{"componentChunkName":"component---src-templates-post-page-template-index-tsx","path":"/2018/07/17/deep-learning-intro/","result":{"data":{"markdownRemark":{"id":"416940c6-a69e-5574-bbfc-320a2786a133","tableOfContents":"<ul>\n<li><a href=\"#artificial-neural-network%EB%9E%80\">Artificial Neural Network란?</a></li>\n<li><a href=\"#%EC%95%94%ED%9D%91%EA%B8%B0%EC%9D%98-%EB%8F%84%EB%9E%98\">암흑기의 도래</a></li>\n<li><a href=\"#%EB%8B%A4%EC%8B%9C-%EC%9E%AC%EA%B8%B0%EC%9D%98-%EC%8B%9C%EA%B0%84\">다시 재기의 시간</a></li>\n</ul>","excerpt":"이번 포스팅에서는 딥러닝이 무엇인지, 기존의 뉴럴네트워크와 다른 점이 무엇인지에 대해서 포스팅하려고 한다. Artificial Neural Network란? 인류는 과거부터 생각하는 기계를 만드려는 노력을 해왔다. 그 과정에서 다양한 시도들이 있었고, 결국 고안해낸 방법은 인간의 뇌를 프로그래밍해보자는 것이였다. 이런 발상이 가능했던 것은 현대에 이르러 인간의 뇌의 구조를 어느 정도 알 수 있었기 때문이기도 하고 이 구조가 생각보다 단순하다는 점도 있었다. 인간의 뇌는 이 이라고 불리는 세포들의 집합체이다. 이 들은 그냥 어떠한 …","html":"<p>이번 포스팅에서는 딥러닝이 무엇인지, 기존의 뉴럴네트워크와 다른 점이 무엇인지에 대해서 포스팅하려고 한다.</p>\n<h2>Artificial Neural Network란?</h2>\n<p>인류는 과거부터 생각하는 기계를 만드려는 노력을 해왔다. 그 과정에서 다양한 시도들이 있었고, 결국 고안해낸 방법은 <strong>인간의 뇌를 프로그래밍</strong>해보자는 것이였다. 이런 발상이 가능했던 것은 현대에 이르러 인간의 뇌의 구조를 어느 정도 알 수 있었기 때문이기도 하고 이 구조가 생각보다 단순하다는 점도 있었다.</p>\n<center>\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 640px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/3823b410149889c0e9aefb86b3f6e80e/21b4d/neural.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 53.75%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAABwklEQVR42n2SW2/TQBCF/eOREEK8AT8AISgiQlVb0pCGXqI2KcVNHDuxY2/ttRPFleMrm8SOL2sz8UYCqYLzsPvg+c4czyxX1fJ8bzKZmKaZ53n1D5VlCWeWZXEcb7fboii4shYhK8dxgKSU/h/2/UDTNOgUhiEHgG0/fmteDgajNE2xbsFZ7ipLMGIAk6roo6E8EuQHZDAvDi5JlFsn3cZBq9Pufv7Y1FQdY2wYBkII4mW1iiJ3nOjksNs+7UJPZreDAz+660tnX/ne9b0oTAn5lSQxJALvIAhUVZtOFVlRVbF/1GidtXqq8gCN97A8RpBQGhrCHZIEraxoVf79qzTJq0dLMG+egZeJ56Ig/4l9ey31b4R2k3/z+ujw0xD9bHjah3W0oNkKRkpp7ls/JlevVuEcimlBN+uYzWIHG/pswI/bx/zzl++uzkUL3S/xd1M4sPi3xvA97r3A4pfIw/G2iOPNuhaY7uGpgk6PLy86vbGALjq3S9efL9wwIrutppsViVhImJnv+/AWdF2HVe9h1/UMA0sjOUkSe+GkaUaLHErh09J1Yeae5wL2dPMcjGo2m7EYT0UIsW07qsX6MzH4N1O3XWVgB46dAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"neural\" title=\"\" src=\"/static/3823b410149889c0e9aefb86b3f6e80e/6af66/neural.png\" srcset=\"/static/3823b410149889c0e9aefb86b3f6e80e/69538/neural.png 160w,\n/static/3823b410149889c0e9aefb86b3f6e80e/72799/neural.png 320w,\n/static/3823b410149889c0e9aefb86b3f6e80e/6af66/neural.png 640w,\n/static/3823b410149889c0e9aefb86b3f6e80e/d9199/neural.png 960w,\n/static/3823b410149889c0e9aefb86b3f6e80e/21b4d/neural.png 1280w\" sizes=\"(max-width: 640px) 100vw, 640px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n  </a>\n    </span>\n  <br>\n</center>\n<p>인간의 뇌는 이 <code class=\"language-text\">뉴런</code>이라고 불리는 세포들의 집합체이다. 이 <code class=\"language-text\">뉴런</code>들은 그냥 어떠한 신호를 받은 후에 변조한 다음 다시 전달하는 세포인데, 뇌는 결국 이 뉴런들이 그물망처럼 연결되어있는 구조인 것이다.\n결론적으로 인간의 뇌의 구조는 굉장히 복잡하게 <code class=\"language-text\">연결</code>되어있지만 그 <code class=\"language-text\">연결체</code>인 뉴런 자체는 놀랍도록 단순한 구조로 되어있었다는 것이 된다.</p>\n<p>이 <code class=\"language-text\">뉴런</code>들은 <code class=\"language-text\">수상돌기</code>에서 input신호를 받아 <code class=\"language-text\">축색돌기</code>로 output신호를 전송하는 구조인데 이때 다음 <code class=\"language-text\">뉴런</code>으로 신호가 전달되기 위해서는 일정 기준, 즉 <code class=\"language-text\">threshold</code> 이상의 전기 신호를 넘겨야한다. 좀 더 자세히 알아보자면 대략 다음 순서를 따른다고 한다.</p>\n<hr>\n<ol>\n<li>뉴런에 연결되어 있는 여러 개의 시냅스로 부터 신호를 받는다.</li>\n</ol>\n<p>이때 신호는 분비된 화학물질의 양(<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span></span>)과 분비되는 시간(<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span></span></span></span></span>)의 곱으로 나타내어 질 수 있다.\n2. 여러 개의 시냅스로부터 받은 여러 개의 신호를 합친다.\n3. 다음 시냅스로 전달하기 전에 특정한 값(<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">b</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">b</span></span></span></span></span>)이 더해진다.\n4. 이 값이 특정 임계점을 넘어가면 신호가 다음 시냅스로 전달된다.</p>\n<hr>\n<center>\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 512px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/6d395b79311597e65462e2af18844938/36dd4/hoe.jpg\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAGAABAAMBAAAAAAAAAAAAAAAAAAECAwX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHvZXoaoCwQD//EABkQAQEBAAMAAAAAAAAAAAAAAAEAQREhMf/aAAgBAQABBQJhSO7cDg2PP//EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQMBAT8BH//EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQIBAT8BH//EABwQAAEDBQAAAAAAAAAAAAAAAAEAEDERICFBUf/aAAgBAQAGPwJCvMvEHdv/xAAeEAEAAgIBBQAAAAAAAAAAAAABABEhMUFhcYGRwf/aAAgBAQABPyFUdYFYIZazECxsY3rv5DbKvQmIu/EQTZxHU0dp/9oADAMBAAIAAwAAABAzwAD/xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/EB//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAECAQE/EB//xAAeEAEAAgIDAAMAAAAAAAAAAAABABEhMUFxoYHB4f/aAAgBAQABPxAELpGOWD6qURYNh+QWYFiQWOiqFoeX35LVUpMD5AbMxrAyxQAIbA0qvsm3qedP/9k='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"hoe\" title=\"\" src=\"/static/6d395b79311597e65462e2af18844938/36dd4/hoe.jpg\" srcset=\"/static/6d395b79311597e65462e2af18844938/0913d/hoe.jpg 160w,\n/static/6d395b79311597e65462e2af18844938/cb69c/hoe.jpg 320w,\n/static/6d395b79311597e65462e2af18844938/36dd4/hoe.jpg 512w\" sizes=\"(max-width: 512px) 100vw, 512px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n  </a>\n    </span>\n  <small>어라...? 생각보다 심플한데...?</small>\n</center>\n<p>사람의 뇌가 이렇게 간단한 원리도 작동한다니…그럼 이걸 기계로도 만들 수 있지 않을까? 라는데서 출발한 것이 바로 <code class=\"language-text\">Artificial Neural Network</code>인 것이다. 이러한 뉴런의 작동방식은 다음과 같이 도식화 될 수 있다.</p>\n<center>\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 640px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/500df46f498950f6283ef37cc89af689/aeb8d/artificial-neural-network.jpg\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 50.625%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAd5FCKD/xAAWEAEBAQAAAAAAAAAAAAAAAAAQATH/2gAIAQEAAQUCbh//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAZEAACAwEAAAAAAAAAAAAAAAAAIRARMVH/2gAIAQEAAT8hG6Hw3n//2gAMAwEAAgADAAAAEPMP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAGxAAAgIDAQAAAAAAAAAAAAAAAREAITFRkRD/2gAIAQEAAT8QZOxyEui1ROI9XZUoorMGB5//2Q=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"artificial neural network\" title=\"\" src=\"/static/500df46f498950f6283ef37cc89af689/c08c5/artificial-neural-network.jpg\" srcset=\"/static/500df46f498950f6283ef37cc89af689/0913d/artificial-neural-network.jpg 160w,\n/static/500df46f498950f6283ef37cc89af689/cb69c/artificial-neural-network.jpg 320w,\n/static/500df46f498950f6283ef37cc89af689/c08c5/artificial-neural-network.jpg 640w,\n/static/500df46f498950f6283ef37cc89af689/aeb8d/artificial-neural-network.jpg 651w\" sizes=\"(max-width: 640px) 100vw, 640px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n  </a>\n    </span>\n  <br>\n</center>\n<p>이 도식을 다시 수식으로 나타내면 다음과 같다.</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>x</mi><mi>i</mi></msub><msub><mi>w</mi><mi>i</mi></msub><mo>+</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">  f(\\sum\\limits_{i=1}^n x_i w_i + b)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:2.9291em;vertical-align:-1.2777em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6514em;\"><span style=\"top:-1.8723em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.05em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.2777em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mclose\">)</span></span></span></span></span></div>\n<p>이때 이 함수 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span></span></span></span></span>를 <code class=\"language-text\">Activation Funcntion</code>이라고 하며, 이 함수는 함수 내부의 값이 <code class=\"language-text\">threshold</code>를 넘어가면 <code class=\"language-text\">1</code>을 리턴하고 아니면 <code class=\"language-text\">0</code>을 리턴하는 함수이다.</p>\n<p>이것이 하나의 <code class=\"language-text\">뉴런</code>이라고 생각하면 이 <code class=\"language-text\">뉴런</code>을 여러 개 모아본다면 대략 아래와 같은 구조가 될 것이다.</p>\n<center>\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 520px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/8c5079be24758bf721e5270eaa520a7b/ddb38/single-layer-network.jpg\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 100.62500000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAGAABAAMBAAAAAAAAAAAAAAAAAAECAwX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHuZ3oaggFwf//EABkQAAMAAwAAAAAAAAAAAAAAAAACMQEQIf/aAAgBAQABBQIaabmBI8P/xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/AR//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAECAQE/AR//xAAZEAACAwEAAAAAAAAAAAAAAAAQMQARIEH/2gAIAQEABj8CHQ6jx//EABwQAQADAAIDAAAAAAAAAAAAAAEAESExURBhwf/aAAgBAQABPyFaXmO9WnRMzIdXb+TsPYlkq1ffhOCf/9oADAMBAAIAAwAAABBwAAD/xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/EB//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAECAQE/EB//xAAdEAACAgIDAQAAAAAAAAAAAAABEQAhMVFBgZGh/9oACAEBAAE/EGILdgmuoBdpeQzDQHJlUDQ9QAExXga8lAtgauFYErkXDRkb3Pmn/9k='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"single layer network\" title=\"\" src=\"/static/8c5079be24758bf721e5270eaa520a7b/ddb38/single-layer-network.jpg\" srcset=\"/static/8c5079be24758bf721e5270eaa520a7b/0913d/single-layer-network.jpg 160w,\n/static/8c5079be24758bf721e5270eaa520a7b/cb69c/single-layer-network.jpg 320w,\n/static/8c5079be24758bf721e5270eaa520a7b/ddb38/single-layer-network.jpg 520w\" sizes=\"(max-width: 520px) 100vw, 520px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n  </a>\n    </span>\n  <br>\n</center>\n<p>그리고 이런 형태의 기계는 이미 1950년대에 개발되어 <code class=\"language-text\">AND</code>나 <code class=\"language-text\">OR</code>문제 같은 선형방정식은 풀 수 있을 정도였다.</p>\n<h2>암흑기의 도래</h2>\n<p>자 그럼 여기서 한가지 의문이 생긴다.</p>\n<blockquote>\n<p><em>아니 지금은 21세기하고도 18년이나 지난 2018년인데, 1950년대에 이미 저기까지 개발이 됐으면 지금은 로봇이 나 대신 일도 해주고 어? 빨래도 해주고 어? 해야하는 거 아니냐!</em></p>\n</blockquote>\n<p>하는 생각이 들 수도 있다. 우선 아까 말한 <code class=\"language-text\">AND</code>와 <code class=\"language-text\">OR</code>를 다시 보자.</p>\n<center>\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 301px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/f87f23e2080e37f8112b3b9c36b731fe/fb933/or.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 111.25%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAWCAYAAADAQbwGAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABUklEQVR42q2UzU7CQBDHZwkUUZGmBfEDFQxqJHwkcOPoyYN3D76KiRevPoqv49mLD+HF9T90tpUNK2zLJL+03c7+d2Znd4jWmwJt0Ae75GHK8V0Fj+AVXDl8vcXrYA9UfCafyKTCVgJz0AUB2AeH1mLKitq8h6C5KqsbEVHicLlmr8z4Mbhw+UZSgO0Ydj3uZ4Ilj+ItR6Zl4A3n7JNowO8vmwv+a40PoqFOqp0uVMQiThlCtzq5GYVF06JAaAoaRUVTwS/c2W+isZa7m1d06dhApA4moLyNlJU8W4h0lDdKO8KFwA9RR2ddxlswkEMdWJH2jKgVKRcudl29prSnyqquA6FrcGSJ8veZS7AtKwaS+o6hQ1S7xxgO/vQdW8CL8pj4ctFqQmoc9pO0o1iILMIHZPGcpB4eZOPse/qn6ywM/2mWs71zZHeuucoTkj0/Nwq/gg4uC2jd4OgAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"or\" title=\"\" src=\"/static/f87f23e2080e37f8112b3b9c36b731fe/fb933/or.png\" srcset=\"/static/f87f23e2080e37f8112b3b9c36b731fe/69538/or.png 160w,\n/static/f87f23e2080e37f8112b3b9c36b731fe/fb933/or.png 301w\" sizes=\"(max-width: 301px) 100vw, 301px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n  </a>\n    </span>\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 314px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/8f39aacbb882bcd4bc1eb9e0b43e1935/5b158/and.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 105.62500000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAVCAYAAABG1c6oAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABLUlEQVR42qWUS27CMBCGxwkUIShUBfMQVSmPLnivgB234BRwA67Moqu2rMyfxhGWlWlix9KnsWLn98x4PET8CEAfSDAAVXIYImX+Ck7gDC7gM2Wv0yjDtTpsDTRAyTqUFQ7BB7dBeXh0AC0tXD3Glq7w6otoYuTTDPcFvHEpGOh8CR1emCz8EI2/id5TPG3o/9icSm7hRrRU8bpT+JygiER+ieawbUtU+AgmF1MCa6VrMY+nMutEiNQR/ga2XCRkW/QZoitlXFwRD4W2XeR0lhV6Xg//9qCUhupRo/6CpihqdIT5kPNUujz6RCAKXcXvnQoJGsIhd0Ed443mKYsKaP5X4F3dAAKjNWURcGJTsNcNQjoQdageGNme7sDCoxs/gS3X/gOPjh518JX99Q7bxDBuFb9FLwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"and\" title=\"\" src=\"/static/8f39aacbb882bcd4bc1eb9e0b43e1935/5b158/and.png\" srcset=\"/static/8f39aacbb882bcd4bc1eb9e0b43e1935/69538/and.png 160w,\n/static/8f39aacbb882bcd4bc1eb9e0b43e1935/5b158/and.png 314w\" sizes=\"(max-width: 314px) 100vw, 314px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n  </a>\n    </span>\n</center>\n<p><code class=\"language-text\">AND</code>와 <code class=\"language-text\">OR</code>는 선형방정식이기 때문에 1950년대에 개발한 <code class=\"language-text\">Single Layer Network</code>를 적용한 기계로도 이런 문제를 푸는 건 별로 어렵지 않았다.</p>\n<p>여기까지 성공한 사람들은 <strong>대박이다. 이제 금방 기계가 걷고 뛰고 말도 할 수 있겠구나!</strong> 라고 생각했지만.. <code class=\"language-text\">XOR</code>가 등장하면 어떨까?</p>\n<center>\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 281px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/c0b2a78291b2791a491b0e6ccd63d2d3/6b1e2/xor.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 104.375%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAVCAYAAABG1c6oAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAA9klEQVR42mNgIAxYgZgTiLmBmAuImRnIBExQWg+Iy4E4A4gbgFgMKs6ISyMjDkmYmCAQqwOxNhArQ11KlGu4CVhANBAAYjkg1sBhICMaJgjUgNgRGuCwMFMkFE5IQAQacXAAijUeqAQDNEbZSfAhGzaLQYKiDFQEBA38D4y4/yRE1oC40PknA4MOlM1IkYEg7/5jYIgG4iVANgtFLvwPzYJA2g2Igyl2IZKBEf+p4WWYZiAt9R+SrykPQ5rEMj0NZMKW9diRCk4BaN7GVtIgY1gBbAXEQuiFCbILmUgsD1mxqRcGYi2oKyWAWBxKE4PFoI4BleYSALBSH0JnOftvAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"xor\" title=\"\" src=\"/static/c0b2a78291b2791a491b0e6ccd63d2d3/6b1e2/xor.png\" srcset=\"/static/c0b2a78291b2791a491b0e6ccd63d2d3/69538/xor.png 160w,\n/static/c0b2a78291b2791a491b0e6ccd63d2d3/6b1e2/xor.png 281w\" sizes=\"(max-width: 281px) 100vw, 281px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\" decoding=\"async\">\n  </a>\n    </span>\n  <br>\n</center>\n<p>와 이건 어떻게 선을 그어도 도저히 답이 없다. <code class=\"language-text\">XOR</code>는 두개의 인풋이 <code class=\"language-text\">같지 않으면 true</code>인 논리식이다. 굉장히 단순해보였지만 <code class=\"language-text\">XOR</code>는 선형방정식이 아니기 때문에 직선으로는 50%의 정확도밖에 낼 수 없었다. 여기까지 직면한 사람들은 좌절하게 된다.</p>\n<blockquote>\n<p>We need to use MLP, Multi Layer Perceptrons.\nNo one on earth had found a viable way to train MLPs good enough to learn such simple functions.</p>\n<p><strong>Perceptrons(1969)</strong> <em>Marvin Minsky</em></p>\n</blockquote>\n<p>결국 1969년 <strong>Marvin Minsky</strong> 가 <code class=\"language-text\">Single Layer Network</code>로는 <code class=\"language-text\">XOR</code>문제를 풀 수 없다는 것을 수학적으로 증명한다. <code class=\"language-text\">Multi Layer Network</code>로는 가능한데 아무도 학습시킬 수 없다고 했단다.</p>\n<p>이 이유에 대해서 좀 더 알아보고 싶어서 <code class=\"language-text\">Perceptrons</code>의 구문을 찾아보니</p>\n<blockquote>\n<p>it ought to be possible to devise a training alhorithm to optimize the weights in thie using, say, the magnitude of a reinforcement signal to communicate to the net the cost of an error. We have not investigated this.</p>\n<p><strong>Perceptrons(1969)</strong> <em>Marvin Minsky</em></p>\n</blockquote>\n<p>라고 한다. 구글링 하다보니까 <code class=\"language-text\">Perceptrons</code>라는 책은 <code class=\"language-text\">XOR</code>가 <code class=\"language-text\">Single Layer Network</code>로 왜 학습이 안되는 지에 대해서 집중적으로 설명하고나서</p>\n<blockquote>\n<p>어…음 Multi Layer Network로 학습시키면 되는 건 알겠는데 어떻게 해야하는 지는 아직 잘 모르겠다.</p>\n</blockquote>\n<p>정도로 쓴 책이라는 의견도 있었다.\n어찌됐던 이 책으로 인해 많은 사람들이 실망을 하게 되고 이로 인해 <code class=\"language-text\">Neural Network</code>라는 학문 자체가 암흑기에 빠지게 된다.</p>\n<h2>다시 재기의 시간</h2>\n<p>1974년 <strong>Paul Werbos</strong> 는 자신의 박사학위 논문에 <code class=\"language-text\">Backpropagation</code>이라는 알고리즘을 게재하게 된다.</p>\n<p>그러나 슬프게도 아무도 관심을 가지지 않았고 심지어 <code class=\"language-text\">Perceptrons</code>의 저자인 <strong>Marvin Minsky</strong> 마저도 관심을 안가져줬다고 한다. 심지어 1982년도에 다시 논문을 발표하게 됐는데 이때도 그냥 묻혔다고 한다…</p>\n<p>그러다가 1986년, <strong>Geoffrey Hinton</strong> 이 독자적으로 이 알고리즘을 다시 발견하고 발표하게 되면서 주목을 받게된다. 어쨋든 이 알고리즘으로 인해 <code class=\"language-text\">Multi Layer Network</code>의 학습이 가능하다는 사실이 알려지고 다시 <code class=\"language-text\">Neural Network</code> 학문은 활기를 띄게 된다.</p>\n<p><code class=\"language-text\">Backpropagation</code>이라는 알고리즘의 구조는 간단하다. 그냥 말 그대로 에러를 output에서 가까운 쪽부터 뒤로(Back) 전파(Propagation)하는 것이다. 그래서 <code class=\"language-text\">역전파알고리즘</code>이라고도 불린다.\n이 <code class=\"language-text\">Backpropagation</code>에 대해서는 <a href=\"/2018/07/19/deep-learning-backpropagation\">다음 포스팅</a>에서 다시 다루도록 하겠다.</p>\n<p>이상으로 Deep Learning 첫번째 포스팅을 마친다.</p>","fields":{"slug":"20180717-deep-learning-intro","path":"/2018/07/17/deep-learning-intro/","lang":"ko"},"frontmatter":{"title":"[Deep Learning 시리즈] 딥러닝이란 무엇일까?","date":"2018-07-17","categories":["프로그래밍","머신러닝"],"tags":["머신러닝","딥러닝","머신러닝 기초","딥러닝 기초","튜토리얼","Machine Learning","Deep Learning","Backpropagation","역전파","신경"],"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/26718e1f085d210bba3e45681f40c1fa/3a812/thumbnail.jpg","srcSet":"/static/26718e1f085d210bba3e45681f40c1fa/1cda5/thumbnail.jpg 80w,\n/static/26718e1f085d210bba3e45681f40c1fa/b1a47/thumbnail.jpg 160w,\n/static/26718e1f085d210bba3e45681f40c1fa/3a812/thumbnail.jpg 320w,\n/static/26718e1f085d210bba3e45681f40c1fa/698e2/thumbnail.jpg 640w","sizes":"(min-width: 320px) 320px, 100vw"},"sources":[{"srcSet":"/static/26718e1f085d210bba3e45681f40c1fa/3e5ca/thumbnail.webp 80w,\n/static/26718e1f085d210bba3e45681f40c1fa/b72f1/thumbnail.webp 160w,\n/static/26718e1f085d210bba3e45681f40c1fa/fc5c5/thumbnail.webp 320w,\n/static/26718e1f085d210bba3e45681f40c1fa/c5332/thumbnail.webp 640w","type":"image/webp","sizes":"(min-width: 320px) 320px, 100vw"}]},"width":320,"height":320}}},"jumbotron":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/26718e1f085d210bba3e45681f40c1fa/e1182/thumbnail.jpg","srcSet":"/static/26718e1f085d210bba3e45681f40c1fa/23110/thumbnail.jpg 750w,\n/static/26718e1f085d210bba3e45681f40c1fa/e1182/thumbnail.jpg 1000w","sizes":"100vw"},"sources":[{"srcSet":"/static/26718e1f085d210bba3e45681f40c1fa/6858b/thumbnail.webp 750w,\n/static/26718e1f085d210bba3e45681f40c1fa/c1538/thumbnail.webp 1000w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5}}}}},"allMarkdownRemark":{"edges":[]}},"pageContext":{"slug":"20180717-deep-learning-intro","previous":{"fields":{"slug":"20170514-paypal-express-checkout","path":"/2017/05/14/paypal-express-checkout/","lang":"ko","postGroup":"20170514-paypal-express-checkout"},"frontmatter":{"title":"페이팔의 Express Checkout Restful API 사용하기"}},"next":{"fields":{"slug":"20180719-deep-learning-backpropagation","path":"/2018/07/19/deep-learning-backpropagation/","lang":"ko","postGroup":"20180719-deep-learning-backpropagation"},"frontmatter":{"title":"[Deep Learning 시리즈] Backpropagation, 역전파 알아보기"}},"lang":"ko","postGroup":"20180717-deep-learning-intro"}},"staticQueryHashes":["3129619726","3523904809","376081736","650499039"],"slicesMap":{}}