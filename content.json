{"pages":[{"title":"","text":"","link":"/404.html"},{"title":"About Evan Moon","text":"Intro.안녕하세요. 물리와 우주 덕후 프로그래머 문동욱입니다. 현재 서울에 거주하며 대한민국의 IT 회사를 돌아다니면서 프론트엔드 개발자로 일하고 있습니다 :) 메인 포지션은 웹 프론트엔드이기는 하지만, 딱히 지식을 가려먹는 타입은 아니라서 폭 넓고 보편적인 지식을 쌓으려고 하고 있습니다. 딱히 프로그래밍과 관련된 것이 아니라도 필요하다 싶으면 그냥 공부하는 편입니다. 글쓰기를 좋아하기 때문에 이 블로그를 운영하고 있긴 한데, 딱히 기술적인 것이 아니라도 그냥 쓰고 싶은 주제가 있으면 아무거나 쓰기 때문에 기술 블로그와 에세이 블로그의 혼종 느낌으로 운영하고 있습니다. Github LinkedIn Email 저서커피 한 잔 마시며 끝내는 VueJS Github 구매하기 이 책에서는 VueJS에 대한 API뿐만 아니라 어떻게 활용하면 되는지에 대해 효과적으로 설명한다. 이러한 활용 예제는 실전 애플리케이션을 구축해나가는 과정을 통해 쉽고 효과적으로 학습한다. 어플리케이션은 실무와 동일하게 REST API를 제공하는 백엔드와 통신을 통해 데이터를 받아온 후 클라이언트에서 상태 관리를 하는 과정으로 진행되며, 이때 필요한 REST API는 Github을 통해 프로젝트를 클론받는 방식으로 제공하고 있다. 그뿐만 아니라 실전 애플리케이션을 구축해나가는 과정에서 겪을 수 있는 트러블 슈팅과 필자의 실무 경험을 바탕으로 하는 조언도 함께 학습한다. 관련 포스팅 흔한 개발랭이의 작가 입문기 Toy Project필요해서 만든 김에 공유하고 싶거나, 재미 혹은 개인 만족을 위해서 만드는 프로젝트들. 딱히 완성이라고 할만한 프로젝트는 없으며, 기능은 시간 날 때마다 꾸준히 추가 중이다. Solar System TS TypeScript, Vue.js, Three.js(WebGL)을 사용해 제작한 실시간 태양계 시뮬레이터. 중력을 사용하여 구현하려고 했으나 천체의 질량 데이터가 미묘하게 맞지 않는건지 뭔지 궤도가 제 멋대로 돌아서 일단 보류. 대신 케플러 궤도 6요소와 케플러 방정식 + 레가르 다항식을 사용하여 현재 날짜에 해당 행성이 어디에 있는지 위치를 추적하는 방향으로 구현. 관련 포스팅 [JavaScript로 천체 구현하기] 케플러 6요소 알아보기 [JavaScript로 천체 구현하기] 행성의 움직임을 구현해보자 [JavaScript로 중력 구현하기] 1. 중력이란 무엇일까? [JavaScript로 중력 구현하기] 2. 코딩하기 Simple ANN TypeScript와 React, D3, ChartJS를 사용해 제작한 간단한 인공 신경망. 원래는 회사 세미나 발표 용도로 간단하게 만들었는데 생각보다 재밌어서 계속 건드는 중. 레이어의 개수와 노드의 개수, 학습 횟수 등을 설정하고 학습 과정에서 $y$값의 변화와 Loss의 변화를 시각화하였다. 추후 여러 개의 Activation Function을 선택할 수 있는 기능과 레이어마다 노드의 개수를 다르게 하는 등 네트워크 커스터마이징을 좀 더 다채롭게 할 수 있는 기능을 추가할 예정. 관련 포스팅 Deep Learning이란 무엇인가? - Intro Deep Learning이란 무엇인가? - Backpropagation TypeScript를 사용하여 간단한 인공 신경망 개발 삽질기 Simple Waveform Visualizer 오디오 파일을 업로드해서 여러가지 오디오 이펙터를 걸어볼 수 있는 어플리케이션. 현재 개발된 이펙터는 Compressor,Reverb, Delay, Filter (LPF, HPF), EQ, Distortion, Tremolo 정도. 다음 단계는 오실레이터를 사용하여 신디사이저를 만들거나, 트랙을 나누어서 동시에 여러 오디오 소스를 재생하며 이펙터를 사용할 수 있도록 변경할 예정. 관련 포스팅 컴퓨터는 어떻게 소리를 들을까? [JavaScript로 오디오 이펙터를 만들어보자] 소리의 흐름을 파악하자 [JavaScript로 오디오 이펙터를 만들어보자] 나만의 소리 만들기 Zarbis 현재 Vue -> React, TypeScript로 포팅 중이라 몇 가지 기능은 구현되지 않았습니다. 시계 + 날씨 + 미세먼지 정보 + 이쁜 사진 대시보드. 집에 노트북이 남길래 하나를 대시보드 전용으로 쓰려고 만든 웹 어플리케이션. 크롬 익스텐션인 Momentum를 쓰고 있었지만 비염이 심한 본인을 위해 미세먼지 농도도 함께 보고 싶었는데 얘네가 해당 기능 제공을 안해줘서 하나 새로 만들어서 쓰는 중. 배경사진은 날씨에 따라 알맞은 사진으로 변경되어 사용자가 굳이 구석에 있는 날씨 정보를 눈여겨 보지 않더라도 한 눈에 현재 날씨를 알아볼 수 있도록 함. .buttons { width: 100%; display: flex; } .buttons a.button { flex-grow: 1; } a.button.is-linkedin { background-color: #0077B5; border-color: #0077B5; color: #FFF; } @media screen and (max-width: 720px) { .buttons a.button { width: 100%; margin-right: 0; } } .project-links { margin-bottom: 16px; }","link":"/about/index.html"}],"posts":[{"title":"[JavaScript로 천체 구현하기] 케플러 6요소 알아보기","text":"이번 포스팅에서는 태양계 시뮬레이션을 개발하면서 제일 애먹었던 천체의 궤도와 위치 계산에 대해서 알아보려고 한다. 필자는 고등학교 시절 수포자였기 대문에 필자와 같이 수포자였던 분들을 위해 최대한 간단하게 설명하는 것을 목표로 하고 있다. 먼저 궤도를 그리는 방법과 행성의 위치를 추정하는 방법에 대해서는 구글에 널리고 널렸지만, 이를 이해하기 위해서는 기본적인 천문학용어들에 대한 지식이 있어야했다.고1 지구과학시간에 배우는 용어들이라는데 필자는 고등학교때 잠만 잤기 때문에 하나도 기억이 나지 않아서 다시 찾아봐야했다. 필자는 천체물리학과를 나온 것도 아닌 일개 개발자이므로 각각의 용어 정리에 대해서는 틀린 점이 있을 수 있다. 간단한 용어 정리케플러 6요소를 알아보기 전에 천체 물리학에서 사용하는 간단한 용어들을 한번 알아보자. 원일점(aphelion), 근일점(perihelion)행성의 궤도는 타원이기 때문에 초점이 2개이다. 보통 이 중 하나의 초점에 항성이 위치하게 되며, 각각 원일점과 근일점은 항성에서 가장 멀리 떨어지거나 가장 가까운 한점을 의미한다. 포물선 궤도는 원일점이 무한대이므로 근일점만을 가지게 된다. 천구(The celestial sphere)관측자를 중심으로 하여 임의의 반지름을 가진 가상적인 구를 의미한다. 천구의 적도(The celestial equator)행성의 적도가 황도에 투영된 것. 위 그림의 천구에서 세로선에 해당하는 것이 천구의 적도이다. 천구의 황도(The ecliptic)행성의 공전 궤도면이 황도에 투영된 것. 지구의 황도는 적도로부터 지구의 자전축만큼(약 23.5도) 기울어져 있다. 적경(Right Ascension), 적위(Declination)이 값들은 천구의 적도를 기준면으로 사용하는 적도 좌표값이다. 적경은 춘분점을 기준으로 천구의 적도를 따라 항성이 움직이는 방향으로 측정되며 단위는 시, 분(1시간은 15도)이다.적위는 천구의 북쪽을 +로 남쪽을 -로 지정했을 때 천구의 적도면에서 항성까지 잰 각이며, 항상 +90에서 -90사이의 값을 가진다. 천구 상의 춘분점은 적경과 적위가 모두 0이다. 황경(Ecliptic longitude), 황위(Ecliptic latitude)적경, 적위와 같은 개념이나, 기준면이 행성의 적도면이 아닌, 황도면이다. 궤도 교점(Orbital node), 승교점(Ascending node), 강교점(Descending node)궤도 교점은 천체의 궤도면과 기준면이 만나는 두 점을 말한다. 기준면은 정의에 따라 달라질 수 있으며, 태양계의 경우 기준면은 지구의 궤도면이다.승교점은 천체가 기준면을 북쪽(보통 위 쪽)으로 지나가는 점을 의미하며, 강교점은 천체가 기준면을 남쪽(보통 아래 쪽)으로 지나가는 점을 말한다. 케플러 6요소기본 용어 정리가 되었으면 이제 궤도의 모양과 크기, 방향을 결정하는 케플러 6요소를 알아보자. 장반경(Semi-major axis) 행성의 궤도는 타원이기 때문에 반지름이 아닌 장반경과 단반경으로 정의한다. 장반경은 궤도 상에서 궤도 중심과 가장 거리가 먼 한 지점을 의미하며, 단반경은 궤도 상에서 궤도 중심과 가장 거리가 가까운 한 지점을 의미한다.원일점, 근일점과의 차이는 측정의 기준이 되는 점이 항성이 아닌 궤도의 중심이라는 것이다.포물선 궤도나 쌍곡선 궤도는 장반경이 무한대 이므로 단반경만을 가지게 된다. 이심률(Eccentricity) 이심률은 간단히 말하면 타원이 얼마나 찌그러져있는지에 대한 값이다. 일반적으로 이심률 $e$는 $$e = \\sqrt{1-k\\frac{b^2}{a^2}}$$ 로 정의할 수 있으며, 이때 $a$는 장반경, $b$는 단반경이다.그러나 케플러 궤도에서 사용하는 사용되는 이심률은 약간 공식이 달라진다. $$e = \\sqrt{1+\\frac{2EL^2}{m_\\text{red}\\alpha^2}}$$ 이 공식의 해인 $e$를 궤도 이심률이라고 부른다. 여기서 $E$는 총 궤도 에너지, $L$은 각운동량, $m_{red}$는 환산 질량을 나타내며, $\\alpha$는 역제곱 법칙 중심력의 계수이다.역제곱 법칙이란 어떤 힘의 크기가 거리의 제곱에 반비례하는 것을 가리키는 말이다. 이 경우의 어떤 힘이란 바로 중력이 된다. 하지만 필자는 이심률이 이미 주어진 데이터를 사용할 예정이므로, 사실 굳이 이렇게 까지 계산할 필요가 없다. 그냥 일반적인 이심률 공식과 계산법이 다르다는 정도만 알고 넘어가자. 케플러 궤도의 이심률은 다음 4가지의 형태로 나타난다. 원 궤도: $e = 0$ 타원 궤도: $0 < e < 1$ 포물선 궤도: $e = 1$ 쌍곡선 궤도: $e > 1$ 지구의 궤도 이심률은 현재 약 0.0167으로 거의 원형에 가깝다. 태양계 행성 중 가장 이심률이 큰 수성은 0.2056의 값을 가지고 있고, 이로 인해서 수성은 근일점에서 원일점보다 두 배 정도 태양 복사를 받는다.원래는 0.248의 값을 가지고 있던 명왕성이 이 분야의 대빵이었지만 지금은 다들 알다시피 퇴출되었다..아디오스 혜성의 경우 이심률이 상당히 다양한 편인데, 보통 주기 혜성들의 경우는 0.2에서 0.7의 값을 가지며 몇몇 궤도가 매우 찌그러진 혜성들은 거의 1에 육박하는 이심률을 가진 것도 있다.예를 들어 혜성 중 많이 이름이 알려진 핼리 혜성의 경우 이심률이 0.967이며, 궤도가 매우 찌그러져있어서 한번 공전하는데 76년이나 걸린다. 참고로 다음 핼리 혜성의 접근 시기는 2061년이다. 기울기(Inclination)항성의 황도면과 궤도 간의 기울기이다. 기울기가 90도를 초과하면 이 물체는 기울기가 0~90인 물체와 반대방향으로 공전하고 있다고 보면 된다. 승교점 적경(Longitude of Ascending Node)궤도가 남(-)에서 북(+)으로 지나면서 행성의 황도면과 교차하게 되는 지점까지의 적경을 의미한다.춘분점의 적경이 0이므로 춘분점을 기준으로 계산하면 된다. 근일점 편각(Argument of periapsis)궤도 근점으로부터 승교점까지의 각도로, 궤도 면에서 타원의 방향을 결정한다. 간단하게 말하면 승교점부터 근일점까지의 적위를 의미한다. 근일점 통과 시각(T)행성이 궤도의 근일점을 통과한 시각 다음 포스팅에서는 궤도를 코드로 구현해볼 예정이다. 이상으로 천문학 용어 정리 포스팅을 마친다.","link":"/2017/05/03/calculate-orbit-1/"},{"title":"[Deep Learning이란 무엇인가?] 딥러닝이란?","text":"이번 포스팅에서는 딥러닝이 무엇인지, 기존의 뉴럴네트워크와 다른 점이 무엇인지에 대해서 포스팅하려고 한다. Artificial Neural Network란?인류는 과거부터 생각하는 기계를 만드려는 노력을 해왔다.그 과정에서 다양한 시도들이 있었고, 결국 고안해낸 방법은 인간의 뇌를 프로그래밍해보자는 것이였다. 이런 발상이 가능했던 것은 현대에 이르러 인간의 뇌의 구조를 어느 정도 알 수 있었기 때문이기도 하고 이 구조가 생각보다 단순하다는 점도 있었다. 인간의 뇌는 이 뉴런이라고 불리는 세포들의 집합체이다. 이 뉴런들은 그냥 어떠한 신호를 받은 후에 변조한 다음 다시 전달하는 세포인데, 뇌는 결국 이 뉴런들이 그물망처럼 연결되어있는 구조인 것이다.결론적으로 인간의 뇌의 구조는 굉장히 복잡하게 연결되어있지만 그 연결체인 뉴런 자체는 놀랍도록 단순한 구조로 되어있었다는 것이 된다. 이 뉴런들은 수상돌기에서 input신호를 받아 축색돌기로 output신호를 전송하는 구조인데 이때 다음 뉴런으로 신호가 전달되기 위해서는 일정 기준, 즉 threshold 이상의 전기 신호를 넘겨야한다. 좀 더 자세히 알아보자면 대략 다음 순서를 따른다고 한다. 뉴런에 연결되어 있는 여러 개의 시냅스로 부터 신호를 받는다.이때 신호는 분비된 화학물질의 양($x$)과 분비되는 시간($w$)의 곱으로 나타내어 질 수 있다. 여러 개의 시냅스로부터 받은 여러 개의 신호를 합친다. 다음 시냅스로 전달하기 전에 특정한 값($b$)이 더해진다. 이 값이 특정 임계점을 넘어가면 신호가 다음 시냅스로 전달된다. 그냥 순서만 보면 꽤 간단해 보인다. 그럼 이걸 기계로도 만들 수 있지 않을까? 라는데서 출발한 것이 Artificial Neural Network인 것이다. 이 뉴런의 작동방식은 다음과 같이 도식화 될 수 있다. 수식으로 나타내면 다음과 같다. $$f(\\sum\\limits_{i=1}^n x_i w_i + b)$$ 이때 이 함수 $f$를 Activation Funcntion이라고 하고 이 함수는 함수 내부의 값이 threshold를 넘어가면 1을 리턴하고 아니면 0을 리턴하는 함수이다.이것이 하나의 뉴런이라고 생각하면 이 뉴런을 여러 개 모아본다면 대략 아래와 같은 구조가 될 것이다. 그리고 이런 형태의 기계는 이미 1950년대에 개발되어 AND나 OR문제 같은 선형방정식은 풀 수 있을 정도였다. 암흑기의 도래자 그럼 여기서 한가지 의문이 생긴다. 아니 지금은 21세기하고도 18년이나 지난 2018년인데, 1950년대에 이미 저기까지 개발이 됐으면 지금은 로봇이 나 대신 일도 해주고 어? 빨래도 해주고 어? 해야하는 거 아니냐! 하는 생각이 들 수도 있다. 우선 아까 말한 AND와 OR를 다시 보자. AND와 OR는 선형방정식이기 때문에 1950년대에 개발한 Single Layer Network를 적용한 기계로도 이런 문제를 푸는 건 별로 어렵지 않았다.여기까지 성공한 사람들은 대박이다. 이제 금방 기계가 걷고 뛰고 말도 할 수 있겠구나! 라고 생각했지만.. XOR가 등장하면 어떨까? 와 이건 어떻게 선을 그어도 도저히 답이 없다. XOR는 두개의 인풋이 같지 않으면 true인 논리식이다. 굉장히 단순해보였지만 XOR는 선형방정식이 아니기 때문에 직선으로는 50%의 정확도밖에 낼 수 없었다. 여기까지 직면한 사람들은 좌절하게 된다. We need to use MLP, Multi Layer Perceptrons.No one on earth had found a viable way to train MLPs good enough to learn such simple functions. Perceptrons(1969)Marvin Minsky 결국 1969년 Marvin Minsky 가 Single Layer Network로는 XOR문제를 풀 수 없다는 것을 수학적으로 증명한다. Multi Layer Network로는 가능한데 아무도 학습시킬 수 없다고 했단다.이 이유에 대해서 좀 더 알아보고 싶어서 Perceptrons의 구문을 찾아보니 it ought to be possible to devise a training alhorithm to optimize the weights in thie using, say, the magnitude of a reinforcement signal to communicate to the net the cost of an error. We have not investigated this. Perceptrons(1969)Marvin Minsky 라고 한다. 구글링 하다보니까 Perceptrons라는 책은 XOR가 Single Layer Network로 왜 학습이 안되는 지에 대해서 집중적으로 설명하고나서 어…음 Multi Layer Network로 학습시키면 되는 건 알겠는데 어떻게 해야하는 지는 아직 잘 모르겠다. 정도로 쓴 책이라는 의견도 있었다.어찌됐던 이 책으로 인해 많은 사람들이 실망을 하게 되고 이로 인해 Neural Network라는 학문 자체가 암흑기에 빠지게 된다. 다시 재기의 시간1974년 Paul Werbos 는 자신의 박사학위 논문에 Backpropagation이라는 알고리즘을 게재하게 된다.그러나 슬프게도 아무도 관심을 가지지 않았고 심지어 Perceptrons의 저자인 Marvin Minsky 마저도 관심을 안가져줬다고 한다. 심지어 1982년도에 다시 논문을 발표하게 됐는데 이때도 그냥 묻혔다고 한다…그러다가 1986년, Geoffrey Hinton 이 독자적으로 이 알고리즘을 다시 발견하고 발표하게 되면서 주목을 받게된다. 어쨋든 이 알고리즘으로 인해 Multi Layer Network의 학습이 가능하다는 사실이 알려지고 다시 Neural Network 학문은 활기를 띄게 된다.Backpropagation이라는 알고리즘의 구조는 간단하다. 그냥 말 그대로 에러를 output에서 가까운 쪽부터 뒤로(Back) 전파(Propagation)하는 것이다. 그래서 역전파알고리즘이라고도 불린다.이 Backpropagation에 대해서는 다음 포스팅에서 다시 다루도록 하겠다. 이상으로 Deep Learning 첫번째 포스팅을 마친다.","link":"/2018/07/17/deep-learning-intro/"},{"title":"[JavaScript로 천체 구현하기] 행성의 움직임을 구현해보자","text":"이번 포스팅에서는 저번 포스팅에 이어 실제 궤도의 모양과 크기, 위치, 방향을 정의하고 JavaScript코드로 작성을 해보려고 한다. 실제 어플리케이션을 작성할 때는 TypeScript를 사용하였으나, 편의상 JavsScript ES6로 포스팅을 진행한다.궤도를 구하는 방법은 저번 포스팅의 용어 정리에서 언급했던 케플러 6요소를 이용하면 된다. 데이터 정의지구의 궤도 데이터는 다음과 같다. 12345678910111213141516171819const AU = 149597870;const EARTH_ORBIT = { base: { a: 1.00000261 * AU, e: 0.01671123, i: -0.00001531, o: 0.0, l: 100.46457166, lp: 102.93768193 }, cy: { a: 0.00000562 * AU, e: -0.00004392, i: -0.01294668, o: 0.0, l: 35999.37244981, lp: 0.32327364 }}; base프로퍼티에 들어있는 값은 궤도의 기본 요소들을 의미하며 이 값들은 천문학에서의 역기점인 J2000때 측정된 값을 의미한다.J2000은 2000년 1월 1일 정오를 의미한다.그리고 cy프로퍼티에 있는 값들은 1세기당 궤도 요소들의 변화량을 의미한다. a는 장반경, e는 이심률, i는 기울기, o는 승교점 적경, l은 평균 경도, lp는 근일점 경도를 의미한다.이 중 장반경의 경우 AU값으로 선언되어있기 때문에 1AU를 km로 환산한 값인 149597870를 곱해줘야 한다. 이때 1AU는 태양과 지구 사이의 거리를 의미한다. 하지만 이 데이터에는 케플러 6요소 중 근일점 편각과 근일점 통과시각이 없다.그렇기 때문에 필자는 근일점 경도와 승교점 적경을 사용하여 근일점 편각을 구하려고 한다.그리고 근일점 통과 시각의 경우 평균근점이각을 구할 때 필요한데, 근일점 통과 시각을 사용하여 구하는 것보다 근일점 경도와 평균 경도를 사용하는 방법이 훨씬 공식이 간단하기 때문에오히려 편해진 상황이다. 시간 설정과 궤도 요소 계산위에서 잠깐 말했듯이 천문학에서는 우리가 평소 사용하는 그레고리력이 아닌 율리우스력을 사용한다. 율리우스력이란 원래 BC 4713년 1월 1일 월요일 정오를 기점으로 계산한 날짜 수 이다. 하지만 기원전 4713년부터 서기 2017년까지의 날짜를 세면 자릿수가 너무 커지기 때문에1976년 IAU(국제천문연맹)에서 결정한 J2000을 사용하는 것이다. 이제 날짜 요소들을 선언해보자.12345678910const DAY = 60 * 60 *24;const YEAR = 365.25;const CENTURY = 100 * YEAR;const J2000 = new Date('2000-01-01T12:00:00-00:00');let today = new Date();let epochTime = (today - J2000) / 1000;// Date객체끼리 연산하면 값이 ms로 나오기 때문에 1000으로 나눠서 초 단위로 나오게끔 환산해준다let tDays = epochTime / DAY; // 일수로 환산let T = tDAys / CENTURY; // 몇 세기나 지났는지 환산 이 글을 작성하고 있는 지금은 2017년 5월 3일 22시 27분이다. 계산 결과 J2000으로부터 약 6332.06028991일이 지났으며 0.17336236세기가 지났다는 것을 알 수 있었다.그럼 이제 아까 선언한 데이터 중 지구의 한 세기당 변화량에 방금 구한 0.17336236을 곱한 후 궤도의 기본 요소 값에 더해주면 J2000이후 오늘 이 시점의 궤도 요소 값이 나올 것이다. 123456789101112131415const keys = Object.keys(EARTH.base);let computed = { time: epochTime };computed = keys.reduce((carry, el) => { const variation = EARTH.cy || 0; carry[el] = EARTH.base[el] + (variation * T); return carry;}, computed);/* a: 149598406.2031184 e: 0.016703615925039474 i: -0.0022597770311920135 l: 6341.400827688619 lp: 102.9937254119609 o: 0.0*/ 이제 현재 시점 기준의 궤도 요소 값을 구했으면 부궤도 요소를 구할 차례이다.가장 먼저 케플러 6요소 중 하나지만 우리의 데이터에는 없는 근일점 편각을 구해보도록 하자. 근일점 편각 w는 근일점 경도에서 승교점 적경을 뺀 값이다.하지만 지구의 승교점 적경 o는 0.0이므로 근일점 경도와 근일점 편각이 일치하게 된다. 12computed.w = computed.lp - computed.o;/* w = 102.9937254119609 - 0.0 = 102.9937254119609 */ 다음은 평균근점이각(mean anomaly)를 구해야한다. 평균근점이각은 어떤 물체가 공전 속도와 공전 주기를 유지한 채 정확한 원 궤도로 옮겨간다고 가정했을 때 물체와 궤도 근점간의 각거리를 의미한다. 위 그림에서 실제 물체의 궤도는 회색 궤도이나, 저 궤도를 원이라고 가정한 빨간 궤도에서의 각거리를 구하는 것이다. 저 두 궤도의 공전 주기는 같기 때문에 같은 지역을 같은 시간 동안 지나간다. 근데 그림을 보면 원 궤도는 각 호들의 넓이가 일정하지만 타원 궤도인 회색 궤도는 일정하지 않다.타원 궤도를 도는 물체는 초점과의 거리에 따라 각속도가 변하기 때문이다. 그래서 좀 더 구하기 쉬운 평균근점이각을 먼저 구한 후 타원 궤도에 이를 적용하게 된다. 참조링크: 평균근점이각 위키참조링크: 케플러 행성운동법칙 위키 - 제 2법칙: 면적속도 일정의 법칙 평균근점이각을 구하는 공식은 여러 개가 있으나 필자는 근일점 경도와 평균 경도값을 가지고 있기 때문에 여러 개의 공식 중 가장 쉬워보이는 녀석을 골라서 사용할 수 있다. 이때 평균근점이각 $M$은 평균 경도 $l$과 근일점 경도 $\\omega$의 차로 나타내어 질 수 있다. $$M = l-\\omega$$ 12computed.M = computed.l - computed.lp/* M = 6341.42938740066 - 102.99372566842652 = 6238.435661732234 */ 이제 지금까지 구한 궤도들의 단위를 바꿔줘야 한다. 우리가 가지고 있는 지구 궤도 데이터의 수치들은 모두 digree단위인데, 이후 계산의 편의를 위해 모두 radian으로 바꾸어 주겠다. 1234567const DEG_TO_RAD = Math.PI / 180;computed.a *= 1000; // 149598406204.91275computed.i *= DEG_TO_RAD; // -0.000039441031811489665computed.o *= DEG_TO_RAD; // 0.0computed.w *= DEG_TO_RAD; // 1.7975796293754245computed.M *= DEG_TO_RAD; // 108.8812424710587 이제 평균근점이각을 구했으니 편심이각(Eccentric anomaly)를 구해야 한다. 편심이각은 그림으로 보는 게 더 직관적으로 이해하기가 쉽다. 이 그림에서 물체의 위치를 P로 그에 따른 편심이각은 E로 나타내어지고 있다.타원의 중심은 C이고 타원의 초점은 F이다.이때 편심이각 E는 타원의 중심에 꼭지점 하나를 찍고 궤도 장반경과 같은 길이의 빗변 a를 그은 후, 장반경 e와 수직하면서도 P에 닿도록선분을 그어 만들어진 직각삼각형에서 관찰되는 각이다. 평균근점이각과 마찬가지로 편심이각도 여러 개의 정의로 나타내어질 수 있는데, 필자는 이미 평균근점이각을 구했기 때문에 평균근점이각으로부터 유도되는 공식을 사용한다. $$E = \\frac{M+e{sinM}}{1-e{cosM}}$$ 여기서 e는 이심률, M은 평균근점이각을 의미한다. 그러나 한가지 슬픈 사실은 여기서 나오는 E값이 근사값이라는 것이다.그렇기 때문에 보통 이 공식은 Newton-Raphson method를 사용하여 진행된다. Newton-Raphson method메소드는 계속적인 급수 형태로 계산되어지며, 계산을 반복할 수록 더 정확한 근사치를 뱉어준다고 보면 된다. 값의 오차는 이심률이 높을 수록 더 커지게 되는데, 데이터 상 지구의 이심률은 0.0167703정도니까 오차율이 그렇게 크진 않을 것이라고 예상된다.그럼 코드를 작성해보자. 1234567891011121314151617function getEccentricity(callback, x0, maxCount) { let x = 0; let x2 = x0; for(let i = 0; i < maxCount; i++) { x = x2; x2 = callback(x); }}function kepler(e, M) { return x => { return x + (M + e * Math.sin(x) - x) / (1 - e * Math.cos(x)); };}computed.E = getEccentricity(kepler(computed.e, computed.M), computed.M, 6);/* E =108.8962365500302 */ 행성의 위치 도출하기자, 지금까지는 이 파트를 위한 지루한 과정이었다. 필요한 값들을 모두 구했으니 이제 행성의 위치를 도출할 수 있게 되었다.이 값들이 있다면 근일점 쪽을 X축으로 하는 황도좌표평면에 대한 직각 좌표 값을 계산할 수 있다.그리고 이 좌표값의 유클리드거리 r과 진근점이각 v를 구할 수 있다. 진근점이각은 항성과 궤도의 근일점 기준으로 어느 각도에 행성이 위치하고 있는지를 나타내는 각이다.즉, 행성은 항성으로부터 r만큼의 거리만큼 떨어져 있고 궤도의 근일점으로부터 v만큼 돌아간 위치에 존재하고 있다는 것이다.거리랑 각을 알면 3d scene내에서의 로컬좌표를 정의할 수 있다. 12345678910111213141516const RAD_TO_DEG = 180 / Math.PI;computed.pos = {x: null, y: null};computed.pos.x = computed.a * (Math.cos(computed.E) - computed.e);computed.pos.y = computed.a * (Math.sqrt(1 - (computed.e * computed.e))) * Math.sin(computed.E);computed.r = Math.sqrt(Math.pow(computed.pos.x, 2) + Math.pow(computed.pos.y, 2));computed.v = Math.atan2(computed.pos.y, computed.pos.x);computed.r /= (1000 * AU);computed.v *= RAD_TO_DEG;/* r = 1.0081888306835929 AU v = 120.17208256525967 도*/ 이 포스팅 작성 당시 지구는 태양으로부터 1.0081...AU 떨어진 위치에 지구 궤도의 근일점으로부터 약 120도 돌아간 곳에 위치하고 있다는 결과를 얻을 수 있다.사실 제일 마지막 pos를 구하는 공식과 진근점이각을 구하는 공식은 아직 이해가 잘 안된다.하지만 중요한 것은 이런 공식을 사용해서 내가 원하는 뭔가를 만들어 볼 수 있다는 것이 아닐까 생각한다. 이상으로 행성 위치 게산하기 포스팅을 마친다.전체 소스는 Solarsystem 프로젝트 깃허브 레파지토리에서 확인해볼 수 있다.","link":"/2017/05/03/calculate-orbit-2/"},{"title":"[JavaScript로 중력 구현하기] 2. 코딩하기","text":"이번 포스팅에서는 저번 포스팅에 이어 중력을 직접 JS로 구현해보려고 한다. 개발환경은 JavaScript ES7, babel, Webpack, Three.js을 사용하였다. 필요한 상수 값들 선언먼저 프로그램에서 사용할 상수 값들 부터 선언하겠다. 123456789101112export const initOptions = { framerate: 60, G: 250, // 내 맘대로 중력상수 START_SPEED: 30, // 초기화 시 물체들의 속도 OBJECT_COUNT: 40, // 초기화 후 렌더 될 물체 개수 TRACE_LENGTH: 100, // 물체의 이동 궤적 길이 MIN_MASS: 400, // 물체들의 최소 질량 MAX_MASS: 3000, // 물체들의 최대 질량 DENSITY: 0.15 // 물체들이 렌더되는 밀도};export const SPHERE_SIDES = 20;export const MASS_FACTOR = 0.01; 원래라면 6.6742e-11의 값을 가지는 중력 상수는 250이나 준 이유는 이 물체들의 질량이 너무 작기 때문이다.실제 중력상수 값을 이용해서 계산을 하고 그 중력의 영향이 눈에 보일 정도가 되려면 질량도 행성급으로 커야한다. MASS_FACTOR는 나중에 렌더할 때 물체의 질량에 비례하도록 구의 크기를 설정하려고 하는데 질량 값들이 400~3000이다보니까 구의 부피가 너무 커질 것 같아서 그걸 보정하기 위해 선언한 상수이다.일종의 압축률 비슷한 느낌으로 나중에 구의 scale값을 정의할 때 물체의 질량에 MASS_FACTOR를 곱해줘서 일정한 비율로 크기를 줄일 예정이다. 이 포스팅은 ThreeJS를 설명하기 위한 포스팅이 아니므로 Scene과 Renderer의 선언 및 초기화 등에 대해서는 생략하고 지나가겠다. Mover 클래스 선언 자 그럼 이제 실제 움직일 놈들을 구현하자. 이름을 Object로 하고 싶었지만 알다시피 JS에서 Object라는 이름은 Build-in Object가 이미 가지고 있다. 그래서 다른 이름을 고심하다가 그냥 Mover라고 했다. 만들던 중에 다른 사람들이 구현한 것도 좀 찾아보고 그러다보니 알게된 건데 다들 Mover라고 하더라…. 1234567891011121314151617181920212223242526272829303132333435import { SPHERE_SIDES, MASS_FACTOR } from 'src/constants';import { Vector3, SphereGeometry, Line, MeshPhongMaterial, PointLight, Mesh, Geometry} from 'three';export class Mover { constructor(mass, velocity, location, id, scene) { this.uid = `mover-${id}`; this.location = location; this.velocity = velocity; this.acceleration = new Vector3(0.0, 0.0, 0.0); this.mass = mass; this.alive = true; this.geometry = new SphereGeometry(100, SPHERE_SIDES, SPHERE_SIDES); this.vertices = []; this.line = new Line(); this.color = this.line.material.color; this.basicMaterial = new MeshPhongMaterial({ color: this.color, specular: this.color, shininess: 10 }); this.mesh = new Mesh(this.geometry, this.basicMaterial); this.mesh.castShadow = false; this.mesh.receiveShadow = true; this.position = this.location; this.parentScene = scene; }} Mover클래스로 생성된 객체는 constructor 실행 시 넘겨받은 랜덤한 질량값과 속도, 위치 값을 가지고 있다.그리고 acceleration값을 초기화 해준다. 가속도는 어떠한 방향으로 움직이는 속도이므로 3개의 원소를 가진 벡터로 선언해 주었다. 그리고 alive 멤버 변수는 Mover끼리 충돌판정이 나면 Mover 두개를 하나로 합쳐서 더 큰 질량을 가진 Mover로 만들 예정이기때문에 얘가 죽었나 살았나를 판별하기 위한 값이다. Mover 객체들 렌더하기이후 Scene에는 초기화 시 Mover들을 그려주는 로직을 써주었다. 123456789101112131415161718192021222324252627282930313233343536373839404142export default { reset() { const movers = this.movers; if(movers) { // movers리스트 초기화 movers.forEach(v => { this.scene.remove(v.mesh); this.scene.remove(v.selectionLight); this.scene.remove(v.line); }); } movers = []; for (let i = 0; i < parseInt(this.options.MOVER_COUNT); i++) { const mass = this.getRandomize(this.options.MIN_MASS, this.options.MAX_MASS); const maxDistance = parseFloat(1000 / this.options.DENSITY); const maxSpeed = parseFloat(this.options.START_SPEED); const velocity = new Vector3( this.getRandomize(-maxSpeed, maxSpeed), this.getRandomize(-maxSpeed, maxSpeed), this.getRandomize(-maxSpeed, maxSpeed) ); const location = new Vector3( this.getRandomize(-maxDistance, maxDistance), this.getRandomize(-maxDistance, maxDistance), this.getRandomize(-maxDistance, maxDistance) ); // 랜덤한 속도, 위치, 질량을 가진 Mover를 생성 movers.push(new Mover(mass, velocity, location, i, this.scene)); } // Mover가 초기화될 때 만들어진 Mesh, Line, Light 객체를 Scene에 넣는다 movers.forEach(v => v.addMover()); this.movers = movers; }, getRandomize(min, max) { return Math.random() * (max - min) + min; }} 이제 Scene은 여러 개의 Mover를 담은 리스트인 movers를 가지게 되었다. 이제 렌더되는 동안 계산만 하면 끝! 인데…전 포스팅에서 말했듯이 여러 개의 물체에 대한 중력을 구하는 다체문제는 해를 구할 수가 없다.그래서 우리는 movers를 순회하며 한번 순회할 때마다 다른 Mover들과의 중력을 이체문제로 모두 계산하는 로직을 만들어야한다. Mover 객체들 운동 시키기123456789101112131415161718192021222324let movers = this.movers;movers.forEach((o1, i) => { if(!o1.alive) return false; movers.forEach((o2, j) => { if(o1.alive && o2.alive && i !== j) { // o1 -> o2의 거리 const distance = o1.location.distanceTo(o2.location); // o1, o2의 반지름 r1, r2 const r1 = (o1.mass / MASS_FACTOR / MASS_FACTOR / 4 * Math.PI) ** (1/3); const r2 = (o2.mass / MASS_FACTOR / MASS_FACTOR / 4 * Math.PI) ** (1/3); if(distance","link":"/2017/05/06/gravity-via-js-2/"},{"title":"[JavaScript로 중력 구현하기] 1. 중력이란 무엇일까?","text":"이번 포스팅에서는 만유인력의 법칙을 이용하여 중력을 구현해보려고 한다.지표면 상에서 한 방향으로 작용하는 중력이 아니라 랜덤한 질량을 가진 여러 개의 물체를 랜덤한 좌표에 뿌려놓고 서로의 운동에 어떻게 간섭하는 지를 살펴볼 수 있는 시뮬레이션을 만들어 볼 예정이다. 중력이란?먼저 이 포스팅은 물리학도가 아닌 필자같은 수학을 잘 못하는 사람을 대상으로 한 포스팅이기에 먼저 중력의 개념부터 제대로 짚고 넘어가려고 한다. 뉴턴께서 가라사대, 중력이란 두 개의 질량을 가진 물체가 서로 끌어당기는 힘이라고 했다.사실 뉴턴은 만유인력의 법칙이라는 공식을 만들면서 어떻게 중력이 작용하는가?는 알아냈지만 왜 중력이 생기는 것인지는 몰랐다. 아무래도 신께서만 아실 일이지…하고 넘어간 듯 싶다.그리고 뉴턴은 질량을 가진 물체가 중력을 가진다고 했었는데 이는 빛은 질량이 없기 때문에 중력에 영향을 전혀 받지않는다는 얘기가 된다. 하지만 현대에 와서 밝혀진 사실에 의하면 빛도 중력에 영향을 받는다.영화 인터스텔라에서도 보지 않았던가?(100% 확실한 시각화 묘사라고는 할 수 없지만 이론물리학자인 킵 손이 일반상대성이론을 계산해서 3D로 시각화했다고 한다.)블랙홀 같이 중력이 어마어마한 곳에서는 이 현상이 눈에 띄게 관찰되는데, 이를 중력렌즈효과라고 부른다. 블랙홀 뒤를 지나는 빛의 휘어짐 중력은 1915년 아인슈타인의 일반상대성이론에서 에너지에 의한 시공간의 휨으로 정의되게 된다.그래서 시공간 상의 물체의 질량이 급격하게 증가하거나 감소할 때 시공간에 파동이 생기게 되는데 이것이 바로 작년 초에 발견된 중력파이다.이렇게 발생된 중력파는 빛과 같은 속도로 움직인다고 한다. 그래서 갑자기 태양계에서 태양이 뿅!하고 사라지게 되면 태양 주위를 공전하던 행성들이 바로 궤도를 이탈해서 날아가는 게 아니다.예를 들어서 지구같은 경우에는 태양에서 나온 빛이 지구까지 약 8분 20초 정도 걸리니까 태양이 갑자기 사라져도 8분정도는 없어진 태양이 있던 위치를 그대로 공전하는 것이다.그러다가 8분 후 마지막에 태양에서 나온 빛이 지구에 도달하는 순간 지구는 궤도를 이탈할 것이다. 큰 질량을 가진 물체일수록 시공간을 더 많이 휘게 만든다 그렇다고 만유인력의 법칙이 틀렸다는 것이 아니라, 상대성이론은 만유인력의 법칙이 놓친 점을 보완해준 것에 더 가깝다.사실 중력이 아주 큰 공간(아주 많이 휘어진 공간)이 아니라면 만유인력의 법칙으로도 충분히 설명이 가능하다. 예를 들어 지구는 전체적으로 보면 커다란 구이지만 지구에 사는 우리가 주변을 돌아보면 이게 구인지 평면인지 알수가 없다.이와 같이 휘어진 정도가 아주아주 작아 유클리드 공간과 거의 유사한 공간을 유사-유클리드 공간이라고 한다.그리고 이런 유사-유클리드 공간에서는 만유인력의 법칙만 적용해도 충분하다. 아폴로 우주선을 달에 보낼때도 만유인력의 법칙만으로 충분했다고 한다. 그래서 결론적으로 중력을 구하는 것 = 시공간의 곡률을 구하는 것으로 볼 수 있지만,필자는 어렵고 복잡한 일반상대성이론까진 안쓸 것이고 쓰지도 못하고…그리고 만유인력의 법칙만으로도 간단한 시뮬레이션은 구현하기에 충분하다. 그래서 구현은 어떻게?일단 설명을 쉽게 하기 위해 랜덤한 질량을 가진 2개의 물체를 랜덤한 좌표에 랜덤한 방향과 속도를 가지게 설정한 후 그냥 공간 상에 뿌린다고 가정하자.이 물체 2개의 중력의 영향을 계산하는 것은 그다지 어렵지 않다. 여기서 만유인력의 법칙이 등장한다. F12=−Gm1m2∣r12∣2r12^F_{12} = -G{\\frac{m_1 m_2}{\\vert{r_{12}\\vert}^2}}\\hat{r_{12}}F​12​​=−G​∣r​12​​∣​2​​​​m​1​​m​2​​​​​r​12​​​^​​ 이 공식은 물체1이 물체2에게 가하는 중력 F를 구하는 공식이다. G는 중력상수를 의미하고,이 중력상수는 6.67384 * 10^-11이다. 그리고 분자의 m은 각 물체의 질량이고 분모인 r은 물체1과 물체2의 유클리드거리를 의미한다.그리고 마지막에 곱해주는 r은 물체1에서 물체2를 바라보고 있는 단위벡터를 의미한다. 그리고 중간에 있는 식을 보면 분모에 거리가 들어가있고 분자에 질량이 들어가있다.이는 두 물체간의 거리가 멀어질수록 분모가 커지니까 값은 작게 나오고, 두 물체의 질량이 커질수록 분자가 커지니까 값은 크게 나온다는 것을 의미하며,간단하게 말해서 중력은 두 물체 간 거리에 반비례하고 두 물체 간 질량의 곱에 비례한다라고 할 수 있다. 혹시 선형대수학을 안배우신 분들이 있을 지 모르니 간단하게 설명하면,벡터라는 놈은 공간 상에서의 방향을 의미한다는 정도만 알아두셔도 좋을 것 같다.지금 우리가 계산하려고 하는 공간은 3차원 공간이기 때문에 벡터는 (x, y, z) 3개의 좌표값을 가지게 된다. 우리가 어릴 때 배웠던 만유인력의 법칙은 F=Gm1m2r2F = G\\frac{m_1 m_2}{r^2}F=G​r​2​​​​m​1​​m​2​​​​ 의 꼴을 가지는데 이 공식의 결과는 1 5.2 1204같은 스칼라 값을 가지게 된다. 스칼라는 어떠한 물리량만을 의미하는 것이기 때문에중력이 얼마나 센지는 알 수 있어도 중력이 어느 방향으로 센지는 알 수가 없다. 그래서 힘의 방향을 표현해줘야 우리가 이 공식을 제대로 사용할 수 있고, 그렇기 때문에 마지막에 벡터를 곱해주는 것이다. 그리고 공식 맨 앞에 -1이 붙어있는 이유는 벡터의 방향 때문이다. 마지막에 우리가 곱한 벡터는 물체1 -> 물체2의 방향이다.하지만 중력은 내 쪽으로 끌어당기는 힘이지 밀어내는 힘이 아니기 때문에 따라서 힘의 방향을 물체1 o2를 바라보는 벡터를 구한다. let force = new Vector3().subVectors(o1.location, o2.location); // 위에서 구한 방향벡터의 길이를 구한 후 절대값으로 변환해준다. const distance = Math.sqrt(force.length() ** 2); // 위에서 구한 방향벡터를 단위벡터로 바꿔준다. force = force.normalize(); // 중력 스칼라 구하기 const strength = -(G * o1.mass * o2.mass) / (distance ** 2); // 방향벡터에 위에서 구한 중력 스칼라를 곱해준다 force = force.multiplyScalar(strength); return force;} 이제 여기서 나온 값을 계산 대상이 되는 현재 물체의 위치값에 계속 더해주면 물체는 그 방향으로 운동하게 될 것이다.근데 문제는 실질적으로 우리는 2개만 뿌릴 것이 아니라는 것이다.저 만유인력의 법칙은 2개의 물체 간의 중력 만을 생각하고 계산하고 있다. 근데 우리는 여러 개의 물체에 동시에(!) 받는 중력을 계산하고 싶은데? 아쉽게도 이런 다체문제는 수리물리학분야에서 손꼽히는 난제이며,1887년에 앙리 푸엥카레가 삼체 이상 문제의 일반해를 구하는 것이 불가능하다라고 했단다.그래서 우리는 저 위에 있는 이체문제의 만유인력의 법칙을 사용해서 중력을 근사적으로 구하는 수 밖에 없다.다음 포스팅에서는 직접 JavaScript코드로 시뮬레이션을 구현해보도록 하겠다. 다음 포스팅에서는 간단한 중력 모델 샘플을 코드로 구현해볼 예정이다.이상으로 JavaScript로 중력 구현하기 첫번째 포스팅을 마친다.","link":"/2017/05/06/gravity-via-js-1/"},{"title":"Universal Server Side Rendering이란?","text":"이번 포스팅에서는 최근 모던 웹 어플리케이션에서 많이 사용하고 있는 Universal SSR에 대해서 설명하고자 한다.Server Side Rendering과 Single Page Application의 방식을 간단하게 알아보고 이 두 렌더 방식을 조합한 Universal SSR의 방식을 설명한다. Server Side Rendering을 수행하는 Multi Page ApplicationSSR 방식은 원래 전통적인 웹 어플리케이션에서 사용하던 방식이다. 최근에는 SPA(Single Page Application)과 대조하여 MPA(Multi Page Application)이라고도 불린다.SSR 어플리케이션은 라우팅이 수행된 후 새로운 페이지가 서버에 요청되면 싶으면 그때마다 HTML를 렌더한 후 클라이언트에서 전체 페이지를 다시 내려받는다.대략적인 실행 순서는 다음과 같다. 클라이언트가 서버에 example.com/products/12 URL로 요청을 보낸다. 서버에서는 해당 URL과 연결되어있는 메소드가 실행되고 알맞는 HTML Template파일을 찾는다. Database에서 12번 상품의 데이터를 가져온다. 가져온 데이터와 HTML Template을 사용해 최종 HTML을 렌더한다. 클라이언트로 HTML을 내려준다. 최종적으로 사용자가 뷰를 본다. 사용자가 렌더된 페이지를 본 이후에도 Ajax를 사용하여 데이터를 추가로 더 받아올 수도 있겠지만 일단 사용자가 완성된 페이지를 보는 시점은 6번 과정이 끝난 이후기 때문에 Ajax와 같은 예외는 생략했다.또한 2번 과정에서 어째서 이 방식의 어플리케이션이 MPA라고 불리는 지 알 수 있는데, 각 페이지에 매칭된 HTML Template이 따로 존재하기 때문이다.다음과 같은 방식의 장단점은 다음과 같다. 장점 서버에서 완성된 HTML을 내려주기 때문에 SEO(Search Engine Optimization)에 최적화되어 있다. 매 페이지에서 필요한 리소스만 로딩하기 때문에 초기 로딩속도를 최적화할 수 있다. 단점 매 페이지 로딩 시마다 새로운 리소스를 요청해야하므로 전체적인 트래픽이 증가한다. 페이지 이동 시 마다 새로고침이 되며 전체 페이지를 다시 렌더하므로 로딩 시간이 길어진다. Client Side Rendering을 수행하는 Single Page Application최근 들어 많은 수의 Frontend 개발자가 Client Side Rendering을 수행하는 A.K.A SPA(Single Page Application)를 개발한다. 즉, 서버에서 실제로 다운로드 받는 페이지는 단 1개이고 그 이후 JavaScript를 통해 동적인 렌더링을 실시하는 어플리케이션을 의미한다.대략적인 실행 순서는 다음과 같다. 클라이언트가 서버에 example.com/products/12 URL로 요청을 보낸다. 서버에서는 뭐가 됐던 요청 URL이 exmplate.com으로 시작하면 index.html을 찾아서 내려준다. 그리고 추가로 JavaScript Bundle을 같이 내려준다. 예를 들면 Webpack같은 모듈러로 빌드하면 나오는 bundle.js같은 파일이 되겠다. bundle.js을 실행한 클라이언트가 api.example.com/products/12 API를 사용하여 12번 상품의 데이터를 서버에 요청한다. 서버는 Database에서 12번 상품의 데이터를 가져온 후 클라이언트에 데이터를 내려준다. 클라이언트는 받아온 데이터를 사용하여 뷰가 렌더한다. 최종적으로 사용자가 뷰를 본다. 아까에 비해서 뭔가 복잡해졌다. 이 방식이 SPA인 이유는 2번 과정에 있다. 보통 Nginx나 Apache같은 서버 엔진의 설정에 해당 url을 선언하고 조건에 일치하는 url로 요청이 들어왔을 경우 index.html파일을 찾아서 보내준다. 어떤 url이든 조건에 일치하게 되면 index.html 하나만 보내주기 때문에 Single Page인 것이다.그리고 클라이언트는 현재 12번 상품의 데이터를 가지고 있지 않기 때문에 추가적으로 API 호출을 하여 12번 상품의 데이터를 받아와야 한다.그럼 SPA의 장단점도 한번 살펴보자. 장점 초기 로딩 시 서버로부터 모든 정적 리소스를 내려받은 후에는 페이지 이동 시 필요한 데이터만 내려받으므로 로딩 속도가 빠르고 전체적인 트래픽을 감소시킬 수 있다. 페이지 이동 시 새로고침이 되지 않으므로 사용자 경험(UX)가 향상된다. 단점 초기 로딩 시 현재 페이지에서 사용하지않는 모든 정적 리소스를 받으므로 초기 로딩속도가 느리다. SEO에 취약하다. SSR vs SPA각 방식의 대략적인 실행 흐름과 장단점을 살펴보았는데, SSR과 SPA 두 방식 모두 로딩 속도라는 장단점을 가지고 있다. 뭐가 다른 걸까?SSR의 장점은 초기 로딩속도이다. SPA는 첫 로딩 시 전체 어플리케이션에서 사용하는 모든 정적 리소스를 내려받기 때문에 초기 로딩속도는 느리지만 그 이후에는 추가적으로 리소스를 다운로드 받을 필요가 없기 때문에 이후 구동 속도가 빠른 것이다.반면 SSR은 현재 페이지에서 필요한 리소스만 로딩하면 되기 때문에 초기 로딩속도는 SPA에 비해서 빠를 수 있다. 그러나 받아온 정적 리소스를 어딘가에 저장하고 있는 게 아니기 때문에 페이지를 이동할때마다 저번 페이지에서 받아왔던 리소스라고 하더라도 처음부터 다시 받아와야한다.그래서 어플리케이션 초기화 후 페이지 이동 시 로딩 시간은 SSR이 더 느릴 수 있다. 하지만 SPA방식의 어마무시한 단점은 바로 SEO다. SEO는 Search Engine Optimization의 약자로 직역 그대로 검색엔진최적화를 의미한다.검색 엔진은 기본적으로 크롤링을 해서 페이지를 수집하는 방식으로 이루어져있는데, 문제는 크롤링을 하는 봇들이 JavaScript를 실행할 수 있는 능력이 없다는 것이다.최근 구글 크롤러 같은 경우는 JavaScript 실행능력이 있다고 하지만 개인적으로 아직까지 그렇게까지 신뢰가 가는 정도는 아니라고 본다. 게다가 Facebook과 같은 SNS에 페이지 공유를 했을때에는 og meta tag를 크롤러가 읽어야지 공유된 사이트의 정보가 올바르게 표시되는데, JavaScript가 실행되지 않은 페이지를 크롤러가 읽었을 때는 그냥 빈페이지밖에 없으니 정보를 제대로 표시해주지 못하는 문제도 있었다. 필자도 이 문제를 해결하기 위해 #!(해쉬뱅)이라던가 _esacped_fragment_이러단가 Pre rendering 같은 방법들을 사용해봤었지만 검색엔진사에서 권장하는대로 어플리케이션을 작성해도 SSR방식에 비해 데이터를 제대로 못긁어가는 건 어쩔 수 없었다. 새로운 개념의 Server Side Rendering의 등장SSR을 택하자니 SPA의 장점이 아깝고, SPA를 택하자니 SSR의 장점이 아깝다. 그럼 어떻게 해야할까? 그래서 나온 방식이 최근에 많이 사용하고 있는 두 방식을 적당히 짬뽕한 방식이다.사용자의 첫 요청시에만 SSR을 수행하고, 그 이후는 SPA처럼 동적인 렌더링을 수행하는 것이다. 이 방식은 아래와 같은 장점을 가진다. 장점 첫 요청을 SSR로 완성된 HTML을 내려줌으로써 SEO와 초기 렌더링 속도문제를 해결 이후 클라이언트에서 렌더링을 수행함으로써 SPA의 장점인 페이지 이동 시 빠른 렌더 속도도 그대로 가져감 Frontend 프레임워크 3대장인 Angular, React, Vue 모두 이러한 SSR 방식을 공식으로 지원하기 때문에 Client와 Server를 같은 Context로 묶을 수 있음. 즉, 내가 만든 컴포넌트는 클라이언트에서 렌더를 수행하든 서버에서 렌더를 수행하든 동일하게 실행된다. 하지만 모든 기술에는 Trade-off가 있는 법…단점은 뭐가 있을까? 단점 코드가 복잡하다. 어플리케이션 구동 순서를 확실하게 파악하고 있지 않다면 진짜 헷갈린다. 서버에서 렌더링을 수행하므로 단순 리소스 서빙보다는 아무래도 CPU를 많이 사용하게 되고, 부하가 걸릴 수 있다. 서버에 익숙하지 않은 Frontend 개발자의 경우 클라이언트처럼 개발을 진행하게 되면 의도하지 않은 버그가 생길 수 있다. 특히 2번과 3번 같은 경우 필자가 간과했던 부분인데, 클라이언트에서는 아무 문제 없었을 부분이 서버에서는 치명적인 실수가 되어 버그로 돌아오는 경험을 했다.다음 포스팅에서는 필자가 회사에 Vue-ssr을 도입했던 경험과 어떤 실수를 했는지에 대해서 적어보고 회고하려고 한다. 이상으로 Universal Server Side Rendering이란? 포스팅을 마친다.","link":"/2018/09/25/universal-ssr/"},{"title":"정렬 알고리즘 정리 (Bubble, Selection, Insertion, Merge, Quick)","text":"이번 포스팅에서는 대표적인 정렬알고리즘 5가지와 대략적인 빅오표기법에 대해서 정리하려고 한다.먼저, 그 5가지 정렬알고리즘은 다음과 같다. 버블정렬(Bubble sort) 선택정렬(Selection sort) 삽입정렬(Insertion sort) 병합정렬(Merge sort) 퀵정렬(Quick sort) 그리고 이 알고리즘들의 성능은 빅오표기법으로 표현하므로, 빅오표기법에 대한 설명도 간단히 하고 넘어가려한다. Big O 표기법과 시간복잡도알고리즘들의 성능을 판단하는 지표로는 시간 복잡도(Time Complexity)와 공간 복잡도(Time Complexity)가 있다. 시간 복잡도는 알고리즘의 수행시간을 의미하는 지표이며, 공간 복잡도는 알고리즘의 메모리 사용량을 의미한다. 보통 알고리즘에 대해서 공부하다보면 이 알고리즘의 시간복잡도는 O n입니다 혹은 O의 n제곱입니다 이런 식으로 이야기하거나 $O(n)$ 이런 식으로 작성되어있는 것을 볼 수 있었을 것이다. 이게 바로 빅오(Big O) 표기법이다. 말로 풀어보자면 $O(n)$의 의미는 다음과 같다. 이 함수는 n만큼의 데이터가 주어졌을 때, “최악”의 경우 n만큼의 리소스를 소모한다 이때 위에서 말한 리소스는 시간복잡도라면 시간이고 공간복잡도라면 메모리공간이 될 것이다.하지만 보통 정렬알고리즘을 평가할때는 주로 시간복잡도에 집중하므로 여기서는 시간복잡도만 살펴보도록 하겠다. 먼저 시간복잡도에 대한 이해를 더 하기위해 이진탐색 알고리즘의 시간복잡도를 살펴보자. 혹시 이진탐색을 잘모르는 사람은 술게임인 업다운을 생각해보자. 다들 잘 알겠지만 업다운은 다른 사람이 생각한 임의의 숫자를 맞춰야하는 게임이다. 이때, 가장 질문을 최소화할 수 있는 방법은 무엇일까? 바로 첫 질문에 중간 값인 50을 부르는 것이다. 50을 부르고 상대방이 업 또는 다운을 하게되면 우리는 반대쪽에 있는 50개의 수는 버리고 나머지 50개만 다시 생각하면 되는 것이다. 만약 운좋게 상대방이 생각한 수가 50이라면 바로 게임이 끝나고 상대방은 벌주를 먹게 된다. 어쨌든 이 게임에서 최악의 경우는 계속 업&다운을 반복하다가 $O({\\log}N)$번만에 끝나는 것이고 최선의 경우는 찾고자 하는 값을 첫 번째 추측으로 맞춰버린 상황. 즉, 시간복잡도는 $O(1)$이 된다. 보는 바와 같이 최선의 결과와 최악의 결과 간 차이는 늘 있을 수 밖에 없기 때문에 보통 알고리즘을 평가할 때는 주로 최악 의 경우를 생각한다.그리고 이런 최악 의 경우를 표현할 때 바로 빅오 표기법을 사용하는 것이다. 자주 나오는 것들은 보통 $O(1)$, $O(n)$, $O(n^2)$, $O({\\log}N)$ 정도가 있는데 알기 쉽게 2차원 그래프로 그려보면 다음과 같이 나타난다. 빅오 표기법의 계산방법과 같은 더 자세한 내용은 다른 포스팅에서 다시 설명하도록 하겠다. 정렬알고리즘 정렬알고리즘은 컴퓨터 공학에서 중요시되는 문제 중 하나로, 어떤 데이터셋이 주어졌을 때 이를 정해진 순서대로 나열하여 재배치하는 문제이다.실제 개발을 하다보면 불규칙한 데이터들을 정렬 후 탐색해야하는 경우가 꽤나 많이 발생하게 되는데 이때 상황에 맞는 알고리즘을 사용하여 효과적으로 문제를 해결할 수 있느냐가 핵심이라고 볼 수 있다. 예를 들어 1부터 10까지 적혀있는 공이 불규칙하게 들어있는 주머니에서 공을 하나씩 꺼내어 작은 수부터 큰 수의 순서로 공을 나열한다고 생각해보자.보통 이런 경우 사람도 어렵지 않게 쓱쓱 정렬해낸다. 하지만 컴퓨터가 주로 다루는 데이터는 10,000개일수도 10,000,000개일수도 있다. 그리고 데이터베이스 같은 경우는 이론상 무한 개의 데이터를 다룰 수 있어야 한다. 이때 데이터가 정렬되어 있지 않다면 순차적으로 하나씩 데이터를 봐가면서 탐색해야하지만, 데이터가 이미 정렬되어있다면 위에서 예시로 들었던 이진탐색(Binary Search)와 같은 강력한 알고리즘을 사용할 수도 있다.(사실 이게 제일 큰 이유이다) 자 그럼 대표적인 정렬알고리즘인 버블정렬, 선택정렬, 삽입정렬, 병합정렬, 퀵정렬을 한번 살펴보도록 하자. 버블정렬(Bubble sort) 버블정렬은 거의 모든 상황에서 최악의 성능을 보여주지만, 이미 정렬된 자료에서는 1번만 순회하면 되기 때문에 최선의 성능을 보여주는 알고리즘이다. 이미 정렬되어 있는 데이터를 왜 정렬하냐는 의문이 들 수 있지만, 정렬알고리즘 자체는 데이터가 정렬되어 있는지 아닌지 모르고 작동하는 것이기 때문에 의미는 있다. 버블정렬은 다음과 같은 순서로 작동한다. 0번째 원소와 1번째 원소를 비교 후 정렬 1번째 원소와 2번째 원소를 비교 후 정렬… n-1번째 원소와 n번째 원소를 비교 후 정렬 한번 순회할 때마다 마지막 하나가 정렬되므로 원소들이 거품이 올라오는 것처럼 보여서 버블정렬이라고 부른다. 원리도 직관적이라서 구현하기 편하긴 하지만 꽤나 비효율적인 정렬 방식이다. 그래서 보통 처음 배울 때 한번 짜보고 나면 실무에서 쓰는 경우는 거의 못 봤다.(물논 시간이 없다면 쓸 수도 있다…) 버블정렬의 구현코드는 다음과 같다. 12345678910111213141516function bubbleSort (input) { const len = input.length; let tmp = null; for (let i = 0; i < len; i++) { for (let j = 0; j < len; j++) { if (input[j] > input[j + 1]) { // Swap tmp = input[j]; input[j] = input[j + 1]; input[j + 1] = tmp; tmp = null; } } } return input;} 선택정렬(Selection sort) 선택정렬은 주어진 자료들 중에 현재 위치에 맞는 자료를 찾아 선택하여 위치를 교환하는 정렬 알고리즘이다.한번 순회를 돌게되면 알고리즘 상 전체 자료 중 가장 작은 값의 자료가 0번째 인덱스에 위치하게 되므로 그 다음 순회부터는 1번 인덱스부터 순회를 돌며 반복하면 된다. 선택정렬은 다음과 같은 순서로 작동한다. 0번 인덱스 ~ n번 인덱스 중 가장 작은 값을 찾아 0번째 인덱스와 swap한다 1번 인덱스 ~ n번 인덱스 중 가장 작은 값을 찾아 1번째 인덱스와 swap한다… n-1번 인덱스 ~ n번 인덱스 중 가장 작은 값을 찾아 n번째 인덱스와 swap한다 선택정렬은 현재 자료가 정렬이 되어있던말던 무조건 전체 리스트를 순회해가며 검사하기 때문에 최선의 경우든 최악의 경우든 한결같이 $O(n^2)$의 시간복잡도를 가지고 있다.선택정렬의 구현코드는 다음과 같다. 12345678910111213141516function selectionSort (input) { const len = input.length; let tmp = null; for (let i = 0; i < len; i++) { for (let j = 0; j < len; j++) { if (input[i] < input[j]) { // Swap tmp = input[j]; input[j] = input[i]; input[i] = tmp; tmp = null; } } } return input;} 삽입정렬(Insertion sort) 삽입정렬은 주어진 자료의 모든 요소를 앞에서부터 차례대로 정렬된 자료 부분과 비교하여 자신의 위치를 찾아 삽입하는 정렬이다. 사실 인간이 직접 정렬하는 순서와 제일 흡사하다고 할 수 있는 정렬인데, 직접 손안에 있는 카드를 정렬한다고 생각해보자. 대충 다음과 같은 순서로 행동할 것이다. 카드를 하나 고른다. 내가 가지고 있는 카드를 쭉 살펴본다. 현재 카드가 들어가야할 순서에 카드를 껴넣는다. 삽입정렬은 이 순서와 비슷하게 작동한다. 0번 인덱스는 건너뛴다. 0~1번 인덱스 중 1번 인덱스 값이 들어가야할 위치를 찾아서 넣는다 0~2번 인덱스 중 2번 인덱스 값이 들어가야할 위치를 찾아서 넣는다… 0~n번 인덱스 중 n번 인덱스 값이 들어가야할 위치를 찾아서 넣는다 삽입정렬은 최선의 경우 전체 자료를 한번만 순회하면 되기때문에 $O(n)$의 시간복잡도를 가지지만 최악의 경우 $O(n^2)$의 시간복잡도를 가진다. 선택정렬의 구현코드는 다음과 같다. 123456789101112131415161718function insertionSort (input) { const len = input.length; for (let i = 1; i < len; i++) { // 두번째 카드부터 시작 const value = input[i]; // 카드를 잡는다 let j = i-1; for (;j > -1 && input[j] > value; j--) { // 이미 정렬된 카드들을 뒤에서부터 살펴보다가 // 살펴본 카드가 현재 카드보다 크다면 // 살펴본 카드를 뒤로 한칸 보낸다 input[j+1] = input[j]; } // 뒤로 보내는 행위가 끝나면 // 현재 카드보다 작은 카드의 한칸 뒤에 // 현재 카드를 위치시킨다 input[j+1] = value; } return input;} 병합정렬(Merge sort) 병합정렬은 일종의 분할 정복법 중 하나로 큰 문제를 작은 여러 개의 문제로 쪼개서 각각을 해결한 후 결과를 모아서 원래의 문제를 해결하는 방법이다.병합정렬이라는 이름 그대로 주어진 자료를 잘게 쪼갠 뒤 합치는 과정에서 정렬을 하는 알고리즘이다. 순서를 살펴보면 다음과 같다. [5, 0, 4, 1]라는 자료를 받았다 length가 1이 될때까지 자료 리스트를 반으로 쪼갠다 [5, 0] [4, 1]가 된다. [5] [0] [4] [1]가 된다 각 리스트의 길이가 1이 되었으므로 병합을 시작한다 왼쪽의 0번 인덱스와 오른쪽의 0번 인덱스를 비교하여 적은 값을 먼저 병합. [5]와 [0] 중 0이 더 작으므로 새로운 리스트에 0을 먼저 병합한다 [0, 5] 생성 왼쪽의 0번 인덱스와 오른쪽의 0번 인덱스를 비교하여 적은 값을 먼저 병합. [4]와 [1] 중 1이 더 작으므로 새로운 리스트에 1을 먼저 병합 [1, 4] 생성 이제 [0, 5]와 [1, 4]를 병합한다. 이 리스트들은 정렬되었기 때문에 작은 인덱스일 수록 작은 값을 가진다는 것이 보장되어있다. 왼쪽의 0번 인덱스와 오른쪽의 0번 인덱스를 비교하여 적은 값을 먼저 병합 [0] 생성. [5]와 [1, 4]가 남았다 왼쪽의 0번 인덱스와 오른쪽의 0번 인덱스를 비교하여 적은 값을 먼저 병합 [0, 1] 생성. [5]와 [4]가 남았다 왼쪽의 0번 인덱스와 오른쪽의 0번 인덱스를 비교하여 적은 값을 병합 [0, 1, 4] 생성. [5]가 남았다 값이 남았으므로 그냥 병합해준다. [0, 1, 4, 5] 정렬완료 볼드 처리된 왼쪽의 0번 인덱스와 오른쪽의 0번 인덱스를 비교하여 적은 값을 먼저 병합 을 보자.같은 방식으로 계속 반복하여 병합하고 있기 때문에 보통 병합정렬은 재귀함수로 구현한다. 또한 병합정렬은 항상 $O({n\\log}n)$의 시간복잡도를 가지기 때문에 효율적이다. 그러나 원소의 개수만큼 리스트를 쪼개고 따로 저장하고 있어야하기 때문에 $O(n)$의 공간복잡도를 가진다. 한마디로 메모리를 팔아 수행속도를 얻는 경우라고 할 수 있다. 병합정렬의 구현코드는 다음과 같다 12345678910111213141516171819202122232425262728function merge (left, right) { const result = []; while (left.length && right.length) { if (left[0] pivot) j--; while (i < j && arr[i]","link":"/2018/10/13/sort-algorithm/"},{"title":"TypeScript를 사용하여 간단한 인공 신경망 개발 삽질기","text":"이번 포스팅에서는 저번 포스팅에 이어 TypeScript를 사용하여 간단한 인공신경망을 만들어본 것을 간단하게 정리하려고 한다. 이 어플리케이션은 현 직장에서 진행하는 Tech 세미나의 발표용으로 작성한 것이기 때문에 상당히 개발 시간이 촉박했다. 그래서 머릿속으로 생각해놓았던 기능을 전부 구현하지는 못했고 추후 기능을 더 추가해볼 예정이다.예전에 JavaScript를 사용하여 완전 하드코딩한 ANN을 작성해본 경험이 있었기 때문에 그 코드를 재활용해볼까 생각했었다.(이젠 커밋 로그에서만 볼 수 있는 그 코드…)근데 막상 뜯어보니까 재활용할 수 있는 부분이 딱히 없어서(완전 하드코딩이었음) 그냥 처음부터 다시 짜기로 했다. 개발에 들어가기에 앞서서 그냥 추상적으로 생각했던 설계와 기능은 이러했다. 하드코딩은 그만! 구조적인 설계를 하자. 레이어 개수, 한 레이어당 노드 개수는 자유자재로 변할 수 있어야 한다. Loss가 줄어드는 과정이나 Weight들의 변화를 시각화해서 보면 좋을 것 같다! 어플리케이션 인풋이나 초기 Weight 값을 어플리케이션 내에서 직접 변경할 수 있도록 하자! 이중 4번은 세미나 PPT도 만들어야 하므로 결국 시간 부족으로 하지 못했고, 1~3번까지는 어떻게든 시간 내에 구현에 성공했다. 뉴런의 연결에 따라 달라지는 로스일단 처음에는 ANN의 핵심 기능인 Weight를 업데이트하는 기능을 어떻게 구현할 것인가를 고민해야했다. 사실 Forward propagation을 진행할 때에 Back propagation 때 필요한 대부분의 값을 미리 계산해놓을 수 있다. 우선, 저번 포스팅에서 사용했던 예시를 가져와서 설명을 진행하려고 한다.예시의 자세한 내용은 링크에서 확인하자.Deep Learning이란 무엇인가? - Backpropagation Back propagation에서 Weight를 업데이트하는 공식은 다음과 같다. ∂E∂w=∂E∂a∂a∂z∂z∂w\\begin{aligned} \\frac{\\partial E}{\\partial w} = \\frac{\\partial E}{\\partial a} \\frac{\\partial a}{\\partial z} \\frac{\\partial z}{\\partial w} \\\\ \\end{aligned}​​∂w​​∂E​​=​∂a​​∂E​​​∂z​​∂a​​​∂w​​∂z​​​​​ $\\frac{\\partial E}{\\partial a}$: 뉴런의 아웃풋($a$)이 에러($E$)에 영향을 끼친 기여도 $\\frac{\\partial a}{\\partial z}$: 인풋($x$)과 Weight($w$)의 곱($z$)이 뉴런의 아웃풋($a$)에 영향을 끼친 기여도 $\\frac{\\partial z}{\\partial w}$: Weight($w$)가 인풋 x Weight($z$)에 영향을 끼친 기여도 사실 뉴런의 Weight를 업데이트할 때 2번과 3번의 식은 변하지 않는다.변하는 것은 1번 뉴런의 아웃풋이 에러에 영향을 끼친 기여도 뿐이다. 더 정확히 말하면 상황에 따라 기여도를 계산하는 방법만 바뀐다. 다음 2가지 케이스를 살펴보자. 뉴런의 아웃풋이 특정 에러에만 영향을 끼친 경우 이 네트워크에서 최종 에러를 MSE로 구한다고 가정했을 때 에러는 다음과 같이 나타낼 수 있다. E1=(target1−a20)2E2=(target2−a21)2E=12(E1+E2)\\begin{aligned} E_1 = (target_1 - a_{20})^2 \\\\ \\\\ E_2 = (target_2 -a_{21})^2 \\\\ \\\\ E = \\frac{1}{2}(E_1 + E_2) \\end{aligned}​E​1​​=(target​1​​−a​20​​)​2​​​​E​2​​=(target​2​​−a​21​​)​2​​​​E=​2​​1​​(E​1​​+E​2​​)​​ 위 식에서도 볼 수 있듯이 $a_{20}$의 경우 $E_1$에는 영향을 끼칠 수 있지만 절대 $E_2$에는 영향을 끼칠 수 없다. 아예 식에 그 변수 자체가 없다. 그렇기 때문에 우리는 $\\frac{\\partial E}{\\partial a_{20}}$을 계산할 때 아예 $E_1$을 제외한 나머지 에러를 모두 0으로 간주하고 계산할 수 있다. ∂E∂a20=−(t1−a20)\\begin{aligned} \\frac{\\partial E}{\\partial a_{20}} = -(t_1 - a_{20}) \\end{aligned}​​∂a​20​​​​∂E​​=−(t​1​​−a​20​​)​​ 뉴런의 아웃풋이 여러 에러에 영향을 끼친 경우 다시 한번 레이어를 보자. 이번에는 좀 더 안쪽에 있는 $a_{10}$이 전체 에러 $E$에 얼마나 영향을 끼쳤는지 알아내야한다.그냥 이어져 있는 선만 봐도 이 놈은 여기저기 다리를 뻗고 있다는 것을 알 수 있다. $a_{10}$를 인풋으로 사용한 뉴런은 물론이고 이 뉴런이 내보낸 아웃풋을 사용한 뉴런 등 많은 것들이 영향을 받았을 것이다. 그래서 이번에는 아까처럼 다른 변수를 무시하거나 하는 짓은 못한다. 다 계산해줘야한다. ∂E∂a10=∂E1∂a10+∂E2∂a10\\begin{aligned} \\frac{\\partial E}{\\partial a_{10}} = \\frac{\\partial E_1}{\\partial a_{10}} + \\frac{\\partial E_2}{\\partial a_{10}} \\end{aligned}​​∂a​10​​​​∂E​​=​∂a​10​​​​∂E​1​​​​+​∂a​10​​​​∂E​2​​​​​​ 여기서 $\\frac{\\partial E}{\\partial a_{10}}$이 의미하는 것은 마지막 레이어부터 a_10을 아웃풋으로 내보낸 뉴런이 속한 레이어의 바로 뒤 레이어까지 전파된 에러를 의미한다.그리고 이 에러를 구성하는 $\\frac{\\partial E_1}{\\partial a_{10}}$와 $\\frac{\\partial E_2}{\\partial a_{10}}$ 등은 이렇게 구한다. ∂E1∂a10=∂E1∂a20∂a20∂z20∂z20∂a10∂E2∂a10=∂E2∂a21∂a21∂z21∂z21∂a10\\begin{aligned} \\frac{\\partial E_1}{\\partial a_{10}} = \\frac{\\partial E_1}{\\partial a_{20}} \\frac{\\partial a_{20}}{\\partial z_{20}} \\frac{\\partial z_{20}}{\\partial a_{10}}\\\\ \\\\ \\frac{\\partial E_2}{\\partial a_{10}} = \\frac{\\partial E_2}{\\partial a_{21}} \\frac{\\partial a_{21}}{\\partial z_{21}} \\frac{\\partial z_{21}}{\\partial a_{10}}\\\\ \\end{aligned}​​∂a​10​​​​∂E​1​​​​=​∂a​20​​​​∂E​1​​​​​∂z​20​​​​∂a​20​​​​​∂a​10​​​​∂z​20​​​​​​​∂a​10​​​​∂E​2​​​​=​∂a​21​​​​∂E​2​​​​​∂z​21​​​​∂a​21​​​​​∂a​10​​​​∂z​21​​​​​​​ 이 식에서 나타난 $\\frac{\\partial E_1}{\\partial a_{20}}$과 $\\frac{\\partial E_2}{\\partial a_{21}}$의 경우는 위에서 본 1번과 같은 케이스이므로 1번처럼 계산하면 될 것이다. 필자는 이 식을 코드로 구현할 때 헷갈렸던 부분이 하나 있었다.한번 $\\frac{\\partial E_1}{\\partial a_{10}}$을 구하는 공식을 보자. 뉴런이 가지고 있지 않은 변수가 2개 존재한다. $E_1$: 뒤쪽 레이어에서 전파된 로스 $a_{10}$: 앞쪽 레이어 뉴런의 아웃풋 그래서 처음에는 뭐야, 이거 이터레이션 할때 앞뒤 레이어에 다 접근해서 가져와야하나? 예외처리도 두번 해줘야하고 귀찮네… 라고 생각했었는데… 네, 생각해보니까 이 값은 그냥 이 뉴런이 받는 인풋이었습니다. 계속 변수에만 집중하다보니 놓치고있던 쩌는 사실이었다. 모자란 필자의 두뇌에 3초간 묵념한 후 다음 단계로 넘어갔다. 의사코드 작성여기까지 생각이 든 후 간략한 의사코드를 먼저 작성했다.원래는 연습장에 끄적끄적 작성했지만 여기서는 Syntax Highlighting을 위해 TypeScript 문법으로 작성하겠다. 기본적인 클래스는 Network, Layer, Neuron 총 3개로 생각했었고 Forward propagation만 Back propagation만 어떻게 구현할 지 고민을 많이 했었기 때문에 의사코드도 Back propagation에 관련된 코드만 작성했다. NeuronNeuron 클래스에서는 Back propagation이 진행되면 Neuron 객체가 가지고 있는 Weight를 업데이트하고 이때 Forward Propagation 때 미리 계산해놓았던 $\\frac{\\partial a}{\\partial z}$를 사용하여 뉴런의 아웃풋이 전파된 에러에 영향을 끼친 기여도인 $\\frac{\\partial E}{\\partial a}$도 함께 계산해야한다. neuron.ts1234567891011121314151617181920212223class Neuron { private activationFunction미분값: number; private weights: number[]; // 뉴런의 Weight들 private weight미분값들: number[]; // 뉴런의 인풋이 전체 에러에 영향을 끼친 기여도들 public weight들업데이트 (에러미분꼴: number, 학습속도: number) { const 새로운weight들 = this.weights.map((weight, index) => { // wx값이 에러에 영향을 끼친 기여도를 구한다 const loss = 에러미분꼴 * this.activationFunction미분꼴; // weight는 x가 wx값에 영향을 끼친 기여도와 같다. // loss랑 곱해주면 x가 에러에 영향을 끼친 기여도를 알 수 있다. this.weight미분값들[index] = loss * weight; // x는 weight가 wx값에 영향을 끼친 기여도와 같다. // loss랑 곱해주면 w가 에러에 영향을 끼친 기여도를 알 수 있다. return weight - (학습속도 * (loss * this.inputs[index])); }); this.weights = 새로운weight들; }} LayerLayer 클래스는 Back propagation 때 이터레이션을 돌리면서 가지고 있는 뉴런들의 메소드를 호출한다.이때 마지막 레이어라면 MSE의 미분값인 $-(target_i - output_i)$를, 마지막 레이어가 아니라면 다음 레이어에 있는 뉴런들의 weights 배열을 업데이트할 때 미리 계산해놓은 weights미분값들 배열에서 필요한 원소들을 가져와 모두 더한 후 현재 레이어의 Neuron들에게 전달해준다. layer.ts123456789101112131415161718192021class Layer { public 다음레이어: Layer; private 뉴런들: Neuron[]; public 뉴런들업데이트 (전파된에러들: any[], 학습속도: number) { if (다음레이어) { // 마지막 레이어가 아니기 때문에 넘어온 에러 중 // 이 뉴런의 아웃풋과 함께 계산된 weight의 인덱스를 지정해서 사용해야한다. this.뉴런들.forEach((뉴런, index: number ) => { const loss = 전파된에러들.reduce((a: number, b: number[]) => a +_b[index], 0); neuron.updateWeights(loss, 학습속도); }); } else { // 마지막 레이어라면 각 뉴런이 영향을 준 에러에 대한 기여도만 넘겨줘야 한다. this.뉴런들.forEach((뉴런, index: number) => { neuron.updateWeights(loss[index], 학습속도); }); } }} 메소드를 보면 현재 업데이트 할 레이어가 마지막 레이어가 아니라면 에러의 자료형이 number[]에서 number[][]으로 바뀐다. 그 이유는 뉴런 내에서 Weight들이 가지고 있는 index값이 있어야 계산이 편하기 때문이다. 1[[wp_0_0, wp_0_1, wp_0_2], [wp_1_0, wp_1_1, wp_1_2]] 다음 레이어의 에러는 이런 2차원 배열 형태로 리턴된다. 이때 만약 레이어의 0번째 인덱스에 있는 뉴런의 weight들을 업데이트 하려고 한다면, 다음 레이어의 에러 중 이 뉴런과 연결되어있는 에러만 뽑아와야한다. 이 그림에서 보듯 a10a_{10}a​10​​와 함께 계산에 사용된 Weight변수는 다음 레이어에 있는 뉴런들의 weights 배열의 0번 원소로 저장되어 있다. 즉, 업데이트하고자 하는 뉴런의 인덱스가 0이면 다음 레이어에 있는 모든 Neuron.weights[0]에만 영향을 주었다고 할 수 있고 그렇기 때문에 참조해야하는 에러도 Neuron.weightPrimes[0]인 것이다. NetworkNetwork 클래스는 레이어와 뉴런의 생성, forward propagation이나 Back propagation 등 네트워크의 동작을 제어, 통합된 결과나 에러를 관리 정도의 책임을 가진다. network.ts12345678910111213141516171819202122232425class Network { 학습속도: number; 레이어들: Layer[]; 인풋레이어: Layer; 아웃풋레이어: Layer; 전체에러미분값들: number[]; // -(target_i - output_i)들 public backPropagation () { const 뒤집힌레이어들 = [...this.레이어들]reverse(); const 학습속도 = this.학습속도; 뒤집힌레이어들.forEach(레이어 => { let 에러들: any = []; if (레이어.id === this.아웃풋레이어.id) { 에러들 = this.전체에러미분값들; } else { // Back propagation이 진행 중이라 다음 레이어의 계산이 먼저 끝나있다. // private 멤버변수에 접근하면 에러나지만 의사코드니까 그냥 넘어가자 에러들 = 레이어.다음레이어.뉴런들.map(뉴런 => 뉴런.weightPrimes); } 레이어.뉴런들업데이트(에러들, 학습속도); }); }} 마무으리대충 이렇게 작성이 되었다면 이제 메인함수에서 이터레이션을 돌리면 된다. app.ts123456const network = new Network();for (let i = 0; i < 학습횟수; i++) { network.forwardPropagation(); network.backPropagation(); console.log(network.getResults());} 뭐 대충 이런 식으로 하면 될 거 같다. forward propagation은 그냥 이터레이티브하게 쭉쭉 계산만 하면 되므로 어렵지 않았지만 Back propagation은 뭔가 직관적으로 와닿지 않아서 처음에 조금 힘들었다. 소박하게 완성된 모습 그래도 어떻게든 세미나 시간에 맞춰서 네트워크 구현을 했고, d3를 사용해서 소박한 시각화도 하고나니 뿌듯하긴 했다. 다음에 시간나면 레이어마다 Activation Function을 변경할 수 있거나 Loss Function도 변경할 수 있게 개선해보고 싶다. 이상으로 TypeScript를 사용하여 간단한 인공 신경망 개발 삽질기 포스팅을 마친다.전체 소스는 깃허브 레파지토리에서 확인할 수 있고 라이브 데모는 여기에서 확인 가능하다.","link":"/2019/02/26/simple-ann/"},{"title":"AWS와 함께 간단하게 HTTP/2 적용하기","text":"이번 포스팅에서는 AWS(Amazon Web Service) 환경에서 HTTP/2 프로토콜을 적용하는 방법에 대해서 설명하려고 한다. AWS의 Cloud Front와 Application Load Balancer는 자체적으로 HTTP/2 프로토콜을 사용할 수 있는 기능들을 제공해주고 있기 때문에 별도의 작업 없이 간단하게 HTTP/2 프로토콜을 적용할 수 있다. HTTP/2란?HTTP/1은 이미 세상에 나온지 30년이 다 되어가는 프로토콜로, 웹 어플리케이션을 위한 프로토콜이라기 보다는 문서를 위한 프로토콜로 설계되었기 때문에 모던 웹 어플리케이션과 같이 무거운 페이로드, 빈번한 통신 등의 환경에서는 여러 가지 비효율적인 점이 많다. 그래서 HTTP/2는 현대적인 통신을 지원하기 위해 다음과 같은 목표를 가지고 고안되었다. 전체 요청, 응답 다중화를 통한 지연 시간 단축 비대한 HTTP 헤더 필드의 효율적인 압축을 통해 프로토콜 오버헤드를 최소화 요청 우선 순위 지정 서버 푸시 지원 HTTP/2는 기존 HTTP/1 프로토콜을 사용하고 있는 어플리케이션을 수정하지 않고도 모든 핵심 개념(메소드, 상태 코드, URI 및 헤더 필드)을 공유하도록 설계되었다. 대신 HTTP/2는 클라이언트와 서버 간 데이터 프레임과 전송 방식을 수정하는 방식으로 통신 효율을 높였다. 또한 클라이언트와 서버 간 통신에서 가장 효과가 큰 것은 응답 다중화가 정식으로 지원된다는 것이다. 이 말은 하나의 연결만 으로 여러 리소스를 주고 받을 수 있다는 뜻이다. [출처] csstrick.com HTTP/1에서는 프로토콜 차원의 응답 다중화가 지원되지않았고 응답의 병렬 처리는 브라우저가 책임을 가졌다. 그렇기 때문에 브라우저의 정책에 따라 요청의 병렬처리가 가능한 개수가 달랐다. Internet Explorer: 출처 별 10개~11개 Chrome: 출처 별 6개 Firefox: 출처 별 6개 Opera: 출처 별 6개 그렇기 때문에 프론트엔드 개발자들은 어플리케이션 초기화 시 최대한 리소스 요청을 줄이기 위해 모든 JavaScript와 CSS 파일을 하나의 번들로 묶어 index.js나 style.css와 같은 파일로 만들고 Minify나 Uglify 등의 기법을 사용하여 용량을 최대한 줄이기도 하고, 여러 개의 이미지를 요청하지 않기 위해 하나의 커다란 이미지를 다운로드받아 마스킹해서 사용하는 스프라이트 방식과 같은 방식을 사용했다. 하지만 HTTP/2는 출처 별로 최대 128개의 병렬 요청을 처리할 수 있으므로, 이제 우리는 하나의 큰 파일이 아닌 작은 여러 개의 파일로 나눠서 동시에 요청하고 받아올 수 있는 등의 HTTP/1.1에서는 하지 못했던 성능 개선의 여지를 만들 수 있게되었다. AWS Cloud Front에서 HTTP/2 사용하기AWS는 2016년 9월부터 Cloud Front에서 HTTP/2를 지원해주기 시작했다. 간단한 세팅 만으로 HTTP/1.1에서 HTTP/2로 변경할 수 있다. 또한 HTTP/2를 지원하지않는 하위 버전의 브라우저에서 요청을 받는다면 HTTP/1.1으로 프로토콜을 변경하여 응답하는 기능 또한 가지고 있다. AWS CloudFront HTTP/2 세팅 메뉴얼에서 HTTP/2를 세팅할 수 있는 방법을 자세히 설명해주고 있기 때문에 프로토콜을 변경하는 과정은 전혀 어려움이 없었다. CloudFront Distributions 변경하기먼저 Cloud Front 대시보드로 이동하면 현재 등록되어있는 배포판들의 목록이 보인다. 이 중 HTTP/2 프로토콜을 적용하고 싶은 배포판을 선택한 후 상단의 Distribution Settings을 선택한다. Distribution Settings을 클릭해서 배포판 설정 화면으로 들어가면 현재 배포판에 대한 여러 가지 정보가 있는 화면이 나온다. 이 정보들 중 이 배포판이 지원하고 있는 프로토콜에 대한 정보도 함께 담겨있다. 이제 상단의 Edit 버튼을 눌러 배포판의 설정을 변경하도록 하자. 밑으로 쭉 내리다보면 Supported HTTP Versions라는 항목이 있다. 이미 저 스샷이 모든 걸 설명해주고 있기 때문에 마무리는 스킵하겠다. Elastic Beanstalk에서 HTTP/2 사용하기Elastic Beanstalk은 Web 서버나 Worker와 같이 친숙한 웹 어플리케이션이나 서비스를 간편하게 배포하거나 컨트롤할 수 있는 서비스이다. Elastic Beanstalk에서 환경을 생성할 때 사용할 언어나 서버 엔진등을 설정해놓으면 해당 설정을 사용하여 다른 환경으로 복사할 수도 있고 Auto Scaling이나 Load Balancing과 같은 귀찮은 설정이 필요한 작업들도 간단한 몇개의 설정만 건드려주면 알아서 다 해주기 때문에 꿀이 따로 없다. 또한 프로젝트의 루트에 .ebextensions 디렉토리를 생성하고 내부에 쉘 스크립트 파일을 넣어놓으면 파일 정렬 순서에 따라서 배포할 때마다 해당 스크립트들을 실행시킬 수도 있어서 굉장히 유연하다.(node-sass가 말썽부려서 rebuild 해야할 때 아주 유용하다) 환경을 생성할 때 Classic Load Balancer와 Application Load Balancer 중 하나의 로드 밸런서를 선택할 수 있는데 Classic Load Balancer는 기존의 ELB를 의미한다. Elastic Beanstalk에서 HTTP/2를 사용하고 싶다면 Application Load Balancer를 선택하도록 하자. 물론 ELB를 고르고 직접 세팅하는 방법도 있지만 필자는 굳이 어려운 길을 선택하지 않았다. Application Load Balancer란?AWS는 지난 2016년에 L7(Application) 계층에서 작동하는 Application Load Balancer(ALB)를 공개하였다. 기존에 사용되던 로드밸런서인 Elastic Load Balancer(ELB)는 L4(Network) 계층에서 동작하기 때문에 HTTP나 HTTPS와 같은 Application Layer에서 사용되는 프로토콜을 인지하지도 못하고 이에 따라서 유연하게 처리하지도 못했지만 ALB는 Application 계층에서 작동하기 때문에 직접 HTTP 헤더를 까보고 이에 따른 유연한 부하 분산이 가능한 것이 장점이다.예를 들면 동일한 호스트로 요청을 보내더라도 /a 경로로 요청을 보내면 a 서버로 보내고 /b 경로로 요청을 보내면 b 서버로 보내는 등의 유연한 라우팅이 가능하게 된다는 것이다. 하지만 무엇보다 좋은 점은 위에서 설명했듯이 HTTP/2 프로토콜과 WebSocket을 자체적으로 지원한다는 것이다. 만약 기존에 ELB, 즉 Classic Load Balancer를 사용하고 있던 환경에서 HTTP/2 프로토콜을 사용하고 싶다면 직접 세팅하거나 ALB로 마이그레이션 해야한다.하지만 로드 밸런서를 마이그레이션해도 기존 ELB에 연결되어있던 인스턴스들과 자동으로 연결해주지는 않기 때문에 Elastic Beanstalk을 사용하고 있다면 그냥 환경을 다시 만드는 게 정신건강에 이롭다.(처음에 쉽게 가보려다가 안되서 실망한 1인) 설정하기먼저 Elastic Beanstalk 환경에 접속하여 새로운 환경을 생성하자. 이후 Web server environment를 선택하면 환경 이름이나 사용할 언어등 간단한 세팅을 할 수 있는 화면으로 이동한다. Elastic Beanstalk에서는 다양한 언어를 제공해주고 있으니 입맛대로 골라담아보자. 이렇게 기본적인 세팅을 하고서 하단의 Configure more options를 클릭하면 좀 더 디테일한 설정을 할 수 있는 화면으로 이동할 수 있다. 만약 손이 미끄러져서 Create environment를 클릭하면 환경이 생성되는 동안 10분 정도는 그냥 날리게 되므로 눈 크게 뜨고 클릭하도록 하자. 제대로 클릭했다면 디테일 설정화면으로 이동하게 되는데 상단의 Configuration presets를 확인해보면 아마 기본 값으로 Low cost가 선택되어 있을 것이다. 이 옵션에서는 로드 밸런서를 사용할 수 없으므로 우리는 Custom configuration을 선택해야한다. Custom configuration 옵션을 선택하면 하단의 Load balancer 카드에 Modify 버튼이 활성화 되었을 것이다. 해당 버튼을 클릭하면 이제 드디어 로드 밸런서를 선택할 수 있는 화면이 나타난다. 3개 다 맛있어보이지만 한번에 하나만 먹을 수 있으므로 욕심 부리지말고 Application Load Balancer를 선택하자. 그리고 밑으로 스크롤을 조금만 내려보면 로드 밸런서 설정 메뉴들이 보인다. 우리는 이 중 Listener를 생성해주어야한다. ALB의 리스너는 구성한 프로토콜이나 포트를 사용한 요청을 확인하는 프로세스라고 보면 된다. 기본 설정에는 HTTP 프로토콜을 사용하여 80포트로 들어오는 요청에 대한 리스너만 있기 때문에 우리는 HTTPS 프로토콜을 사용하여 443포트로 들어오는 요청에 대한 리스너를 만들어주면 된다. 리스너를 생성했다면 하단의 Save 버튼을 눌러 로드 밸런서 설정을 저장하고 나머지 설정도 입맛대로 설정한 후 Create environment를 클릭하면 드디어 환경이 올라가기 시작한다. 참고로 좀 오래 걸린다. 서버에 별도 설정을 안했는데?따로 안해도 된다. ALB에 HTTPS 리스너를 가지고 있다면 이 리스너가 알아서 해준다. 만약 브라우저가 HTTP/2를 지원하는 브라우저라면 리스너도 HTTP/2로 응답할 것이고 만약 HTTP/1.1만 지원하는 브라우저인터넷익스플로러라면 리스너도 HTTP/1.1 프로토콜로 응답할 것이다. 그리고 HTTPS 리스너가 HTTP/2 요청을 받더라도 로드 밸런서에 연결된 인스턴스들과는 각각의 HTTP/1.1 프로토콜로 통신하기 때문에 서버에서는 그냥 평소대로 HTTP/1.1에 대한 처리만 하면 된다. 자세한 내용은 Elastic Load Balancing 사용 설명서의 HTTP Connections을 읽어보도록 하자. HTTP/2, 잘 적용 됐니?환경을 생성했다면 이제 끝이다. 가장 간단하게 테스트해볼 수 있는 방법은 역시 curl을 사용하는 것이다. 여러분이 생성한 환경에 요청을 해봐도 되고 아니면 그냥 아무데나 찔러보자. 생각보다 많은 서비스들이 HTTP/2를 사용하고 있다. 12345678$ curl --http2 -I https://www.naver.com/HTTP/2 200server: NWSdate: Thu, 13 Jun 2019 14:59:09 GMTcontent-type: text/html; charset=UTF-8cache-control: no-cache, no-store, must-revalidatepragma: no-cache 크롬 브라우저를 사용한다면 HTTP/2 and SPDY indicator 크롬익스텐션을 설치하는 방법도 있다. 이런 귀여운 아이콘이 HTTP/2나 SPDY 프로토콜 사용 여부를 알려줄 것이다. 근데 SPDY는 아직까지 구글말고는 쓰는 곳을 본 적이 없는듯…? 또는 크롬 브라우저의 개발자 도구 Network 탭에서 테이블 헤드에 마우스 우클릭을 하면 Protocol 컬럼을 활성화 시킬 수도 있다. 그러면 네트워크 탭에서 HTTP/2 프로토콜로 통신하여 받아온 리소스는 h2라고 표시가 된다. Waterfall로 비교해보자왼쪽이 HTTP/1.1, 오른쪽이 HTTP/2이다. HTTP/2 쪽의 붉은 라인은 이미지 요청 에러인데, 스테이징 환경에서 리소스 출처 문제로 인해 발생하는 403에러이기 때문에 무시해도된다. .inline { display: inline !important; vertical-align: top; } HTTP/2 쪽의 Waterfall은 상당히 많은 수의 요청이 동시에 처리되고 있는 것을 확인할 수 있다. HTTP/2의 이런 특징을 이용하면 기존에 하나로 번들링하고 있던 JavaScript 파일을 여러 개로 Chunking하여 파일 용량을 줄이거나, 기존에 스프라이트로 사용하던 이미지도 개별 요청을 통해 동시에 받아옴으로써 로딩 속도를 조금 더 단축시킬 수도 있다. 이상으로 AWS와 함께 간단하게 HTTP/2 적용하기 포스팅을 마친다.","link":"/2019/06/13/http2-with-aws/"},{"title":"JavaScript 배열(Array)의 발전과 성능에 대해서 자세히 알아보기","text":"이 포스팅은 2017년 9월 2일에 Paul Shan이 작성한 Diving deep into JavaScript array - evolution & performance를 번역한 글입니다. 포스팅을 시작하기 전에 이 포스팅은 JavaScript 배열의 구문에 관한 것을 알려주거나 예제를 보여주는 등의 기본적인 내용은 아니라고 먼저 얘기해두고 싶다. 이 포스팅에서는 메모리 표현, 최적화, 구문에 따라 달라지는 동작의 차이, 성능 및 최근의 JavaScript 배열이 어떻게 발전했는지에 관해서만 설명할 것이다. 필자가 JavaScript를 처음 시작했을 때 필자는 이미 C, C++, C# 등의 언어에 익숙한 상태였다. 그래서 다른 C/C++ 개발자들처럼 JavaScript와의 첫 만남이 그리 좋지는 못했다. 그 중 필자가 JavaScript를 좋아하지 않았던 가장 큰 이유는, 바로 배열이다. JavaScript의 배열은 Hash Map이나 Dictionary로 구현되었고 연속적이지 않기 때문에 필자는 이 언어가 배열을 제대로 구현할수도 없는 B급 언어라고 생각했다. 그러나 그 이후 JavaScript에 대한 필자의 이해도는 많이 달라졌다. (역주) 기존 언어에서 구현했던 배열은 생성 시에 특정 범위의 메모리를 할당하고 연속적으로 데이터를 저장했지만 JavaScript는 메모리를 미리 할당해놓지 않고 동적 할당하므로 연속적으로 원소가 저장되지 않는다. 리스트와 동일한 방식. JavaScript의 배열이 실제로는 배열이 아닌 이유자바스크립트에 관한 설명들을 시작하기 전에 배열이 무엇인지부터 설명을 먼저 해야할 것 같다.배열은 연속적인 메모리 로케이션들의 묶음을 사용하여 값을 저장하는 데 사용된다. 여기서 중요한 포인트는 연속성(continuous)과 인접성(contiguous)이라는 단어이다. 위 그림은 배열의 메모리 상태의 예시를 표현한 것이다. 이 배열은 4 bit로 이루어진 4개의 블록을 가지고 있고 총 16 bit의 메모리 블록을 사용하고 있다. 이제 필자가 tinyInt arr[4];를 선언했고 1201부터 시작해서 이 메모리 블록들을 포착했다고 가정해보자. 이제 필자가 어떤 포인트로부터 a[2]를 읽으려고 한다면 a[2]의 메모리 주소를 찾기 위한 간단한 수학 계산이 이루어진다. 1201 + (2 x 4)와 같은 식으로 1209의 주소를 바로 참조할 수 있다. 역주: 메모리 시작 주소 + (찾고자 하는 인덱스 x 한 블록에 할당된 메모리 블록 개수)로 계산한 것이다. 배열은 이렇게 원하는 원소에 바로 접근할 수 있다. JavaScript에서의 배열은 Hash Map이다. 이것은 다양한 자료 구조를 사용해서 구현될 수 있고, 그 중 하나가 바로 Linked List이다. 만약 JavaScript 내에서 우리가 var arr = new Array(4);로 배열을 선언하면 이 배열은 상단의 그림과 같은 구조를 생성한다. 따라서 만약 우리가 a[2]를 읽고 싶다면 무조건 1201부터 탐색해나가면서 a[2]를 찾아나가야 한다는 것이다. 이것이 바로 JavaScript의 배열과 진짜 배열이 다른 점이다. 분명히 JavaScript의 배열을 탐색하는 것은 원래의 Linked List 탐색보다는 계산이 적다. 그러나 배열의 길이가 길어질수록 인생이 고달파지는 건 똑같다. JavaSciprt 배열의 발전예전에는 친구가 컴퓨터에 256MB 짜리 램을 사용한다고 하면 부러움을 느끼던 시절도 있었지만 요즘엔 보통 8GB 정도의 램을 사용한다. 이와 비슷하게 JavaScript 또한 많은 발전을 이루었다. V8, SpiderMonkey, TC39, 증가하고 있는 웹 사용자들의 피나는 노력으로 인해 JavaScript는 전 세계의 필수 요소가 되었다. 이렇게 거대한 유저 베이스를 가지고 있다면 분명히 성능 향상 또한 필요하다. 최근의 JavaScript 엔진은 모든 요소가 동일한 타입을 가지고 있는 배열인 경우 연속적으로 메모리를 할당한다. 훌륭한 프로그래머는 항상 배열을 동일한 타입으로 사용하며 JIT(Just in Time) 컴파일러는 이런 배열에 대해서 C 컴파일러와 같은 배열 계산을 수행한다.그러나 이런 동일한 타입 배열에 다른 타입의 원소를 삽입하려고 할 때 JIT는 전체 배열의 구조를 해제하고 다시 예전의 배열처럼 비연속적인 메모리를 할당한다. 즉, 만약 우리가 코드를 제대로 작성한다면 JavaScript의 Array 객체는 실제 배열처럼 작동한다는 것이다. 이는 모던 JS 개발자들에게는 정말 좋은 일이다. 이에 더해서 배열은 ES2015 또는 ES6를 통해서 더욱 발전했다. TC39 위원회는 JavaScript에 타이핑된 배열을 추가하기로 결정했고 그래서 우리는 ArrayBuffer를 사용할 수 있게 되었다.ArrayBuffer는 인접한 메모리 블록을 제공하고 우리가 그것을 마음대로 다룰 수 있게 해준다. 그러나 메모리를 직접 다루는 것은 매우 Low Level이고 또 너무 복잡하기 때문에 우리는 View라는 것을 통해서 ArrayBuffer를 다루게 된다. 이미 몇가지 View를 사용할 수 있고 나중에는 더 추가될 예정이다. 123var buffer = new ArrayBuffer(8);var view = new Int32Array(buffer);view[0] = 100; 만약 당신이 Int32Array와 같은 Typed Array에 대해서 더 알고 싶다면 MDN Documentation를 참고하기 바란다. 타이핑된 배열은 굉장히 효율적이다. 타이핑된 배열은 WebGL을 사용하는 사람들이 일반 배열로는 바이너리 데이터를 효과적으로 처리할 수 없는 엄청난 성능 문제에 직면했기 때문에 요청해서 도입된 객체이다.(역주: ThreeJS도 내부적으로는 전부 Typed Array를 사용 중이고, 성능 차이 또한 몸으로 느껴질 정도로 확연하다.) 또한 우리는 SharedArrayBuffer를 사용하여 여러 개의 Web Worker간 메모리를 공유하여 성능을 끌어올릴 수도 있다. 놀랍지 않은가? JavaScript의 배열은 간단한 Hash Map에서 시작해서 이제는 SharedArrayBuffer까지 다루고 있다. 일반 배열 vs 타이핑된 배열 - 성능 비교우리는 JavaScript 배열의 발전에 대해서 이야기 했다. 이제 최근의 배열이 얼마나 좋은지 확인해보자. 필자는 Mac과 Node.js 8.4.0 환경에서 몇 개의 작은 테스트를 해보았다. 일반 배열 – 삽입1234567var LIMIT = 10000000;var arr = new Array(LIMIT);console.time(\"Array insertion time\");for (var i = 0; i < LIMIT; i++) { arr[i] = i;}console.timeEnd(\"Array insertion time\"); 수행 시간: 55ms 타이핑된 배열 – 삽입12345678var LIMIT = 10000000;var buffer = new ArrayBuffer(LIMIT * 4);var arr = new Int32Array(buffer);console.time(\"ArrayBuffer insertion time\");for (var i = 0; i < LIMIT; i++) { arr[i] = i;}console.timeEnd(\"ArrayBuffer insertion time\"); 수행 시간: 52ms 앗…? 예전의 전통적인 배열과 최근 배열의 성능이 비슷한데요…? Nope. 필자는 요즘의 컴파일러는 똑똑하기때문에 같은 타입을 가진 배열은 내부적으로 연속적인 메모리를 가진 배열로 변환한다는 것을 설명했다. 이게 바로 첫번째 예시에서 발생한 일이다. 필자는 new Array(LIMIT)을 사용했지만 내부적으로는 연속적인 메모리 할당을 가진 현대적인 배열을 유지하고 있던 것이다. 이제 첫번째 예시를 수정하여 동일한 자료형을 가지고 있지 않은 배열로 만들고 성능 차이가 있는지 살펴보도록 하자. 일반 배열 – 삽입 (동일하지 않은 자료형)12345678var LIMIT = 10000000;var arr = new Array(LIMIT);arr.push({a: 22});console.time(\"Array insertion time\");for (var i = 0; i < LIMIT; i++) { arr[i] = i;}console.timeEnd(\"Array insertion time\"); 수행 시간: 1207ms 여기서 필자는 3번 라인에 새로운 표현을 추가했을 뿐 나머지는 이전과 전부 동일하지만 성능은 차이가 나기 시작했다. 무려 22배 느려진 것을 확인할 수 있다. 일반 배열 - 읽기12345678910111213var LIMIT = 10000000;var arr = new Array(LIMIT);arr.push({a: 22});for (var i = 0; i < LIMIT; i++) { arr[i] = i;}var p;console.time(\"Array read time\");for (var i = 0; i < LIMIT; i++) { //arr[i] = i; p = arr[i];}console.timeEnd(\"Array read time\"); 수행 시간: 196ms Typed Array - read123456789101112var LIMIT = 10000000;var buffer = new ArrayBuffer(LIMIT * 4);var arr = new Int32Array(buffer);console.time(\"ArrayBuffer insertion time\");for (var i = 0; i < LIMIT; i++) { arr[i] = i;}console.time(\"ArrayBuffer read time\");for (var i = 0; i < LIMIT; i++) { var p = arr[i];}console.timeEnd(\"ArrayBuffer read time\"); 수행 시간: 27ms 결론JavaScript에 타이핑된 배열이 추가된 것은 위대한 첫 발걸음이다. Int8Array, Uint8Array, Uint8ClampedArray, Int16Array, Uint16Array, Int32Array, Uint32Array, Float32Array, Float64Array 등은 네이티브 바이트 순서로 이루어진 뷰를 제공하고, 또 여러분이 직접 DataView를 사용하여 커스텀 뷰를 만들 수도 있다. 앞으로 ArrayBuffer를 사용하기 위해 더 많은 DataView 라이브러리들이 활성화되기를 바란다. JavaScript의 배열이 이렇게 개선된 것은 좋은 일이다. 이제 JavaScript의 배열은 빠르고 효율적이며 강력하고 똑똑하게 메모리를 할당할 수 있게 된 것이다.","link":"/2019/06/15/diving-into-js-array/"},{"title":"좋은 프로그래머란 무엇일까?","text":"2016년 처음 회사에서 개발자로 일을 시작해서 2019년 이 글을 쓰고 있는 지금까지 늘 의문이었던 좋은 프로그래머, 또는 좋은 개발자는 무엇을 뜻하는 것인가에 대해서 글을 한번 끄적여볼까 한다. 사실 대학생 때 처음 프로그래밍이라는 것에 빠져들었을 때는 말 그대로 기술만능주의 였던 것 같다. 사용하는 언어에 잘 어룰리는 패턴을 연구하고 효율적인 알고리즘이 몸에 배어있고 변경사항에 유연하고 확장성이 뛰어난 설계를 하는 그런 것들에 한창 목마를 때였다. 왜 이렇게 생각하게 됐는지는 간단하다. 기술적으로 뛰어난 사람?필자는 2014년 말 대학생때 친구들과 함께 루비콘이라는 팀을 꾸려서 나름 원대한(?) 꿈의 프로젝트를 시작했었는데 이게 지금 생각해보면 어지간히 정신나간 프로젝트였다. 그때 팀 구성이 대략 이랬다. 역할 수준 프론트엔드(나) 학교에서 배운 C와 Java가 아는 전부. 마지막으로 해본 JavaScript는 초4 때. 백엔드 웹 디자인 기능사 자격증 보유. 나름 이 팀에서 유일한 웹 경험자. 백엔드 롤을 맡았지만 서버 따윈 한번도 안해봄. 백엔드2 전자전기전공. 이 분도 DB나 웹에 대한 지식 전무. 바빠서 잘 오지도 못함. UI/UX 디자이너 군인. 아직 전역도 안함. 가끔 휴가나와서 작업함. 기획자 캐나다 교포. 음악 전공. IT 경험 전무함. 팀 구성만 봐도 오합지졸이라는 게 딱 보인다. 다시 써놓고 보니까 진짜 정신나갔던 것 같다. 무슨 자신감이었는지…심지어 전부 웹을 한번도 안해봤다. 무식하면 용감하다는 게 딱 이 꼴인데, 문제는 이 사람들이 모여서 만드려고 했던게 웹 상에서 작동하는 3D 모델 에디터 였다는 거다. 물론 이것도 계획의 일부일 뿐 그 뒤에는 더 많은 삽질이 있었다.(MVP의 중요성을 대학 때 깨닿게 해준 고마운 프로젝트다.) 일단 필자는 프론트엔드를 맡았으니 일단 맨땅에 헤딩하듯 JavaScript와 jQuery로 뭐 어찌어찌 만들어나갔다. 그러다가 3D에서 꽤나 삽질을 많이 했는데, 이때 ThreeJS를 처음 사용해보았다. 사실 정확히 말하자면 Graphics 자체를 처음 해본거다. 그 덕분에 수학을 엄청 공부했고 3달 정도 걸려서 obj파일을 웹 상에 로드한 후 Texture나 Material 등을 변경하거나 빛을 배치해서 공간의 느낌을 바꿀 수 있는 등의 작은 기능을 가진 프로토타입을 만들었었다. 추억의 동국대학교 학림관 옆 까페 한때 이 대머리 아저씨를 우리 엄마보다 더 많이 봤었다. 이때 이걸 완성하고 나서 필자의 느낌은 대략 이랬다. 아 신이시여… 전 제가 만드려고 했던 게 이런건지 상상도 못했습니다… 오일러는 뭐고 쿼터니온은 뭐죠…? 그래도 어찌어찌 고생 끝에 만들었으니 뭔가 자부심도 생겼고 결과물에 대한 애정도 생기더라. 그때 필자의 머릿 속에는 좋은 개발자는 기술적으로 어려운 걸 뚝딱뚝딱 만들 수 있는 사람이라는 생각이 자리잡았던 것 같다. 첫 직장 입사 후 비즈니스에 대해서 달라진 시각그러다가 2016년에 졸업을 하고 한 2주 뒤에 바로 운좋게 한 스타트업으로 취업하게 되었는데, 이때 필자는 개발자가 아닌 다른 직군과 처음 제대로된 협업을 했었다.내가 만든 프로그램에서 장애가 발생하면 고객들이 CS팀으로 문의를 하고 CS팀은 개발팀에게 버그를 리포팅하거나, 혹은 디자이너와 기획자와 아이디어를 공유하면서 프로덕트를 만드는 그런 경험 말이다. 대학생 때는 아무래도 친구들이랑 반 취미로 만드는 프로젝트였기 때문에 이걸 어떻게 만들지? 같은 단순한 고민만 했다면 회사에서는 전혀 달랐다. 여기서는 이런 고민에 더해 다른 고민들도 함께 해야했다. 이 기능을 배포하면 파생될 리스크 혹은 이익이 발생할까? 개발하는 과정에서 시간을 줄이기 위해 외부 솔루션을 쓸까 아니면 장기적으로 보고 직접 개발을 할까? A 기능을 개발하는 데 한달 정도가 걸리는 데 이 시간에 다른 기능을 두개 더 만들 수 있다면 어떤 선택을 해야할까? 이 기능에 관련된 VOC가 많이 들어왔는데 어떤 식으로 해결해야 유저들이 만족할까? 이런 고민들을 그때 당시 디자이너, 개발자, 기획자 분들끼리 자유롭게 의견을 주고 받았는데 이때 깨달은 것이 결국 회사에 가장 중요한 것은 고객과 돈이라는 것이다. 사실 이건 지극히 당연한 말이긴 하다. 그래도 이 생각이 들게 된 것이 지금까지 필자의 개발자로서 가치관에 큰 영향을 줬던 것 같다. 뭐 사실 회사에서 중요한 건 직원의 업무 만족도라던가 문화라던가 다른 것들도 많지만 제일 기본은 고객과 돈인 것 같다.결국 내가 아무리 프로그램을 잘 만들어도 아무도 써주지 않으면 의미가 없는 것이고 아무리 회사의 문화와 복지가 좋더라도 돈이 없으면 결국 그 회사는 망하기 때문이다. 그래서 이때 처음으로 내가 프로그래머라도 기술적인 것만 생각하는 건 좁은 시각일수도 있겠구나.결국 내가 만드는 건 사람이 쓰는 상품이고 사람을 위해 만들어야 하는구나. 라는 생각들이 들기 시작했고 이때부터 본격적으로 “좋은 프로그래머가 되려면 비즈니스나 유저에 대한 생각도 많이 해야하는구나” 라는 생각을 많이 했던 것 같다. 공적인 업무에서 감정은 불필요하다?필자는 지금도 스타트업에서 계속 일을 하고 있다. 그 이유는 규모가 큰 기업에서는 할 수 없는 다양한 일을 내가 다 맡아서 해볼 수 있다는 점도 있고 내 의견을 자유롭게 이야기하고 건강한 토론을 통해 합리적인 의견을 도출해나가는 과정이 재밌는 것도 있다. 그러나 마냥 좋을 수만은 없는 법. 아무래도 사람 모이는 곳이다 보니 서로 간의 감정이 상하거나 팀원들과 회사 간의 갈등이 생기거나 하는 등 다양한 이벤트가 터진다. 특히 분위기가 자유 그 자체다 보니 팀원들도 약간 친구처럼 지내고 있고, 그러다보면 간혹 실수도 하기 마련이다.(물론 아무리 수평적인 조직이고 자유롭다 해도 기본적인 예의는 당연히 다들 지킨다) 누가 등록한건지 모르겠는 정체불명의 짤. @everyone 멘션을 달면 자동으로 짤이 붙는다… 특히 이 중에서도 감정에 관해서 요즘 생각을 많이 하게 된다. 필자는 원래 공과 사는 구분하자라던가 아무리 힘들어도 감정적으로 대응한다면 그건 프로가 아니다라는 말들에 공감을 많이 하는 편이었다. 사실 지금도 공적인 업무에 감정이 껴서 좋을 게 하나도 없다고 생각한다. 그러나 여기에 더해서 요즘에 드는 생각은 이렇다. 아무리 감정을 배제하려고 해도 사람인 이상 그건 불가능한 거 아니야? 뭐, 아무리 똑똑한 사람이든 시니어든 주니어든 결국 사람인데 당연히 힘든 것도 있고 기쁜 것도 있고 할 것 아닌가? 필자는 시니어 프로그래머 중 한분이 이런 걸 징징대는 것 정도로 치부하고 프로페셔널하지 못하다고 하는 경우도 보았다. 물론 저런 감정을 여기저기 발산하고 다닌다면 그 사람은 이상한 사람인게 맞지만 어쩔 수 없이 새어나오는 감정을 무조건 참으라고 하는 것도 좀 이상한 것 같다. 감정은 두뇌의 신호를 받아 호르몬의 분비를 통해 발생한다고 한다. 이건 의지로 통제할 수 있는 것이 아니다. 필자는 개인적으로 회사에서 직접적으로 나와 함께 일하는 PO, 디자이너, 개발자들을 단순히 직장 동료로만 생각하지는 않는다. 단순한 직장 동료보다는 직장 동료와 친구 중간의 어딘가 정도가 맞는 것 같다. 뭐 아 다르고 어 다른 표현 같지만, 어쨌든 일반적인 공적인 관계로만 생각하지는 않는다. 전부 같은 목표를 가지고 열심히 함께 달리고 있는 사람들이고 나이도 다 비슷하고 가치관도 생각보다 크게 다르지 않다. 그래서 가끔은 다른 개발자분들에게 그냥 형님~ 할때도 있다. 실제로 내가 막내니까 다들 형이 맞기도 하고. 뭐 우리 팀은 조금 그런 분위기다. 반말 존댓말 섞어쓰는 조금 친구같은 그런…? 근데 이런 팀원이 뭔가 안좋은 일이 생겨서 감정적으로 어려움을 겪고 있고, 또 그게 티가 난다면 내가 도와줄 수 있는 것 아닌가? 라는 생각이 든다. 그런 감정을 회사까지 끌고 왔다고 뭐라하는 게 아니라. 도와준다고 해도 별로 어려운 것도 아니다. 그냥 아무렇지 않게 “워~ 오늘 얼굴 왜 이래? 커피 한잔 하실?” 하면서 카페가서 같이 이야기도 좀 들어주고 20~30분 노닥거리다 오는 게 전부다. 그렇게만 해줘도 고민이 있던 팀원은 어느 정도 생각도 정리되고 다른 사람한테 이야기하면서 감정도 정리되어 다시 일에 더 집중할 수 있지 않을까? 사람은 감정적인 동물이다. 감정을 배제하고는 아무 것도 이해할 수 없다는 생각이 든다. 이런 점을 간과하고서는 건강한 커뮤니케이션은 없을 것 같다. 지금까지의 결론 결국 요즘에 드는 생각은 직장을 그만 두더라도 다른 사람들에게 “아 그 사람은 좋은 사람이었지. 같이 일할 때 좋았어” 라고 기억될 수 있으려면 프로그래머라고 해도 기술적인 것만 추구해서는 좋은 프로그래머가 될 수 없다 라는 방향으로 많이 기울고 있다. 결국 프로그래머라고 해도 누군가와는 반드시 협업이나 커뮤니케이션을 해야하는 직장인 아닌가? 심지어 프리로 뛰더라도 클라이언트와의 커뮤니케이션은 필수다. 단순히 코딩만 잘하는 프로그래머는 다른 사람들에게 그다지 “같이 일하기 좋은 프로그래머”로 남을 수는 없을 것 같다. 같이 일하고 싶은 사람은 일만 잘하는 사람은 아니다. 사람들과 회의를 할때도 나와 다른 의견이 있거나 또는 그 사람이 나보다 경력이 낮거나 심지어 그 사람이 초등학생이라고 해도 그 사람의 의견을 온전히 존중해줄 수 있는 태도. 다른 사람이 감정적으로 힘들어 할때 프로페셔널하지 못하다고 비난하는 것보다는 그 고민을 들어주고 얼른 원래 페이스로 돌아올 수 있게 도와주는 태도. 또는 그 사람의 감정이 이해는 안되더라도 그 사람이 그런 감정을 느꼈다는 것을 최소한 인지할 수 있는 태도. 이런 태도들을 기본적으로 갖추고 있어야 좋은 프로그래머를 넘어서서 좋은 동료가 될 수 있는 것 아닐까? 물론 필자도 낯을 가리는 성격이라 따뜻한 성격은 아니긴 하다.(어릴 때 부터 첫인상 안좋다는 소리 많이 들음) 그러나 개인의 성격을 떠나 적어도 함께 협업하는 타인에 대한 존중과 배려는 갖춰야 한다고 생각한다. 필자는 개인적으로 기술적인 하드 스킬은 누구나 열심히 하면 그 속도에 차이가 있을지언정 늘긴 는다고 생각하는 편이고, 또 프로그래머라면 굳이 누가 말하지 않아도 자신의 기술 스펙에 대한 노력은 하고 있다고 생각한다. 하지만 커뮤니케이션이나 타인에 대한 배려같은 소프트 스킬은 평소에 신경쓰지 않으면 자신도 모르게 소홀해지기 쉽기 때문에 계속 고민하고 신경쓰고 지속적인 피드백을 받으려는 태도가 중요하다고 생각한다. 사실 최근에도 다른 팀 분한테 말투가 너무 딱딱하고 무섭다는 피드백을 받아서 조금 의기소침해지기도 했지만 그래도 그 이후로 나름 말투에 대해 신경쓰면서 말하고 있는 중이다. 잘 되고 있는 지는 잘 모르겠지만…ㅎㅎ 일단 지금까지 4년 동안 프로그래머로 회사에서 일을 하면서 좋은 프로그래머는 무엇인가?에 대해서 고민했던 과정과 나름 내려본 결론은 이렇다. 아직 필자는 일할 날도 살 날도 많이 남았기 때문에 지금 생각한 결론이 다시 달라질 수도 있겠지만 그래도 “타인을 배려하는 태도”가 중요하다는 사실은 필자가 프로그래머가 아니라 다른 일을 하더라도 적용되는 사실인 것 같다.","link":"/2019/06/06/what-is-good-programmer/"},{"title":"JIRA 프로젝트 이슈 커스터마이징하기","text":"이번 포스팅에서는 Atlassian의 대표 제품 중 하나인 Jira에 대해서 포스팅 하려고 한다. 요즘 많은 IT회사들에서 애자일 개발 방법론을 사용하여 소프트웨어 개발을 진행하고 있다. Jira는 애자일 방법론에서 사용하는 다양한 방법들을 좀 더 쉽고 편하게 사용할 수 있게 도와준다. 사실 애자일 프로세스는 일종의 방법론일 뿐 어떤 원칙이 아니기 때문에 팀마다 그 구성이나 개발 과정이 조금씩 달라지게 될 수 밖에 없고, 또 회고를 통해 이런 저런 방법들을 계속 시도해보기 때문에 어떠한 규칙으로 똑부러지게 정해지는 게 아니다. Jira는 이렇게 매번 달라지는 규칙들에 대응하기 위해서 다양하게 프로젝트를 커스터마이징 할 수 있는 기능을 제공한다. 그러나 기능이 너무 많고 용어가 복잡해서 처음 접하는 사람은 이걸 도대체 어떻게 써야할지 감이 안오는 것도 사실이다. 아마 많은 분들이 그냥 기본적으로 제공해주는 칸반이나 스프린트 기능 정도만 사용하고 있지 않을까 싶다. 그래서 오늘은 Jira를 사용할 때 제일 기본적인 단위를 이루는 Issue를 커스터마이징하는 방법을 설명해볼까 한다. Jira Software란?Jira는 호주의 Atlassian이라는 기업에서 제공하는 애자일 소프트웨어 개발 방법론을 지원하는 일종의 프로젝트 관리 솔루션이다. 간단하게는 칸반보드부터 깊게는 애자일 프로세스에 필요한 스프린트 단위의 관리 기법이나 스프린트를 분석할 수 있는 다양한 보고서까지도 제공해주고 있다. 이런 점이 바로 Trello나 Asana와 같은 다른 프로젝트 관리 툴과 차별화되는 점인 것 같다.(참고로 Trello도 Atlassian이 먹은 지 좀 됐다) 또한 Atlassian에서 제공하는 다른 서비스들인 Bitbucket, Confluence, Source Tree 등과 서로 연동이 잘되어있는 부분도 좋은 것 같다. 예를 들면 어떤 브랜치에서 기능을 개발하다가 해당 브랜치로 Pull Request를 생성하면 Jira에 연결된 이슈가 자동으로 Code Review 상태로 변경되게도 할 수 있고 커밋 메세지에 LU-0000 first commit 처럼 Jira 이슈 번호를 넣어준다면 Jira 이슈 내에 해당 커밋과 브랜치가 자동으로 추적되기도 한다. 이슈에서 추적되고 있는 브랜치와 커밋의 모습 저런 공유 기능으로 인해서 제일 좋은 점은 사실 PO(Product Owner)들의 업무가 많이 편해진다는 것이다. Jira의 스프린트 대시보드는 일종의 상황판이다. 누가 어떤 업무를 어느 정도 진행했는지 브랜치와 커밋을 통해 상세히 알아볼 수도 있고 또 전체 스프린트의 이슈의 몇 퍼센트가 Done 상태인지, 팀이 얼마나 더 빡쎄게 달려야 이 스프린트를 마칠 수 있는지와 같은 정보를 실시간으로 보여준다. 뭐 사실 이러지 저러니 해도 Jira는 툴일 뿐, 애자일에는 일을 효율적으로 하고 싶다라는 팀의 의지가 가장 중요하긴 하지만 그래도 Jira가 굉장히 세심한 기능들을 제공해준다는 건 사실이다. Jira 이슈 커스터마이징 하기 이 문서에 작성되는 사항들은 프로젝트의 Administrator 권한이 있는 사용자만 수행할 수 있습니다. Jira 세팅 메뉴 찾기Jira 메인 화면에 접속하였을 때 Jira 프로젝트 설정에 대해 수정 권한이 있는 사용자라면 왼쪽 메뉴 리스트의 제일 하단에 Jira settings라는 메뉴가 보일 것이다. 이 메뉴에 접속하면 시스템, 프로젝트, 이슈 등 Jira에 대한 모든 설정을 수정할 수 있다. Jira settings 메뉴를 클릭하여 세팅 메뉴가 노출되면 System, Products, Projects, Issues, Apps 총 5개의 메뉴가 보인다. 이 포스팅에서는 이 설정들 중에서 Issues에 대한 설정만을 다룰 것이다. Issues 메뉴 Jira settings 내의 Issues 메뉴를 클릭해보면 이슈에 관련된 수많은 메뉴들이 노출된다. 하나하나 알아보자. Issue Types Issue Types 이슈 타입은 Story, Bug, Task와 같이 Jira 내에서 사용할 수 있는 이슈의 타입을 의미한다.이런 이슈 타입들은 크게 Standard 타입과 Sub-Task 타입으로 나누어진다. 이슈 타입 설명 Standard 부모 이슈로써 자식 이슈를 가질 수 있는 이슈 Sub-Task 부모 이슈의 자식 이슈로만 존재할 수 있는 이슈. 단독으로는 존재 불가능 현재 직장의 애자일 팀에서는 Bug, Epic, Research, Story, Sustain, Task, Sub-task로 총 7개의 이슈 타입을 사용 중이다. 각 이슈 타입의 의미에 대해서는 이슈 타입 밑에 적혀져 있는 설명을 읽어보자. 슬프게도 영어로 되어있지만 그냥저냥 읽을 수 있는 수준이다. 다른 사람들도 이렇게 영어로 설명을 읽어야하는 고통을 경험하게 만들고 싶지 않다면 우리가 희생해서 전부 한글로 번역해두면 된다.이슈 타입들은 맨 마지막 컬럼인 Actions 컬럼의 메뉴를 통해 수정 및 삭제할 수 있다. Edit: 이슈 타입의 이름, 설명, 아이콘 등을 수정 Translate: 이슈 타입 이름과 설명의 번역 내용을 설정 Delete: 되도록이면 쓰지말자. 쳐다도 보지 말자. Issue Type Schemes이슈 타입 스키마는 이슈 타입들을 묶어놓은 일종의 그룹이다. 각 프로젝트에 이슈 타입을 적용할 때 이슈 타입을 각각 할당하는 것이 아닌 이슈 타입 스키마를 할당하게 된다. 또한 이슈 타입 스키마 설정 페이지의 Projects 컬럼에는 해당 스키마가 어떤 프로젝트에서 사용되고 있는지 명시되어 있으므로 수정 및 삭제 시 반드시 이 부분을 확인하고 진행하도록하자. 이슈 타입 스키마도 이슈 타입과 마찬가지로 맨 마지막 컬럼인 Actions 컬럼의 메뉴를 통해 이슈 타입 스키마를 수정, 삭제, 할당, 복사할 수 있다. Edit - 이슈 타입 스키마의 이름, 설명, 기본 이슈 타입, 사용할 이슈 타입의 종류를 설정 Associate - 이슈 타입 스키마를 프로젝트에 할당 Copy - 이슈 타입 스키마를 복사 Delete - 이슈 타입 스키마를 삭제 Sub-tasks서브 태스크들의 이슈 타입을 관리한다. 아무것도 설정하지 않았다면 기본적으로 제공해주는 Sub-task라는 이름의 이슈 타입만 사용하게 되어있다. Workflows워크 플로우는 지라 프로젝트 내 이슈들의 작업 흐름을 의미한다. To Do, In Progress, Done 등이 여기에 해당한다. Workflows 워크 플로우를 자세히 보려면 Actions 컬럼의 View 버튼을 클릭하면 된다. 워크 플로우는 다이어그램과 텍스트 2가지 방식으로 볼 수 있는데 텍스트 방식은 작업의 흐름을 읽기가 불편하니 다이어그램 방식으로 보는 것을 추천한다. 워크 플로우는 Status와 Transition으로 구분된다. To Do, Done과 같은 사각형이 Status를 의미하고 그 사각형들을 이어주는 화살표는 Transition이다. 워크 플로우를 수정하려면 우측 상단의 Edit 버튼을 누르면 된다. 그러면 다이어그램 UI를 사용하여 워크 플로우 수정 화면이 노출된다. PowerPoint나 Google Draw.io와 유사한 UX를 제공해주기 때문에 처음이라도 무리없이 수정할 수 있다. Status를 클릭하면 Status의 정보를 수정할 수 있는 창이 노출되는데, 이 창 내부에서 Allow all statuses to transition to this one에 체크를 하면 이슈가 어떤 Status를 가지고 있는지와 관계없이 자유롭게 현재 Status로 변경할 수 있도록 설정된다. Transition을 클릭하면 Transition 설정 창이 노출된다. 이 창에서는 Transition의 이름, 속성, 트리거, 조건, 유효성 검사 등을 설정할 수 있다. 특히 트리거와 조건은 굉장히 유용하게 사용할 수 있다. 트리거 - 임의의 이벤트가 발생하면 자동으로 해당 Transition을 실행한다. Pull Request가 생성되면 자동으로 이슈를 Code Review 상태로 변경 등 조건 - 이 Transition이 실행되기 위한 조건들을 설정한다. 이슈를 Done으로 바꾸려면 이슈에 할당된 모든 Reporter가 동의해야한다 등 이후 모든 수정을 마쳤다면 상단의 Publish Draft를 클릭하여 설정을 배포하면 된다. 이때 이전 워크플로우를 백업할 것이냐고 물어보는 창이 노출되는데 백업을 할지말지는 그냥 본인 판단하에 알아서 하면된다. 편집한 이슈는 내 마음 속에 저장! 그리고 한번 Edit 버튼을 눌러 워크플로우 수정 화면으로 들어가면 아무 것도 수정하지 않았더라도 Jira는 현재 상태를 자동 저장한다. 그러니까 아무것도 수정하지 않았더라도 상단의 Discard Draft 버튼을 클릭하여 수정 상태를 종료해주도록 하자. Workflow schemesJira는 프로젝트의 이슈 타입마다 다른 워크 플로우를 사용할 수 있게끔 해준다. 워크 플로우 스키마는 이슈 타입마다 워크 플로우를 할당하는 그룹이다. Bug: To Do → In Progress → Resolved Story: To Do → In Progress → Test → Review → Deployed Research: To Do → In Progress → Done 이런 식으로 이슈 타입마다 다른 워크 플로우를 적용할 수 있다. Screens스크린은 스크럼 보드에서 이슈를 클릭했을 때 나오는 이슈의 자세한 정보를 보여주는 화면을 의미한다. 이 메뉴는 다른 메뉴들에 비해 직관적이므로 설명보다는 예시 스크린샷을 주로 보여주겠다. Screens스크린 메뉴에서는 해당 스크린에 들어갈 탭과 필드들을 설정할 수 있다. Screen Schemes이슈를 생성할 때, 수정할 때, 열람할 때 각각 다른 스크린을 할당할 수 있다. 기본적으로 Default로 설정되어있다. 이슈 생성은 보통 실무자들이 플래닝할 때 생성하는 경우가 많으므로 빠르게 생성하기 위해 최소한의 필드만 입력하도록 하고 이후 PO들이 다시 이슈의 내용을 검토하며 자세히 작성하는 경우에 사용하면 좋다. Issue Type Screen Schemes이슈 타입마다 다른 스크린 스키마를 할당할 수 있다. 기본 값으로 Default 이슈와 Bug 이슈로 구분되어있다. Bug 같은 경우는 이슈 특성 상 에스티메이션을 할 수 없는 경우도 많고 뭔가를 개발하는 것이 아닌 고치는 것이기 때문에 이슈에 필요한 정보가 일반 이슈들과는 많이 다르기 때문에 나누어져 있는 것 같다. 참고로 스크린이 아니라 스크린 스키마를 할당하는 것이다. Fields스크린에 들어가는 항목들을 수정할 수 있는 항목이다. Custom Fields 메뉴에서 항목을 추가하고 위에서 설명한 Screens에서 불러와서 사용하면 된다. 상단의 Add custom field를 누르면 다양한 폼을 선택할 수 있다. 폼 종류는 Standard와 Advanced가 있는 데, 필자는 Standard 밖에 사용을 안해봤다. 사실 Standard 폼만 사용하더라도 왠만한 정보는 전부 표현할 수 있다. Issue Features Time Tracking타임 트래킹은 이슈를 진행할때 남기는 워크 로그를 통해 해당 이슈를 수행하는 데 얼마나 걸렸는지를 추적해주는 기능이다. 근데 워크 로그를 매번 남기는 게 생각보다 번거로운 작업인지라 팀 내에 정착이 잘 안됐다.(다들 코멘트를 애용하심…) Issue Linking이슈 링킹에서는 이슈들의 관계를 정의하는 필드에 들어갈 내용을 수정할 수 있다. 하나의 관계에는 자동태와 수동태로 두가지 상세 관계를 설정할 수 있다. 기본적으로 Jira는 6가지의 상태를 제공해준다. Blocks(병목) - blocks, is blocked by Cloners(복제) - clones, is cloned by Duplicate(중복) - duplicates, is duplicated by Issue split(분리) - split to, split from Problem/Incident(문제, 원인, 사건 등) - causes, is caused by Relates(관계됨) - relates to 만약 A 이슈와 B 이슈가 있을 때 A 이슈가 반드시 먼저 끝나야 B 이슈를 작업할 수 있는 상황이 있을 수 있다. 예를 들면 A가 DB 스키마 변경 작업이고 B가 REST API를 개발하는 작업과 같은 상황이다. 모델이 어떻게 변경될지 모르니까 섣불리 API를 개발할 수 없지 않은가? 이런 경우 A blocks B 또는 B is blocked by A로 표현할 수 있다. 만약 A 이슈에 B 이슈로 통하는 blocks 링크를 건다면 B 이슈에 자동으로 is blocked by가 추가된다. A blocks B B is blocked by A 이슈 링킹 기능은 해당 이슈 담당자로 하여금 이 이슈가 어디서 파생된 이슈인지 혹은 어떤 작업이 선행되어야 하는지 명시적으로 알려줄 수 있는 기능이므로 잘 사용하면 커뮤니케이션 비용을 상당히 아낄 수 있다. 이상으로 JIRA 프로젝트 이슈 커스터마이징하기 포스팅을 마친다.","link":"/2019/06/16/jira-customizing-issue/"},{"title":"Webpack Watch의 메모리 누수 고치기","text":"이번 포스팅에서는 최근에 고쳤던 Webpack Watch 기능의 메모리 누수에 대해서 간략하게 남겨보려고 한다. 필자가 회사에서 개발한 프로젝트가 점점 커짐에 따라서 Watch 중에 빌드를 여러 번 돌리게되면 어느 순간 갑자기 out of memory가 뜨면서 프로세스가 죽어버리는 이슈가 발생하였다. 이 문제는 사실 꽤 예전부터 발생했던 이슈지만 계속 비즈니스 이슈를 개발하느라고 외면받고 있던 이슈였는데 우연히 기회가 되어 해당 이슈를 자세히 들여다 볼 수 있었다. 사실 구글에 webpack watch memory leak이라고 검색만 해봐도 같은 이슈로 고통받고 있는 전세계의 동지들이 많다는 것을 알 수 있다. Webpack을 사용하다보면 자주 볼 수 있는 슬픈 화면 일단 이 문제의 가장 유명한 해결 방법은 바로 NodeJS의 --max-old-space-size을 사용하여 Old Space의 영역을 늘리는 것이다. 원래 Old Space의 기본 크기는 64비트 기준 1.4GB 정도이지만 이렇게 메모리가 터지는 경우 Old Space에 4GB나 8GB를 할당할 것을 권장하는 해결 방법이 많다. 사실 이 방법을 사용하면 왠만큼 해결은 된다. 메모리 누수는 일반적으로 GC(가비지 컬렉팅) 때 수집되어야 할 쓰레기가 제대로 수집되지 않아서 발생하는 경우가 대부분인데, 이때 수집되지않은 쓰레기 객체들은 모두 Old Space에 있기 때문이다. 하지만 이 방법은 근본적인 해결 방법이 아니라 그냥 메모리가 터지는 시점을 좀 더 늦춘 것 뿐이다. Old Space에 4GB를 할당해주고 20번 빌드했더니 메모리가 터져서 Old Space에 8GB를 할당했다? 그레봤자 약 40번 정도 빌드하면 언젠간 또 터진다. 그렇다고 이렇게 해결하는 것이 틀린 방법인 건 아니다. 필자도 사실 이미 --max-old-space-size=4096 옵션을 통해 Old Space에 4GB 정도를 할당해주고 있었다. 하지만 이건 마치 뭐랄까, 진통제 같은 느낌이지 근본적인 치료는 아니기 때문에 이번 기회에 필자는 이 이슈의 원인을 찾아내어 제대로 고쳐보고 싶었다. NodeJS 인스펙터 사용하기먼저, 이런 메모리 누수가 고치기 까다로운 이유는 원인 추적이 힘들기 때문이다. 예를 들어 TypeError같은 경우는 로그에 아주 명확하게 어디서, 왜 타입에러가 발생했는지 알려주지만 메모리 누수는 그딴 거 없다. 그냥 쭉쭉 잘 실행되고 있는 듯 보이다가 어느 순간 픽! out of memory를 남기고 프로그램이 죽어버리기 때문이다. 프로그램이 죽으면서 마지막 힘을 짜내어 몇 줄의 Stack Trace를 남겨주긴 하지만, 표면적인 원인만 보여주는 느낌이기 때문에 메모리 누수를 고칠 때는 별 도움이 안되었던 것 같다. 그래서 이런 메모리 누수를 고칠 때는 가장 먼저, 자바스크립트의 Heap을 뜯어봐야한다. 프로그램 내에서 어떤 놈이 메모리를 점점 갉아먹고 있는 지 부터 파악하는 것이다. NodeJS를 실행시킬 때 몇가지 옵션을 사용하면 구글 크롬(Chrome)의 개발자 도구를 사용하여 Heap의 스냅샷을 찍을 수 있다. 1$ node --inspect --inspect-brk server --inspect 옵션을 사용하여 어플리케이션을 시작하고나서 구글 크롬의 아무 창이나 선택한 후 개발자 도구를 켜보면 왼쪽 구석에 NodeJS의 아이콘이 생긴 것을 확인할 수 있다. 그 아이콘을 누르면 NodeJS를 프로파일링할 수 있는 새로운 인스펙터가 나타난다. 그리고 --inspect-brk 옵션은 코드를 실행하기 전 Debug Pause 기능을 이용하여 실행을 멈춰준다. 개발자가 직접 Resume 버튼을 눌러주면 코드가 실행된다. 메모리 누수 원인필자도 맨 처음에 했던 일이 바로 NodeJS의 인스펙터를 사용하여 Heap 스냅샷을 찍어보는 것이었다. 사실 필자는 이 이슈가 일반적인 메모리 누수 상황과는 조금 다르다고 생각했는데, 보통 일반적인 메모리 누수는 대략 이런 시나리오로 그려진다. 객체를 생성! 객체 참조를 해제함! 근데 GC가 안됨…? 뭐지…? 알고 보니 다른 놈이 참조를 하고 있었다는 결말 그렇기 때문에 보통 메모리 누수는 어떤 놈이 해제되어야 할 객체를 참조하고 있는거지?로 시작하는 경우가 많다.그래서 보통 객체 할당을 하고 한번 Heap 스냅샷을 찍고, 다시 객체 참조를 해제한 다음 다시 Heap 스냅샷을 찍은 후 그 두 스냅샷을 비교하는 방법으로 디버깅을 진행한다. 분명히 나는 참조를 해제했는데 뭔가 아직도 유지되고 있는 놈이 보인다? 바로 그 놈이 범인일 가능성이 높다. 이 범인 색출이 오지게 힘들긴 하지만… 하지만 Webpack의 메모리 누수 이슈는 이런 복잡한 이슈는 아닌 것 같았다. Stack Overflow에서 다른 사람들의 사례를 보면 프로젝트가 작을 때는 별 문제 없다가 프로젝트가 커질수록 이런 문제가 발생한다는 사례가 많았고, 필자 또한 그런 상황이었기 때문에 처음부터 방향은 혹시 빌드할 때마다 파일이 계속 누적되고 있나?라는 가설로 잡았기 때문이다. 그래서 가설을 확인하기 위해 일단 빌드를 n번 진행한 후, 메모리 사용량의 변화를 살펴보기 위해 매 빌드마다 Heap 스냅샷을 찍었고, 그 결과 Memory File System 이라는 놈이 점점 비대해지고 있는 것을 발견했다. 빌드 횟수 MFS의 메모리 사용량(byte) 1 21,034,701 2 38,776,735 3 45,209,592 4 51,642,543 7 83,807,008 10 122,404,490 13 180,301,478 여기까지 확인한 후 Memory File System 객체의 내부를 한번 까봤더니 예상한 대로 번들링했던 모든 파일이 계속 누적되고 있는 것을 확인할 수 있었다. 징글징글한 것들… 왜 이런 문제가 발생했을까? 그 이유는 해당 프로젝트의 Webpack 세팅의 번들 파일 이름 패턴이 client-bundle.[chunkhash].js로 되어있었기 때문이다. chunkhash나 hash 옵션은 매 번들링 때 새로운 파일명을 만들어주므로 운영환경에서 발생하는 파일 캐싱 이슈에 대해서 자유롭게 해준다. 하지만 개발 환경에서는 이 옵션이 오히려 독이 된 것이다. Webpack은 개발 서버를 띄울 때 번들링된 파일을 memory-fs라는 라이브러리를 사용하여 저장한다. 이 라이브러리는 그냥 내부적으로 맵(Map)을 가지고 있고 파일 내용은 파일의 각 라인을 원소로 가지고 있는 배열(Array)이다. 즉 번들링된 파일 이름이 main.js이면 다음과 같은 구조를 가진 객체가 생성된다는 것이다. 123const MFS = { 'main.js': ['var a = 1', 'console.log(a)']} 만약 필자가 main.js 파일이나 여기서 종속된 모듈을 수정하면 main.js는 다시 번들링 될 것이고 MFS 객체의 main.js키의 값을 새로 번들링된 파일의 내용으로 변경할 것이다. 그러나 문제는 chunkhash나 hash와 같이 빌드 때마다 파일 이름이 변경되는 경우에 생긴다. 1234const MFS = { 'main.1111.js': ['var a = 1'], // 기존 파일 'main.2222.js': ['var a = 2'] // 새로 번들링된 파일} main.1111.js는 이전 빌드 때 생성된 번들이고 main.2222.js는 이번 빌드 때 생성된 번들이다. chunkhash, hash는 파일을 다시 번들링하면 해쉬값이 변경되므로 번들링된 파일의 이름이 달라지게 되고, 결과적으로 MFS 객체에 저장되어있던 이전 버전 번들은 교체되지 않고 남아있게 된다. 즉, 파일이 누적된다. 이 객체 자체나, 파일 라인들을 담고 있는 배열, 라인들의 값을 가지고 있는 문자열 모두 당연히 메모리에 저장되고, memory-fs가 이 친구들을 계속 참조하고 있으니 결과적으로 Old Space로 넘어간 후에도 계속 GC에 수집되지 않았던 것이다. 해결 방법사실 이 문제를 처음 접했을 때 3가지 정도의 해결 방법을 생각했었다. 개발 환경일 때는 파일 이름 패턴에 hash나 chunkhash를 사용하지 않는다. Webpack의 compiler hook에서 이전 빌드의 번들을 직접 제거(HotModuleReplacement가 사용하는 방법임) 그냥 Old Space를 늘려준다.(자존심을 버린 최후의 방법) 사실 1번 방법인 파일 이름 패턴에서 chunkhash를 제거하기 만으로도 해결되는 문제긴 하지만, 혹시 모르니 다음 차선책까지 생각해둔 것이다. 필자는 보통 이런 문제를 만나면 한번에 해결된다는 기대를 잘 안하는 편이다.(높은 데서 떨어지면 더 아픈 법이다. 아예 기대를 말자) 하지만 다행히도 1번 해결 방법으로 한 방에 해결이 되었다. webpack.client.config.js123456789// ...const isLocal = process.env.NODE_ENV === 'local';// ... module.exports = { output: { filename: isLocal ? 'client-bundle.[name].js' : 'client-bundle.[chunkhash].js', },} 1234567891011121314151617181920212223242526Built at: 2019-08-06 10:29:34 Asset Size Chunks Chunk Names client-bundle.app.js 4.16 MiB app [emitted] app client-bundle.app.js.map 2.98 MiB app [emitted] app client-bundle.chat.js 815 KiB chat [emitted] chat client-bundle.chat.js.map 503 KiB chat [emitted] chat client-bundle.chat~user.js 312 KiB chat~user [emitted] chat~user client-bundle.chat~user.js.map 200 KiB chat~user [emitted] chat~user client-bundle.polyfill.js 6.24 KiB polyfill [emitted] polyfill client-bundle.polyfill.js.map 6.23 KiB polyfill [emitted] polyfill client-bundle.search.js 254 KiB search [emitted] search client-bundle.search.js.map 224 KiB search [emitted] search client-bundle.style-guide.js 135 KiB style-guide [emitted] style-guide client-bundle.style-guide.js.map 105 KiB style-guide [emitted] style-guide client-bundle.user.js 134 KiB user [emitted] user client-bundle.user.js.map 70.2 KiB user [emitted] user client-bundle.vendor.js 4.6 MiB vendor [emitted] vendor client-bundle.vendor.js.map 5.25 MiB vendor [emitted] vendor fontawesome-webfont.eot?674f50d287a8c48dc19ba404d20fe713 162 KiB [emitted] fontawesome-webfont.ttf?b06871f281fee6b241d60582ae9369b9 162 KiB [emitted]fontawesome-webfont.woff2?af7ae505a9eed503f8b8e6982036873e 75.4 KiB [emitted] fontawesome-webfont.woff?fee66e712a8a08eef5805a46892932ad 95.7 KiB [emitted] index.html 804 bytes [emitted] sw.js 4.79 KiB [emitted] sw.js.map 4.2 KiB [emitted] vue-ssr-client-manifest.json 108 KiB [emitted] 파일 이름 패턴에서 chunkhash을 삭제하고 name을 추가했기 때문에 이제 몇 번을 빌드하든 client-bundle.app.js처럼 늘 같은 이름으로 번들이 생성될 것이다. 그리고 위에서 얘기했듯 memory-fs는 파일 이름을 맵의 키로 사용하기 때문에 새로운 파일을 맵에 저장할 때 그 파일과 같은 이름을 가진 이전 빌드의 번들은 자동으로 덮어씌워질 것이다. 아래는 해당 작업을 수행한 후 다시 Heap 스냅샷 사용하여 분석해본 Memory File System 내 dist 객체의 크기이다. 빌드 횟수 MFS의 메모리 사용량(byte) 1 21,031,408 2 21,023,274 3 21,023,224 4 21,023,310 이제 여러 번 빌드를 하더라도 memory-fs가 사용하는 메모리가 점점 증가하지 않게 되었고 프론트엔드 챕터 개발자들은 드디어 쾌적한 환경에서 개발을 계속 할 수 있게 되었다! 마치며사실 이 이슈는 처음 이슈를 접했을 때 세운 가설이 딱 맞아들어가서 빠르게 해결할 수 있었던 케이스였다. 하지만 그 가설을 증명하는데는 거의 하루 종일 걸렸는데, 그 이유는 구글 크롬 인스펙터가 Heap 스냅샷을 한번 찍는데 시간이 너무 오래 걸려서이다. 아니 무슨 한번 찍는데 거의 3~4분이 걸려… 막상 원인을 파악하고 가설도 증명하고나니 해결 방법은 굉장히 심플했는데, 뭔가 손 안대고 코 푼 느낌이랄까…? 뭐 어쨌든 쉽게 해결할 수 있어서 다행이었다. 이상으로 Webpack Watch의 메모리 누수 고치기 포스팅을 마친다.","link":"/2019/08/08/fix-webpack-dev-memory-leak/"},{"title":"실시간 데이터의 평균을 효율적으로 구하기","text":"이번 포스팅에서는 실시간으로 빠르게 쌓이는 데이터들의 평균을 효율적으로 구할 수 있는 방법에 대해서 간단하게 설명하려고 한다. 이런 실시간 데이터의 평균을 구해야하는 경우는 생각보다 꽤 많은데, 서버 엔진의 액세스 로그에 쌓이는 응답들의 평균 응답 시간을 구한다던가, 센서에서 들어오는 값들의 평균을 구한다던가 하는 경우이다. 이때 이런 데이터들은 빠르게는 1ms 정도의 간격으로 수집되는 경우도 비일비재하기 때문에, 데이터를 입력받자마자 빠르게 처리해야하는 성능이 굉장히 중요하다. 이때 우리가 이 데이터들을 가지고 실시간으로 평균을 구해야한다면, 일반적으로 생각나는 평균의 수식은 전체 데이터의 총합 / 데이터 배열의 길이일 것이다. 이렇게 구한 평균 값을 산술 평균이라고 한다. 그러나 우리가 일반적으로 산술 평균을 구하는 방법은 데이터를 실시간으로 빠르게 처리해야하는 시스템과는 별로 맞지 않는 방법이다. 왜 그럴까? 일반적인 산술 평균 공식의 문제점일반적으로 우리가 산술 평균을 구한다고 할때 떠오르는 공식은 다음과 같다. Average(n)=1n∑k=1nxk=x1+x2+...+xnnAverage(n) = \\frac{1}{n}\\sum_{k=1}^{n}x_k = \\frac{x_1 + x_2 + ... + x_n}{n}Average(n)=​n​​1​​​k=1​∑​n​​x​k​​=​n​​x​1​​+x​2​​+...+x​n​​​​ 수식으로 보면 조금 복잡해보일 수 있지만 결국 k=1일 때 n이 될 때 까지 이터레이션을 돌리며 x1 ~ xn까지 전부 더한 다음 마지막으로 1/n을 곱해주는 것, 결국 우리가 학교에서 배웠던 일반적인 평균 알고리즘이다. 이렇게 데이터를 모두 모아서 한번에 연산하는 방식을 배치식(Batch Expression)이라고 부른다. 배치식의 단점은 바로 시간 복잡도가 $O(n)$이라는 것이다. 이건 데이터의 숫자가 적을 때는 큰 문제가 없더라도 계속 해서 누적되는 실시간 데이터를 빠르게 처리해야하는 시스템에서는 치명적인 단점이 된다. 간단하게 1부터 100,000까지의 평균을 반복적으로 구하는 배치식 평균 알고리즘의 수행 시간을 확인해보자. 123456789101112131415let avg = 0;const numbers = [];function average (numbers = []) { const sum = numbers.reduce((prev, current) => prev + current, 0); return sum / numbers.length;}console.time('avg1');for (let k = 1; k < 100001; k++) { numbers.push(k); avg = average(numbers);}console.timeEnd('avg1');console.log(`그래서 평균은? -> ${avg}`); 실시간 데이터 처리를 흉내낸 간단한 코드를 작성했다. numbers 배열에는 반복적으로 새로운 데이터가 입력되고 그때마다 새로운 평균을 구한다. 첫번째 for문은 실시간으로 입력되는 데이터 환경을 구현하기 위한 것이므로 시간 복잡도에서 제외하고 getAvg 함수의 내부만 봐도, 한번 평균을 구할 때마다 지금까지 누적한 전체 데이터의 배열을 처음부터 순회하며 총합을 구하는 모습을 볼 수 있다. 그래서 결국 이 알고리즘이 100,000개 데이터의 평균을 구하는 데 소요된 시간은… 123$ node average.jsavg1: 8146.261ms그래서 평균은? -> 50000.5 8146ms 정도? 대략 8초라는 엄청 나게 긴 시간이다. 실시간 데이터 처리 시스템에서 어떤 값을 처리하는데 8초가 걸린다면 그건 그냥 망한거다.(사실 1초만 넘어도 망했다고 한다.) 게다가 실시간 데이터의 특성 상 데이터의 양은 점점 누적되어 늘어날 수 밖에 없는데, 100,000개 처리에 8초면 1,000,000개 처리면 대충 80초 정도 걸리지 않을까? 사실 배치식의 단점은 시간이 단순히 오래 걸린다는게 아니라 데이터가 늘어나면 수행시간도 비례해서 늘어난다는 것이다. 또한 $n$ 번째 데이터까지 합친 평균을 계산할 때 이전 평균 계산 결과를 전혀 사용하지 못하므로 결국 지금까지 들어온 모든 데이터를 저장하고 있어야 하는데, 이때 불필요한 메모리 자원도 낭비되게 된다. 그럼 데이터가 늘어나도 언제나 수행 속도가 일정한 $O(1)$ 평균 알고리즘이 있을까? 평균 필터 알고리즘있다. 바로 평균 필터(Average Filter) 알고리즘이다. 이 알고리즘은 n번째 데이터가 들어온 데이터 셋의 평균을 구할 때 n-1 까지의 평균을 재사용할 수 있는 알고리즘이다. 이런 알고리즘을 재귀식(Recurrence Expression)이라고 한다.(참고로 재귀식은 재귀함수보다 좀 더 포괄적인 개념이다.) 재귀식인 평균 필터 알고리즘을 사용하면 이전에 들어온 데이터를 전부 저장해놓을 필요도 없고 몇개의 데이터가 누적되든 반드시 $O(1)$의 시간 복잡도를 보장한다. 그래서 이 알고리즘은 센서에서 짧은 간격으로 들어오는 데이터들을 실시간으로 처리해야하는 IOT나 임베디드 분야에서 더 많이 사용된다.(필자도 이 쪽 분야에서 일하는 형한테 처음 배웠다.) 하지만 다짜고짜 수학으로 들어가면 재미가 없으니, 이해를 돕기 위해 이번에는 먼저 코드와 수행시간을 확인해보고나서 이 알고리즘에 대한 설명을 하도록 하겠다. 12345678910111213let avg = 0;function cumulativeAverage (prevAvg, newNumber, listLength) { const oldWeight = (listLength - 1) / listLength; const newWeight = 1 / listLength; return (prevAvg * oldWeight) + (newNumber * newWeight);}console.time('avg2');for (let k = 1; k < 100001; k++) { avg = cumulativeAverage(avg, k, k);}console.timeEnd('avg2');console.log(`그래서 평균은? -> ${avg}`); cumulativeAverage 함수는 이전 데이터까지의 평균과 새로 들어온 데이터, 총 데이터 개수 이렇게 3개의 인자를 사용하는 함수이다. 그리고 간단한 수식을 통해서 바로 평균을 계산한다. 위에서 작성한 average 함수와 가장 큰 차이점이 있다면, 함수 내부에 이터레이션이 없다는 것이다. 위에서 봤던 배치식 평균 알고리즘은 100,000개 데이터의 평균을 실시간으로 구해내는데 8초의 시간이 소요되었지만 이 친구는 급이 다르다. 123$ node average.jsavg: 4.631ms그래서 평균은? -> 50000.5 쨘, 8000ms에서 4ms가 되었다. 뭐 이건 사실 당연한 사실이다. 평균을 구하기 위해 전체 데이터를 순회할 필요가 없이 단순한 연산만 반복하면 되기 때문이다. 데이터의 개수를 1,000,000개로 늘려도 수행시간은 대략 10ms 정도로, 크게 변하지 않는다. 그리고 이 코드에서 데이터의 양을 100,000에서 1,000,000으로 늘렸을 때 시간이 늘어난 이유는 그저 for문이 더 오래 도니까 그런 것인데, 실시간 데이터 처리 시스템에서는 이렇게 이터레이션을 돌면서 데이터를 입력하는 것 아니라 말 그래도 비동기적으로 쭉쭉 데이터가 들어오고 이벤트 리스너를 통해 평균 함수를 실행할 것이므로 결국 실제 사용 사례에서는 항상 균일한 수행 속도를 가지게 된다고 생각할 수 있다. 그럼 이 알고리즘은 어떤 원리로 작동하는 것일까? 무슨 원리로 이렇게 되는건가요?구글에 평균 필터 알고리즘을 검색해보면 이미 많은 분들이 배치식에서 재귀식을 유도해내는 과정을 많이 포스팅 해놓은 것을 볼 수 있었다. 하지만 이런 포스팅들의 특성 상 글쓴이 본인의 아카이브 느낌이 강하기 때문에 친절한 설명은 딱히 없었던 것 같다. 그리고 그렇게 수식을 유도하며 설명하는 방법은 수학과 친하지 않은 사람에게는 직관적인 설명이 될 수 없다. 그래서 필자는 배치식에서 재귀식을 유도하는 방법 보다 좀 더 직관적이고 간단한 느낌으로 설명해보려고 한다. 일단 평균 필터 알고리즘의 공식을 한번 보자. Average(n)=n−1nAverage(n−1)+1nxnAverage(n) = \\frac{n-1}{n}Average(n-1) + \\frac{1}{n}x_nAverage(n)=​n​​n−1​​Average(n−1)+​n​​1​​x​n​​ 이 식의 $n$은 현재까지의 전체 데이터 길이, $Average(n-1)$는 이전 데이터까지의 평균, $x_n$은 이번에 새로 들어온 데이터를 의미한다. 그렇다면 이 식이 의미하는 것이 뭐길래 평균을 구할 수 있도록 해주는 것일까? 이 식에서 중요한 키워드는 바로 가중치이다. 우리가 배치식을 버리고 재귀식을 사용하기 위한 조건은 바로 이전 평균 값을 활용할 수 있을 것이었다. 그러기위해 우리는 새로운 평균을 구하기 위해 사용할 이전 평균 값이 새로운 평균 값에 끼칠 영향을 계산해줘야 하는 것이다. 식을 자세히 보면 이전 평균 값인 $Average(n-1)$에는 $\\frac{n-1}{n}$이 곱해지고 있다. 이때 $\\frac{n-1}{n}$이 의미하는 것이 바로 가중치이다. 또 새로 들어온 데이터인 $x_n$을 보면 $\\frac{1}{n}$이 곱해지고 있는데, 이 또한 새로운 데이터의 가중치이다. 좀 더 빠른 이해를 돕기 위해 간단한 예시를 보면서 설명하도록 하겠다. 12const prevData = [1, 2, 3, 4];const prevAverage = (1 + 2 + 3 + 4) / 4; // 2.5 여기 길이가 4인 간단한 데이터셋이 있다. prevAverage는 여러분이 좀 더 직관적으로 이해할 수 있도록 그냥 배치식을 하드코딩했고, 할당된 값은 1, 2, 3, 4의 평균인 2.5가 된다. 이때 이 데이터셋에 5라는 값이 새로 들어오면 배열의 길이는 5가 될 것이다. 이때 배치식을 사용하며 계산하게 되면 1부터 5까지 모두 더한 후 배열의 길이인 5로 나누게 될 것이고, 재귀식을 사용하면 기존 4개 원소의 평균이었던 2.5에 가중치인 4/5를 곱해주고 새로 들어온 5에도 가중치인 1/5를 곱한 후 서로 더하기만 하면 된다. 한번 아래 코드를 콘솔에서 실행시켜보고 결과를 확인해보자. 1234const batch = (1 + 2 + 3 + 4 + 5) / 5;const recurrenced = (2.5 * (4 / 5)) + (5 * (1 / 5));console.log(batch, recurrenced);// 3 3 결과는 둘 다 3으로 동일하게 출력된다. 즉, 쉽게 말하자면 이전 평균의 값과 새로운 데이터가 새로운 평균에서 얼마 만큼의 비중을 가지고 있는지를 계산해서 더해주는 것이다. 이제 대충 이해가 되었으리라 생각한다. 만약 배치식에서 재귀식을 유도하는 과정이 궁금하신 분은 구글에 평균 필터라고 검색해보면 다른 분들이 작성해놓은 많은 포스팅이 있으니 그 쪽을 참고해보도록 하자. 필자는 개인적으로 그런 수식 유도 과정보다 이 알고리즘이 작동하는 추상적인 개념을 이해하는 것이 더 중요하다고 생각하기 때문에 이 포스팅에서는 별도의 유도 과정을 적지는 않겠다. 이상으로 실시간 데이터의 평균을 효율적으로 구하기 포스팅을 마친다.","link":"/2019/08/11/average-filter/"},{"title":"[Deep Learning이란 무엇인가?] Backpropagation, 역전파 알아보기","text":"이번 포스팅에서는 저번 포스팅에 이어 Backpropagation에 대해서 알아보려고 한다. 전 포스팅에서도 설명했듯, 이 알고리즘으로 인해 Multi Layer Network에서의 학습이 가능하다는 것이 알려져, 암흑기에 있던 Neural Network 학계가 다시 관심을 받게 되었다. Backpropagation이란?Backpropagation은 오늘 날 Artificial Neural Network를 학습시키기 위한 일반적인 알고리즘 중 하나이다.한국말로 직역하면 역전파라는 뜻인데, 내가 뽑고자 하는 target값과 실제 모델이 계산한 output이 얼마나 차이가 나는지 구한 후 그 오차값을 다시 뒤로 전파해가면서 각 노드가 가지고 있는 변수들을 갱신하는 알고리즘인 것이다.다행히 여기까지는 직관적으로 이해가 되지만 필자는 다음 2가지의 원리가 궁금했다. 각 노드가 가지고 있는 weight이나 bias같은 변수들을 어떻게 업데이트할 것인가? Multi Layer Network에서 각 노드나 레이어가 가지고 있는 변수들은 다 제 각각인데 그 값들을 얼만큼 변경하는 지 어떻게 알 수 있는가? 다행히 이 문제들은 Chain Rule이라는 법칙을 사용해 해결할 수 있다고 한다. 한번 차근차근 살펴보자. Chain Rule이란?Chain Rule, 미분의 연쇄법칙이라고도 불리는 법칙이다. 이건 고딩때는 안배우고 대학수학에서 배우기 때문에, 대학 때 이산수학만 배웠던 필자는 이해가 잘되지않아서 고생했다. 먼저 정의부터 보자. 함수 $f, g$가 있을 때$f$와 $g$가 모두 미분 가능하고$F = f(g(x)) = f \\circ g$로 정의된 합성 함수이면 $F$는 미분 가능하다.이때 $F’(x) = f’(g(x)) \\centerdot g’(x)$이다.$t = g(x)$라고 한다면,$\\frac{dy}{dx} = \\frac{dt}{dx} \\frac{dy}{dt}$가 성립한다. 정의를 보면 뭔 말인가 싶을 수 있는데, 먼저 합성함수는 그냥 어떤 함수의 인자로 다른 함수가 주어진 함수이다. 대충 이런 식이랄까? 12345678910function f (g) { return g * 3;}function g (x) { return x + 1;}const x = 3;let F = f(g(x));// F = 12 그럼 미분 가능하다라는 말이 의미하는 것은 뭘까? 보통 미분 = x와 x'간의 기울기를 구한다 정도로 이해하고 있다면 합성함수 어쩌고에서 기울기를 왜 구하지? 라고 생각할 수 있다.하지만 기울기를 구한다라는 말은 변화량을 구한다라고도 할 수 있다.위의 코드를 보자. 그냥 직관적으로 딱 봐도 변수 F를 선언할 때 g에 주는 값을 변경한다면 최종적으로 F값이 바뀐다는 것을 알 수 있을 것이다. 1234F = f(g(4));// F = 15F = f(g(2));// F = 9 즉 Chain Rule이란 쉽게 얘기하자면 1. x가 변화했을 때 함수 g가 얼마나 변하는 지와 그로인해 2. 함수 g의 변화로 인해 함수 f가 얼마나 변하는 지를 알 수 있고 3. 함수 f의 인자가 함수 g이면 최종 값 F의 변화량에 기여하는 각 함수 f와 g의 기여도를 알 수 있다는 것이다.방금 전 위에서 예시로 든 합성함수 F의 식에 들어가는 변수는 x 하나였다. 그럼 변수가 여러 개면 어떻게 되는 걸까? 이변수함수 $z = f(x, y)$에서 $x = h(s,t), y = g(s,t)$일 때$f(x,y), g(s,t), h(s,t)$가 모두 미분 가능하면 ∂z∂s=∂z∂x∂x∂s+∂z∂y∂y∂s∂z∂t=∂z∂x∂x∂t+∂z∂y∂y∂t\\begin{aligned} \\frac{\\partial z}{\\partial s} = \\frac{\\partial z}{\\partial x}\\frac{\\partial x}{\\partial s} + \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial s} \\\\ \\\\ \\frac{\\partial z}{\\partial t} = \\frac{\\partial z}{\\partial x}\\frac{\\partial x}{\\partial t} + \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial t} \\\\ \\end{aligned}​​∂s​​∂z​​=​∂x​​∂z​​​∂s​​∂x​​+​∂y​​∂z​​​∂s​​∂y​​​​​∂t​​∂z​​=​∂x​​∂z​​​∂t​​∂x​​+​∂y​​∂z​​​∂t​​∂y​​​​​ 로 나타내어질 수 있다. 이 말인 즉슨 $s$나 $t$가 얼만큼인지는 모르지만 어쨌든 변했을 때, 함수 $z$의 변화량을 저런 식으로 구할 수 있다는 것이다.그리고 $\\partial$는 편미분을 뜻하는 기호인데, 메인이 되는 변수 하나를 남겨두고 나머지 변수는 그냥 개무시하는 미분법이다. 그래서 $s$와 $z$의 관계를 구하는 식에서는 아예 $t$가 없는 것을 알 수 있다.여기까지 이해가 되었다면 이제 본격적으로 Backpropagation이 어떻게 진행되는 지 살펴보도록 하자. Forward-propagation이제 직접 Backpropagation이 어떻게 이루어지는 지 한번 계산해보자.그 전에 먼저 Forward Propagation을 진행해야한다. 초기화한 $w$값과 input인 $x$을 가지고 계산을 진행한 뒤 우리가 원하는 값이 나오는 지, 나오지 않았다면 얼마나 차이가 나는지를 먼저 구해야한다.필자가 이번 계산에 사용할 모델은 아래와 같다. 이 모델은 2개의 input, 2개의 output을 가지고 2개의 Hidden Layer를 가진 2-Layer NN 모델이다.이제 각 변수에 값을 할당해보자. 먼저 필자가 output으로 원하는 $y_1$의 값은 0.2, $y_2$의 값은 0.7이다.그리고 input으로는 $x_1$에 0.2, $x_2$에 0.5를 넣어주었고, 각각의 $w$값은 그냥 느낌 가는대로 넣어놓았다. 필자는 이 계산에서 Activation Function으로 Sigmoid함수를 사용하고, Error Function은 Mean Squared Error함수를 사용하려고 한다. 먼저 Layer0에서 받을 값부터 계산해보자. 보통 행렬로 계산한다. z10=[x1x2]×[w100w200]z11=[x1x2]×[w110w210]\\begin{aligned} z_{10} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\times \\begin{bmatrix} w^0_{10} & w^0_{20} \\end{bmatrix} \\\\ \\\\ z_{11} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\times \\begin{bmatrix} w^0_{11} & w^0_{21} \\end{bmatrix} \\\\ \\end{aligned}​z​10​​=[​x​1​​​x​2​​​​]×[​w​10​0​​​​​w​20​0​​​​]​​z​11​​=[​x​1​​​x​2​​​​]×[​w​11​0​​​​​w​21​0​​​​]​​​ 저 행렬 곱을 풀어보면 다음과 같이 되고 결국 $wx$들의 합의 형태로 나타난다. z10=x1w100+x2w200=(0.2×0.1)+(0.5×0.3)=0.02+0.15=0.17z11=x1w110+x2w210=(0.2×0.2)+(0.5×0.1)=0.04+0.05=0.09\\begin{aligned} z_{10} = x_1w^0_{10} + x_2w^0_{20} = (0.2\\times0.1) + (0.5\\times0.3) = 0.02 + 0.15 = 0.17 \\\\ \\\\ z_{11} = x_1w^0_{11} + x_2w^0_{21} = (0.2\\times0.2) + (0.5\\times0.1) = 0.04 + 0.05 = 0.09 \\\\ \\end{aligned}​z​10​​=x​1​​w​10​0​​+x​2​​w​20​0​​=(0.2×0.1)+(0.5×0.3)=0.02+0.15=0.17​​z​11​​=x​1​​w​11​0​​+x​2​​w​21​0​​=(0.2×0.2)+(0.5×0.1)=0.04+0.05=0.09​​​ $z_{10}$와 $z_{11}$의 값을 구했으면 이제 Activation Function을 사용하여 $a_{10}$와 $a_{11}$값을 구해보자.필자가 사용할 Activation Function인 Sigmoid의 수식은 다음과 같다. $$\\sigma = \\frac{1}{1 + e^{-x}}$$ 이걸 매번 손으로 계산하면 너무 번거롭기 때문에 JavaScript를 사용해 다음과 같이 함수를 하나 만들어 놓고 사용했다. 123function sigmoid (x) { return 1 / (1 + Math.exp(-x));} a10=σ(z10)=0.54a11=σ(z11)=0.52\\begin{aligned} a_{10} = \\sigma(z_{10}) = 0.54 \\\\ \\\\ a_{11} = \\sigma(z_{11}) = 0.52 \\\\ \\end{aligned}​a​10​​=σ(z​10​​)=0.54​​a​11​​=σ(z​11​​)=0.52​​​ 다음 레이어도 같은 방식으로 값을 계속 구해보면 다음과 같은 값들을 구할 수 있다. z10=0.17a10=0.54z11=0.09a11=0.52z20=0.27a20=0.57z21=0.43a21=0.61\\begin{aligned} z_{10} = 0.17 \\\\ a_{10} = 0.54 \\\\ \\\\ z_{11} = 0.09 \\\\ a_{11} = 0.52 \\\\ \\\\ z_{20} = 0.27 \\\\ a_{20} = 0.57 \\\\ \\\\ z_{21} = 0.43 \\\\ a_{21} = 0.61 \\\\ \\end{aligned}​z​10​​=0.17​a​10​​=0.54​​z​11​​=0.09​a​11​​=0.52​​z​20​​=0.27​a​20​​=0.57​​z​21​​=0.43​a​21​​=0.61​​​ 결국 $y_1$와 $y_2$는 각각 $a_{20}$과 $a_{21}$과 같으므로, 우리는 최종 output값을 구하게 되었다.근데 우리가 처음에 원했던 $y_1$과 $y_2$는 0.2와 0.7이었는데, 우리가 구한 output은 0.57과 0.61으로 거리가 있다.이제 Mean Squared Error함수를 사용하여 에러 $E$를 구할 차례이다.결과값으로 얻기를 바라는 값을 $t$로, 실제 나온 값을 $y$라고 할 때 에러 $E$는 다음과 같다. $$E = \\frac{1}{2}\\sum(t_i - y_i)^2$$ 필자같은 수포자를 위해 쉽게 설명하자면, 그냥 마지막 Output Layer에서 뱉어낸 $y$들과 하나하나 레이블링했던 $\\hat{y}$가 얼마나 차이나는 지 구한 다음에 그 값들의 평균을 내는 것이다.결국 ANN을 학습시킨다는 것은 이렇게 구한 에러 EEE의 값을 0에 근사시킨다고 볼 수 있다.여기서 나온 $E$값을 이제 Backpropagation하면 되는 것이다. 이것도 매번 손으로 계산하기 귀찮으니까 그냥 함수를 하나 만들자. 1234567891011121314function MSE (targets, values) { if (values instanceof Array === false) { return false; } let result = 0; targets.forEach((target, i) => { result += (0.5 * ((target - values[i]) ** 2)); }); return result;}MSE([0.2, 0.7], [0.57, 0.61]); // 0.072 이제 여기서 구한 에러 $E$값을 사용하여 Backpropagation을 진행해보자. BackpropagationFrontend Propagation을 통해서 구해진 값을 다시 그림으로 살펴보면 다음과 같다. 필자는 이 중 현재 0.4로 할당되어 있는 $w^1_{10}$값을 업데이트 하려고 한다.그러려면 $w^1_{10}$이 전체 에러인 $E$에 얼마나 영향을 미쳤는지, 즉 기여도를 구해야한다. 이때 위에서 설명한 Chain Rule이 사용된다. $E$에 대한 $w^1_{10}$의 기여도를 식으로 풀어보면 다음과 같다. ∂E∂w101=∂E∂a20∂a20∂z20∂z20∂w101\\begin{aligned} \\frac{\\partial E}{\\partial w^1_{10}} = \\frac{\\partial E}{\\partial a_{20}} \\frac{\\partial a_{20}}{\\partial z_{20}} \\frac{\\partial z_{20}}{\\partial w^1_{10}} \\\\ \\end{aligned}​​∂w​10​1​​​​∂E​​=​∂a​20​​​​∂E​​​∂z​20​​​​∂a​20​​​​​∂w​10​1​​​​∂z​20​​​​​​​ 먼저 $\\frac{\\partial E}{\\partial a_{20}}$부터 차례대로 풀어보자. 원래 우리가 구한 $E$는 아래와 같은 식이였다. $$E = \\frac{1}{2}((t_1 - a_{20})^2 + (t_2 -a_{21})^2)$$ 여기서 $a_{20} = y_1, a_{21} = y_2$이기 때문에 치환해주었다. 하지만 $\\frac{\\partial E}{\\partial a_{20}}$는 편미분식이기 때문에 지금 구하려는 값과 상관없는 $a_{21}$는 그냥 0으로 생각하고 풀면된다. ∂E∂a20=(t1−a20)∗−1+0=(0.2−0.57)×−1=0.37\\begin{aligned} \\frac{\\partial E}{\\partial a_{20}} = (t_1 - a_{20}) * -1 + 0 = (0.2 - 0.57) \\times -1 = 0.37 \\\\ \\end{aligned}​​∂a​20​​​​∂E​​=(t​1​​−a​20​​)∗−1+0=(0.2−0.57)×−1=0.37​​​ 이 계산 결과가 의미하는 것은 전체 에러 $E$에 대하여 $a_{20}$, 즉 $y_1$가 0.37만큼 기여했다는 것을 의미한다.이런 식으로 계속 계산해보자. ∂a20∂z20=a20×(1−a20)=0.57×(1−0.57)=0.25\\begin{aligned} \\frac{\\partial a_{20}}{\\partial z_{20}} = a_{20} \\times (1 - a_{20}) = 0.57 \\times (1 - 0.57) = 0.25 \\\\ \\end{aligned}​​∂z​20​​​​∂a​20​​​​=a​20​​×(1−a​20​​)=0.57×(1−0.57)=0.25​​​ ∂z20∂w101=a10+0=0.54\\begin{aligned} \\frac{\\partial z_{20}}{\\partial w^1_{10}} = a_{10} + 0 = 0.54 \\\\ \\end{aligned}​​∂w​10​1​​​​∂z​20​​​​=a​10​​+0=0.54​​​ ∂E∂w101=0.37×0.25×0.54=0.049\\begin{aligned} \\frac{\\partial E}{\\partial w^1_{10}} = 0.37 \\times 0.25 \\times 0.54 = 0.049 \\\\ \\end{aligned}​​∂w​10​1​​​​∂E​​=0.37×0.25×0.54=0.049​​​ 최종적으로 $E$에 $w^1_{10}$가 기여한 값은 0.049이라는 값을 계산했다.이제 이 값을 학습식에 넣으면 $w^1_{10}$값을 업데이트 할 수 있다.이때 값을 얼마나 건너뛸 것이냐 또는 얼마나 빨리 학습시킬 것이냐 등을 정하는 Learning Rate라는 값이 필요한데, 이건 그냥 사람이 정하는 상수이고 보통 0.1보다 낮은 값으로 설정하나 필자는 0.3으로 잡았다. w101+=w101−(L∗∂E∂w101)=0.4−(0.3×0.049)=0.3853\\begin{aligned} w^{1+}_{10} = w^1_{10} - (L * \\frac{\\partial E}{\\partial w^1_{10}}) = 0.4 - (0.3 \\times 0.049) = 0.3853 \\end{aligned}​w​10​1+​​=w​10​1​​−(L∗​∂w​10​1​​​​∂E​​)=0.4−(0.3×0.049)=0.3853​​ 이렇게 해서 필자는 새로운 $w^1_{10}$값인 0.3853을 얻었다. 이런 식으로 다른 $w$값을 계속 업데이트 해보자.이번에는 Layer1보다 한 층 더 깊숙히 있는 Layer0의 $w^0{10}$값을 업데이트 할 것이다. 보다시피 $w^0_{10}$은 $w^1_{10}$보다 많은 값에 영향을 미치고 있다.전체 에러 $E_t$에 $w^0_{10}$가 기여한 정도는 다음과 같이 나타낼 수 있다. ∂Et∂w100=(∂E1∂a10+∂E2∂a10)∂a10∂z10∂z10∂w100\\begin{aligned} \\frac{\\partial E_t}{\\partial w^0_{10}} = (\\frac{\\partial E_1}{\\partial a_{10}} + \\frac{\\partial E_2}{\\partial a_{10}}) \\frac{\\partial a_{10}}{\\partial z_{10}} \\frac{\\partial z_{10}}{\\partial w^0_{10}} \\end{aligned}​​∂w​10​0​​​​∂E​t​​​​=(​∂a​10​​​​∂E​1​​​​+​∂a​10​​​​∂E​2​​​​)​∂z​10​​​​∂a​10​​​​​∂w​10​0​​​​∂z​10​​​​​​ 그럼 먼저 $\\frac{\\partial E_1}{\\partial a_{10}}$부터 구해보자. ∂E1∂a10=∂E1∂a20∂a20∂z20∂z20∂a10=−(t1−a20)×a20×(1−a20)×w101=−(0.2−0.57)×0.57×(1−0.57)×0.4=0.03627\\begin{aligned} \\frac{\\partial E_1}{\\partial a_{10}} = \\frac{\\partial E_1}{\\partial a_{20}} \\frac{\\partial a_{20}}{\\partial z_{20}} \\frac{\\partial z_{20}}{\\partial a_{10}}\\\\ \\\\ = -(t_1 - a_{20}) \\times a_{20} \\times (1 - a_{20}) \\times w^1_{10} \\\\ \\\\ = -(0.2 - 0.57) \\times 0.57 \\times (1 - 0.57) \\times 0.4 \\\\ \\\\ = 0.03627 \\end{aligned}​​∂a​10​​​​∂E​1​​​​=​∂a​20​​​​∂E​1​​​​​∂z​20​​​​∂a​20​​​​​∂a​10​​​​∂z​20​​​​​​=−(t​1​​−a​20​​)×a​20​​×(1−a​20​​)×w​10​1​​​​=−(0.2−0.57)×0.57×(1−0.57)×0.4​​=0.03627​​ 마찬가지로 $\\frac{\\partial E_2}{\\partial a_{10}}$도 구해본다. ∂E2∂a10=∂E2∂a21∂a21∂z21∂z21∂a10=−(t2−a21)×a21×(1−a21)×w111=−(0.7−0.61)×0.61×(1−0.61)×0.5=−0.0107\\begin{aligned} \\frac{\\partial E_2}{\\partial a_{10}} = \\frac{\\partial E_2}{\\partial a_{21}} \\frac{\\partial a_{21}}{\\partial z_{21}} \\frac{\\partial z_{21}}{\\partial a_{10}}\\\\ \\\\ = -(t_2 - a_{21}) \\times a_{21} \\times (1 - a_{21}) \\times w^1_{11} \\\\ \\\\ = -(0.7 - 0.61) \\times 0.61 \\times (1 - 0.61) \\times 0.5 \\\\ \\\\ = -0.0107 \\end{aligned}​​∂a​10​​​​∂E​2​​​​=​∂a​21​​​​∂E​2​​​​​∂z​21​​​​∂a​21​​​​​∂a​10​​​​∂z​21​​​​​​=−(t​2​​−a​21​​)×a​21​​×(1−a​21​​)×w​11​1​​​​=−(0.7−0.61)×0.61×(1−0.61)×0.5​​=−0.0107​​ 이제 $\\frac{\\partial E_1}{\\partial a_{10}}$와 $\\frac{\\partial E_2}{\\partial a_{10}}$를 전부 구했으니 $\\frac{\\partial E_t}{\\partial w^0_{10}}$를 구할 차례이다. ∂Et∂w100=(∂E1∂a10+∂E2∂a10)∂a10∂z10∂z10∂w100=(0.03627+(−0.0107))×0.2484×0.54=0.0034\\begin{aligned} \\frac{\\partial E_t}{\\partial w^0_{10}} = (\\frac{\\partial E_1}{\\partial a_{10}} + \\frac{\\partial E_2}{\\partial a_{10}}) \\frac{\\partial a_{10}}{\\partial z_{10}} \\frac{\\partial z_{10}}{\\partial w^0_{10}} \\\\ \\\\ = (0.03627 + (-0.0107)) \\times 0.2484 \\times 0.54 \\\\ \\\\ = 0.0034 \\end{aligned}​​∂w​10​0​​​​∂E​t​​​​=(​∂a​10​​​​∂E​1​​​​+​∂a​10​​​​∂E​2​​​​)​∂z​10​​​​∂a​10​​​​​∂w​10​0​​​​∂z​10​​​​​​=(0.03627+(−0.0107))×0.2484×0.54​​=0.0034​​ 이로써 $w^0_{10}$이 전체 에러 $E_t$에 0.0034만큼 기여한다는 걸 알아냈다.이제 이 값을 사용하여 $w^0_{10}$값을 업데이트하자.Learning Rate는 아까와 동일한 0.3이다. w100+=w100−(L∗∂Et∂w100)=0.1−(0.3×0.0034)=0.09897\\begin{aligned} w^{0+}_{10} = w^0_{10} - (L * \\frac{\\partial E_t}{\\partial w^0_{10}}) = 0.1 - (0.3 \\times 0.0034) = 0.09897 \\end{aligned}​w​10​0+​​=w​10​0​​−(L∗​∂w​10​0​​​​∂E​t​​​​)=0.1−(0.3×0.0034)=0.09897​​ 코딩하기필자는 도저히 이걸 8번이나 손으로 풀 수 있는 사람이 아니기 때문에 JavaScript를 사용하여 위에 설명했던 공식을 간단하게 코드로 작성해보았다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687function sigmoid (x) { return 1 / (1 + Math.exp(-x));}function MSE (targets, values) { if (values instanceof Array === false) { return false; } let result = 0; targets.forEach((target, i) => { result += (0.5 * ((target - values[i]) ** 2)); }); return result;}// 인풋 초기화const x1 = 0.2;const x2 = 0.5;// 타겟 값 초기화const t1 = 0.2;const t2 = 0.7;// Weights 초기화const w0 = [[0.1, 0.2], [0.3, 0.1]];const w1 = [[0.4, 0.5], [0.1, 0.3]];const learningRate = 0.3;const limit = 1000; // 학습 횟수// 두번째 Layer의 Weight들을 업데이트function updateSecondLayerWeight (targetY, y, prevY, updatedWeight) { const v1 = (-(targetY - y)) + 0; const v2 = y * (1-y); const def = v1 * v2 * prevY; return updatedWeight - (learningRate * def);}// 첫번째 Layer의 Weight들을 업데이트function updateFirstLayerWeight (t1, t2, y1, y2, w1, w2, a, updatedWeight) { const e1 = (-(t1 - y1)) * y1 * (1-y1) * w1; const e2 = (-(t2 - y2)) * y2 * (1-y2) * w2; const v1 = a * (1-a); const v2 = a; const def = (e1 + e2) * v1 * v2; return updatedWeight - (learningRate * def);}// 학습 시작let i = 0;for (i; i < limit; i++) { let z10 = (x1 * w0[0][0]) + (x2 * w0[1][0]); let a10 = sigmoid(z10); let z11 = (x1 * w0[0][1]) + (x2 * w0[1][1]); let a11 = sigmoid(z11); let z20 = (a10 * w1[0][0]) + (a11 * w1[1][0]); let a20 = sigmoid(z20); let z21 = (a10 * w1[0][1]) + (a11 * w1[1][1]); let a21 = sigmoid(z21); let e_t = MSE([t1, t2], [a20, a21]); console.log(`[${i}] y1 = ${a20}, y2 = ${a21}, E = ${e_t}`); // 계산된 기여도들을 사용하여 새로운 Weight로 업데이트 const newW0 = [ [updateFirstLayerWeight(t1, t2, a20, a21, w1[0][0], w1[0][1], a10, w0[0][0]), updateFirstLayerWeight(t1, t2, a20, a21, w1[1][0], w1[1][1], a11, w0[0][1])], [updateFirstLayerWeight(t1, t2, a20, a21, w1[0][0], w1[0][1], a10, w0[1][0]), updateFirstLayerWeight(t1, t2, a20, a21, w1[1][0], w1[1][1], a11, w0[1][1])] ]; const newW1 = [ [updateSecondLayerWeight(t1, a20, a10, w1[0][0]), updateSecondLayerWeight(t2, a21, a10, w1[0][1])], [updateSecondLayerWeight(t1, a20, a11, w1[1][0]), updateSecondLayerWeight(t2, a21, a11, w1[1][1])] ]; // 업데이트된 Weight들을 반영한다 newW0.forEach((v, i) => { v.forEach((vv, ii) => w0[i][ii] = vv); }); newW1.forEach((v, i) => { v.forEach((vv, ii) => w1[i][ii] = vv); });}console.log(`t1 = ${t1}, t2 = ${t2}`); 계산된 결과들을 보면 처음에 Multi Layer Network에 넣었던 x1과 x2가 점점 t1과 t2로 수렴하는 것을 볼 수 있다.1000번 돌린 결과를 전부 볼 수는 없으니까 처음과 중간, 마지막 진행 상황을 첨부한다. 다음 포스팅에서는 좀 더 구조화한 네트워크를 만들어보려고 한다. 이상으로 Backpropagation 포스팅을 마친다.","link":"/2018/07/19/deep-learning-backpropagation/"},{"title":"클라이언트 사이드 렌더링 최적화","text":"이번 포스팅에서는 필자의 현직장에서 진행했던 클라이언트 사이드 렌더링 최적화에 대해서 적어보려고 한다. 크롬 브라우저의 Audits 탭에서 현재 페이지의 퍼포먼스나 SEO 점수와 같은 지표를 확인해볼 수 있다. 이 지표는 Google Chrome 팀에서 제공하는 Lighthouse라는 툴을 사용하여 측정된다. 또한 측정된 지표를 JSON 포맷으로 Export하여 저장하고 Lighthouse의 Report Viewer 페이지에서 다시 확인해볼수도 있다. 아래 링크들을 살펴보면 Lighthouse에 대해서 더 잘 알 수 있을 것이다. Lighthouse Github 레파지토리Google Devloper의 Lighthouse 문서Lighthouse Report Viewer 최적화를 진행하게 된 이유?필자는 회사에서 숨고라는 서비스를 개발하고 있다. 숨고에서는 1주 단위의 스프린트로 업무를 진행하고 있는데, 마침 이번 주에 다른 이슈에 의해 필자가 진행할 이슈가 병목에 걸려버려서 시간이 붕 떴다.그래서 뭘 할까 찾아보다가 숨고의 고수 찾기 페이지에서 Audits를 한번 돌렸는데 First Meaningful Paint 항목이 거의 2초가 걸리는 것을 확인했다. 숨고의 고수 찾기 페이지를 분석한 결과 해당 페이지는 SEO 최적화 대상 페이지 중 하나이기 때문에 이 참에 First Meaningful Paint 시간을 1초 아래까지 줄여보자라는 목표를 가지고 최적화를 진행하게 되었다. 사실 해당 페이지는 이 작업 이전에도 동료들과 함께 여러 차례에 걸쳐 최적화 작업을 진행해왔기 때문에 페이지 자체의 로딩 속도는 나쁜 편은 아니다. 그러나 이전의 최적화는 클라이언트보다는 렌더 서버에 초점을 맞춰서 진행해왔기 때문에 클라이언트 렌더링의 병목 지점을 확인한 것은 사실 이번이 처음이다. 문제점 파악일단 Lighthouse에서 감사 결과로 지목했던 다양한 문제점 중 지금 당장 짧은 시간 안에 해결할 수 있는 문제들을 중심으로 파악했다. 애초에 이 이슈는 스프린트에 들어갔던 이슈가 아니라 우연히 시간이 남아서 하게 된 일이기 때문에 자칫 야크 털깎기에 빠지거나 욕심을 부려서 너무 오랜 시간을 끌게되면 정작 중요한 비지니스 이슈를 처리하지 못하기 때문이다. 그 중 필자가 생각하기에 짧은 시간안에 우선 개선할 수 있는 부분은 다음과 같았다. Text 압축 미사용보통 text/html이나 application/javascript 등 텍스트나 코드로 취급되는 리소스들은 gzip 압축을 사용한다. 하지만 현재 렌더 서버는 text/html 타입만 gzip 압축을 수행하고 있었다. 오프스크린 이미지 지연이 필요함오프스크린 이미지란 코드 상에는 존재하지만 화면 밖에 있거나 CSS 스타일로 인해 감춰져 있는 등 실제로 유저에게는 보여지고 있지 않은 이미지를 의미한다. 이런 이미지는 당연히 지연 로딩(Lazy Loading)을 하는 것이 좋을 거라 생각했다. 네트워크 페이로드 크기가 너무 크다이 문제는 상기한 Text 압축 미사용과 관련있다. 말 그대로 리소스를 한번 요청했을 때 받아와야하는 데이터의 크기가 너무 크다는 것을 의미한다. 이 이슈는 gzip 압축과 Code Chunking 등으로 해결할 수 있다. 기본 스레드 작업 최소화 하기브라우저에서 웹 애플리케이션을 초기화 할 때 JavaScript를 실행하는 시간이 너무 오래 걸려서 화면에 요소를 렌더링하는 행위에 병목이 발생한 것을 의미한다. 이 문제를 해결하기 위한 방법은 진짜 마이너한 최적화부터 조금만 손봐도 큰 효과를 볼 수 있는 방법까지 여러가지 방법이 떠올랐는데, 마이너한 최적화는 사실 하나마나므로 최소 비용으로 최대의 효과를 얻을 수 있는 방법을 골라야 했다. 웹폰트가 로드되는 동안 텍스트가 계속 표시되는지 확인하기아무런 조치도 취하지 않았을 경우 브라우저들은 각각의 정책에 따라 웹폰트를 렌더하는 방법이 다르다. Chrome, Firefox, Safari, Opera 같은 Webkit 진영의 브라우저들은 웹폰트의 다운로드가 완료될 때 까지 폰트가 적용된 텍스트를 보여주지 않는 FOIT(Flash of Invisible Text) 방식으로, IE, Edge는 웹폰트의 다운로드가 완료될 때 까지 텍스트를 기본 폰트가 적용된 상태로 노출시키는 FOUT(Flash of Unstyled Text) 방식으로 웹폰트를 렌더한다. 그렇기 때문에 Lighthouse는 사용자가 웹폰트 다운로드 완료 여부와 상관없이 페이지의 내용을 확인할 수 있는 FOUT 방식을 사용할 것을 권고하였다. 조치이미지 지연 로딩이미지 지연 로딩은 HTML5의 IntersectionObserver를 사용하면 간단하게 구현할 수 있다. 그러나 한가지 걱정되었던 것은 “지연 로딩을 하면 구글의 Search Engine Bot이 이미지를 긁어가지 못하는 것이 아닌가?”라는 것이다. 그래서 다른 사람들은 어떤 방식으로 생각하는 지 알고 싶어 리서치를 조금 해봤는데 태그를 사용하는 방법, XML을 사용하여 이미지 사이트맵을 만드는 방법 등 몇가지 방법이 있었고 혹자는 구글의 Search Engine Bot이 지연 로딩을 사용하여 불러오는 이미지도 전부 인덱싱하기 때문에 신경안써도 된다는 얘기도 있었다. 그래서 이 문제에 관해 PO(Product Owner)와 토의해보았다. 그 결과 어차피 고수 찾기 페이기에 있는 이미지는 유저 프로필 이미지 밖에 없고, 이 이미지들은 인덱싱이 되든 안되든 SEO에 큰 영향이 없을 것으로 판단되어 별도의 조치 없이 그냥 지연 로딩을 적용하기로 하였다. 숨고 프론트엔드는 Vue를 사용하고 있기 때문에 Vue에서 제공해주는 Directive와 HTML5의 IntersectionObserver API를 사용하여 이미지 지연 로딩 기능을 구현하였다. 일단 유저의 프로필 이미지를 렌더하고 있는 컴포넌트 내부에서 유저 프로필을 img 태그가 아니라 CSS의 background-image 속성을 사용하여 렌더하고 있기 때문에 필자가 생각했던 이 디렉티브의 인터페이스는 대충 이랬다. 123456 { entries.forEach(entry => { if (entry.isIntersecting) { const imageURL: string = entry.target.getAttribute('data-lazy-background-image'); if (!imageURL.length) { return; } entry.target.style.backgroundImage = imageURL; observer.unobserve(entry.target); } }); }) : null; IntersectionObserver의 constructor는 요소가 뷰포트로 들어오거나 벗어나는 등 행위가 발생했을 때 Observer가 호출할 콜백 함수를 전달 인자로 받는다. 과거 scroll 이벤트로 비슷한 행동을 처리했지만 이벤트는 동기적으로 반응하기 때문에 메인 스레드의 응답성, 간단하게 말하면 퍼포먼스에 영향을 준다. scroll 이벤트는 워낙 빈번하게 호출되기 때문에 scroll 이벤트 핸들러에서 조금만 많은 처리를 해도 화면이 뚝뚝 끊기는 등 문제가 발생하는 모습을 볼 수 있다.(이벤트 옵션 중 passive 옵션을 사용하면 어느 정도 이 현상을 방어할 수는 있다.) 그러나 Observer는 비동기적으로 동작하기 때문에 메인 스레드의 처리 스택과 독립적으로 실행된다. 브라우저 지원률이 90%가 안된다는 점만 해결된다면 진짜 좋을 텐데…이제 Observer를 구현했으니 디렉티브를 뚝딱 만들면 된다. lazy-background-image.directive.ts1234567891011121314151617181920import { Vue } from 'vue-property-decorator';import { VNode, VNodeDirective } from 'vue';Vue.directive('lazy-background-image', { bind (el: any, binding: VNodeDirective, vnode: VNode) { if (isSupportIntersectionObserver) { if (!el.style.backgroundImage) { return; } el.setAttribute('data-lazy-background-image', el.style.backgroundImage); el.style.backgroundImage = ''; intersectionObserver.observe(el); } }, unbind (el: any) { if (isSupportIntersectionObserver) { intersectionObserver.unobserve(el); } },}); 이렇게 디렉티브를 사용하여 이미지 지연 로딩을 한 결과 최초 요청하는 이미지 개수를 60개에서 39개로 대폭 줄일 수 있었다. 컴포넌트 지연 로딩숨고 프론트엔드는 SEO를 위해 첫 요청은 SSR(서버사이드렌더링)을 하지만 그 외에는 일반적인 SPA와 동일한 방식으로 작동한다. 그렇기 때문에 처음 애플리케이션이 초기화될 때 애플리케이션 내에서 사용될 모든 JavaScript와 CSS를 받아온다. 이 방식은 애플리케이션이 작을 때는 딱히 문제가 되지 않지만 애플리케이션이 커질수록 번들의 용량도 비례하여 늘어나므로 점점 부담이 되기 마련이다. 그래서 Webpack에서 제공하는 기능인 Dynamic Import를 사용하여 해당 페이지에서 사용하는 코드만을 비동기적으로 로드하여 사용하도록 변경하였다. router/search.ts12345// Syncimport SearchPro from '@/pages/Search/SearchPro';// Asyncconst SearchPro = () => import(/* webpackChunkName: \"search\" */ 'src/pages/Search/SearchPro'); webpackChunkName 주석을 사용하면 일정 단위의 모듈들을 하나의 청크로 묶어줄 수 있다. 숨고는 현재 HTTP/1.1 프로토콜을 사용하고 있으므로 한번에 요청할 수 있는 리소스의 개수가 6개 정도로 한정되어있다.(이 개수는 브라우저의 정책에 따라 조금씩 다르다)청크의 개수가 너무 많아지면 오히려 로딩 속도가 느려질 수 있으므로 관련있는 모듈을 묶어주어 청크의 개수가 너무 많아지지 않도록 조절하였다. 여기까지는 솔직히 별로 어려울 것도 없고 순조로웠는데 CSS를 별도의 번들로 분리하기 위해 사용하는 mini-css-extract-plugin에서 문제가 발생했다. 이 플러그인이 Dynamic import로 불러온 CSS 모듈을 처리하는 방식 때문에 SSR 사이클에서 ReferenceError: document is not defined라는 참조 에러가 발생했던 것이다. node_modules/mini-css-extract-plugin/dist/index.js123456var linkTag = document.createElement(\"link\");linkTag.rel = \"stylesheet\";linkTag.type = \"text/css\";linkTag.onload = resolve;linkTag.href = fullhref;head.appendChild(linkTag); 원래 코드는 엄청 거대하지만 간단하게 한번 추려보자면 여기가 문제가 발생한 부분이다.SSR 사이클은 NodeJS 프로세스에서 실행되므로 당연히 document고 나발이고 없기 때문에 참조 에러가 발생한 것이다. 다행히 빌드 설정은 client.config와 server.config로 나눠서 관리되고 있기 때문에 적절한 조치를 취해줄 수 있었다. 클라이언트 사이드 렌더링 사이클은 문제 없으므로 최초 요청 시 Express가 Vue를 컴파일할때만 손봐주면 된다. 역시 StackOverflow에 필자와 같은 삽질을 했었던 전 세계의 개발랭이들이 이미 열띤 토론을 통해 mini-css-extract-plugin의 SSR 관련 이슈에 대해 어느 정도 결론을 내놓은 것을 발견했다. 위아 더 월드. 다양한 방법들이 논의되었지만 필자는 css-loader의 exportOnlyLocals 옵션을 사용하는 방법을 선택했다. 해당 이슈에는 css-loader/locals로 사용하라고 되어있지만 이 이슈가 논의된 이후에 css-loader가 업데이트 되었기 때문에 이제는 옵션 객체를 사용해야 한다. webpack.server.config.js12345678910111213141516171819module.exports = merge(baseConfig, { // ... module: { rules: [ { test: /\\.scss$/, use: [ { loader: 'css-loader', options: { exportOnlyLocals: true } }, 'postcss-loader', 'sass-loader' ] }, ] } // ...}) 이렇게 문제는 해결했지만 일단 이 기술의 안정성이 프론트엔드 챕터 내에서 충분히 확인되지 않았다고 판단되어 일단 원래 목표였던 고수 찾기 페이지에 관련된 컴포넌트들만 지연 로딩하고 있는 중이다. 향후 안정성이 검증되면 점진적으로 커버리지를 넓혀 적용할 예정이다. 텍스트 컨텐츠 gzip 압축 적용이건 그냥 Nginx 설정에 gzip 관련 설정들을 추가해주면 된다. 1234567891011server { # ... gzip on; gzip_disable \"msie6\"; gzip_proxied any; gzip_comp_level 6; gzip_buffers 16 8k; gzip_http_version 1.1; gzip_types text/plain text/css application/json application/javascript text/xml application/xml text/javascript;} 사실 숨고는 IE6 지원 따위 하지도 않지만 유저가 아예 페이지를 못보는 상황보다는 망가진 페이지라도 보는게 낫다고 생각하여 혹시 몰라 추가해두었다. 컴포넌트 지연 로딩과 텍스트 컨텐츠 gzip 압축 적용을 마치고나서 한번 번들의 크기와 로딩 속도를 확인해 보았다. JSBeforeAfter CSSBeforeAfter JS before의 제일 상단에 있는 변태같은 1.2MB 크기의 번들이 node_modules 라이브러리를 묶어놓은 vendor이다. 항상 이 놈을 볼때마다 눈물이 나고 마음이 아팠는데 이렇게 간단하게 끝낼 수 있는 걸 왜 이리 시간을 끌었나 싶었다. SEO가 필요한 페이지는 인증 과정을 기다리지 않고 바로 페이지를 렌더하도록 변경사실 이건 숨고의 기술 부채와도 관련이 있는 내용이다. 현재 로그인한 사용자의 정보를 받아오는 API가 엄청 느리다. 보통 때는 응답 시간이 1.5초 정도이고 트래픽 피크 타임때는 2초에 가까워 지는 경우도 있다. 이건 DB 스키마와도 관련있는 깊은 기술 부채이기 때문에 짧은 시간 안에 해결하기에는 조금 힘든 문제다. 그렇다 하더라도 외부 네트워크 요인 때문에 사용자가 화면을 볼 수 있는 시간이 2초씩이나 딜레이된다는 것은 너무 아깝다. 근데 생각을 해보자. SEO가 필요한 페이지는 무조건 비로그인 유저도 볼 수 있는 페이지이다. 응? 사실 너무나 당연한 사실인데 놓치고 있었다. 그럼 이런 의식의 흐름이 생긴다. SEO가 중요한 페이지의 로딩 시간을 단축시키는 것이 목표였다. SEO가 되고 있는 페이지는 애초에 비로그인 유저도 접근 가능한 페이지다. 인증 API를 기다릴 필요가 없다…? 그렇다면 그냥 client-entry.js12345678910// ...router.onReady(async () => { if (isAllowGuestPage(router.currentRoute)) { init(); } else { await init(); } // ...}); 이렇게 바로 질러버린다. 라우터 퍼미션을 검사해서 비회원 유저도 접근 가능한 페이지는 init의 프로미스를 기다리지 않고 다음 초기화 로직을 실행하도록 변경하였다. 그 결과 비회원 접근 가능 페이지들의 로딩 속도가 1초 정도 더 빨라진 것을 확인할 수 있었다. Before After 2513.9ms 1111.5ms 결과이렇게 이것 저것 열심히 했더니 그래도 조금 빨라지긴 했다. Before After 원래 목표였던 First Meaningful Paint를 1초 아래로 떨어트리는 목표는 달성했지만 다른 수치가 이 정도로 영향을 안받을 줄은 몰랐다. 퍼포먼스 점수도 꼴랑 2점 올라갔다. 다른 팀원들은 “그래도 2점이 어디야~”라고 해주셨지만 뭔가 마음 한켠이 찜찜하다…다음에는 다른 점수를 좀 더 올려보는 걸 목표로 삼아봐야겠다. PWA 세팅해놓으면 Accessibility 점수는 좀 더 올라갈 것 같기도 한데 생각해놓은 다른 이슈들은 백엔드 개발자 분들의 도움이 필요한 것들이 꽤 있어서 혼자서는 힘들 듯하다. 이상으로 클라이언트 사이드 렌더링 최적화 포스팅을 마친다.","link":"/2019/06/03/client-render-optimizing/"},{"title":"우리 집에서 구글까지 가는 길","text":"이번 포스팅에서는 우리 집에서 구글까지 어떤 과정을 통해 통신을 하는지에 대해서 간략하게 얘기해보려고 한다. 필자가 대학에서 배운 것 중 재밌다고 생각했던 것이 몇 개 있는데, 그 중 대표적인 것이 바로 인터넷에 연결되어 있는 모든 컴퓨터는 실제로 케이블을 통해 연결되어있다라는 사실이었다. 혹자는 당연하다고 생각할 수 있는 이 사실이 그땐 굉장히 신기했던 것 같다. 매일매일 인터넷을 사용하고 있지만 너무 생활 속에 자연스레 녹아있는 요소이다 보니 그 원리에 대해서는 그다지 신경쓰지 않았기 때문이다. 그러다가 학교에서 네트워크에 대해 점점 자세히 배우면서 내가 당연하게 사용하고 있는 이 혜택들이 사실은 굉장히 체계적이고 복잡한 시스템이구나라는 사실을 알게 되었다. 그래서 이번 포스팅에서는 필자의 집에서 구글 서버에 접속하는 과정을 통해 그 시스템에 대해서 간단하게 풀어보려고 한다. 물론 IP 주소를 기반으로 대략적인 추정을 하는 것이기 때문에 정확한 정보는 아니겠지만 대략적으로 이런 과정을 통해 해외에 있는 서버에 접속할 수 있다는 사실을 중심으로 풀어갈 것이다. IP, Internet Protocol먼저 우리 집에서 구글까지의 경로를 알기 위해서 인터넷하면 빠질 수 없는 IP에 대해서 간단하게 알아보자. IP 주소라는 단어는 컴퓨터 관련 전공을 하신 분이 아니더라도 굉장히 익숙한 단어이다. 일반적인 환경에서 우리가 인터넷에 접속할 때 이 IP 주소가 우리 집 주소의 역할을 한다고 생각하면 된다. 마찬가지로 우리 집에서 구글까지 가는 길목에 있는 지점들도 모두 IP 주소를 가지고 있고, 우리는 그 IP 주소를 기반으로 이 지점이 어디인지 대략적으로 추측해볼 수 있다. IP에 관해서 굉장히 기본적인 내용을 설명할 예정이므로 이미 다 아시는 분은 그냥 건너뛰셔도 무방하다. IPv4의 구조일반적으로 우리가 접할 수 있는 192.168.0.1과 같은 형태는 IPv4이다. IPv4는 8비트로 구성된 4개의 필드로 구성되고 이 하나하나의 필드를 옥탯(Octet)이라고 부른다(참고로 Oct으로 시작하는 용어는 8을 뜻하는 게 많다). 예를 들어 192.168.0.1은 이진법으로 전환하면 다음과 같이 나타낼 수 있다. 1210진법: 192.168.0.12진법: 11000000.10101000.00000000.00000001 한 옥탯에 8비트밖에 가지지 못하기 때문에 옥탯은 00000000 ~ 11111111, 즉 십진법으로는 0~255의 수를 가지게 되는 것이다. 그렇기 때문에 IPv4로 생성할 수 있는 주소의 개수는 $2^{32} = 4,294,967,296$로 약 43억개이다.43억개라니… 꽤 큰 숫자 같아 보인다. 처음에 이 주소 체계를 만들 때만 해도 이런 생각을 했던 것 같다. 에이 43억개나 있는데 어느 정도는 버티겠지? ... 생각보다 전세계의 인터넷 사용자가 빠르게 늘면서 결국 2011년 2월 4일부로 모든 주소가 소진되어 현재는 IPv4의 할당이 중지되었다. 다행히 그 동안 손놓고 있지는 않았고, 전세계의 똑똑한 분들이 이미 대책을 다 세워두었으니 너무 걱정하진 말자. IPv4의 클래스이렇게 IP 주소는 무제한으로 막 퍼줄 수 있는 자원이 아니라 유한한 자원이다 보니, 특정 대역마다 사용처를 나누어서 관리하게 된다. 이때 나누어진 대역을 클래스라고 부르며 A~E의 총 5개의 클래스로 나누어서 관리한다. 각 클래스는 이진법으로 표현한 IPv4인 00000000.00000000.00000000.00000000 중 첫번째 필드에 있는 8비트에 제한을 만들어서 표기하기 때문에 첫번째 필드의 숫자만 보면 해당 IPv4 주소가 어느 클래스에 속해있는 지 알 수 있다. 이 제한은 바로 첫번째 필드의 최상위 비트를 강제하는 방식이다. 예를 들어 A 클래스는 무조건 비트가 0으로 시작해야하기 때문에 00000000(0)에서 01111111(127)사이의 첫번째 필드를 사용할 수 있고 C 클래스는 무조건 비트가 110로 시작해야하기 때문에 11000000(192)부터 11011111(223)까지의 첫번째 필드를 사용할 수 있는 식이다. 클래스 첫번째필드 주소범위 용도 A 0xxxxxxx 0.0.0.0 ~ 127.255.255.255 대규모 네트워크 B 10xxxxxx 128.0.0.0 ~ 191.255.255.255 중규모 네트워크 C 110xxxxx 192.0.0.0 ~ 223.255.255.255 소규모 네트워크 D 1110xxxx 224.0.0.0 ~ 239.255.255.255 멀티캐스트 E 1111xxxx 240.0.0.0 ~ 255.255.255.255 연구/개발 또는 미래 대비로 예약 특수한 목적의 IPv4 주소이렇게 클래스를 나누고 그 클래스 내부에서도 특수한 용도를 위해 미리 예약해놓은 IPv4 주소들이 있다. 이 예약 주소들은 RFC 5735에 의해 정의되어 있으며 목적에 벗어난 용도로 할당 받는 것이 불가능하다. 이 포스팅에서는 간략하게 대표적인 몇가지만 설명하고 넘어가겠다. 클래스 주소 용도 A 0.0.0.0 ~ 0.255.255.255 할당되지 않은 메타 주소 A 10.0.0.0 ~ 10.255.255.255 대규모 사설 네트워크 A 100.64.0.0 ~ 100.127.255.255 Carrier-grade NAT용 주소 A 127.0.0.0 ~ 127.255.255.255 자기 자신. 일명 루프백(Lookback) C 192.168.0.0 ~ 192.168.255.255 소규모 사설 네트워크. 공유기를 쓰면 많이 보인다. 우리 집에서 구글까지의 경로를 보자자, 드디어 이 포스팅의 메인 내용을 시작할 수 있게 되었다. 집에서 맥북을 열고 웹 브라우저에 google.com을 입력하면 웹 브라우저는 전 세계 어딘가에 있는 구글 서버에 페이지를 보내달라는 요청을 보낼 것이다. 하지만 필자의 맥북과 구글 서버까지는 직접 연결되어 있지 않다. 그렇기 때문에 필자가 보낸 요청은 구글 서버까지 가기 위해 여기저기를 들러가면서 험난한 여정을 겪는다. 그렇다면 이제 필자가 집에서 보낸 요청이 구글 서버까지 도달하기 전에 어디를 들리는 지 한번 알아보자. traceroute 사용하기필자는 Unix 기반의 MacOS를 사용하고 있기 때문에 MacOS 기준으로 설명하겠다. traceroute 유틸리티를 사용하면 내가 보낸 패킷이 어떤 경로를 통해 구글 서버까지 도달하는 지 알아볼 수 있다. 12345678910111213$ traceroute -q 1 google.comtraceroute to google.com (172.217.25.196), 64 hops max, 52 byte packets 1 192.168.25.1 (192.168.25.1) 3.077 ms 2 211.0.0.0 (211.0.0.0) 5.928 ms 3 100.71.51.17 (100.71.51.17) 4.693 ms 4 10.44.255.240 (10.44.255.240) 4.328 ms 5 10.222.18.10 (10.222.18.10) 7.319 ms 6 10.222.16.201 (10.222.16.201) 4.883 ms 7 10.222.16.41 (10.222.16.41) 3.853 ms 8 72.14.204.124 (72.14.204.124) 34.784 ms 9 108.170.242.193 (108.170.242.193) 37.751 ms10 108.170.233.79 (108.170.233.79) 36.643 ms11 nrt13s50-in-f4.1e100.net (172.217.25.68) 35.416 ms 두번째 홉은 필자의 디테일한 위치가 노출될 수 있는 공인 IP를 출력하므로 보안 상 마스킹했다. 사실 필자가 사용하고 있는 ISP(Internet Service Provider)인 SK 브로드밴드는 관리 지역 내 IP 주소 임대기간이 1시간이기도 하고 필자는 인구밀도가 높은 곳에 거주하고 있으므로 저 IP 주소가 노출된다고 해도 위치를 알기는 쉽지 않지만 그래도 불안하기 때문에 가린다. 누가 우리 집 알아내면 어떡해… traceroute의 결과로 출력되는 하나의 row를 홉(Hop)이라고 부르는데, 필자의 집에서부터 구글의 서버까지 필자의 패킷이 거쳐간 경로를 의미하는 것이다. 홉의 IP 주소 오른쪽에 출력된 시간은 홉이 응답한 시간을 의미한다. 재밌는 것은 8번 홉부터 갑자기 응답 시간이 한자리 수에서 두자리 수로 확 뛴다는 것인데, 여기서부터는 패킷이 국내를 벗어나 해외로 빠져나간 것이다. 그렇다면 이 IP 주소들은 어디를 의미하는 걸까? 좀 더 자세히 알아보자!이제 이 IP 주소들이 각각 어디를 의미하는지, 어떤 회사가 소유하고 있는 어떤 디바이스인지 좀 더 자세히 알아보고 싶다. 간단하게 알아보는 방법은 whois 유틸리티를 사용하는 방법이 있다. 1234567$ whois 172.217.25.68NetRange: 172.217.0.0 - 172.217.255.255CIDR: 172.217.0.0/16NetName: GOOGLENetHandle: NET-172-217-0-0-1Parent: NET172 (NET-172-0-0-0-0)NetType: Direct Allocation MacOS의 whois 유틸리티는 5개의 whois 서비스에 요청을 보내 받아온 결과를 출력해주는 방식이다. 참고로 대한민국의 whois 서비스는 KISA(한국인터넷진흥원)의 후이즈검색에서 제공하고 있다. whois 도메인 담당 구역 whois.arin.net 아메리카 whois.nic.mil 미국(현재 작동안함) whois.ripe.net 유럽 whois.apnic.net 아시아 태평양 whois.jprs.jp 일본 그러나 whois 서비스는 기본적으로 도메인이 등록된 IP 주소의 소유주에 대한 정보를 보여주는 서비스라서 도메인이 등록되지 않은 게이트웨이나 라우터에 대한 정보는 나오지 않는다. 그래서 필자는 IP2Location이라는 사이트를 이용하여 한번 찾아볼 것이다.(근데 이것도 잘 나올 것 같지는 않다) 먼저 첫번째 홉인 192.168.25.*은 필자 집의 공유기에서 할당해준 사설 IP 이고 두번째 홉은 ISP(Internet Service Provider)가 필자에게 부여해준 공인 IP이나, 보안 상 마스킹 했으므로 세번째 홉인 100.71.51.17부터 한번 살펴보겠다. 100.71.51.17 앗 역시 생각보다 정보가 별로 안나왔다. 대신 해당 IP 주소가 Carrier Grade NAT에 사용된다는 정보를 출력했다. 위에서 설명한 대로 A 클래스의 100.64.0.0 ~ 100.127.255.255 대역이 Carrier Grade NAT에 사용된다는 것은 이미 표준으로 정의된 것이기 때문에 사실 뻔한 결과를 출력한 것이다. 그럼 Carrier Grade NAT이 뭐길래 중간에 필자의 패킷이 들렀다 가는걸까? Carrier Grade NAT우선 NAT은 Network Address Translation의 약자로 대개 사설 네트워크에 속한 여러 개의 호스트가 하나의 공인 IP를 사용하여 인터넷에 접속하기 위한 기술이다.필자는 SK 브로드밴드의 IPTV나 Wi-Fi 등 여러 개의 서비스를 사용 중이다. 그렇다고 IP 주소를 가입한 서비스마다 나눠주기에는 주소가 매우 부족한 상황이기 때문에 SK 브로드밴드는 필자에게 단 하나의 공인 IP 주소를 할당해주고 NAT 기능이 있는 공유기를 집에 설치해, 필자의 집에 사설 네트워크(Private Network)를 구축해준 것이다. Carrier Grade NAT(CGN)은 NAT의 스케일 큰 버전이라고 생각하면 된다. 필자의 집에 설치된 공유기는 필자의 집 내부에 사설망을 구성하는 데에 그치지만 CGN은 백본망에 구성되는 NAT이다. 쉽게 말하면 거대 공유기랄까…? 필자가 알기로는 원래 스마트폰의 LTE같이 모바일 망에 직접 연결되는 무선 인터넷 디바이스를 NAT로 관리하기 위해 백본망에서 CGN을 운영한다고 알고 있었는데, 왜 집에 있는 공유기를 사용 중인 필자의 패킷이 CGN을 타는지는 잘 모르겠다. SK 브로드밴드가 IPv4가 많은 편은 아니긴 한데… 너무 모자라서 유선 인터넷에도 CGN을 달았나…?(혹시 아시는 분은 좀 알려주세요…궁금해요…) 10.44.255.240 ~ 10.222.16.41여기서부터는 A클래스 사설망에 할당되는 IP이므로, 추측하건데 SK 브로드밴드의 백본망 내부인 것 같다. ISP들은 자신들의 망 구조를 자세히 알려주지 않기 때문에 정확히 어디에 위치한 노드인지는 알 수 없지만 SK 브로드밴드의 망 구조 상 대략 이쯤 되는 것 같다. 여기서 부터 다른 국내 ISP와 연결되거나 해외망으로 연결된다 72.14.204.124 ~ 108.170.233.79 드디어 구글이 소유하고 있는 디바이스가 나타났다! 근데 위치가 네덜란드 암스테르담이다. 구글은 미국 캘리포니아에 있는 회사인데 왜 뜬금없이 암스테르담이 출현했는지 모르겠다. 뭔가 이상하기 때문에 유럽을 담당하는 whois 서비스인 whois.ripe.net에 물어봐야겠다. 12345whois -h whois.ripe.net 72.14.204.124inetnum: 69.194.128.0 - 76.255.255.255netname: NON-RIPE-NCC-MANAGED-ADDRESS-BLOCKdescr: IPv4 address block not managed by the RIPE NCC 역시 여기서 관리하고 있는 주소가 아니라고 한다. 그렇다면 아메리카를 담당하는 whois.arin.net에 물어보자. 12345678910111213whois -h whois.arin.net 72.14.204.124NetRange: 72.14.192.0 - 72.14.255.255CIDR: 72.14.192.0/18NetName: GOOGLENetHandle: NET-72-14-192-0-1Parent: NET72 (NET-72-0-0-0-0)NetType: Direct AllocationOriginAS:Organization: Google LLC (GOGL)RegDate: 2004-11-10Updated: 2012-02-24Ref: https://rdap.arin.net/registry/ip/72.14.192.0 오 이번에는 제대로 된 정보가 나왔다. 72.14.204.124 주소를 가진 디바이스는 구글이 소유하고 있는 무언가인듯하다. IP2Location은 이제 필자에게 신뢰를 잃었기 때문에 가차없이 버리기로 하고 DB IP라는 새로운 솔루션을 사용하겠다.DB IP를 사용하여 검색해보니 위치가 Ashburn, Loudoun, Virginia, United States로 잡히고 Connection Type은 Hosting으로 되어있다. 그 이후 9번 홉인 108.170.242.193과 10번 홉인 108.170.233.79 또한 버지니아 주에 위치한 호스팅 서버인 것으로 표시되었다. 여기에 도대체 뭐가 있길래 다 여기에 옹기종기 모여있는걸까…?조금 구글링을 해보니 구글이 Ashburn에서 일할 사람을 찾고 있는 공고를 발견했다. 또한 같은 버지니아 주의 옆 동네인 Sterling과 Reston으로도 채용공고가 많이 올라와있다. 날 뽑아줘요…엉엉 공고를 쭉 스캔해보니 주로 Network Engineer, Administrative Business Partner, Data Center, Google Cloud와 같은 키워드가 많이 보이는 걸로 봐서는 아무래도 여기에 구글 데이터센터가 있나보다. 좀 더 정보를 찾아보니 구글이 버지니아에서 일하는 인원을 두배 늘린다는 기사도 올해 2월에 작성되었었다. 여기에 구글 데이터센터가 있는 것은 아무래도 확실한 것 같다. 172.217.25.68(google.com) 자 드디어 마지막 google.com 도메인의 종착점인 172.217.25.68 주소를 가진 서버이다. 이 주소도 역시 DB IP를 통해 검색해본 결과 일본에 있는 서버라고 한다. 사실 한국에 있는 필자가 미국에 있는 서버에서 계속 정보를 받는 것보다 가까운 일본에 있는 서버에서 정보를 받는 게 훨씬 빠르기 때문에 이건 당연하긴 하다. 필자의 패킷은 태평양을 건너 미국에 갔다가 다시 일본으로 건너가는 기묘한 루트를 그리고 있었다는 것이다… 자세하게 파고 들어가면 왜 이런 루트를 그리는지 알 수 있겠지만 너무 포스팅이 길어질 것 같아 여기까지만 파보겠다.(다음에 좀 더 자세히 파보는 걸로…)늘 아무 생각없이 접속하던 구글도 자세히 들여다보면 이렇게 다양한 경로를 통해 나에게 전달된다. 새삼 신기한 사실이다. 이상으로 우리 집에서 구글까지 가는 길 포스팅을 마친다.","link":"/2019/06/22/my-home-to-google/"},{"title":"JavaScript의 let과 const, 그리고 TDZ","text":"이번 포스팅에서는 JavaScript ES6에서 추가되었던 let과 const 키워드에 대해서 자세히 포스팅하려고 한다. 부끄럽지만 지금까지 필자는 let과 const는 호이스팅이 되지 않는다고 생각하고 있었다. 하지만 얼마 전 친구와 대화하던 중에 let과 const도 호이스팅 대상체이지만 TDZ라는 특수한 영역을 사용하여 참조를 방어하는 것임을 알게 되었다. 다른 분: 근데 var와 다르게 let이랑 const는 왜 참조 에러가 발생하는 건가요?필자: let이랑 const는 호이스팅 안될 거에요.친구: let이랑 const도 호이스팅 되는데…? TDZ에 들어가 있어서 참조 에러 나는거야필자: 된다고??? TDZ는 또 뭐여? 호…호이스팅이 된다고…? 이런 부끄러운 일을 겪고 이번 기회에 변수 선언 키워드들에 대해 제대로 공부도 할겸 이번 포스팅을 작성하게 되었다. var 키워드var키워드는 JavaScript ES5까지 변수를 선언할 수 있는 키워드로 사용되었다. var 키워드는 다른 언어랑 조금 다른 방식으로 작동했기에 다른 언어를 사용하다가 JavaScript로 처음 입문한 개발자들의 멘탈을 털어버리는 데 혁혁한 공을 세웠다. var 키워드의 특징은 다음과 같다. 변수의 중복 선언이 가능하다.123var name = 'Evan';var name = 'Evan2';console.log(name) // Evan2 이 코드는 변수 선언부가 가까이 붙어있으니 한눈에 아 name이 두번 선언되었구나라고 한눈에 알 수 있지만 첫번째 선언부와 두번째 선언부 사이에 500줄의 코드가 있다면 이제 문제가 심각해진다. 이런 변수의 중복 선언 허용은 의도하지 않은 변수의 변경이 일어날 가능성이 충분하다. 호이스팅 당한다.호이스팅은 쉽게 얘기해서 스코프 안에 있는 선언들을 모두 스코프의 최상단으로 끌어올리는 것을 의미한다. 호이스팅은 JavaScript 인터프리터가 코드를 해석할 때 변수 및 함수의 선언 처리, 실제 코드 실행의 두단계로 나눠서 처리하기 때문에 발생하는 현상인데 이게 또 굉장히 사람 헷갈리게 만든다. 12console.log(name); // undeinfedvar name = 'Evan'; 필자도 처음 JavaScript를 시작했을 때 아니 이게 왜 참조 에러가 안나지?라고 생각했었다. 사실 호이스팅이 발생하면서 이 코드는 아래와 같은 방식으로 동작한다. 1234var name; // 선언부를 제일 위로 끌어올린다.console.log(name);name = 'Evan'; 물론 이건 JavaScript 인터프리터가 내부적으로 코드를 이런 방식으로 처리한다는 거지 실제 코드 라인이 변경되거나 하는건 아니다.어쨌든 이 호이스팅을 당하게 되면(호이스팅은 왠지 “당한다”는 표현이 잘 어울린다. 나도 여러번 당했기 때문에…) 인터프리터 언어임에도 불구하고 개발자가 코드를 읽는 순서와 코드가 실행되는 순서가 달라지게 되는 것이기 때문에 JavaScript에 입문할 때 헷갈리게 만드는 요인 중 하나다. 함수 레벨 스코프대부분의 프로그래밍 언어는 블록 레벨 스코프를 사용하지만 JavaScript는 역시 다르다. var 키워드로 선언된 변수는 함수 레벨 스코프 내에서만 인정된다. 이건 사실 JavaScript에 익숙한 개발자들이라면 큰 문제가 되지는 않지만 역시 늅늅이 시절에는 굉장히 헷갈린다. 1234567(function () { var local = 1;})();console.log(local); // Uncaught ReferenceError: local is not definedfor (var i = 0; i < 10; i++) {}console.log(i); // 10 함수 스코프만 인정되기 때문에 심지어 for 문 내부에서 선언한 변수 i도 외부에서 참조 가능하다. var 키워드 생략 가능변수를 선언할 때 var 키워드를 붙혀도 되고 안붙혀도 된다. 역시 자유의 상징 JavaScript 답다. 너무 자유로워서 개발자가 한시도 긴장의 끈을 놓을 수 없게 만든다. 12345678var globalVariable = 'global!';if (globalVariable === 'global!') { globlVariable = 'global?' // 오타 냄}console.log(globalVariable) // global!console.log(globlVariable) // global? 실수로 globalVariable 변수를 globlVariable 변수로 오타를 냈다. 순진한 개발자는 globalVariable 변수의 값이 global?로 변경되었으리라 기대를 하겠지만 아쉽게도 그 값은 오타낸 변수명인 globlVariable이 가져갔다. 이런 경우도 간단한 코드에서는 디버깅이 쉽지만 조금만 코드가 복잡해져도 눈물이 흐르는 케이스이다. 이런 것들에게 한번 걸리면 얄짤없이 11시까지 일하고 택시타고 집에 가야한다 let과 const 키워드의 등장var 키워드의 경우 전역 변수를 남발하기가 쉽고 또 로컬 변수라고 해도 변수의 스코프가 너무 넓기 때문에 변수의 선언부와 호출부가 너무 멀리 떨어져 있거나 값이 의도하지 않게 바뀌는 경우를 추적하기 힘들다.그래서 2015년에 발표된 JavaScript ES6에는 새로운 변수 선언 키워드로 let과 const가 추가되었다. let 키워드는 var와 마찬가지로 변수를 선언할 때 사용하는 키워드이고 const 키워드는 상수를 선언할 때 사용하는 키워드이다.즉 const 키워드는 리터럴 값의 재할당이 불가능하다. 12const callEvan = 'Hello, Evan!';callEvan = 'Bye, Evan!'; // Uncaught TypeError: Assignment to constant variable. 그럼 이 친구들이 기존의 var 키워드와 다른 점은 무엇일까? 위에서 설명했던 var 키워드의 특징은 변수의 중복 선언이 가능하다. 호이스팅 당한다. 블록 레벨 스코프가 아닌 함수 레벨 스코프를 사용한다 var 키워드는 생략이 가능하다. 이상 4개 였다. 여기에 대조되는 let과 const 키워드의 특징부터 먼저 살펴보자. let과 const 키워드의 특징우선 위에서 설명한 var 키워드의 특징과 대조되는 점 부터 살펴보자. 변수의 중복 선언이 불가능하다.이 친구들은 var 키워드와는 다르게 한번 키워드를 사용해서 선언한 변수는 재선언이 불가능하다. 12345678var name = 'Evan';var name = 'Evan2'; // 아무 일도 일어나지 않았다...let name = 'Evan';let name = 'Evan2'; // Uncaught SyntaxError: Identifier 'name' has already been declaredconst name = 'Evan';const name = 'Evan2'; // Uncaught SyntaxError: Identifier 'name' has already been declared 이로써 나도 모르게 변수를 두번 선언해서 값이 변경되어 야근하게 되는 일을 방어할 수 있게 되었다. 호이스팅 당한다?이게 바로 필자가 이 포스팅을 작성하게 된 이유이다. 필자는 let이나 const 키워드가 호이스팅 되지 않는 줄 알았다.그러나 위에서 설명했듯이 호이스팅은 JavaScript 인터프리터가 코드를 해석하는 과정에서 발생하는 일이기 때문에 let이나 const라고 한들 피해갈 수 있을리가 없다. 그렇다면 이 문제를 어떻게 해결했을까? 변수 선언 키워드에 따라 다른 에러가 발생한다1234567console.log(name); // undefinedvar name = 'Evan';console.log(aaa) // Uncaught ReferenceError: aaa is not definedconsole.log(name); // Uncaught ReferenceError: Cannot access 'name' before initializationlet name = 'Evan'; 첫번째는 호이스팅 당한 var 키워드를 사용하여 선언한 변수를 호출한 모습이다. 당연히 참조 에러따윈 나지 않고 깔끔하게 undefined가 출력된다.두번째는 아예 선언한 적이 없는 변수를 호출하는 모습이다. Uncaught ReferenceError가 발생하고 메세지는 aaa is not defined라고 한다.세번째는 let 키워드를 사용하여 선언한 변수를 선언부 이전에 호출한 모습이다. 두번째와 마찬가지로 Uncaught ReferenceError가 발생했지만 에러 메세지는 Cannot access 'name' before initialization라고 나온다. V8 엔진을 뜯어보자이 두개의 에러는 전혀 다른 에러로, V8 엔진 내부에서 사용하는 MESSAGE_TEMPLATE에도 엄밀히 구분되어 있고 실제 호출되는 케이스도 다르다. 12T(NotDefined, \"% is not defined\")T(AccessedUninitializedVariable, \"Cannot access '%' before initialization\") V8 엔진의 깃허브 레파지토리을 클론받아서 살펴본 결과 내부적으로 var 키워드로 선언된 JS 객체와 let과 const로 선언된 JS 객체를 분기로 갈라놓은 코드가 굉장히 많았다. 코드를 계속 분석해보면서 var, let, const 중 어떤 키워드를 사용하여 값을 선언하든 호이스팅은 항상 이루어진다는 것을 알 수 있었다. V8 엔진 내부의 호이스팅 플래그인 should_hoist 값을 JavaScript 객체에 할당할 때 변수 선언 키워드에 대한 구분을 하지않고 무조건 true를 할당한다. 그렇다면 var 키워드와 let, const 키워드의 차이는 어디서 오는 것일까? 변수를 선언할 때, 즉 V8이 변수 객체를 생성할 때는 전부 동일하게 처리하지만 변수를 위해 메모리에 공간을 확보하는 초기화 단계에서는 이 키워드들을 다르게 처리한다. 12345static InitializationFlag DefaultInitializationFlag(VariableMode mode) { DCHECK(IsDeclaredVariableMode(mode)); return mode == VariableMode::kVar ? kCreatedInitialized : kNeedsInitialization;} 그러나 이후 진행 로직을 보면 DefaultInitializationFlag라는 함수를 통해 V8 엔진 내부에서 사용되는 VariableKind라는 타입을 반환하는데, 이때 var 키워드를 사용하여 선언한 변수는 kCreatedInitialized 값을, 그 외의 키워드인 let과 const로 선언한 변수는 kNeedsInitialization 키워드를 반환하고 있다. 즉 let, const 키워드로 선언한 리터럴 값은 호이스팅은 되나 특별한 이유로 인해 “초기화가 필요한 상태”로 관리되고 있다. 라고 말할 수 있다. 초기화가 필요한 상태는 무엇을 의미할까?JavaScript 인터프리터 내부에서 변수는 총 3단계에 걸쳐 생성된다. 선언 (Declaration): 스코프와 변수 객체가 생성되고 스코프가 변수 객체를 참조한다. 초기화(Initalization): 변수 객체가 가질 값을 위해 메모리에 공간을 할당한다. 이때 초기화되는 값은 undefined이다. 할당(Assignment): 변수 객체에 값을 할당한다. var 키워드를 사용하여 선언한 객체의 경우 선언과 초기화가 동시에 이루어진다. 선언이 되자마자 undefined로 값이 초기화 된다는 것이다. v8/src/parsing/parser.cc1234// Var 모드로 변수 선언 시auto var = scope->DeclareParameter(name, VariableMode::kVar, is_optional, is_rest, ast_value_factory(), beg_pos);var->AllocateTo(VariableLocation::PARAMETER, 0); V8 엔진의 코드를 보면 kVar 모드로 변수 객체를 생성한 후 바로 AllocateTo 메소드를 통해 메모리에 공간을 할당하는 모습을 볼 수 있다.그러나 let이나 const 키워드로 생성한 변수 객체는 다르다. v8/src/parsing/parser.cc123456789// kLet 모드로 변수 선언 시VariableProxy* proxy = DeclareBoundVariable(variable_name, VariableMode::kLet, class_token_pos);proxy->var()->set_initializer_position(end_pos);// Const 모드로 변수 선언 시VariableProxy* proxy = DeclareBoundVariable(local_name, VariableMode::kConst, pos);proxy->var()->set_initializer_position(position()); kLet 모드나 kConst 모드로 생성한 변수 객체들은 AllocateTo 메소드가 바로 호출되지 않았고 대신 소스 코드 상에서 해당 코드의 위치를 의미하는 position값만 정해주는 것을 볼 수 있다. 바로 이 타이밍에 let 키워드나 const 키워드로 생성된 변수들이 TDZ(Temporal Dead Zone) 구간에 들어가는 것이다. 즉, TDZ 구간에 있는 변수 객체는 선언은 되어있지만 아직 초기화가 되지않아 변수에 담길 값을 위한 공간이 메모리에 할당되지 않은 상태 라고 할 수 있는 것이다. 이때 해당 변수에 접근을 시도하면 얄짤없이 Cannot access '%' before initialization 에러 메세지를 만나게 되는 것이다. 블록 레벨 스코프를 사용한다함수 레벨 스코프를 사용하는 var 키워드와 다르게 let과 const는 블록 레벨 스코프를 사용한다. 블록 레벨 스코프를 사용하지 않기 때문에 블록 내부에서 선언한 변수 또한 전역 변수로 취급된다. 1234567var globalVariable = 'I am global';if (globalVariable === 'I am global') { var globalVariable = 'am I local?';}console.log(globalVariable); // am I local? 그러나 let과 const 키워드의 경우에는 블록 내부에서 선언한 변수는 지역 변수로 취급된다. 123456789let globalVariable = 'I am global';if (globalVariable === 'I am global') { let globalVariable = 'am I local?'; let localVariable = 'I am local';}console.log(globalVariable); // I am globalconsole.log(localVariable); // Uncaught ReferenceError: localVariable is not defined 이 경우 블록 내부에서 선언된 localVariable은 지역 변수로 취급되어 전역 스코프에서는 참조가 불가능하다. 참고로 let과 const는 호이스팅도 블록 단위로 발생한다. 123456let name = 'Global Evan';if (name === 'Global Evan') { console.log(name); // Uncaught ReferenceError: Cannot access 'name' before initialization let name = 'Local Evan';} 이 경우 if문 내부에서도 전역 변수로 선언한 name 변수의 값인 Global Evan이 출력될 것이라고 생각할 수 있지만 if문의 블록 내부에서도 호이스팅이 발생하여 지역 변수인 name의 선언부가 블록의 최상단으로 끌어올려졌기 때문에 참조 에러가 발생한다. let, const 키워드는 생략이 불가능하다.1234name = 'Evan'// 상기 코드는var name = 'Evan'// 과 같다 변수 선언 키워드를 사용하지 않으면 var 키워드를 사용한 것으로 취급되기 때문에 무조건 써줘야 한다. const 키워드의 특징let 키워드의 경우 위에서 설명한 이유들을 제외하면 변수를 선언할 때 사용한다는 점에서 var 키워드와 동일한 역할을 한다고 할 수 있다.그러면 기존의 var 키워드와 다른 역할을 하는 const에 대해서 조금 더 알아보자. 상수를 선언할 때 사용한다위에서 설명했듯이 const는 상수를 선언할 때 사용하는 키워드이다. 상수는 어떠한 불변 값을 의미한다. 즉, 한번 const 키워드를 사용하여 선언한 값은 두번 다시 변경할 수 없다는 뜻이다. 12const maxCount = 30;maxCount = 40; // Uncaught TypeError: Assignment to constant variable. 만약 const 키워드로 선언한 값을 재할당하려고 시도하면 친절한 에러메세지와 함께 불가능하다고 알려준다. 하지만 여기에 중요한 점이 있다. 바로 Call by reference 호출 방식을 사용하는 타입을 const 키워드로 선언 했을 때다. 12const obj = { name: 'Evan' }obj = { name: 'John' } // Uncaught TypeError: Assignment to constant variable. 이 경우 당연히 obj 변수가 바라보는 값 자체의 참조를 변경하려고 했기 때문에 에러가 발생한다. 그러나 객체 내부의 프로퍼티들은 const 키워드의 영향을 받지 않는다. 123const obj = { name: 'Evan' }obj.name = 'John'console.log(obj) // { name: 'John' } 이건 Call by reference 호출 방식을 사용하는 다른 타입인 Array도 마찬가지다. const 키워드를 사용하여 선언했더라도 push나 splice 등으로 배열 내부의 원소를 변경하는 행위에는 아무런 제약이 없다. 반드시 선언과 동시에 초기화 해줘야 한다let 키워드의 경우 명시적으로 선언만 했더라도 인터프리터가 해당 코드 라인을 해석함과 동시에 묵시적으로는 undefined가 할당되며 초기화된다. 12let hi;console.log(hi); // undefined 그러나 const 키워드를 사용하는 경우 반드시 선언과 동시에 값을 할당해줘야 한다. 1const hi; // Uncaught SyntaxError: Missing initializer in const declaration 결론반드시 JavaScript ES5로 코드를 작성해야하는 안습한 경우를 제외한다면 var 키워드는 이제 더이상 사용하지 않는 것을 추천한다. 그렇다면 남은 것은 let과 const인데 이 친구들을 어떻게 사용하는 것이 좋을까? 라는 고민이 생긴다사실 var 키워드만 쓰다보니까 생각도 안해본 사실인데, 생각보다 프로그램 내부에서 반드시 변수에 값을 재할당 해야하는 경우는 많지 않다. 필자도 별다른 생각없이 코딩하다가 어느 날 작성했던 소스코드를 봤는데 대부분 const 키워드를 사용하여 변수를 선언했고 let 키워드는 몇군데 사용하지 않은 것을 알 수 있었다. 만약 let 키워드를 사용해야한다면 절대 전역 스코프에서는 사용하지말고 가능하면 블록 스코프를 작게 만들고 그 내부에서 사용하는 것을 추천한다. 그리고 const 키워드의 경우 값을 재할당하려고 하면 바로 에러를 뿜뿜 해주기 때문에 개발자가 의도하지 않게 변수의 값이 재할당되는 슬픈 상황을 방지할 수 있다. 그렇기에 const 키워드를 잘 활용하여 안전한 코딩 라이프를 즐기고 다들 야근하지말고 칼퇴하시길 바란다. 빨리 집에 가서 밥먹고 넷플릭스봐야지! 이상으로 JavaScript의 let과 const, 그리고 TDZ 포스팅을 마친다.","link":"/2019/06/18/javascript-let-const/"},{"title":"프로그래머는 수학을 잘해야할까?","text":"이번 포스팅에서는 필자가 많이 받은 질문 중 하나인 프로그래머는 수학을 잘해야할까?라는 질문에 대해서 한번 이야기 해볼까 한다.물론 이 주제는 전 세계의 많은 개발자들 간에도 의견이 갈리는 내용이기 때문에 그냥 지나가는 개발자 한명의 생각일 뿐이라고 생각해줬으면 좋겠다. 사실 필자도 수학을 잘하는 편이 아니라 그냥 컴퓨터 공부하신 다른 분들처럼 학교에서 배우긴 했는데 졸업하고 나서는 잘 기억안나는 그냥 그 정도의 수준이다. 게다가 필자는 손으로 푸는 계산이 굉장히 약하기 때문에 수학 성적이 좋았던 편도 아니다.(사칙연산을 잘 틀린다.) 그래도 인터넷에서 많은 분들이 이 주제에 대해서 이야기 해주시기도 했고, 실제로 개발자가 아닌 분들에게 이런 질문을 받은 적도 있어서 필자도 한번 이 주제에 대한 필자의 생각을 끄적여볼까한다. 프로그래밍은 수학이다일단 근본적으로 우리가 사용하고 있는 이 컴퓨터는 사실 0과 1을 사용하는 계산기다. 그렇기 때문에 컴퓨터에는 아무래도 수학적인 내용이 많이 들어갈 수 밖에 없고, 프로그래밍을 할 때도 알게 모르게 많이 녹아있는 수학적인 개념들이 많다. 그래서 사실 필자는 프로그래머라면 수학을 조금은 할 줄 알아야한다고 생각하는 편이다. 이 얘기를 들은 여러분은 어? 난 수학을 잘 못하는데 지금 개발을 하고 있는데?라고 생각하실 수도 있겠다. 그러나 여기서 필자가 말하는 수학은 무슨 선형대수학이나 미적분같은 고등수학을 말하는 것이 아니다. 필자가 얘기하는 수학은 대부분 명제나 집합, n진법과 같이 이미 우리가 중고등학교에서 배웠던 정도의 수준을 이야기하는 것이다. 물론 이 개념들도 결국 파고들면 파고들수록 점점 추상적이고 어려운 개념들이 나오지만 솔직히 그렇게까지 알 필요는 없다고 생각한다. 우리가 무슨 수학을 연구하는 사람도 아니고, 우리는 그냥 프로그래머로써 프로그래밍에 필요한 정도만 알고 있으면 된다. 중요한 것은 수학이라는 키워드에 쫄지 않는 것이다. 그래서 이번 포스팅에서는 필자가 생각했을 때 프로그래밍에 도움이 되는 수학 개념 3가지에 대해서 가볍게 한번 이야기 해보려고 한다. 수학에 쫄지 말자!요즘 핫한 주제인 머신러닝이나 인공 신경망 같은 경우 호기심을 자극하는 키워드지만, 구글링을 해보면 나오는 검색 결과는 우리의 공부 의욕을 꺾어놓는다. …해답이 어떤 데이터에 의존적인 경우, 비용은 관측값에 대한 함수가 되어야 하며, 그렇지 않을 경우에는 데이터와 관련된 어떤 것도 모델링할 수 없게 된다. 많은 경우 비용은 근사될수만 있는 통계로 주어진다.간단한 예로, 어떤 분포 $\\mathcal {D}$에서 뽑아낸 데이터 쌍 $(x, y)$에 대해 비용 $C = E[(f(x) - y)^{2}]$을 최소화하는 모델 $f$를 찾는 문제를 생각해보자. 실용적으로는 분포 $\\mathcal {D}$에서 유한한 $N$개의 샘플만을 뽑아낼 수 있으므로, 이 예의 경우 $\\hat{C} = \\frac{1}{N}\\sum_{i=1}^{N}(f(x_i) - y_i)^{2}$, 즉 전체 데이터 집합이 아니라 데이터의 샘플에 대한 비용만 최소화될 수 있을 것이다.… 인공 신경망 - 학습위키 백과 난 분명 수식을 보고 있는데 숫자보다 영어가 더 많이 보이고… 솔직히 저런 수식을 처음 보면 무슨 외계어 같기도 하고 무슨 말을 하는 지 도통 알 수가 없다. 게다가 간단한 예랍시고 설명하고 있는 것은 전혀 간단하지 않게 생겼다.(빡침) 이런 것들이 바로 우리의 공부 의욕을 깎아먹는 수학의 모습이다. 하지만 저 수식이 진짜 어렵고 복잡한 의미일까? 저 기호와 알파벳이 어떤 의미인지만 알면 우리는 이 수식을 코드로 포팅할 수 있는데, 막상 짜놓고 보면 굉장히 간단하다.그럼 딱 봤을 때 제일 복잡해보이는 $\\hat{C} = \\frac{1}{N}\\sum_{i=1}^{N}(f(x_i) - y_i)^{2}$를 값 C를 구하는 코드로 한번 작성해보겠다. 1234567891011121314151617const inputs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10];const outputs = [10, 9, 8 ,7, 6, 5, 4, 3, 2, 1];const N = inputs.length;function exampleFunction (x) { return x + 1;}function getC (x, y) { let result = 0; for (let i = 0; i < N; i++) { result += (exampleFunction(x[i]) - y[i]) ** 2; } return result / N;}const C = getC(inputs, outputs); 쨘, 이제 아까 저 수식이 어떤 내용인지 조금 더 이해가 잘될 것이라고 생각한다. 혹시 저 수식이 어떤 내용인지 이해하지 못했더라도 코드는 이해할 수 있으니 말이다. 그럼 먼저 이상한 기호가 있어서 어려워 보이는 $\\sum_{i=1}^{N}$를 한번 살펴보자. $\\sum$(Sum이라고 읽는다) 기호는 그냥 뒤에 붙어 있는 값을 반복문 돌리면서 더하라는 뜻이다. 즉, $\\sum_{i=1}^{N}$은 i를 1로 초기화하고 N까지 반복문 돌리면서 값을 더해라라는 의미인 것이다. 우리가 for문을 사용할 때 관습적으로 사용하는 변수 $i$도 여기서 파생된 것이다. 그러면 $\\sum$의 뒤쪽으로 오는 $(f(x_i) - y_i)^{2}$에 포함된 $x_i$나 $y_i$의 의미도 자연스럽게 이해가 될 것이다. 그냥 어떤 집합의 $i$번째 원소를 말하는 것이다. 그리고 마지막에 $\\frac{1}{N}$을 곱해주는데, $N$번 반복하면서 모두 더한 값을 $N$으로 나누면 뭘까? 네, 평균입니다. 여기까지 보고나면 이제 더 이상 저 수식이 낯설게만 느껴지지는 않을 것이다. 그리고 위에서도 한번 얘기했지만 필자가 말하고 싶은 것은 저 수식을 코드로 작성할 수 있냐 없냐가 아니다. 저런 수식도 코드로 바꿔보면 그렇게 어려운 수식이 아니라는 것을 말하고 싶은 것이다. 쫄 필요가 없다! 그냥 우리에게 for문은 익숙하지만 $\\sum$이 익숙하지 않을 뿐이다. 만약 어떤 수식을 보고 모르는 기호가 나오면 한번 의미를 찾아보자. 물론 그 중에는 의미 자체가 어려운 기호도 있다.($\\int$같은 적분 기호…?) 하지만 대부분의 경우 그냥 알파벳은 변수, 기호는 규칙(프로그래밍에서의 명령문)이나 특별한 함수를 의미하기 때문에 하나하나 뜯어보고 코드로 작성해보면 생각보다 단순한 경우가 많다. 이제 더 이상 수학이 별 거 없다는 걸 알았으니 수학이라는 단어 자체에 쫄지 말고 한번 씹고 뜯고 맛보고 즐겨보자. 알면 도움이 되는 수학 개념 3가지자 그럼 이제 본론으로 들어가서, 우리가 프로그래밍을 할 때 도움이 될만한 수학의 3가지 개념들을 살펴보자. 사실 필자가 이야기하고 싶은 것은 개념이기 때문에 위에서 예로 들었던 것처럼 수식이 나오는 대수학 같은 건 아니다. 그리고 이 포스팅에서 설명하는 개념들은 사실 수학보다 논리학에 가까운 느낌이기 때문에 반드시 숫자와 연관지어서 생각할 필요는 없다. 이 개념들은 우리가 중학교, 고등학교에서 이미 배웠던 개념들이다. 다만 수능을 보고 나면 딱히 쓸 일이 없기 때문에 까먹었을 뿐이다. 물론 이과 계열의 전공을 택하신 분이나 문과 중에서도 경제학처럼 수학과 연관이 깊은 전공을 택하신 분은 계속 수학을 공부하기 때문에 까먹지 않았을 수 있지만 그 외의 경우는 사실 까먹을 수 밖에 없다. 뭐 어쨌든 필자가 설명할 개념들은 전부 어릴 때 한번 정도는 들어봤던 논리에 대한 개념들이니까 조금은 가벼운 마음으로 한번 즐겨보도록 하자. 명제(Proposition)자 처음으로 이야기하고 싶은 개념은 중학교 1학년인가? 2학년 때 배우는 명제이다. 사실 명제는 굳이 프로그래밍을 위해서가 아니더라도 그냥 논리적인 사고를 하기 위해서 기본적으로 알고 있어야 하는 개념이다. 명제는 참, 거짓과 같은 논리적인 진릿값을 가지는 것을 말한다. 즉, 참과 거짓을 구분할 수 없는 문장은 명제로 치지 않는다.즉, 에반은 키가 크다와 같은 문장은 명제로 성립될 수 없다는 것이다. 다른 사람이 필자를 봤을 때 키가 크다고 생각할 수도 있고 작다고 생각할 수도 있는 주관성이 들어가는 문장이기 때문이다. 그렇다면 명제는 우리가 프로그래밍을 할 때 어디에 사용된다는 걸까? 명제가 말하는 참과 거짓은 우리가 프로그래밍할 때 사용하는 True, False나 1, 0과 동일한 개념이다. 즉, 우리가 중학생 때 수학 교과서로 배웠던 명제는 조건식과 동일하다고 할 수 있다. 간단한 코드를 한번 보자. 12345678const array = ['a', 'b', 'c'];if (array.includes('a')) { console.log('array 변수에는 a가 들어있다.');}else { console.log('array 변수에는 a가 들어있지 않다.');} 이 코드에서 필자가 제시한 명제는 array 변수에 담긴 배열에 \"a\"라는 원소가 포함되어있다이고 이 명제가 참일 경우 if문 내부의 코드가, 거짓일 경우에는 else문 내부의 코드가 실행된다. 이처럼 조건문에 사용되는 조건은 반드시 명제여야하므로 명제에 익숙한 프로그래머는 어떤 요구사항을 들었을 때, 그 요구사항을 충족할 수 있는 명제를 빠르게 제시해나갈 수 있다. 이 명제라는 개념은 필자가 앞으로 설명할 다른 개념들이나 다른 수학의 개념에서도 가장 기초가 되는 개념이기 때문에 우리가 중학교에 입학하자마자 배우는 것이다. 수학은 긴가민가한 학문이 아니라 정확한 질문과 정확한 답을 제시할 수 있어야하는 능력이 필요한 학문이기 때문에 명제가 가장 기초가 된다. 집합(Set)다음으로 이야기 할 것은 명제와 마찬가지로 우리가 중학교 입학하고나서 배웠던 집합이다. 집합도 명제와 마찬가지로 알게 모르게 많이 사용되는 개념이기 때문에 집합에 대한 확실한 개념을 알고 있다면 프로그래밍할 때 많은 도움을 준다. 동글동글 귀여운 벤다이어그램과 프로그래밍은 왠지 거리가 멀어보이지만 사실 우리는 저 개념을 매일매일 프로그래밍할 때 사용하고 있다. 바로 논리식을 작성할 때 말이다. 논리식은 위에서 말한 명제처럼 True, False 둘 중에 하나로 평가받을 수 있는 식을 이야기한다. 보통 우리는 명제 여러 개를 논리 연산자로 묶은 식을 많이 사용한다. 이 논리식과 집합이 무슨 관계가 있다는 걸까? 사실 우리가 사용하는 논리연산자는 &&(AND) = 교집합, ||(OR) = 합집합으로 대응되기 때문에 복잡한 논리식을 마주쳤을 때 그 논리식을 벤다이어그램으로 그려보는 것도 가능하다. 그리고 우리가 어릴 때 외웠던 드 모르간의 법칙도 논리식에 그대로 적용된다.(사실 드 모르간의 법칙은 집합에 대한 법칙이라기보다 좀 더 포괄적인 논리학의 법칙이다.) 드 모르간의 법칙을 벤다이어그램으로 나타낸 모습 .inline { display: inline !important; vertical-align: top; } 필자는 드 모르간 법칙이 빛을 발하는 순간이 바로 자연어를 논리식으로 변환할 때라고 생각한다. 보통 회사에서 비즈니스 로직을 짜다보면 PO들이 어떤 기능의 작동 여부에 조건을 추가하는 경우가 많은데 문제는 조건을 추가할 때 기존에 있던 조건들까지 모두 생각하면서 말해주지 않는다는 것이다. 필자가 지금까지 일을 하면서 겪은 조건 중에 가장 복잡했던 경우를 예로 들어보겠다. 필자는 예전에 멤버십 결제 기능을 개발한 적이 있었는데 문제는 결제 수단을 입력할 수 있는 폼의 렌더 조건이 굉장히 복잡하다는 것이었다. 물론 처음부터 이렇게 복잡한 건 아니였고, 기능이 추가됨에 따라 점점 조건이 복잡해진 케이스이다. 당시 조건이었던 논리식을 자연어로 그대로 써보겠다. 조건 1. 사용자의 멤버십이 해지 상태가 아니고 결제 수단도 가지고 있지 않다.조건 2. 사용자가 결제수단을 가지고 있고 결제 수단을 핸드폰으로 가지고 있으며, 현재 사용자가 고른 상품이 현재 사용자가 가지고 있는 상품이 아니고 현재 멤버십이 해지 예약상태가 아니다. (조건 1 || 조건 2)이면 결제 수단 등록 폼이 활성화 된다. 조건 version 1 뭐 사실 저 사단이 난건 1차적으로 개발자인 필자의 잘못이긴 하지만 굳이 핑계를 대자면 시간에 쫓겨서 맨날 야근하면서 개발하다보니 저런 괴물같은 논리식이 탄생해버렸다…저렇게 개판쳐놓고나서도 다른 할 것도 너무 많았기 때문에 일단 묻어놓고 다른 프로젝트를 또 개발하던 중에 PO가 필자에게 이야기 했다. 에반, 저희 결제 수단 등록 폼에 조건 하나만 더 추가할 수 있을까요? 저 얘기를 듣고 다시 저 코드를 보고 PO 얼굴을 한번 본 뒤, 저걸 어떻게든 뜯어고쳐야겠다는 결론에 다다른 필자는 조용히 노트북을 들고 화이트 보드 앞으로 간 후, 저 복잡한 조건들을 어떻게든 이해할 수 있는 수준으로 만들기 위해서 발버둥쳤는데 그 결과가 이것이다. 조건 1. 사용자가 멤버십 가입 상태가 아니고, 사용자가 등록한 결제 수단이 카드가 아니다.조건 2. 사용자가 멤버십 가입 상태이고 사용자가 등록한 결제 수단이 휴대폰이며, 현재 구매하려고 선택한 멤버십이 나의 멤버십과 다른 상품이다.조건 3. 사용자의 결제 수단 정보가 없다. (조건 1 || 조건 2 || 조건3)이고 사용자가 선택한 결제 수단이 카드라면 결제 수단 등록 폼이 활성화 된다. 조건 version 2 사실 이것도 간단한 논리식은 아니지만 그래도 조건 1, 조건 2, 조건 3만 읽어 보았을 때 이전의 조건에 비해서 어떤 상태인지 좀 더 알아보기 쉬워졌다.(라고 자기합리화를 해봅니다.) 이 당시 필자가 논리식을 정리할 때 사용했던 방법이 벤다이어그램과 드 모르간 법칙이었다. 벤다이어그램으로 논리식을 펼쳐놓음으로써 여러 개의 논리식 중 사실 같은 명제이지만 역의 꼴을 취하고 있는 친구들을 쉽게 찾을 수 있었고 겹치는 명제들을 골라서 합치고 좀 더 알아보기 쉬운 단위로 조건을 나누고해서 저렇게라도 만들어 놓은 것이다. 물론 이 코드는 언젠가 개선을 해야한다…언젠가… 그리고 추가적으로 이런 논리식 외에도 데이터베이스에 질의를 던질 때 사용하는 SQL의 JOIN 개념도 보통 벤다이어그램으로 표현한다. 이런 경우 복잡한 논리식이나 SQL의 JOIN문을 보고 벤다이어그램이 바로 머리 속에 떠오른다면 그냥 코드나 자연어로 이해하는 것 보다는 좀 더 직관적이고 빠르게 이해할 수 있지 않을까? 라는 생각을 해본다. 수학적 귀납법(Mathematical Induction)수학적 귀납법(Mathematical Induction)은 수학에서 사용하는 증명 방법 중 하나이다. 주로 어떤 명제가 모든 자연수에 대하여 성립함을 보이기 위해 사용한다. 수학적 귀납법이 무엇인가를 자세히 알아보기 전에 먼저 우리는 논리학의 논증법 투톱인 귀납논증과 연역논증에 대해서 알아야한다. 간단하게 이야기하자면 귀납논증은 지금까지 그래왔으니까 앞으로도 그럴 것이다라는 느낌이고 연역논증은 전제가 맞다면 결론도 반드시 맞다라는 느낌이다. 이걸 너무 자세히 설명하면 글이 길어지기 때문에 간단한 예시로 간만 보겠다. 먼저 귀납논증은 이런 느낌이다. 2000년 여름은 더웠다, 2001년 여름도 더웠다….2019년 여름도 더웠다. 그러므로 여름에는 반드시 덥다. 귀납적인 이런 논증 방식은 모든 전제가 참이라고 해도 반드시 결론도 참이라는 법이 없다. 당장 위의 예시만 봐도 2020년 여름에는 기상이변으로 인해서 눈이 올 수도 있지 않을까?(투모로우…?) 즉, 귀납논증은 언제나 오류가 존재할 확률이 있다. 여기까지만 보면 왠지 허점투성이 논증법인 것 같지만 그래도 현대 과학은 귀납논증을 통해 끊임없는 가설을 제시하고 그걸 증명함으로써 발전해왔으므로 상당한 가치가 있는 논증법이라고 할 수 있다. 반면에 연역논증은 이런 느낌이다. 맥북은 애플이 만든다. 내 컴퓨터는 맥북이다. 그러므로 내 컴퓨터는 애플이 만들었다. 연역논증 중에서 가장 대표적인 사례인 삼단논법이다. 이게 바로 위에서 얘기한 어떤 부분적인 전제가 맞다면 결론도 반드시 맞다라는 의미이다. 만약 결론이 거짓이라면 전제 중 하나도 무조건 거짓이다. 즉, 연역논증은 이미 전제에 담겨있던 것을 증명하는 데는 탁월하지만 귀납논증처럼 새로운 지식을 탐구하기에는 부적절하다. 하지만 우리가 프로그래밍을 할때는 새로운 지식을 탐구하는 것이 아니라 그냥 내 코드가 오류없이 완벽한가를 증명하기 위한 논증법을 사용해야하므로 귀납논증보다는 연역논증이 더 알맞다. 필자가 이 두 논증법을 전부 설명한 이유는 바로 수학적 귀납법이 귀납논증이 아니라 연역논증이기 때문이다. 아니…귀납법이라며…? 수학적 귀납법은 어떤 명제 $P$가 있을 때 다음 2가지만 충족시키면 모든 자연수에 대해서 $P$가 성립한다는 것을 의미한다. $P(1)$은 참이다 $P(n)$이 참이면 $P(n + 1)$도 참이다. 그러므로 명제 $P$는 모든 자연수에 대해서 참이다. 이렇게만 얘기하면 또 머리가 아파지니까 예시를 살펴보자. 수학적 귀납법은 보통 도미노로 예시를 많이 들기 때문에 필자도 도미노를 예로 설명하겠다. 맨 처음에 있는 도미노가 쓰러진다. ($P(1)$이 참) 무작위로 고른 $n$번째 도미노가 쓰러질 때 항상 $n+1$번째에 세워진 도미노도 쓰러진다. ($P(n)$이 참이면 $P(n + 1)$도 참) 그러므로 맨 처음에 있는 도미노를 쓰러트리면 반드시 모든 도미노가 순서대로 쭉쭉 쓰러진다. 이것이 수학적 귀납법의 논리 전개 방식이다. 간단하게 얘기하자면 전제가 참이라는 것을 먼저 보인 후에 그 전제에서 보편적인 결론을 이끌어 내는 것이다. 이런 수학적 귀납법은 알고리즘의 정당성을 검증할 때 아주 유용하게 쓰일 수 있다. 왜냐면 알고리즘이란 것은 굉장히 보편적인 규칙이고, 어떤 형태로든 반드시 반복적인 요소를 가지고 있기 때문이다. 그럼 한번 유명한 알고리즘인 $n!$, 팩토리얼을 구하는 알고리즘을 한번 수학적 귀납법으로 풀어보자. 12345678function factorial (n) { if (n < 1) { return 1; } else { return n * factorial(n - 1); }} $n = 0$인 경우 $n! = 1$이다. $n! = n \\times (n - 1) \\times (n - 2) \\times … 3 \\times 2 \\times 1$ 이다. $(n + 1) \\times n! = (n + 1) \\times n \\times (n - 1) \\times … 3 \\times 2 \\times 1$ 이다. 그러므로 이 논리는 참이다. 이런 식의 논리적인 사고방식은 당장 코딩할때 직접적인 도움이 되지는 않겠지만, 복잡한 문제를 만났을 때 일반화된 해결법을 찾아낼 수 있는 능력을 키워준다. 사실 이런 논증법을 적용할 수 있는 문제는 일상에서도 얼마든지 찾아볼 수 있으므로 평소에도 한번 이렇게 생각하는 습관을 들여보는 것도 나쁘지 않다.(연애할 때는 절대 이러지 말자.) 마치며사실 이 포스팅에서 설명한 저런 것들 다 몰라도 느낌적인 느낌으로 프로그래밍을 잘 할수는 있다. 하지만 곰곰히 생각해보자. 저런 것들을 모르고 프로그래밍을 하고 있었다고 생각했던 분들도 그냥 이론으로 정리하지 않았을 뿐이지 알게모르게 저 개념들을 전부 사용하고 있었을 것이다. 그리고 필자가 생각했을 때 수학을 배우면 가장 좋은 점은 내가 만들고 싶은 것을 만들때 적어도 이론에서 막히는 일은 없다는 것이다. 필자가 예전에 작성했던 포스팅인 행성 궤도 계산이나 역전파 알고리즘같은 포스팅만 봐도 수식이 많이 나와서 어려워 보일 수 있다. 사실 필자도 저 친구들을 처음 만들 때 학교에서 배운 수학 같은 건 이미 가물가물한 상태였기 때문에 거의 처음부터 다시 공부해서 결국은 저 프로젝트를 완성할 수 있었다. 물론 선형대수학부터 시작해서 오일러 회전, 쿼터니온 등 이름만 들어도 토할 것 같은 이론들이 처음에는 필자에게도 상당한 두려움으로 다가왔지만 일단 이해가 안되더라도 문서를 계속 보고 조금이라도 이해되는 부분이 있다면 코드로 작성한 후에 하나하나 실행시켜보면서 공식의 매커니즘을 눈으로 직접 보다보니까 어느 순간부터는 그래도 처음보다 많이 익숙해졌던 것 같다.(근데 사실 지금도 잘 모른다.) 수학은 그렇게 무서운 친구가 아니다. 위에서 예시로 나왔던 인공신경망 알고리즘의 수식도 처음 보면 뭔가 어려워보이고 복잡해보이지만 막상 코드로 풀어보니 별 거 아니였던 것처럼 말이다. 여러분은 이미 프로그래머로써 알게 모르게 수학이나 논리학의 개념이 몸에 배어있는 사람인데 이제 와서 수학을 겁내는 것도 좀 이상하지 않은가? 이제는 그런 마음을 다 털어버리고 한번 수학과 친해져보자. 수학은 그냥 프로그래밍 언어처럼 여러분이 상상하는 것을 실현시켜줄 수 있는 도구라고 생각하자. 이상으로 프로그래머는 수학을 잘해야할까? 포스팅을 마친다.","link":"/2019/07/17/programmer-with-math/"},{"title":"컴퓨터가 만드는 랜덤은 정말로 랜덤할까?","text":"이번 포스팅에서는 랜덤에 대해서 한번 이야기 해볼까 한다. 랜덤이란 어떤 사건이 발생했을 때 이전 사건과 다음 사건의 규칙성이 보이지 않는, 말 그대로 무작위로 발생하는 패턴을 이야기한다. 우리가 사용하고 있는 컴퓨터도 랜덤한 패턴을 만들어야 할 때가 있고 또 실제로도 만들고 있다. 하지만 컴퓨터는 사실 그냥 기능이 많은 계산기에 불과하다. 계산기는 입력된 값을 가지고 이리 저리 가지고 놀다가 결과값을 내놓는 물건이다. 근데 이런 계산기가 어떻게 랜덤한 결과를 만들어낼 수 있는 것일까? 우리는 이 질문에 대한 답을 찾기 전에 근본적으로 랜덤이란 것이 무엇인지부터 생각해봐야한다. 진짜 무작위라는 것이 존재하기는 하는 걸까? 랜덤(Random)이란 무엇일까?먼저, 랜덤이란 무엇일까? 위에서 설명했듯이 무작위로 발생하는 어떠한 패턴이다. 사람마다 의견이 분분하겠지만 지금 필자 머리 속에 떠오른 대표적인 랜덤은 바로 도박이다. 도박의 가장 위험한 점이 비록 이번 판에는 잃었지만 다음 판에는 나도 딸 수 있을거야!라는 희망인데, 이런 희망은 도박할 때 사용하는 게임들이 랜덤에서 기반하는 게임이라는 생각에서 출발하기 때문이다. 즉, 어느 정도 운빨게임이어야 한다는 것이다. 대법원 판례 2006도736에도 도박의 정의를 재물을 걸고 우연에 의하여 재물의 득실을 결정하는 것이라고 이야기하고 있다. 대표적인 도박인 파칭코, 섰다, 포커, 주식 등만 살펴봐도 대충 감이 온다. 필자는 도박에서 사용하는 게임을 잘 모르기 때문에 누구나 해봤을 법한 가벼운 도박을 예로 들어보겠다. 필자가 중고등학생 시절을 보낸 2000년대에 전국의 중, 고등학교에서 널리 행해졌던 놀이인 판치기이다. 판치기는 워낙 전국적으로 유행했기 때문에 필자 또래의 독자분들이면 독자분들이면 한번 쯤은 해봤거나 아니면 친구들이 하는 걸 보기는 했을 거라 믿는다. 판치기하다가 선생님들한테 걸리면 교무실로 끌려가서 바로 빠따행이었다. 그래도 혹시 판치기가 뭔지 모르는 분들이 있을 수 있으니 일단 간단하게 룰을 설명하겠다. 적당히 두꺼운 교과서를 준비한다. 국사책이나 물리책처럼 적당히 두꺼운 책을 사용하자. 각자 준비한 동전을 교과서에 올린다. 보통 100원을 건다. 순서대로 교과서를 손으로 때려서 동전을 뒤집는다. 모든 동전을 뒤집어서 같은 면으로 만든 사람이 판돈을 모두 가져간다. 상식적으로 교과서를 손으로 때렸을 때 n개의 동전이 뒤집어 질 것이라는 계산을 하기란 쉽지 않다. 즉, 판치기는 어느 정도 랜덤에 기반한 게임이고 그래서 도박의 기본적인 특성인 사행성을 가질 수 있는 것이다. 참고로 형법 제 246조 1항과 국내 도박법 판례 상 학교에서 하는 판돈이 몇백원 정도인 판치기는 일시 오락으로 판정받아서 무죄이지만, 아무리 판치기라고 해도 몇백만원 수준의 돈이 왔다갔다 하는 수준이면 도박죄로 처벌받을 수 있다는 점 알아두자.(물론 저 정도 돈이 있는 사람들이면 판치기말고 다른 걸 한다…) 그럼 이 쯤에서 처음의 그 질문을 한번 던져보겠다. 이게 정말로 랜덤일까? 예전에 필자의 학창시절을 생각해봐도 학교에 1~2명씩 판치기의 절대 고수들이 있었는데 이 친구들은 자신이 원하는 동전만 정확히 뒤집을 수 있는 능력을 보유하고 있었다. 예전에 TV에서도 판치기의 고수라고 나온 분이 있었는데 이 분은 뭐 거의 닥터 스트레인지 수준이었다. 사실 판치기도 결국은 물리 법칙 내에서 돌아가는 판이기 때문에 판돈으로 걸린 동전의 무게, 판을 내려치는 힘, 판으로 사용된 교과서의 탄력성 등 몇가지 변수를 알면 어느 동전이 뒤집어 질지도 계산할 수 있을 것이다. 물론 이렇게 계산하는 것이 쉬운 일은 아니기 때문에 우리는 진정한 의미의 랜덤이 아니더라도 이 정도면 그냥 랜덤하다고 치는 것이다. 즉, 우리 주변에서도 진정한 의미의 랜덤은 그렇게 많지 않다는 것을 알 수 있다. 단지 우리가 랜덤하다고 생각할 뿐이다. 컴퓨터에서의 랜덤사실 우리가 일상에서 어느 정도 우연성이 있다면 랜덤하다고 하듯이 컴퓨터도 진정한 의미의 랜덤을 만들어 내는 것이 아니라 어느 정도 납득할만한 우연성을 만들어 내고 이를 랜덤하다고 한다. 게다가 컴퓨터로 뭔가를 만드려면 어떠한 규칙을 만들어줘야 하는데, 어떠한 규칙으로 규칙이 없는 랜덤을 생성한다는 말 자체가 모순이다. 그래서 우리는 랜덤은 아니지만 랜덤에 가까운 유사 랜덤(Pseudo Random)밖에 만들 수 밖에 없는 것이다. 또한 컴퓨터는 기본적으로 숫자를 기반으로 하는 일종의 계산기이기 때문에 랜덤한 사건을 만들기 위해서는 난수, 즉 랜덤한 수를 뽑아 낼 수 있어야 한다. 그럼 컴퓨터는 어떻게 이런 난수 생성을 하는 걸까? 중앙제곱법(Mid Square Method)중앙제곱법은 폰 노이만이 1949년에 고안한 유사 난수 생성법으로, 임의의 숫자를 제곱한 다음 이 숫자의 일부분을 가져와서 새로운 난수로 만들어내는 방법이다. 임의의 4자리 난수를 만든다고 가정해보자. 초기 값은 심플하게 1234로 가겠다. 페이즈 대상값 제곱값 난수 0 1234 1522756 1522756 1 5227 27321529 27321529 2 3215 10336225 10336225 이런 식으로 계속 중앙에 있는 값을 빼와서 제곱하면서 임의의 난수를 생성하는 방법이 바로 중앙제곱법이다. 뭔가 딱봐도 예측하기 쉬워보인다. 아무래도 폰 노이만 형이 활동하던 1950년대에 개발된 알고리즘이고, 그 당시 컴퓨터의 성능은 눈물나기 그지 없었으므로 최대한 간단한 방법을 사용한 것이다. 그래서 이 방법은 요즘에는 거의 사용되지 않는다. 선형합동법(Linear Congruential Method)선형합동법은 C의 rand함수에서 사용하는 알고리즘이며 다음과 같은 재귀 관계식으로 정의되며 난수들의 수열을 반환한다. $$X_{n+1} = (aX_n + c) \\mod m$$ 여기서 $X$는 난수로 된 수열이고 나머지는 그냥 임의의 정수이다. 참고로 ANSI C 표준은 $m = 2^{31}, a = 1103515245, c = 12345$로 정해져있다. 그럼 간단하게 한번 자바스크립트를 사용해서 구현해보자. 12345678910111213141516171819202122let m = 2 ** 31;const a = 1103515245;const c = 12345;function rand (x) { return (((x * a) + c) % m);}function getRandomNumbers (randCount = 1) { // 현재 시각을 사용하여 초기값을 설정한다. const initial = new Date().getTime(); const randomNumbers = [initial]; for (let i = 0; i < randCount; i++) { randomNumbers.push(rand(randomNumbers[i])); } randomNumbers.shift(); return randomNumbers;}console.log(getRandomNumbers(10)); 코드를 브라우저 콘솔에 붙혀넣고 실행시켜보면 인자로 주었던 10만큼의 길이를 가진 난수 배열이 출력된다. 1(10) [1163074432, 465823232, 1719475776, 1744670976, 790949120, 552540416, 896259328, 1473241344, 1074855168, 575793408] 선형합동법의 특징은 이전에 생성된 난수를 활용한다는 것이며 최대 $m$만큼 경우의 수를 가지므로 최악의 경우 $m$ 만큼의 반복 주기를 가진다. 필자가 m 변수를 let 키워드로 선언한 건 이 이유다. 한번 브라우저에서 m 변수에 작은 수를 할당한 다음에 getRandomNumbers 함수를 다시 호출해보자. 난수의 경우의 수와 동일한 값이 출현이 눈에 띄게 커진다는 것을 확인해볼 수 있다. 선형합동법은 계산이 굉장히 간단하고 빠르기 때문에 초창기부터 컴퓨터에 널리 사용되었다. 그러나 선형합동법은 난수에 주기성이 있고 생성되어 나오는 난수들 사이에 상관 관계가 존재하기 때문에 마지막으로 생성된 난수와 그 외 변수들만 알면 그 다음에 생성될 난수를 모두 예측할 수 있다. 문제는 그 변수들이 ANSI C 표준으로 정해져 있어서 누구든지 다 알 수 있다는 점이다. 즉, 조금 지식이 있는 사람이면 rand 함수의 결과를 보고 다음 난수를 미리 예상할 수 있다는 것이다. 그래서 이 알고리즘은 난수가 예측당해도 상관없는 경우나 임베디드처럼 메모리를 많이 사용하지 못하는 제한된 상황에서 주로 사용한다. 메르센 트위스터(Mersenne Twister)메르센 트위스터는 엑셀, MATLAB, PHP, Python, R, C++ 등에서 사용하고 있는 난수 생성 알고리즘이며, 1997년에 마츠모토 마코토와 니시무라 다쿠지가 개발한 알고리즘이다. 메르센 트위스터라는 이름은 이 알고리즘의 난수 반복 주기가 메르센 소수인데서 유래했다. 메르센 소수라고 하면 뭔가 대단한 수 같은데 사실 별 거 없다. 메르센 수는 $M_n = 2^{n} - 1$으로 나타내며 식 그대로 2의 n제곱에서 1이 모자란 수를 말하는 것이고 메르센 소수는 그냥 이 메르센 수 중에서 소수인 것을 고른 것이다. 보통 $2^{19937} - 1$의 난수 반복 주기를 가지는 MT19937이 많이 사용되는데, C++에서도 이 알고리즘을 채택해서 사용하고 있다. 이 알고리즘의 동작 원리를 간단하게 설명하면 다음과 같다. seed를 사용하여 624 만큼의 길이를 가진 벡터를 생성. seed는 보통 하드웨어 노이즈나 오늘 날짜를 사용한다. 이 벡터를 사용하여 624개의 유사 난수를 만든다. 이 벡터에 노이즈를 준 후 다시 2번을 반복. 이 노이즈를 주는 행위를 Twist한다고 한다. 이때 3번의 Twist하는 과정에서 GFSR(Generalized Feedback Shift Register라는 방법을 사용한다. GFSR은 자료가 많지 않고 대부분 논문같은 학술 자료만 있는 상황이라 필자가 자세히 알아보지는 못했으나, 열심히 구글링하고 논문들을 뜯어본 결과 LFSR(Linear Feedback Shift Register)를 약간 변형한 방법이라는 정보를 얻을 수 있었다.(정보가 너무 제한적이라 LFSR 기반이 맞는지는 정확하지 않다.) LFSR은 이전 상태 값들을 가져와서 선형 함수를 통해 계산한 후 그걸 사용해서 다음 값을 만들어 내는 방법이다. 이때 사용하는 함수는 보통 XOR를 많이 사용하고 맨 처음 값을 시드(Seed)라고 부른다. LFSR를 간단히 설명하자면, 우선 몇개의 메모리 주소를 골라놓고 초기화된 인풋인 시드를 레지스터에 밀어넣는다. 그러면 오른쪽으로 한칸씩 비트가 밀리게(Shifting) 된다. 그러면 우리는 끝에서 삐져나온 한개의 비트를 아웃풋에서 얻게 된다. 그 다음 미리 골라놨던 메모리 주소에 접근해서 값을 빼온 다음에 순서대로 하단에 위치한 3개의 XOR 게이트에 통과시키면 다음 인풋이 나오고 그걸 또 레지스터에 밀어넣는걸 반복하는 것이다. 메르센 트위스터도 결국은 LFSR가 약간 변형된 GFSR를 사용하여 난수를 생성하기 때문에 초반에 시드를 생성해줘야한다. 메르센 트위스터의 자세한 알고리즘은 위키피디아의 Mersenne Twister - Algorithmic_detail에서 확인할 수 있다. 필자는 2002년에 마츠모토 마코토와 니시무라 다쿠지가 자신들의 메르센 트위스터 알고리즘을 개선해서 다시 작성한 C의 MT19937 알고리즘 코드를 보고 자바스크립트로 한번 포팅해보았다. 이 코드는 최대 32bit 길이의 난수를 사용하도록 작성되어있다. 64bit용 알고리즘은 MT19937-64라고 또 따로 있다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263const N = 624;const M = 397;const F = 1812433253;const UPPER_MASK = (2 ** 32) / 2; // 0x80000000const LOWER_MASK = UPPER_MASK - 1; // 0x7fffffffconst MATRIX_A = 0x9908b0df;class MersenneTwister { constructor () { const initSeed = new Date().getTime(); this.mt = new Array(N); this.index = N + 1; this.seedMt(initSeed); } seedMt (seed) { let s; this.mt[0] = seed >>> 0; for (this.index = 1; this.index < N; this.index++) { this.mt[this.index] = F * (this.mt[this.index - 1] ^ (this.mt[this.index - 1]) >>> 30) + this.index; this.mt[this.index] &= 0xffffffff; } } int () { let y; const mag01 = new Array(0, MATRIX_A); if (this.index >= N) { let kk; if (this.index === N + 1) { this.seedMt(5489); } for (kk = 0; kk < N - M; kk++) { y = (this.mt[kk] & UPPER_MASK) | (this.mt[kk + 1] & LOWER_MASK); this.mt[kk] = this.mt[kk + M] ^ (y >>> 1) ^ mag01[y & 1]; } for (; kk < N - 1; kk++) { y = (this.mt[kk] & UPPER_MASK) | (this.mt[kk + 1] & LOWER_MASK); this.mt[kk] = this.mt[kk + (M - N)] ^ (y >>> 1) ^ mag01[y & 1]; } y = (this.mt[N - 1] & UPPER_MASK) | (this.mt[0] & LOWER_MASK); this.mt[N - 1] = this.mt[M - 1] ^ (y >>> 1) ^ mag01[y & 1]; this.index = 0; } y = this.mt[this.index++]; y ^= (y >>> 11); y ^= (y > 18); return y >>> 0; };} 코드 내부에 사용된 상수 F = 1812433253, MATRIX_A = 0x9908b0df, this.seedMt(5489) 등은 필자가 임의로 넣은 수가 아니라 MT19937의 표준 계수로 정해져 있는 수이다. 사실 알고리즘 자체가 너무 복잡해서 필자도 다 이해하지는 못했다.(대학 때 공부를 열심히 안한 죄…) 그래도 일단 이렇게 코드로 한번 직접 작성해보면 글로만 읽을 때보다는 확실히 조금 더 이해가 되기 때문에 C로 작성된 원래 코드를 자바스크립트로 포팅한 것이다. 필자가 참고한 C 라이브러리 코드는 여기서 확인할 수 있다. 12const rand = new MersenneTwister();console.log(rand.int()); // 2145258025 작성한 코드를 직접 실행해보니 랜덤한 난수가 잘 생성이 되는 것 같다. 사실 난수가 잘 생성되는지 정확하게 보려면 결과를 시각화한 후 난수의 분포도를 봐야하지만 귀찮은 관계로 일단 패스하겠다. 참고로 이 알고리즘을 만든 마츠모토 마코토의 홈페이지에 들어가면 이 알고리즘이 왜 이런 이름을 가지게 되었는지 나와있다. 마코토: Knuth 교수님이 이 이름 발음하기 너무 힘들대.다쿠지: …… [며칠 후] 마코토: 하이 타쿤, 메르센 트위스터는 어때? 메르센 소수를 사용하기도 했고, 원형은 Twisted GFSR(Generalized Feedback Shift Register)니까.다쿠지: 글쎄…?마코토: 왠지 제트코스터 같이 들리는 걸? 빨라보이기도 하고 기억하고 쉽고 발음하기 좋고. 그리고 이건 비밀인데 이 이름엔 우리들의 이니셜이 숨어있지!(Makoto, Takuji)다쿠지: ….마코토: 자자자 MT로 가는거다?다쿠지: 음…그래 나중에 우리는 Knuth 교수님으로부터 “좋은 이름인 것 같다”라는 편지를 받았다. 뭔가 깨발랄한 마코토형과 시니컬한 다쿠지형 케미가 돋보인다. 뭔가 이런 대단한 알고리즘을 개발한 사람이지만 사람 냄새나는 네이밍 과정인듯. XOR 시프트(XOR shift)XOR 시프트도 메르센 트위스터와 마찬가지로 Linear Feedback Shift Register을 기반으로 한 의사 난수 알고리즘이다. 어떻게 보면 메르센 트위스트의 하위호환이라고 볼 수도 있는데, 메르센 트위스트와 원리는 비슷하지만 구현이 훨씬 간단하고 작동이 빠르기 때문에 왕왕 사용된다. XOR 시프트 알고리즘은 여러 종류가 있는데, 32bit, 64bit, 128bit의 수를 사용하며 각각 $2^{32} -1$, $2^{64} - 1$, $2^{128} - 1$의 메르센 수 난수 반복 주기를 가진다. 근데 이게 TestU01이라는 난수 품질 테스트 프레임워크의 테스트를 통과하지 못해서 조금 문제가 있었다. 이걸 통과해야 ANSI C 표준이 될 수 있기 때문이다. 그래서 몇가지 변종 알고리즘이 나오게 되었는데 그 중 하나가 128bit를 사용하는 XOR 시프트 128을 개량한 XOR 시프트 128 +라는 알고리즘이다.(뭔가 게임 후속작 제목 같다…) 참고로 V8, SpdierMonkey, JavaScriptCore 등 메이저 자바스크립트 엔진들이 채택한 난수 생성 알고리즘이 XOR 시프트 128 +이다. 이 알고리즘은 2016년 11월에 Sebastiano Vigna라는 분에 의해 학회에서 발표되었다. 논문 내용은 여기에서 확인할 수 있다. 전반적인 원리는 위에서 설명한 LFSR와 비슷하니, V8엔진에서 Math.random 메소드를 어떻게 구현했는지 한번 살펴보자. 원래 코드는 C++로 작성되어있으나 여러분이 브라우저에서 코드를 쉽게 실행시켜볼 수 있도록 동일한 로직을 자바스크립트로 포팅해서 작성하겠다. V8 내부 소스 코드는 v8/src/base/utils/random-number-generator.h의 XorShift128 메소드에 작성되어 있다. xorshift128plus.js12345678910111213141516171819const state = [1827981275, 1295982171];function xorshift128plus () { let s1 = state[0]; let s0 = state[1]; state[0] = s0; s1 ^= s1 > 17; s1 ^= s0; state[1] = s1; console.log('레지스터의 현재 상태 -> ', state); return state[0] + state[1];}console.log('초기 레지스터 상태 -> ', state);for (let i = 0; i < 5; i++) { console.log('난수 -> ', xorshift128plus());} 1234567891011초기 레지스터 상태 -> [ 1827981275, 1295982171 ]레지스터의 현재 상태 -> [ 1295982171, 867440954 ]난수 -> 2163423125레지스터의 현재 상태 -> [ 867440954, 1393243966 ]난수 -> 2260684920레지스터의 현재 상태 -> [ 1393243966, 37812574 ]난수 -> 1431056540레지스터의 현재 상태 -> [ 37812574, 833890405 ]난수 -> 871702979레지스터의 현재 상태 -> [ 833890405, 1661667227 ]난수 -> 2495557632 출력된 로그를 보면 레지스터의 상태가 어떻게 변하고 있는지 알 수 있다. [1]번 인덱스에 있던 값이 [0]번 인덱스로 옮겨지고 [1]번 인덱스에 새로운 난수를 꽂아넣은 후 두 원소를 더해서 출력하고 있다는 걸 알 수 있다. 계산 과정을 보면 [1]번 인덱스에 새로운 값을 넣을 때는 시프팅되어 왼쪽으로 삐져나온 값인 [0]번 인덱스의 값을 사용하여 XOR 시프팅을 진행하고 그 값을 다시 [1]번 인덱스에 넣어준다. 그리고 시프팅하는 상수인 23, 17은 XOR 시프팅 128+ 알고리즘을 개발할 때 연구를 통해 찾아낸 최적의 상수이기 때문에 사용하는 것이다. 해당 논문을 보면 뭐 이것저것 시도해본 다음에 결과를 일일히 테스트해서 나온 결과를 비교한 도표도 함께 첨부되어있다. 마치며사실 처음에는 가벼운 마음으로 랜덤에 대한 이야기를 해보려고 했는데 예상보다 수학적인 내용이 많이 나와서 왠지 어려운 포스팅이 되어버린 것 같다. 사실 컴퓨터라는 계산기로 진정한 의미의 난수를 만드는 것은 거의 불가능하다. 최근에 미국에서 양자컴퓨터를 사용해서 진정한 의미의 난수를 생성하는 데 성공했다고 하지만 양자컴퓨터는 아직은 우리와 너무나도 먼 이야기이기 때문에 논외로 치겠다. 필자도 사실 수학을 그다지 좋아하는 편은 아니다. 하지만 이렇게 수학적인 연구를 통해 생성 규칙이 없는 난수를 만드려는 많은 사람들의 도전을 보면, 편하게 상위 레이어에서 코딩하고 있는 필자로써는 이 분들에게 굉장히 감사함을 느낀다.(난 안될꺼야 아마…) 연구원 분들이 기반 알고리즘을 만들어주시면 열심히 상용 어플리케이션에 써먹겠습니다…!!! 필자는 이 포스팅을 작성하면서 논문과 위키피디아를 엄청 들여다봤는데 오랜만에 영어와 수식을 너무 많이 봐서 머리에 과부하가 걸린 상태다. 그리고 다른 건 몰라도 메르센 트위스트 같은 경우는… 아니 너무 복잡하다 인간적으로… 참고로 이거 만든 마츠모토 마코토님은 지금 히로시마 대학교의 수학 대학원에서 조교수로 근무하고 계신데, 이 정도 머리가 되야 강단에 설 수 있는 건가라는 생각도 든다. 완전 어나더레벨… 이상으로 컴퓨터가 만드는 랜덤은 정말로 랜덤할까? 포스팅을 마친다. 참고문헌 XorShift - WikipediaThere’s Math.random(), and then There’s Math.random() - V8 DevFuther scramblings of Marsaglia’s xorshift generators - Sebastiano VignaThe Xorshit128+ random number generator fails BigCrush - Daniel Lemire","link":"/2019/07/14/what-is-random/"},{"title":"개발자가 조직문화에 대해 관심을 가져야 하는 이유","text":"우리는 개발자이기 이전에 어떤 조직의 구성원이기 때문에 조직문화로부터 생각보다 많은 영향을 받고 있다. 그렇기 때문에 우리는 한 조직의 일원으로 이 조직이 건강한 조직인지, 우리가 활기차고 생산성있게 일하고 있는지 끊임없는 관심을 가져야 한다. 하지만 필자는 대부분의 개발자들이 기술이나 개발 문화에 대해서는 많은 관심을 기울이지만 조직 문화에 대해서는 별로 관심이 없는 모습을 종종 보아왔다. 쌓여있는 업무를 처리하거나 매년 쏟아져나오는 프레임워크와 새로운 패러다임을 공부하기도 벅차기도하고 개발팀 내부에서 조직문화 때문에 발생하는 이슈가 별로 없기 때문이기도 한 것 같다. 실제로 필자가 지난 4년 간 개발자로 일해오며 시니어 개발자들에게 조직 문화에 대한 질문을 했을 때 돌아오는 대답은 대략 이랬다. 조직문화…? 조직문화 중요하죠. 근데 지금 이슈 쳐내기도 너무 정신이 없어서… 그리고 필자에게 이런 대답을 해주셨던 시니어 개발자들은 직장 내에서 진짜로 겁나 바쁜 상태인 경우가 많았다. 하지만 이렇게 얘기할 수 있는 것도 사실 조직문화라는 키워드가 개인의 우선 순위에서 밀렸기 때문이라고 볼 수 있기 때문에 상대적으로 그 분들에게 조직문화에 대한 중요성이 와닿지 않은 경우라고 생각한다. 그래서 이 포스팅에서는 우리같은 조직원들이 왜 이런 조직문화에 관심을 가져야하는지, 또 우리가 조직문화를 위해 당장 실천할 수 있는 게 뭔지 한번 이야기 해보려고 한다. 물론 좋은 조직문화 형성에는 조직의 리더인 CEO의 역할이 가장 중요하기는 하지만 필자는 CEO였던 적도 없고, 또 앞으로도 그냥 코딩이나 하면서 살고 싶지 CEO 같은 건 별로 할 생각이 없으므로 조직원의 입장에서만 이 이야기를 풀어볼 것이다. 굳이 개발자가 조직문화에 까지 관심을 가져야하나요?여러분들 중에서는 이런 생각이 드는 분도 분명히 있을 것이다. 굳이 개발자가 조직문화까지 관심을 가지는 게 맞는 것일까? 조직문화는 컬쳐 담당자나 CEO가 관심을 가지고 개선을 주도하는 것이 맞지 않나? 사실 맞다. 올바른 조직문화의 정착은 컬쳐팀의 주요 목표이기도 하고 간혹 컬쳐팀이 없다면 HR팀이 담당하는 경우도 있다. 그리고 CEO가 조직문화의 개선에 관심이 없으면 아무리 직원들이 조직문화의 개선을 외쳐도 개선이 되지 않는 것도 맞다. 그렇다면 다시 이런 질문을 던져볼 수 있겠다. 그럼 조직문화는 내 담당도 아니고… 나는 개발자니까 프로그래밍을 잘하는 것에 집중해야 하지 않을까? 음, 그 말도 맞다. 개발자로써 당연히 프로그래밍을 잘하면 좋다. 사실 프로그래밍을 잘한다는 사실 자체 보다는 프로그래밍을 잘하고 싶은 욕구가 있어야하는 것이 더 중요하다고 생각한다. 이건 개발자라는 직업을 가진 사람으로써 기본적인 욕구이자 직업적인 책임이다. 냉정해보이지만 사실 당연한 개념인데, 어떤 사람이 오버워치 프로게이머인데 티어가 브론즈라고 생각해보자. 여기서부터 벌써 응? 브론즈?라는 생각이 들 것이다. 게다가 이 사람이 5년 동안 심해에만 머물러 있다면 이건 뭔가 잘못된 거다.(필자는 골드다.) 하지만 어쨌든 프로그램으로 먹고 사는 사람에게 프로그래밍을 잘한다는 것은 결국 패시브 스킬 같은 것이다. 좋은 개발자는 프로그래밍을 넘어선 부분도 생각해야한다. 결국 우리는 컴퓨터와 사람 사이에서 일하는 사람들이기 때문에 컴퓨터 바깥의 세상으로부터 자유롭지 못하기 때문이다. 그 컴퓨터 바깥의 세상에는 나와 함께 일하는 팀원이 있고, 그 너머에는 우리 팀 전체, 또 그 너머에는 우리 회사가 있다. 다시 강조해서 말하지만 우리는 절대 컴퓨터랑만 일하는 사람들이 아니라는 점을 명심하자. 심지어 여러분이 세계 곳곳을 여행하며 원격 근무하는 디지털 노마드라고 하더라도 한국의 업무 시간에 맞춰서 새벽에 스카이프에 접속해야하는 일은 비일비재할 것이다. 결국 우리는 어쩔 수 없이 조직에 영향을 받을 수 밖에 없고, 좋은 조직문화는 결국 조직원 개개인의 행복이나 업무의 성취감, 비전에도 영향을 끼친다. 이것이 필자가 생각하는 개발자도 조직문화에 관심을 가져야하는 이유이다.조직문화에 관심을 가진다는 것은 당연히 좋은 조직문화에 대해서 고민한다는 것도 포함한다. 좋은 조직문화는 보통 하루아침에 뚝딱 만들어지는 것이 아니기 때문에 우리 같은 팀원들과 CEO같은 리더들의 노력이 함께 어우러져야 겨우겨우 조직에 자리잡을 수 있다. 그렇다면 우리가 당장 내일부터 조직문화를 위해, 즉 우리 팀을 위해 할 수 있는 행동은 어떤 것이 있을까? 우리부터 먼저 행복해지자필자는 개인이 행복하면 조직도 행복하다라고 생각하는 사람이다. 여러분이 매일 아침 출근하는 길이 지옥같고 사무실에서는 아무 재미없는 일만 하느라 멍하게 코딩만 한다면 그 조직은 건강한 조직일까? 필자 생각에는 아닐 가능성이 높다. 게다가 이런 문제는 흔히 말하는 사회생활 때문에 겉으로 보기에는 잘 드러나지 않는다. 사실 필자가 이야기하는 개인이 행복하면 조직도 행복하다라는 말은 뜬구름 잡는 이야기가 아니다. 이미 행복과 같은 긍정적인 감정을 가진 사람이 업무 효율성이나 집중도, 능력 발전도가 더 좋다는 연구 결과가 이미 많이 나와있기 때문이다. 개인의 이러한 발전은 결과적으로 조직에도 긍정적인 영향을 끼칠 수 밖에 없다. 이 분야에서 나름 알려진 개념은 긍정심리자본이 있는데, 이 개념은 경영학자인 프레드 루선스(Fred Luthans)가 2000년에 창시한 개념이다. 이 개념은 이미 논문도 많고 연구도 활발히 진행되고 있는 개념이기 때문에 한번쯤 구글링해보는 것을 추천한다. 구글에 “긍정심리자본”이라고 검색하면 많은 학술자료가 나온다 그럼 조직 안에서 개인이 행복해지려면 어떻게 해야할까? 사실 조직 자체에서 조직원들이 행복할 수 있도록 관심을 가지고 고민하며 도와주는 것이 좋긴 하지만 아쉽게도 아직까지 많은 조직들은 이런 행복의 힘에 대해서 깊게 생각하고 있지 않은 것이 현실이다. 하지만 그렇다고 개개인이 아무 것도 할 수 없는 것은 아니다. 조직원 개개인의 작은 행동들이 모여서 자연스럽게 암묵적인 조직의 분위기를 만들 수 있기 때문이다. 게다가 아무리 조직이 조직원들의 행복에 관심을 가지고 탑다운(Top-down)식으로 행복한 조직문화를 만들었다고 해도 결국 조직이 만든 명시적인 조직문화와 개개인들이 만든 암묵적인 조직문화가 일치하지 않는다면 겉으로는 그럴싸해보일지 몰라도 실제로는 안에서부터 곪아 터질 수 있다. 마치 이런 느낌이랄까 그렇기 때문에 조직이 만들어가는 명시적인 조직문화와 동시에 조직원들이 암묵적인 조직문화를 만들어 나가는 것이 중요하다. 그럼 좀 더 구체적으로 우리가 암묵적인 조직문화를 만들어나가기 위해 어떤 행동과 마음가짐을 가져야 할 지 살펴보자. 동료를 존중하자필자는 중학교 1학년이었던 2004년부터, 군대에 입대하는 해인 2011년까지 비보이로 활동했었다. 그냥 학교 동아리가 아니라 나름 CYON BBOY CHAMPION SHIP에서 서울 3위를 했던 프로 비보이 크루의 일원이었던 적도 있었다. 한 7~8년 정도 힙합씬에 있었다는 소리다. 오른쪽에 브이하고 있는 게 필자. 거의 10년 전이니 확실히 어려보인다. 부럽… 힙합은 Respect라는 정신을 굉장히 강조하는 데 이는 말 그대로 서로를 존중하는 것을 의미한다. 비보이들이 배틀에서 각종 제스쳐와 춤으로 상대를 서로 도발하고 싸우긴 하지만 그 이면에는 상대방을 나와 같은 비보이로 생각한다는 정신이 깔려있는 것이다. 필자가 고등학생 때에는 찔랭이 동네 비보이부터 날고 기는 프로 팀까지 모두 연습을 하러 오는 유명한 연습실인 문래YMCA(현 영등포구 문래청소년수련관)란 곳이 있었는데, 필자도 주말마다 여기서 비보잉 연습을 하면서 많은 형들의 조언을 들을 수 있었다. 이 당시 필자는 세계적으로 유명했던 한 비보이로부터 우연히 조언을 들은 적이 있었는데, 정확한 문맥은 기억안나지만 그 분의 말 한마디는 아직도 똑똑히 기억난다. 누군가 비보잉을 하고 있다면, 그 사람이 춤을 시작한지 하루가 되었든 10년이 되었든 우리는 다 같은 비보이다. 그러니까 상대방을 Respect하는 습관을 들여라. 이 말은 사실 필자의 가치관 형성에도 굉장히 많은 영향을 줬다. 그리고 필자는 이 Respect 정신을 비보잉을 하던 음악을 하던 프로그래밍을 하던 항상 똑같이 적용해왔다. 어떤 분이 프로그래밍을 하는 직업을 가지거나 지금 프로그래밍을 공부하고 있는 분이라고 하면 그 자체만으로도 그 분을 개발자로써 존중해야 한다는 것이다. 이 정신을 마음 속에 품고 다른 개발자와 이야기를 나눈다면 더 이상 그 사람의 경력은 중요하지 않다. 어제 처음 입사한 신입 개발자든 이 바닥에서 10년 넘게 구른 시니어 개발자든 우리는 개발자라는 사실 앞에서 평등하다. 물론 시니어는 그 동안 겪어온 짬이 있기 때문에 주니어보다 더 프로그래밍을 잘할 수 있다. 하지만 순수하게 기술력만을 놓고 본다면 시니어보다 잘하는 주니어도 많다는 사실을 잊지 말자. 단순히 상대방이 나보다 경력이 적다는 이유로, 어리다는 이유로 무시하면 안된다는 것이다. 뭐 가끔은 얘기하다보면 상대방이 논리적으로 틀린 말을 할 수도 있지만 그렇다고 이런 것도 몰라?라고 면박주는 것이 아니라 잘못된 것에 대해서 제대로 피드백을 줄 수 있는 성숙한 마음을 가지자. 또 누군가 나에게 피드백을 준다면 그 사람이 초등학교 갓 졸업한 14살짜리 프로그래머일지라도 그 피드백에 대해서 고민 해볼 수 있는 마인드를 가져야한다. 상대방이 프로그래밍을 업으로써 본인이 먹고 살수 있다면 신입이든 시니어든 개발자는 개발자다. 그러니까 항상 같이 일하는 동료를 존중하는 마음을 가지자. 동료들이 팀 내에서 자기 능력을 존중받는다고 느낀다면 결국 여러분에게도 그 존중은 돌아오게 되어있다. 서로 존중받지 못하는 팀에 개인의 행복은 있을 수 없다. 조직 안에서 나의 행복을 찾아보자요즘 출판되고 있는 자기계발서에서 흔하게 하는 뻔한 이야기라고 생각할 수 있지만, 위에서 한번 설명했듯 이게 틀린 말은 또 아니기 때문에 대충 넘겨서도 곤란하다. 그리고 필자가 말하고 싶은 행복이라는 것은 생각보다 거창한 개념이 아니다. 사실 필자는 아 행복해~라는 감정과는 굉장히 거리가 먼 사람이다. 그렇다고 불행하다는 것은 아니고 내가 지금 행복한 상태인가?와 같은 질문을 스스로에게 해본 적이 별로 없기 때문에 난 지금 행복해!라는 생각도 안하는 것이다. 그래서 필자가 말하는 행복은 단지 불행하지 않은 상태라고 봐도 무방하다. 개발자다운 이분법적인 생각(!불행 === 행복)일수도 있지만, 행복이란 것은 자기 마음먹기 나름이기 때문에 필자는 결국 행복한 게 맞는 것 같다. 하지만 우리가 일반적으로 생각하는 행복은 뭐랄까… 연봉이 20% 인상되었을 때와 같은 행복인 경우가 많은 것 같다.(정확하게 뭐라 말로 표현 못하겠다) 하지만 이런 류의 행복은 쳇바퀴와도 같아서 내가 연봉이 20% 올랐어도 결국 금방 또 다음 단계의 행복을 찾아나서게 된다. 오 연봉이 4천이 됐다! 근데 세금을 너무 많이 떼가네…? 한달에 400만원만 벌어봤으면 좋겠는데… 물론 연봉이 올라서 느끼는 감정도 행복일 수 있지만 필자가 말하는 행복은 이런 행복이 아니다. 그것보다는 좀 더 소박한, 필자가 방금 얘기한 것 같이 난 딱히 불행한 상태가 아니잖아? 그럼 나름 행복한 것 같은데?과 같은 작은 행복이다. 그리고 이 행복의 기준은 사람마다 너무 다르기 때문에 콕 찝어서 뭐라 말하기는 힘들다. 어떤 사람은 행복의 기준이 무조건 돈일 수도 있고 어떤 사람은 그냥 아침에 출근할 때 상쾌한 바람 냄새만 맡아도 행복할 수 있기 때문이다. 그렇기 때문에 한번 필자의 기준을 토대로 이야기해보겠다. 필자의 기준도 어떤 이에게는 이해가 안될 수도 있겠지만 그냥 이런 사람도 있구나 정도로 받아들이도록 하자. 일단 필자는 굉장히 직선적인 성격이다. 아니다 싶으면 아니라고 말하고 맞다 싶으면 맞다고 말하는 그런 사람이다. 직장에서는 50명 가까운 사람들이 모여있는 자리에서 지금 우리가 하고 있는 데이터 기반 의사결정이 진짜로 우리 유저들을 위한 것이 아니라 순전히 우리의 지표를 위한 결정이라고 비판하며 우리 회사가 잘못하고 있는 점을 거의 한시간 동안 하나하나 지적한 적도 있었다. 필자의 이런 행동들을 보고 어떤 사람들은 자신의 마음을 대변해줘서 고맙다고 하는 사람도 있었지만 반면에 너무 직선적이라 걱정된다고 해주시는 분도 있었다.(다른 회사는 더 하면 더 하다는 말과 함께…) 물론 필자도 이런 성격이 사회 통념 상 안좋게 보일 수도 있다는 사실은 당연히 알고 있고, 또 그냥 있는 듯 없는 듯 살며 사회에 맞출 수도 있다. 군대에서는 필자의 직선적인 성격을 다 숨기고 그냥 까라면 까는 충실한 군인이기도 했기 때문에 뭐 지금도 그렇게 다시 하라고 한다면 못할 건 없다. 그리고 CEO를 포함한 전직원이 있는 자리에서 회사 전체를 비판하며 나 혼자만 외롭게 얘기한다는 것은 솔직히 쉬운 일은 아니다. 이런 상황에서 왠만한 사람은 다 긴장할 수 밖에 없고, 혹시 내가 틀렸으면 어떡하나라는 생각도 들기 마련이다. 그래서 이 얘기를 하기로 마음먹은 후에는 거의 매일 새벽 2~3시까지 논문부터 학술자료나 사례까지 닥치는 대로 찾아봤었다. 하지만 이런 장애물을 다 넘어서 결국 필자가 이렇게 행동할 수 있었던 이유는 필자가 어떤 조직의 조직원으로써 느끼는 행복의 기준이 바로 내가 만드는 것이 사람들을 도울 수 있는 무언가가 되는 것이기 때문이다. 그렇기 때문에 데이터 기반 의사결정을 비판할 때도 많은 사람들 앞에서 난 전환율이나 매출같은 지표를 올리기 위한 프로덕트를 만드는 것이 아니라, 진짜로 사람들이 우리 서비스를 통해 좋은 경험을 얻어갈 수 있는 그런 프로덕트를 만들고 싶다. 라고 자신의 의견을 이야기할 수 있었던 것이다. 그리고 저렇게 한바탕 이야기하고 난 뒤 많은 분들이 공감을 해주셔서 결국 회사가 좋은 방향으로 바뀌긴 했다. 이게 진짜 잘한건지 아닌건지는 아직도 고민 중이긴 하지만… 어쨌든 이런 과정을 통해 결국 필자는 내가 이 프로덕트를 만들면 유저에게 어떤 도움이 되는지를 공감하면서 프로덕트를 개발할 수 있었고 결과적으로 만족했다. 조직 내에서 자기만의 행복을 찾는 방법은 이렇게 적극적인 방법부터 조심스러운 방법까지 아주 다양한 방법이 있으니 한번 퇴근 후 집에서 내 행복의 기준은 무엇일까라는 고민을 한번쯤은 다들 해보았으면 좋겠다. 내가 컨트롤할 수 없는 것에 집착하지 말자이건 조금은 스님같은 이야기라 공감 못하시는 분들도 있을 수 있는데, 필자의 경험상 이 내용과 가장 알맞는 예시는 바로 돈이다. 필자는 재작년인 2017년까지만 해도 돈에 굉장히 민감했었다. 이게 어느 정도였냐면, 한달에 30만원으로 살던 시절이었다. 교통비, 담배값, 밥값 다 합쳐서 30만원이다. 대학생 때 그렇게 살던게 직장인이 되어서도 계속 습관처럼 굳어진 것인데, 이 내면에는 돈을 빨리 모아서 집이라도 하나 사놔야지라는 욕구가 있었던 것 같다. 뭐 이런 생각은 필자 나이 또래의 모두가 하는 생각이니 그렇게 특별한 생각은 아니다. 근데 필자는 조금 도가 지나쳤던 것이, 한달에 30만원으로만 살면 옷을 사거나 머리를 하거나 친구들이랑 술 한잔하는 그런 건 솔직히 힘들다. 그리고 결정적으로 돈은 내가 모으고 싶다고해서 그렇게 팍팍 모이는 것이 아니다. 필자가 한달에 30만원씩 쓰면서 돈을 모으고 모아도 필자의 목표에는 거의 다가가지 못하고 있었다. 그래서 어느 날 갑자기 이렇게 돈만 모으면서 사는 것에 대해서 회의감도 들고 조금 지쳐서 무기력한 상태가 지속되었던 적이 있었는데, 그 당시 필자가 다녔던 직장의 CTO와 티타임을 가지며 내가 이렇게 사는 게 맞는 지 모르겠다는 느낌의 얘기를 나눴었던 적이 있다. 근데 사실 필자의 마음은 이미 답정너였다. 그냥 엄살부리지말고 더 열심히 모아!라는 소리가 듣고 싶었던 것이다. 하지만 그 형님에게서는 의외의 대답이 돌아왔다. 어차피 마티즈 탈 운명인 놈이 아무리 열심히 해봤자 소나타정도 타겠지. 벤츠는 절대 못타.마찬가지로 에반이 돈을 아무리 열심히 모아도 운좋게 뭐라도 되지 않는 이상 방 2개인 집이 방 3개인 집이 되는 그 정도 차이야. 이 말은 어떻게 보면 굉장히 냉정하게 들릴 수 있는 말이었지만 필자는 이때 많은 것을 느꼈던 것 같다. 방 두칸이 세칸이 되면 내가 행복해질까? 하는 그런 고민을 그때부터 했던 것 같다. 오랜 기간 고민을 해봤지만 꼴랑 방 한칸이 필자 인생에서 그렇게 큰 비중을 차지할 수는 없을 것 같았기에 그냥 생긴대로 살자라는 결론을 내리게 되었다.(그리고 본격적으로 주식을 시작했다…) 물론 위에서 한번 얘기했던 연봉도 마찬가지다. 연봉이 오르면 기분이 좋을 수는 있다. 하지만 그 좋은 기분은 잠깐이다. 사람 마음이라는 것이 워낙 간사하기 때문에 내가 연봉이 20%가 올라도 한 6개월 정도 지나면 또 20%를 올리고 싶어질 것이다. 하지만 좋은 기회로 이직이라도 하지 않는 이상 그건 불가능하다. 게다가 연봉이 2천만원에서 3천만원이 될 때는 상승폭이 크게 느껴지지만 3천에서 4천, 4천에서 5천이 되면 세금을 점점 많이 떼어가기 때문에 실질적으로 들어오는 실수령액은 그렇게 큰 차이가 나지 않는다. 필자가 이런 이야기를 하는 이유는 바로 돈이라는 것이 내 뜻대로 되는 것이 아니기 때문이다. 만약 돈을 많이 벌고 싶어하는 사람들이 전부 돈을 많이 벌 수 있었다면 세상에는 부자들 밖에 없을 것이다. 연봉이 오르는 것, 책 집필 같이 새로운 수입이 생기는 것 등은 어느 정도 운빨도 필요하다. 100% 실력으로 승부할 수 있는 것이 아니라는 것이다. 그래서 필자는 이런 생각 끝에 그냥 열심히 코딩이나 하자. 돈은 언젠가 따라오겠지.라는 마음으로 나름 행복하게 지내고 있다. 마음대로 컨트롤할 수 없는 돈에 집착하는 것은 뭔가 짝사랑과 비슷한 느낌이라 굉장히 힘든 것 같다. 그냥 남는 돈은 구글이나 엔비디아같은 대형주에 부어놓고 하늘에 맡기도록 하자. 마치며필자가 여러 번 이야기 했듯이 사실 좋은 조직문화의 형성에는 반드시 조직의 리더인 CEO의 관심이 수반되어야한다. 하지만 아직까지는 CEO가 조직문화까지 관심을 가지는 경우가 많지 않고, 또 관심이 있다고 해도 우선순위에서 밀리는 경우가 많기 때문에 현실적으로 회사에서 알아서 좋은 조직문화를 만들어주겠지라고 기대하기에는 조금 무리가 있다고 생각한다. 하지만 위에서 설명했듯이 당장 우리도 팀을 위해, 조직을 위해 할 수 있는 일들이 있다. 나부터 행복해지자라는 말은 언뜻 보면 나만 행복해지자는 말 같지만 처음에 이야기 했듯이 결국 나 개인의 행복은 우리 팀의 행복이 될 수도 있고, 더 나아가서 암묵적으로 행복한 조직문화를 형성할 수도 있다. 사실 필자가 이야기한 내용들은 필자의 경험을 기반으로 한 주관적인 내용들이기 때문에 공감하지 못할 수도 있다. 하지만 필자가 이 글을 통해 하고 싶었던 말은 나처럼 행복해져라가 아니라 지금 당장 여러분이 조직문화를 위해 할 수 있는 작은 일들이 있다는 것이다. 우리는 비록 월급받고 일하는 월급쟁이 개발자이지만 결국 어떤 조직의 일원이고, 또 그 조직을 위해서 우리가 할 수 있는 일은 프로그래밍말고도 많이 있다고 생각한다. 그리고 우리가 그렇게 조직문화에 대해서 관심과 개선하고 싶다는 의지를 가지고 조금씩 노력하다보면 어쩌면 10년 후에 새로 입사한 개발자들은 우리보다 더 재밌고 행복하고 건설적인 환경에서 일할 수도 있지 않을까? 이상으로 개발자가 조직문화에 대해 관심을 가져야 하는 이유 포스팅을 마친다.","link":"/2019/08/06/developer-with-organizational-culture/"},{"title":"2년 동안 근무했던 회사를 떠나며","text":"오늘, 2019년 8월 16일을 마지막으로 지난 2년 동안 근무했던 회사를 떠나게 되었다. 지금은 사무실에서 말년의 여유를 즐기며 이 포스팅을 서두를 작성하고 있다. 그래서 이번 포스팅에서는 지난 2년 간 필자가 이 회사를 다니면서 느꼈던 점이나 입사 당시와 비교해서 지금 달라진 점들에 대해서 회고를 진행하려고 한다. 필자가 처음 이 회사에 입사했을 때는 직원 수 13명 정도에 아직 Series A 투자도 받지 못했던 신생 스타트업이었지만, 지금은 어느 덧 120억 정도의 Series B 투자도 받고 직원 수도 50명 정도 되는 규모의 회사가 되었다. 필자도 회사가 이렇게 성장하는 것을 직접 경험하는 것이 처음이었기에 재밌는 점도 있었지만 한편으로는 조직이 커짐에 따라 변해가는 사내 문화와 조직의 분위기를 보면서 규모가 작을 때에 대한 향수를 느끼기도 했었다. 어쨌든 필자는 이제 휴식과 재정비를 위해 오늘을 마지막으로 회사를 그만 두게 되었으니 입사 당시의 기억부터 2년이 지난 오늘까지의 기억을 되짚어 보며 한번 회고를 진행해보도록 하겠다. 내가 만든 레거시 코드필자는 이 회사에 입사하기 전에 아는 분과 함께 하던 스타트업을 시원하게 말아먹고 2개월 정도 혼자서 토이 프로젝트로 이것저것 만들면서 놀고 있는 한량 개발자였다. 하루는 홍대에 있는 필자의 단골 카페(지금은 없어진 홍대 디자이너스 라운지…)에서 혼자 코딩을 하고 있었는데 왠 이상한 외국인 아저씨가 링크드인으로 면접 제안 메세지를 보낸 것이다. 그래서 몇 마디를 나눠보니 그 분이 지금 만들고 있는 서비스도 나름 재밌을 것 같았고, 회사의 분위기나 비전도 필자와 잘 맞을 것 같아서 그 다음 주에 바로 면접을 진행했고, 면접이 끝난 그 자리에서 바로 합격 통보를 받았다. 그 당시 주고 받았던 질답 중에 지금까지 기억나는 질답은 딱 이거 하나다. CTO: 혹시 VueJS 하실 수 있으세요? 저희가 이런 거를 만들어 보려고 하는데…(다른 서비스의 컴포넌트를 보여줌)필자: Vue는 거의 안 써보긴 했는데… 뭐 개인적으로 공부 하면서 하면 만들 수 있을 것 같네요. 면접이 끝나고 나서 생각해보니 이 사람들 도대체 뭘 보고 날 뽑은거지...?라는 생각이 들 정도로 프론트엔드에 대한 깊은 질문이나 코드 테스트도 없었는데, 입사 첫 날에 바로 현재 상황을 파악할 수 있었다. 일단 필자가 이 회사의 첫번째 프론트엔드 개발자였다. CTO가 기존 Django로만 작성된 프로젝트를 운영하다가 프론트엔드 프레임워크의 필요성을 느끼고 React를 도입하려다가 한번 실패하고 차선책으로 Vue를 어떻게든 돌아가게만 적용해놓은 상황이었다. 하지만 당시 CTO는 백엔드 개발자였기 때문에 프론트엔드에 전문성을 가진 누군가가 이 상황을 타개해줬으면 했던 것이고 그래서 필자가 간택당한 것이다. 그래서 필자가 입사한 다음 했던 첫 작업이 바로 gulp와 webpack을 사용해서 프론트엔드 빌드 프로세스를 만들고, 번들링되어 나온 파일들을 Django와 연동시키는 그런 작업이었다. 하지만 당시 비즈니스 상황 상 프론트엔드 어플리케이션을 Django로부터 완전 분리해서 작성할 수가 없었기에 Django가 서버에서 자체 템플릿 엔진을 사용하여 한번 HTML을 렌더하고 클라이언트에서 Vue가 2차 렌더링을 진행하는 방식으로 적용할 수 밖에 없었다. 그리고 2019년 현재 구글에 Django Vue라고 검색해보면 Django 템플릿과 Vue를 동시에 사용할 수 있는 방법을 설명한 몇 개의 포스팅이 나오지만 필자가 이 작업을 진행했던 2017년 5월에는 그런 레퍼런스가 단 한개도 존재하지 않았기 때문에 모든 것을 필자 스스로 설계하고 아이디어를 짜내서 연동해야했다. 당시에 이 작업으로 포스팅을 작성했으면 좋았을 것 같은데 입사한 지 얼마 되지 않았을 때라 포스팅이고 나발이고 일단 전체 코드를 파악하는 것만 해도 정신이 없었다. 결국 이 어플리케이션은 현재 프론트엔드 챕터에게 레거시라고 불리며 필자를 포함한 그 누구도 건드리기 싫어하는 어플리케이션이 되었는데, 뭐 사실 그때 당시에는 그게 최선이긴 했기 때문에 당시 결정에 후회는 없다. 하지만 필자가 작성한 코드가 시간이 지나서 다른 개발자들에게 레거시라고 불리게 되는 경험은 처음이었기에 나름 흥미로웠다. 이 코드는 사실 그 당시 필자가 만들면서도 이거 나중에 분명히 문제될 것 같은데...?라고 생각할 정도로 일반적인 프론트엔드들이 이해하기 힘든 기형적인 구조였다. 문제가 많아서 일일히 나열하기는 힘들지만 대표적인 이슈는 바로 여러 개의 Vue 인스턴스가 존재한다는 것과 DJango를 할 줄 알아야 수정이 가능한 부분이 있다는 것, 그리고 기존 코드에서 bower 패키지를 사용하고 있었기 때문에 bower를 걷어내고 npm으로 완전히 갈아타는 것이 불가능하다는 것이었다. 물론 이 이슈는 현재진행형이다. 이런 것들이 하나하나 쌓여서 결국 기술 부채가 누적되는 것이기 때문에 언젠가 청산을 하긴 해야 했다. 그래서 필자는 퇴사 후 현 직장과 2개월 프리랜서 계약을 체결하고 이 레거시 어플리케이션에 붙어있는 프론트엔드 어플리케이션을 전부 새로 만든 프론트엔드 어플리케이션 프로젝트로 마이그레이션하는 작업을 진행하기로 하였다.(내 똥은 내가 치운다! 라는 느낌이랄까) 기술만능주의에서 조직을 보는 개발자로이 내용은 좋은 프로그래머란 무엇일까?라는 포스팅에서도 한번 언급한 적이 있었는데, 처음 필자가 이 회사에 입사했을 때 필자가 생각하는 좋은 개발자의 기준은 요구사항을 빠르고 완벽하게 개발하는 개발자였고 필자의 개발 스타일은 빠르게는 만들지만 자잘한 버그가 많은 스타일이었다. 그래서 필자는 당시 이 단점을 보완하기 위해 유닛 테스트나 실수를 줄일 수 있는 설계 패턴 등을 통해서 신뢰성이 떨어지는 필자 코드에 대한 방어선을 펼치는 방향으로 노력을 했었다. 하지만 필자의 첫 직장과 다르게 여기서는 CTO가 모든 의사결정을 하지 않고 프론트엔드 어플리케이션에 대한 의사결정은 프론트엔드 개발자가 하는 구조였다. 단순히 필자의 단점을 보완하는 것 뿐만 아니라 비즈니스에 유연하게 대응할 수 있는 설계와 인프라, 현재 어플리케이션이 가지고 있는 문제점의 분석 등 많은 것을 함께 해야했던 것이다. 또한 팀 외부와 소통하는 것 또한 필자가 프론트엔드 개발자로써 직접 해야했다.(특히 마케팅팀과 SEO, 유저 행동 데이터 수집 등에 대해 많은 커뮤니케이션을 했었다) 이 과정에서 필자가 생각하고 있던 개발의 영역이 많이 확장되게 되었는데, 기존에는 내가 어떻게 하면 더 개발을 잘할 수 있을까?를 위주로 고민했다면 이 과정을 겪고 나서는 이 어플리케이션이 어떻게 하면 더 발전할 수 있을까?를 더 고민하게 되었다. 더 나아가서 지금은 어플리케이션 뿐만 아니라 이 팀이 어떻게 하면 더 효율적으로 일할 수 있을까?까지 고민하고 있는 상황이다. 같은 팀이라는 것필자가 입사하고 얼마 되지 않았을 때 회사가 Series A 투자를 받게 되었는데, 그때 CEO가 했던 약속이 우리 투자받으면 해외로 한번 워크샵가자!였었다. 그래서 필자는 입사한지 거의 한달만에 베트남 다낭으로 워크샵을 떠나게 되었다.(개꿀) 대략 3박 4일 정도였던 것 같은데 문제는 한국으로 귀국하기 바로 전날 밤에 터졌다. 필자가 워크샵을 오기 전에 개발했던 기능이 있었는데 이 기능이 Internet Explorer에서 아예 작동하지않고 있었던 것이다. 그 기능이 배포된 지는 대략 3주 정도 되었고, 유저들이 IE에서 해당 기능을 사용하지 못하자 데이터 플로우 차트가 이상하게 그려지기 시작했던 것이다. 당시 사내에 IE를 사용하는 사람이 단 한명도 없었기에 이 문제를 조기에 발견하지 못했다. 하지만 제일 큰 책임은 해당 기능을 개발하고 제대로 크로스 브라우징을 테스트하지않았던 필자에게 있었다. 그래서 필자는 그 이야기를 듣자마자 바로 맥북을 펴고 핫픽스를 하기 시작했는데 그때 시간은 이미 00시를 넘기고 있었다. 그때 들었던 감정은 음, 테스트를 제대로 하지 않았다는 자책감과 나의 부주의함으로 인해 회사에 피해를 끼쳤다는 미안함이었던 것 같다. 어쨌든 이걸 빨리 고쳐야하니 정신없이 분석하면서 원인을 찾게 되었는데, 제일 큰 문제는 바로 Array.prototype.findIndex 메소드를 babel이 제대로 트랜스파일링 해주지 않았던 것이었고 그 외 자잘한 이슈도 몇개 더 있었다.(필자는 이 이후로 babel을 안 믿는다) 다행히 빠르게 버그 원인을 알아냈지만 진짜 문제는 베트남 인터넷이 너무 느려서 소스를 빌드하고 해당 기능의 작동을 확인하는 것도 오래 걸리고 문제가 발생한 부분이 서버와의 API 통신을 담당하는 모듈이라서 해당 모듈을 사용하는 핵심 기능을 전부 테스트해야 했었다. 게다가 그걸 고치는 과정에서 빌드 프로세스도 갑자기 맛이 가는 등 연쇄적으로 문제가 커져서 픽스 시간은 점점 늘어났다. 추억돋는 그 날의 커밋들 결국 이런 저런 이슈들이 겹쳐서 최종 릴리즈는 오전 5시가 넘어서야 할 수 있었다. 이때 중요한 점은 모든 팀원들이 그때까지 필자와 함께 밤을 샜다는 것이다. 사실 이건 100% 프론트엔드 이슈였다. 당시 프론트엔드 개발자는 필자밖에 없었고 나머지는 전부 모바일 앱 개발자, 백엔드 개발자였기 때문에 다른 개발자 분들이 픽스를 도와줄 수 있는 것도 아니었다. 게다가 필자와 함께 밤을 새준 사람 중에는 UI/UX 디자이너도 있었다. 새벽 한시에도 옆을 함께 지켜준 팀원들 이때 필자는 팀원들한테 미안하면서도 굉장히 고마운 마음이 들었다. 당시 당황한 마음으로 버그 픽스하고 있는 필자에게 팀원들은 괜찮으니까 천천히 꼼꼼히 고치라면서 격려도 해주고 먹을 것도 가져다주고 도와줄 게 없는 지 물어봐주기도 했다. 앞으로도 다른 회사에서 또 이런 팀을 만나볼 수 있을까라는 생각이 들 정도로 정말 고마운 분들이었고 다들 각자의 길을 찾아나선 지금도 물론 꾸준히 연락을 주고 받으면서 잘 지내고 있다.(어제는 퇴사 선물도 받았다.) 필자는 사실 개인주의적인 성향이 강한 사람이었는데 이런 경험들은 필자가 팀의 발전에 대해서 꾸준히 고민할 수 있게 만들어준 소중한 경험이 되었다. 다른 사람을 도와주는 사람으로작년 여름 쯤에, 필자가 입사할 때부터 CX팀에서 일하던 분이 보직 변경을 통해 PO(Product Owner)로 오게된 일이 있었다. 물론 이 분의 전공은 이 쪽 분야가 아니기도 하고 프로덕트와 관련된 일을 해본 경험도 거의 없었기 때문에 신입과 다름 없는 상태였다. 그러다가 팀을 스쿼드 편제로 나누게 되면서 이 분과 필자는 같은 스쿼드에서 함께 일을 하게 되었는데, 처음 같이 일을 할때는 사실 별로 이 분에 대해서 신경을 안썼다. 뭐 알아서 잘하시겠지… 혹여나 못하더라도 나나 누군가가 커버할 수 있겠지 정도? 하지만 같이 일을 하게 되면서 그 전까지는 몰랐던 이 분의 새로운 모습들을 많이 보게 되었다. 비록 남들보다 경험은 적을지라도 굉장히 프로덕트를 만든다는 일에 열정적이었고, 또 질문도 굉장히 많이 하시는 편이었는데 질문의 질이 굉장히 좋았다. 그래서였는지 이 분이 필자에게 개발에 대한 질문을 할 때마다 필자도 점점 열의를 가지고 가르쳐드리게 되었는데, 이 분은 항상 그 점에 있어서 필자에게 고맙다는 이야기를 많이 해줬었다. 필자 또한 필자가 알고 있는 지식을 그 분에게 알려드리면서 실제로 그 분이 빠르게 성장하는 모습을 보고 나니 필자도 기분이 굉장히 좋았다. 내가 이미 알고 있는 별 것 아니라고 생각했던 지식이 다른 사람에게는 굉장한 도움이 될 수 있다는 생각을 이때 처음으로 하게 되었던 것 같다. 그리고 그 분의 스쿼드 운영 스타일은 개발자와 디자이너들이 모두 자유롭게 아이디어를 내고 본인이 그걸 취합하고 정리하면서 진행하는 참여형이었는데 그 분의 경험과 실력이 점점 쌓이면서 시너지 효과가 발생해서 팀의 생산성도 높아지고 팀의 사기와 분위기도 많이 좋았던 기억이 난다. 결국 필자가 그 분에게 개발에 대한 지식을 알려줬던 그 행동이 필자와 그 분뿐만 아니라 팀 전체에 영향을 주었던 것이다. 그런 과정을 겪으면서 필자는 개인이 각자 잘하면 팀은 알아서 잘 굴러간다와 같은 생각에 더 이상 동의하지 않게 되었다. 아싸에서 나름 인싸로필자는 원래 사람 만나는 것을 별로 좋아하는 편이 아니었다. 하지만 이 직장에서 많은 일을 겪으면서 뭔가 성격이 좀 변했다고 해야하나…? 갑자기 인싸 짓을 하고 다니기 시작했다. 처음 입사했을 때 필자와 며칠 차이로 입사했던 백엔드 개발자분이 계셨는데, 이 분과는 입사하고 거의 한달 정도는 업무 이야기 외에 별로 말을 안할 정도로 사람한테 관심이 없었다.(이 분도 낯가리는 성격이라 둘 다 말을 잘 안걸었다) 근데 지금은 어떤 분이 새로 입사하면 먼저 말도 걸기도 하고 다른 팀과 같이 술을 마시거나 게임도 하러가는 등 인싸같은 행동을 많이 하게 되었는데, 지금 생각해보면 아마도 작년의 번아웃이 뭔가 계기가 되었던 것 같다. 필자는 2018년 여름 쯤에 한번 번아웃을 크게 겪은 적이 있었는데, 그 전까지 필자는 1년 365일 매일 코딩만 하는 사람이었다. 평일에는 밤 11시까지 야근하고 집에 와서 새벽 2~3시까지 토이 프로젝트를 하고, 주말에는 친구들과 또 다른 토이 프로젝트를 진행하고는 했던 그런 시기였다. 그렇게 일주일 내내 하루에 무조건 코딩을 7시간 씩은 했던 것 같다. 필자는 이 생활을 군대 전역 후 대학교에 복학했던 2014년부터 계속 해왔으니 번아웃이 왔던 2018년에는 대략 5년 정도 그렇게 살아왔던 셈이다. 그러다가 갑자기 번아웃이 찾아왔는데, 그때는 그냥 코딩 자체에 흥미가 떨어져서 아무것도 하기 싫었다. 듬성듬성한 필자의 2018년 깃허브 잔디 지금 생각해봐도 굉장히 당혹스러운 감정이었다. 코딩이 그렇게 즐겁고 재밌어서 5년 내내 매일 7~8시간씩 코딩을 했던 사람이었는데 하루 아침에 갑자기 코딩이 싫어지게 되는 기분이란 꽤나 이상한 감정이다. 내가 왜 이렇게까지 아둥바둥해야하지?라는 회의감도 들었고 이러다가 뒤쳐지는 거 아니야?라는 불안함도 들었다. 개발이 나랑 맞지 않나?라는 생각도 들었던 것 같다. 이 당시 필자가 잘못 했던 것은 바로 일하기 싫은 티를 너무 팍팍 내고 다닌 것이었다. 이 당시 필자의 속마음은 아 너무 재미없고 힘들다. 회사 그만 둘까...?였기 때문에 팀원들과도 거리를 두고 업무 태도도 불량했다. 낮에는 업무를 설렁설렁하고 혼자 야근하면서 일을 끝내는 경우가 많았기 때문에 일이 밀리거나 하지는 않았지만 팀원들과 커뮤니케이션을 많이 못했었다. 나중에 들어보니 당시 필자와 함께 오래 일했던 팀원들은 얘 갑자기 뭔가 맛이 갔는데...?라는 생각을 하고 필자에게 먼저 다가와 주었지만, 필자를 잘 몰랐던 다른 팀 분들은 업무를 제대로 하지 않는다고 생각하기도 했다고 한다. 그래서 당시 팀 리더가 필자에게 티타임을 신청하고 잠깐 이야기를 나눴었는데, 그때 그 분이 했던 말이 아직도 기억에 남는다. 에반이 힘든 건 에반 개인의 문제가 아니라 팀의 문제에요. 팀이 도와줄 수 있는 건 최대한 도와줄게요.대신 뭐 때문에 힘든지는 이야기 해줘야해요. 이 이야기를 듣고 솔직히 감동까지는 아니였지만 맞네. 우리는 팀이었지.라는 생각이 새삼스럽게 다시 들었었다. 그래서 이때부터 필자는 주변에 계신 분들께 필자의 현재 상황을 설명하고 많은 조언을 들을 수 있었는데, 주변에 계신 분들이 전부 똑같이 이야기한 것이 바로 좀 쉬어라였다. 매일 야근하고 주말에도 코딩하고 취미도 없고 연애도 하지 않는 생활이 필자를 지치게 만들었다는 것이다. 이런 피로감은 어느 순간 슬금슬금 생기다가 어떤 임계점을 넘어가는 순간 빵! 터지게 되는 경우가 많고, 지금이 바로 그 타이밍이라고 했다. 그래서 다들 코딩말고 좀 다른 것도 해보면서 쉬라는 조언을 많이 해주셨었다. 평소에도 이런 이야기를 주변에서 많이들 해주셨다 그래서 이때 필자는 거의 매일 술먹고 당구치고 노래방에 가는 등 나름 필자 기준에서는 스트레스를 풀기 위한 방탕한 생활을 했었는데, 이때 개발팀 외에 다른 팀에 있는 사람들과 많이 친해질 수 있었다. 개발팀끼리 놀 때는 거의 프로그래밍이나 조직문화같이 어느 정도 일과 관련된 이야기를 많이 나누었는데, 다른 팀 사람들과 술을 마시게 되니 새로운 이야기들을 들어볼 수 있어서 좋았다. 그렇게 다른 팀들의 이야기를 들었던 경험들은 회사에서 일을 할 때도 필자가 다른 팀의 상황을 조금 더 이해하고 배려할 수 있는 기반이 될 수 있었다. 능력있는 팀원들이 주는 자극이것 또한 주관적인 평가겠지만 필자가 생각했을 때 필자가 지난 2년간 이 직장에서 함께 일했던 팀원들은 대부분 자기주도적인 업무를 하며 능동적인 의사결정과 전문분야에 대한 스킬도 좋았던 것 같다. 물론 개발자만 이야기하는 것이 아니라 다른 팀에 있는 분들도 포함하는 이야기이다. 물론 가장 큰 자극은 같은 팀에서 일하는 개발자들로부터 많이 받게 되었는데, 가장 많은 자극을 받았던 팀원은 아무래도 같은 프론트엔드 개발자가 아닐까 싶다. 이 분은 바로 어제까지 필자와 함께 일했던 시니어 프론트엔드 개발자이다. 이 분은 사실 개발에 엄청난 열정을 가지고 집에서도 공부하고 그런 스타일은 아니다. 하지만 아무렇지 않게 툭툭 던지는 아이디어가 문제를 바로 해결할 수 있을 정도로 신박한 것들이 많았다. 물론 이 아이디어들은 자바스크립트에 대한 깊은 이해가 없이는 나올 수 없는 것들이었고, 이 분이 입사한지 이틀만에 필자가 일주일 동안 고민했던 문제를 단박에 풀어버리는 것을 보고 역시 세상엔 굇수가 많군이라는 생각이 다시 들었던 것 같다. 외부 SDK 스크립트 로드가 끝나는 시점을 알 수가 없어서 어플리케이션 초기화 시 해당 SDK가 담기는 전역 변수가 null인 일종의 타이밍 이슈였는데, 필자는 이걸 setTimeout으로 간신히 작동하게만 만들어 놓고 더 이상 아이디어가 없어서 방치하고 있던 상황이었다. 근데 이 분이 이 문제를 보자마자 isLoaded 변수를 Promise로 선언하고 외부 SDK 로드가 끝나면 해당 프로미스를 resolve하는 방식으로 해결했다.(지금 생각해도 변태같지만 신박한 방법이다.) 그리고 필자가 Express로 작성한 프론트엔드 렌더 서버에 있던 메모리 릭도 이 분이 해결했었는데, 이 문제는 필자가 몇번 개선은 해서 당장은 문제가 없었지만 누수 자체는 그래도 조금씩 발생하고 있었다. 그런 와중에 이 시니어분이 프론트엔드에서 사용하고 있는 외부 패키지들을 최신화했는데 갑자기 메모리 릭이 가속화된 상황이었다. 하지만 필자는 이 문제의 근본적인 원인이 무엇인지 파악하지 못하고 있는 상태였다. 그래서 이 분이 이 이슈를 가져가서 고치기로 했는데, 예상보다 빠른 시간 안에 문제를 해결해서 꽤나 놀랬던 기억이 난다. 드라마틱한 트랜잭션 타임의 변화 클라이언트에서만 사용할 수 있게 설계된 일부 외부 패키지가 서버사이드 렌더링 타이밍 때 사용되면서 발생하는 누수였는데, 필자는 이런 상황이 발생할 수 있을 것이라는 경우의 수조차 생각하지 못했었다. 그 외에도 같이 오래 일했던 백엔드 개발자 분에게도 설계나 비즈니스를 보는 관점 등에 대해서 좋은 영향을 많이 받았고, 야근을 좋아하는 모바일 개발자에게는 프로덕트에 대한 열정을 배웠다. 솔직히 팀원들에게 배운 것들이 너무 많아서 이런 것들을 하나하나 다 나열하자면 한도 끝도 없으니 이 정도만 이야기하도록 하겠다.(아마 본인들은 이 글만 보면 누군지 다 알듯) 마치며지난 2년 동안 이 팀에서 정말 많은 일을 함께 겪었던 팀원들에게 감사의 말을 전하고 싶다. 여러분과 함께 일했던 2년은, 경력과 직종에 상관없이 개발자로써 또는 한명의 인간으로써 굉장히 많은 것을 배울 수 있었던 기회였다. 개발자가 아닌 분들에게서는 개발을 넘어선 다른 분야의 지식도 함께 배울 수 있었다. 필자에게 여러분은 어떤 때에는 프로페셔널한 팀원이었고 어떤 때에는 같이 일하는 친구같은 느낌이었다. 비록 회사를 떠나며 함께 일할 수 없게 되었지만 다른 곳에서 또 만날 수 있게 되기를 바란다. 2년 동안 좋은 추억을 만들어준 여러분께 감사하다는 말을 전하며 이 글을 마친다.","link":"/2019/08/17/leave-the-company/"},{"title":"[JavaScript로 오디오 이펙터를 만들어보자] 소리의 흐름을 파악하자","text":"이번 포스팅에서는 컴퓨터는 어떻게 소리를 들을까? 포스팅에서 진행했던 오디오 파형 그리기에 이어서 오디오에 여러가지 효과를 줄 수 있는 이펙터를 만드는 과정을 설명하려고 한다. HTML5의 Audio API는 오디오에 효과를 줄 수 있는 여러가지 노드를 제공하는데, 대부분의 이펙터는 이 노드들만 사용해도 구현할 수 있을 정도로 완성도있는 API를 제공한다. 또한 이 포스팅은 총 두편에 나눠서 작성될 예정이며, 이번 포스팅에서는 HTML5 Audio API의 개요와 오디오의 음량을 조절할 수 있는 GainNode를 사용하는 방법에 대해서, 다음 포스팅에서는 좀 더 복잡한 오디오 이펙터들에 대한 설명과 제작 과정을 설명할 예정이다. 지난 포스팅에서 이미 오디오에 관한 기본적인 이론을 설명했으니 이번에는 기본적인 이론이 아니라 실제로 녹음실에서 오디오를 어떤 방식으로 컨트롤하고 효과를 주는 지에 대한 방법에 대해 초점을 맞추고 설명을 진행하도록 하겠다. 오디오 신호는 흐르는 것이다일반적인 녹음실에서 우리는 마이크를 통해서 오디오를 녹음하거나 혹은 이미 녹음된 오디오를 Logic Pro나 Cubase와 같은 DAW(Digital Audio Workstation)으로 불러와서 사용하게 된다. 이때 처음으로 받게되는 이 오디오를 소스(Source)라고 한다. 이 소스는 앰프, 컴프레서, 이퀄라이저 등 오디오에 특별한 느낌을 줄 수 있는 여러가지 이펙터들을 지나서 최종적으로 스피커나 헤드폰을 통해서 출력되게 된다. 이 흐름을 알고나면 HTML5의 Audio API가 제공하는 노드(Node)의 개념을 쉽게 이해할 수 있다. 일단 이해를 돕기 위해 필자가 예전에 사운드 엔지니어로 일할 때 사용했던 시스템을 예로 들겠다. 필자가 예전에 사용했던 장비들 사진의 중앙에 있는 커다란 장비는 아마 여러분도 TV에서 몇번 보았던 장비일 것이다. 이 장비는 여러 개의 채널로 나눠진 오디오 소스의 볼륨이나 패닝, 이퀄라이징까지 할 수 있는 일종의 컨트롤 타워 역할을 하는 믹싱 콘솔이다. 그리고 믹싱 콘솔의 오른쪽에 있는 것들이 바로 오디오에 효과를 줄 수 있는 이펙터들이다. 보통은 믹싱 콘솔 양쪽에 가득 채워놓고 쓰는데 저 사진은 아직 녹음실 셋업이 덜 끝났을 때라서 몇가지 장비만 들어가 있다. 그리고 사진에는 나오지 않았지만 따로 콘솔 랙(Console Rack)이라는 선반을 두고 거기에도 이펙터들이 가득 채워져 있다. 그리고 이펙터들의 위쪽을 보면 붉은색 선이 꽂혀있는 것을 볼 수 있는데, 저 장비가 오디오의 흐름을 컨트롤할 수 있는 패치 테이블(Patch Table)이라고 하는 장비이다. 보통 사운드 엔지니어들은 같은 역할을 하는 이펙터라고 하더라도 여러 종류의 장비를 사용하게 되는데, 이는 같은 역할을 하는 이펙터라고 하더라도 장비마다 조금씩 소리가 다를 수 있기 때문이다. 즉, 같은 리버브를 사용한다고 해도 최종적으로 만들고자하는 소리가 어떤 느낌인지에 따라 A 리버브를 사용할 수도 있고 B 리버브를 사용할 수도 있다는 것이다. 그래서 이런 소리의 질을 만드는 고유한 알고리즘은 이펙터 제조 회사들의 기업 비밀이다. 하지만 다른 이펙터를 사용하고 싶을 때마다 장비에 꽂혀있는 케이블을 일일히 하나하나 빼서 다시 다른 장비에 연결하는 것은 비효율적이기도 하고 케이블을 계속 뺐다가 꼈다가 하면 장비에 손상이 갈수도 있기 때문에 모든 장비의 라인을 저 패치 테이블에 연결해놓고 사용하는 것이다. 게다가 케이블은 대부분 장비 뒤쪽에 위치하기 때문에 저 믹싱 콘솔을 앞으로 살짝 밀고 봐야하는데, 딱 봐도 저 큰 장비를 계속 밀었다 당겼다 하기에는 무거워 보이지 않는가? 허리 나간다. 패치 테이블은 대략 이런 느낌으로 정리된다 사운드 엔지니어는 이렇게 복잡한 여러 개의 장비 사이를 흘러다니는 오디오 신호의 흐름을 패치 테이블을 통해서 한번에 파악하고 컨트롤 할 수 있다. 소리를 컨트롤하는 사람에게 오디오 신호의 흐름이라는 개념은 굉장히 중요하다. 필자가 방금 예로 든 하드웨어 장비 뿐만 아니라 소프트웨어로 구현된 이펙터를 사용하려할때도 결국은 이 흐름을 프로그램 내부에서 그대로 구현해줘야하기 때문이다. 위 사진은 전 세계 녹음실 중 90%가 사용하고 있는 Protools라는 DAW의 믹서 창이다. 사진에 강조된 부분에 Vocal Bus라고 적혀있는 곳을 보면 맨 오른쪽 채널은 위쪽에 위치하고 있고 나머지 채널은 아래쪽에 위치하고 있다. I/O 메뉴에서 위쪽은 In을 의미하고 아래쪽은 Out을 의미하기 때문에 이 그림에서 오디오의 흐름은 대략 다음과 같이 나타날 수 있다. 이때 저 네모 하나하나가 HTML5의 Audio API에서 제공해주는 노드와 정확히 같은 개념이다. 즉, 자바스크립트로 저 흐름을 완벽히 동일하게 구현할 수 있다는 뜻이다. 이해를 돕기 위해 저 노드들의 역할에 대해서 조금 더 부가설명을 하자면, 일단 Lead Vox는 말 그래도 보컬의 노래 소스를 가진 노드이고 LeadVxDbl은 노래를 풍부하게 들리게 하기 위해 같은 멜로디를 한번 더 녹음한 것, 즉 더블링 작업을 한 노드이다. 그리고 Vox Fill은 화음을 쌓은 코러스를 담은 노드일 것 같다. 그리고 보컬이 노래한 이 오디오 소스를 모두 Vocal Bus라는 노드로 모으고 있다. 이렇게 하는 이유는 여러 개의 오디오 소스에 이펙터를 각각 사용하면 노드마다 조금씩 소리의 느낌이 달라질 수 있기 때문에 Vocal Bus라는 하나의 노드로 오디오 신호를 모은 다음 해당 노드에만 이펙터를 걸어주는 것이다. 이렇게 하면 모든 노드에 이펙터를 사용하지않고 하나의 노드에만 이펙터를 사용해도 되기 때문에 메모리 비용도 아낄 수 있고, 보컬이라는 하나의 소스에 동일한 느낌을 부여할 수 있다. 그리고 최종적으로 신호가 들어가는 Sub Master 노드는 아마 최종 아웃풋으로 소리가 나가기 전에 한번 더 이펙터 처리를 하고 싶어서 생성한 것일 테고, Sub Master까지 도달한 오디오는 아웃풋, 즉 스피커를 통해 출력되어 우리의 귀로 들어오게 되는 것이다. 결국 in -> out -> in -> out의 계속된 반복이라고 보면 된다. 그래서 필자가 오디오의 흐름이라고 표현하는 것이다. 이제 오디오 소스의 흐름이라는 것이 대략 이해가 되었으면 한번 직접 HTML5의 Audio API를 사용해서 이 흐름을 구현해보도록 하자. 오디오의 음량을 조절하기위에서 이야기했듯이 이번 포스팅에서는 본격적으로 이펙터를 구현해보기에 앞서 오디오의 흐름을 직접 구현해보고 체험해보는 것에 초점을 맞출 것이다. 그래서 이펙터라기에는 조금 애매한 단순한 구조의 흐름을 만들어보려고 한다. 바로 오디오의 음량을 조절하는 흐름이다. HTML5 Audio API의 GainNode를 사용하면 오디오 소스의 음량을 손쉽게 조절할 수 있다. Gain이란 무엇인가요?게인(Gain)이란 쉽게 말하면 입력 볼륨을 의미한다. 게인을 사용하여 마이크에서 오디오 믹서나 녹음기로 오디오 신호를 보낼 때 그 신호량을 컨트롤하는 것이다. 처음 오디오에 입문하시는 분들이 게인(Gain)과 볼륨(Volume)의 차이에 대해 헷갈려하시는데, 쉽게 말하면 게인은 입력 신호를 조절하는 것이고 볼륨은 출력 신호를 조절하는 것이다. 만약 100정도 세기의 신호를 처리할 수 있는 녹음기가 있다고 생각해보자. 이때 우리가 마이크에 대고 80 정도의 세기로 소리를 왁! 지르면 이 녹음기는 무리없이 이 신호를 받아들일 수 있지만, 150의 세기로 소리를 지르게 되면 이 녹음기는 50만큼의 소리를 받아들이지 못하고 그대로 유실시킨다. 회색으로 표시된 부분이 잘려나간 신호이다. 이 현상은 여러분도 살면서 몇번 경험한 현상일텐데, 스피커로 소리를 엄청 크게 틀면 지지직거리는 잡음이 발생하는 것을 들어본 적이 있을 것이다. 이렇게 장비가 처리할 수 있는 신호의 세기를 넘어가는 현상을 클리핑(Clipping)이라고 한다. 말 그래도 신호가 잘려나가는 것이다. 이렇게 잘려나간 신호는 위 그림에서 볼 수 있듯이 머리가 네모 반듯한 사각파의 형태를 가지게 되는데, 이 사각파는 우리가 EDM 등에서 멜로디를 표현할 때 많이 들을 수 있는 신디사이저 리드(Lead) 계열의 쭈와앙~하는 금속성 소리를 낸다. 말로는 잘 이해가 안될테니 한번 음악으로 들어보면서 잠시 쉬어가도록 하자. 아마 클럽 좀 다녀보신 분들은 아! 이 소리할 것이다. 해당 곡의 인트로가 끝나는 35초부터 메인 멜로디를 맡는 악기가 사각파를 사용한 리드이다. 아무래도 리드는 악기로써 파형이 어느 정도 정제된 상태이기 때문에 클리핑이 발생했을 때 나는 소리는 이것보다 더 거칠고 날카롭다. 참고로 이렇게 파형에 따라 소리가 달라진다는 개념은 왜곡계(Distortion) 이펙터를 만들때도 사용하는 개념이기 때문에 기억해두면 좋다. 어쨌든 이런 클리핑 문제 때문에 사운드 엔지니어들은 오디오 소스와 다음 장비 사이에 게인을 조절할 수 있는 장치를 두고, 장비가 받아들일 수 있는 신호의 세기에 맞춰서 알맞게 게인을 조절하여 소스 오디오의 신호가 커지더라도 모든 신호를 다 담을 수 있도록 한다. 반대로 볼륨은 소리를 내보낼 때 얼마나 증폭시킬 것이냐를 의미한다. 많은 분들이 게인과 볼륨을 헷갈려하는 이유가 둘 다 소리를 증폭시키거나 감소시키는 역할을 하기 때문인데, 볼륨은 이미 입력된 신호를 출력할 때 건드리는 것이기 때문에 클리핑이 발생하더라도 볼륨을 줄이면 신호가 돌아오지만 녹음할 때 게인을 잘못 설정하여 유실된 소리는 다시 돌아오지 않는다. 신호 입력 단계에서 이미 유실된 것이기 때문에 영원한 이별인 것이다. 게다가 녹음이라는 특성 상 그 원본 소스는 사람인 경우가 많다. 결국 이 유실된 신호를 다시 살려낼 수 없기 때문에 게인을 잘못 설정하면 다시 녹음을 해야하는 슬픈 상황이 벌어질 수 있는 것이다. 그래서 사운드 엔지니어들은 소리를 녹음할 때 게인을 잘 다루는 것을 엄청 중요하게 생각한다. 사실 게인만 해도 좀 더 깊이 들어가면 하고 싶은 이야기가 많지만, 이 포스팅은 오디오 전문 포스팅이 아니므로 그냥 비슷한 거라고 생각하고 넘어가도 상관없다. 이제 게인이 무엇인지 이해했다면 GainNode와 함께 오디오 소스의 신호 세기를 조절해서 소리의 크기를 변형시켜보자! Gain Node를 사용하여 음량을 조절해보자일단 게인을 사용해보려면 오디오 소스가 필요하다. 오디오 소스는 HTML5의 태그를 사용하거나 사용자가 직접 업로드한 파일에서 추출하는 두가지 방법이 있는데, 필자는 이 중 후자의 방법을 사용하였다. 이것도 엄밀히 말하자면 태그를 사용하여 소스를 추출했을 때와 파일 버퍼에서 직접 추출했을 때는 다른 소스 노드 객체가 생성되긴 하지만 기능상 큰 차이 없으므로 그냥 개인의 취향대로 하면 된다. 12345678910111213141516const audioContext = new (Audiocontext || webkitAudioContext)();document.getElementById('audio-uploader').onchange = evt => { const file = evt.currentTarget.files[0]; if (!file) { return; } const reader = new FileReader(); reader.onload = async evt => { const buffer = await audioContext.decodeAudioData(file); const sourceNode = audioContext.createBufferSource(); sourceNode.buffer = buffer; console.log(sourceNode); }}; 1AudioBufferSourceNode {buffer: AudioBuffer, playbackRate: AudioParam, detune: AudioParam, loop: false, loopStart: 0, …} 우선 간단한 설명을 하자면 buffer 변수에 담긴 오디오 데이터는 raw한 오디오 데이터일 뿐 아직 하나의 노드가 아니기 때문에 사용할 수 없는 상태이다. 그렇기 때문에 createBufferSource 메소드를 사용하여 소스 노드를 생성한 후 해당 소스 노드에 오디오 데이터를 입력해줘야만 비로소 오디오 데이터를 사용할 수 있는 상태가 되는 것이다. 이때 필자는 사용자가 업로드한 파일에서 직접 오디오 버퍼 데이터를 뽑아와서 노드를 만든 것이기 때문에 createBufferSource 메소드를 사용하여 소스 노드를 생성했지만, 만약 태그에서 추출한 오디오 데이터를 사용하여 소스 노드를 생성하고 싶다면 createMediaElementSource 메소드를 사용해야 한다. 그럼 이제 GainNode를 생성하고 소스 노드에 연결만 시켜주면 바로 이 오디오 소스의 음량을 조절할 수 있게 된다. 123const gainNode = audioContext.createGain();sourceNode.connect(gainNode);gainNode.connect(audioContext.destination); 이 코드에서 소스 -> 게인 -> 데스티네이션으로 설정된 오디오의 흐름이 보인다면 사실상 Audio API에 대한 이해는 거의 끝났다고 봐도 무방하다. 위에서도 말했듯이 오디오를 컨트롤할 때는 이 개념을 이해하는 게 제일 중요하기 때문이다. 또한 gainNode가 연결된 audioContext.destination은 최종 아웃풋, 즉 스피커로 향하는 정보를 가지고 있다. 그럼 이제 여기서 오디오의 소리를 증폭시키거나 감소시키려면 어떻게 해야할까? 123456gainNode.gain.value = 1.2;// 또는gainNode.gain.setValueAtTime(1.2, audioContext.currentTime);// 그 후 소스를 재생해보자sourceNode.start(); [Warinig] 너무 값을 크게 올리면 재생했을 때 고막 터집니다. 간단하다. 그냥 GainNode.gain.value에 접근해서 값을 변경해주면 된다. 게인 같은 경우는 값에 직접 접근하여 변경하는 것이 가능하지만 다른 노드의 경우 자신의 값을 직접 변경하는 것이 허용되지 않는 경우가 있는데, 이럴 때는 setValueAtTime 메소드를 사용하면 된다. setValueAtTime 메소드는 일종의 스케줄러같은 개념인데, 두번째 인자로 넘긴 시간이 지난 후에 값을 적용하는 기능을 가지고 있다. 이때 인자로 넘기는 시간의 단위는 초이다. audioContext.currentTime을 인자로 사용하면 곧바로 값의 변경이 적용된다. 필자는 처음에 이런 노드들의 값을 변경할 때 헷갈렸던 것이 하나 있는데, 바로 min과 max이다. 즉, 이 노드가 가지는 값의 범위를 알 수가 없었다. 물론 공식 문서에 다 나와있긴 하지만 그걸 어느 세월에 하나하나 검색해서 보겠는가? 그래서 문서를 조금 더 뒤져보니 이 노드들이 가지는 값은 공통적으로 AudioParam 타입이라는 것을 알 수 있었다. 이 타입은 min, max, defaultValue, value 속성을 가지고 있었고, 이 값들은 input[type=\"range\"]를 사용하여 오디오를 컨트롤할때 유용하게 사용할 수 있다. 1console.log(gainNode.gain); 1AudioParam {value: 1, automationRate: \"a-rate\", defaultValue: 1, minValue: -3.4028234663852886e+38, maxValue: 3.4028234663852886e+38} 이 값을 잘 확인하고 게인의 값을 설정하면 적어도 고막과 이어폰이 터져나가는 불상사는 방지할 수 있을 것이다. 위에서 말했듯이 컴퓨터가 처리할 수 있는 신호의 세기를 넘어가게 되면 클리핑 현상이 발생하면서 찢어지는 듯한 소리가 나기 때문에 만약 이어폰을 끼고 있었다면 농담이 아니고 진짜 귀에 무리가 갈 수도 있다. 자, 이렇게 간단하게 오디오 소스의 게인을 조절해보았다. 나머지 이펙터들도 대부분 이런 느낌으로 구현된다. 간혹 조금 더 복잡한 연결이 필요한 이펙터들이 있긴 하지만 대부분의 경우 간단한 몇개의 노드를 연결하는 것만으로 구현할 수 있기 때문에 그렇게 어렵지 않다. 다음 포스팅에서는 이번에 알아본 개념을 바탕으로 소리를 압축하거나 공간감을 부여하고, 특정 주파수를 잘라내어 소리에 특별한 느낌을 줄 수 있는 다른 이펙터들을 만들어보도록 하겠다. 또한 기회가 된다면 이미 존재하는 오디오 소스를 변형하는 이펙터가 아니라 진짜로 오디오 신호 자체를 만들어 낼 수 있는 오실레이터(Oscillator)를 사용하여 나만의 악기를 만들어볼 수 있는 포스팅도 진행할 예정이다. 이상으로 JavaScript로 오디오 이펙터를 만들어보자 - 소리의 흐름을 파악하자 포스팅을 마친다.","link":"/2019/08/19/javascript-audio-effectors-gain/"},{"title":"나는 어떤 마음으로 소프트웨어를 만들어야 하는가","text":"최근 필자는 산드로 만쿠소의 소프트웨어 장인이라는 책을 읽게 되었는데, 이 책을 읽으며 느낀 점이 많았기 때문에 이번 포스팅에서는 산드로 만쿠소가 이 책을 통해 이야기하고자 하는 것이 무엇인지와 그에 따른 필자의 생각을 한번 편하게 적어보려고 한다. 표지가 깔끔하니 이쁘장하다 산드로 만쿠소가 이 책에서 계속 해서 강조하고 있는 것은 제목 그대로 프로페셔널리즘이다. 저자는 책의 초반에서는 내가 개발자로써, 또는 기술 전문가로써 비전문가인 고객들에게 어떠한 만족을 줄 것인지 혹은 고객이 진짜로 나에게 원하는 것은 무엇인지와 같은 질문을 통해 소프트웨어를 개발하는 장인으로써의 태도를 이끌어 내려고 하고 있다. 그러다가 후반에는 고객들이 원하는 바를 충족시켜주기 위해 개발자로써 어떤 방법들을 사용할 수 있는지 설명하고 있다. 사실 처음에는 아무 생각없이 읽기 시작했는데, 어느 샌가 그치, 이건 맞지, 이건 좀 아닌듯?하면서 빠져들고 있는 자신을 볼 수 있었다. 저자가 예시로 이야기하고 있는 상황들이 개발자로서 자주 경험하게 되는 상황들인 경우가 많고, 책 내에서 필자가 평소에 생각하고 있던 주제들도 많이 다루고 있다보니 더 빨리 빠져들었던 것 같다. 그만큼 재미있다 이 책. 그래서 필자는 이 책에서 재미있게 읽었고 많은 것을 느낄 수 있었던 주제 몇 가지를 한번 소개해보려고 한다. 내 커리어의 주인은 누구인가많은 개발자들이 개발자의 성장에 대한 지원을 해주고 관심을 가지는 회사를 선호한다. 컨퍼런스 가고 싶다고 하면 돈도 내주고 책도 사주고 스터디를 하고 싶다고 하면 사무실도 빌려주거나하는 그런 회사 말이다. 간혹 조직 문화 차원에서 업무 시간에 공식적으로 공부할 수 있는 시간을 내어주는 회사도 있다. 이런 회사들의 특징은 조직원이 성장하는 것이 결과적으로는 회사의 성장이라는 것을 잘 알고 있다는 것이고, 실제로 이런 문화가 있는 조직에는 좋은 개발자들도 많이 몰릴 수 밖에 없다. 자신의 성장을 지원해주는 회사, 얼마나 좋은가? 하지만 이런 지원이 없는 회사를 다니고 있다면 이런 불만이 생길 수도 있다. 옆 동네 철수네 회사는 컨퍼런스 비용도 다 대주고 한다는데 우리 회사는 왜 안해주는거야…? 필자도 회사를 다니면서 우리 회사보다 더 좋은 혜택이 있는 회사 얘기를 들으면 이런 생각을 하긴 했다. 그러나 저자인 산드로는 이런 불만을 가지는 것에 대해서 옳지 않다고 이야기한다. 사실 개발자가 새로운 기술을 공부하거나 컨퍼런스에 참여하거나 하는 등의 자신의 기술을 갈고 닦는 행위는 고객들을 만족시키기 위한 일종의 투자이다. 우리가 만약 몸이 아파서 병원에 갔는데 의사가 환자들에게 자신의 연구 비용 명목으로 진료비의 10%를 추가한다면 기분이 어떻겠는가? 산드로는 이 예시가 개발자들이 회사에 자신의 자기 계발을 위한 배려를 해달라고 강요하는 것과 다르지 않다고 이야기한다. 회사는 고객으로써 나의 기술력을 얻기 위해 나에게 돈을 지불하고 있는 것이지 나의 성장을 위해 돈을 지불하고 있는 것이 아니다. 물론 회사에서 이런 것들을 지원해주면 결국 개발자들의 실력 향상이 되면서 회사에도 좋은 영향을 끼칠 수 있지만 이건 일종의 배려이지 의무가 아니다. 개발자의 자기 계발은 스스로를 위한 투자이기 때문에 회사에서 지원을 해주던 말던 간에 기본적으로 스스로 알아서 해야 한다. 사실 필자도 첫 직장을 다닐 때는 개발자들의 발전을 지원해주고 컨퍼런스비나 도서 구입비를 지원해주는 것이 일종의 의무라고 생각했던 적이 있었다. 이런 지원을 해주지 않는 회사는 조직원의 성장에 별로 관심이 없는 회사이기 때문에 이런 회사에서는 더 이상 내가 성장할 수 없다고 생각했다. 기본적으로 퇴근 후에 따로 토이 프로젝트를 하거나 새로운 기술을 공부하기는 했지만 회사에서 지원을 해주면 더 빨리 성장할 수 있을 것이라고 생각했기 때문에 이런 불만을 가졌던 것이다. 하지만 이 생각은 굉장히 어리고 부끄러운 생각이였다. 회사에서 이런 지원을 해주면 좋긴 하지만, 혹여 저런 지원이 없더라도 필자는 소프트웨어 프로페셔널로서 고객에게 항상 최고의 결과물을 제공해야한다.(그래도 안해주면 아쉽긴 하다) 저자인 산드로는 직장에서 이런 불만을 가지고 있는 팀원에게 당신의 커리어의 주인은 누구인가?라는 질문을 던졌다고 한다. 위에서 구구절절히 이야기한 것들을 단번에 관통하는 명쾌한 질문이다. 결국 내 커리어를 만들어 가는 것은 자기 자신이고, 내가 성장함으로써 가장 큰 이익을 받는 것도 자기 자신이기 때문이다. 프로답게 행동하자산드로는 고용과 피고용의 관계는 창조적인 업무를 할 때 방해되는 모델이라고 이야기한다. 단, 여기서 저자가 이야기하는 고용과 피고용의 관계는 단어 자체가 아니라 고용과 피고용의 관계에서 오는 상명하복 시스템을 의미한다. 쉽게 말하면 아무리 아닌 것 같아도 대표가 까라면 까야하는 그 시스템이다. 사실 이 상명하복 시스템은 아무리 수평적인 문화를 지향하는 기업이라고 하더라도 어느 정도는 암묵적으로 존재하기 마련이다. 직원이 회사에 이건 좀 아닌 것 같다라고 말할 수 있는 기업은 우리나라 뿐만 아니라 세계적으로도 생각보다 많지 않다.(대표와 HR 담당자는 바로 우리 회사가 그런 회사라고 이야기하지만, 이건 직원 얘기를 들어봐야한다.) 하지만 잘 생각해보면 우리는 사실 회사와 계약을 한 것이다. 이 계약은 내가 너의 노예가 되겠다라는 계약은 아니였을 것이다. 회사는 나의 기술력을 원하고, 나는 기술 전문가로서 회사에 나의 기술력을 제공하는 일종의 동반자 관계인 것이다. 결국 전문가는 자신의 전문성을 파는 사람이다 그렇기 때문에 산드로는 회사도 개발자의 고객이라고 이야기한다. 우리는 소프트웨어를 만드는 전문가로서 회사에 나의 기술력을 팔고 있는 것이기 때문이다. 우리가 몸이 아프면 병원에 가서 의사를 찾거나 하수구가 막히면 배관공을 찾는 것과 마찬가지다. 회사가 소프트웨어를 만들고 싶으면, 개발자를 찾는다. 그리고 이러한 전문가들은 자신의 고객이 손해를 볼 것을 알면서도 어떠한 일을 해주기를 원한다고 해도, 그 일이 자신의 소신에 어긋난다면 그것을 쉽게 해주지 않는다. 간단한 예로 수술이 끝나고 마취가 풀리면서 고통이 찾아오면 환자는 의사에게 진통제를 더 투여해달라고 할 수 있다. 이때, 환자의 몸 상태가 진통제를 받아들일 수 없는 상태라면 의사는 진통제를 더 투여하지 않을 것이다. 환자는 그 진통제가 자신에게 어떤 영향을 끼칠지 모르는 상태이지만 의사는 확실하게 알고 있다. 개발자도 마찬가지다. 우리는 기술 전문가이기 때문에 기술적으로 우려되는 부분이 있다면 회사에 전부 이야기 해야한다. 회사는 바로 그런 기술적인 부분을 알고 싶어서 우리를 고용한 것이다. 그 중 대표적인 예는 바로 프로젝트 기간이다. 만약 대충 봐도 한달 이상 걸릴 것 같은 스펙의 프로젝트를 PO나 대표가 와서 2주 안에 끝내달라고 부탁하면 어떻게 해야할까? 많은 개발자 분들이 한번 노력해볼게요. 달려봅시다!라고 말할 것 같다. 왜냐면 저 상황에서 대표가 와서 간곡히 부탁하는데 딱 잘라서 안되는데요?라고 하면 왠지 내가 능력이 없는 것 같기도 하고 나쁜 사람이 되는 것 같기도 하고 그런 느낌? 자, 그럼 우리가 한달 이상 걸릴 프로젝트를 2주 안에 끝내려면 어떻게 해야할까? 네 야근입니다. 저녁 배달 시키실 분? 기본적으로 이런 상황에서 개발자들은 야근을 택할 수 밖에 없다. 또 다른 것으로는 문제가 생길 것 같은 부분을 일단 대충 때우고 넘어가는 등 일을 빠르고 대충 처리하는 방법도 있다. 문제가 생길 것을 뻔히 알면서 일단 끝내고 보자라는 마인드로 하드 코딩을 하거나 스키마 설계를 제대로 하지 않고 모델을 구현한다거나 하는 것들 말이다. 그러나 산드로는 개발자들의 이런 행위에 대해서 프로페셔널하지 못하다라고 이야기한다.(뜨끔) 사실 필자도 회사에서 이런 마음으로 몇번 프로젝트를 진행한 적이 있다. 당연히 제 기간 안에 프로젝트를 끝내는 게 제일 중요하다고 생각했고, 문제가 생길 수 있는 부분을 제대로 해결하지않고 어떻게든 작동하게만 만들어놓고 정신없이 다음 구현사항으로 넘어갔다. 물론 개발에 내공이 쌓이신 분들이라면 대충 하더라도 어느 정도 퀄리티가 나오겠지만 아쉽게도 필자는 그 정도 내공이 있는 개발자는 아니기 때문에 진짜 개판으로 짠 적도 있다. 하지만 이렇게 프로젝트를 진행하는 경우, 나중에 어플리케이션이 커질수록 이런 기술 부채들이 계속 쌓여서 분명히 문제가 발생할 것이라는 또한 우리 모두 알고있는 사실이다. 게다가 아무리 야근을 한다고 해도 프로젝트의 배포 일정을 맞출 수 있으리라는 보장도 없고, 어찌어찌 일정을 맞췄다고 하더라도 이런 상황에서 작성된 코드의 질이 높을리가 없고, 질낮은 코드는 결국 버그를 발생시킬 확률이 높아진다. 아무리 프로젝트 일정에 맞춰 배포를 했다고 해도 버그로 인해 사용자들이 좋지 않은 경험을 하게 만들었다면 그 프로젝트는 제대로 완료되었다고 할 수 없다. 산드로는 이런 상황에서 개발자가 프로페셔널로서 회사에 이렇게 프로젝트를 진행하면 안된다라고 강력하게 말해야 한다고 이야기한다. 즉, 개발자만이 지금 이 코드가 나중에 어떤 문제를 가져올 지 가장 정확하게 알고 있는 사람이고, 우리는 프로페셔널로서 프로젝트를 이렇게 진행하면 추후에 이런 문제점이 발생할 수 있다는 것을 고객인 회사에게 이야기할 의무가 있다는 것이다. 뭐, 이 상황을 어떻게 개발자가 아닌 사람들에게 이해시킬 것인지는 또 다른 문제지만 필자도 일단 이야기를 해야하는 것 자체는 맞다고 생각한다. 이렇게 이야기함으로써 PO나 CEO 등 개발자가 아닌 다른 팀원들이 문제 상황을 제대로 인지하게 되면 새로운 해결 방법이 나올 수도 있기 때문이다. 뭐 스펙 아웃을 해준다던가, 일정을 약간 늘려준다거나 하는 등의 여러 가지 해결책이 있을 것이다. 어쨌든 결론적으로 이 책의 저자가 이야기하고 싶은 것은 우리는 소프트웨어를 만드는 프로페셔널로서 회사에 기술력을 제공하고 있는 것이라는 것이다. 우리는 무리한 일정이 가져올 수 있는 문제를 회사에 이야기해주고 다른 합리적인 대안을 제시할 수 있어야 한다. 단순히 돈 주니까 받는 만큼 시키는대로 해야지라는 생각으로 일하지 말자는 것이다. 또한 회사 입장에서는 개발자가 이런 주장을 한다면 일 쉬엄쉬엄하려고 하네?라고 생각할 것이 아니라 뭔가 켕기는 부분이 있어서 하는 말이라는 것을 확실히 인지하고, 이런 상황에서 프로젝트를 강행하면 얻을 수 있는 것과 잃을 수 있는 것을 잘 판단해야한다. 개발자가 코드를 대하는 태도필자도 저번에 애자일이 도대체 뭐길래? 라는 포스팅에서 한번 다룬 적이 있지만 애자일이라는 방법론은 애자일 선언에서 출발했고, 그 애자일 선언에는 이런 항목이 있다. 포괄적인 문서보다 작동하는 소프트웨어를 만들자 이때 이 작동하는 소프트웨어라는 정의의 범위는 개개인마다 조금씩 다르다. 어떤 개발자는 일단 돌아가기만 하면 됐다고 생각할 수도 있고 어떤 개발자는 설계가 군더더기없이 깔끔하고 유닛 테스트까지 모두 작성되어야 한다고 생각할 수도 있다는 것이다. 하지만 사실 대부분의 경우 애자일 == 기민하게 움직여야한다라는 개념에 사로잡혀서 전자를 택하게 된다. 그러나 잘 생각해보면 일단 돌아가게만 작성된 코드, 그러니까 제대로 된 설계나 추상화나 패턴이 없이 작성된 코드는 지금 당장 작성하기엔 쉬울 지 몰라도 새로운 기능이 추가될 때마다 문제가 발생할 것이다. 한번 우리가 엉망진창인 레거시 코드와 만났을 때 느끼는 감정들을 떠올려보자. 코드를 파악하기가 힘들어서 잘못 수정하면 어디가 어떻게 망가질 지 몰라 손을 대기도 무섭다. 그래서 작은 수정을 할 때마다 전체 기능을 전부 테스트해야지 안심이 된다. 그마저도 자동화가 되어있지 않아서 일일히 손으로 테스트 해야한다. 일단 이런 경우도 작동하는 소프트웨어의 범주에는 들어간다. 자, 이제 이런 레거시 코드에 어떤 기능을 새로 추가하거나 기존의 기능을 개선해야 한다고 생각해보자. 우리가 처음에 애자일을 도입하면서 의도했던 것처럼 기민하게 움직일 수 있을 것인가? 필자는 아니라고 생각한다. 처음에는 기능 개발이 빠르기 때문에 기민하게 움직이는 것처럼 보일지 몰라도 점점 누적되는 기술 부채들 때문에 결국 나중에는 기민하게 움직일 수 없게 될 것이다. 게다가 애자일에서 기민하게 움직인다는 것은 빠르게 개발을 끝낸다가 아니다. 자주 변경되는 요구사항에 유연하게 대처할 수 있는 기민함을 말하는 것이다. 산드로는 소프트웨어를 만들어내는 프로페셔널이라면 단순히 작동하는 소프트웨어가 아니라 정교하며 솜씨있게 만들어진 소프트웨어를 추구해야 한다고 말한다. 정교하며 솜씨있게 만들어진 소프트웨어란, 오래 전에 작성한 코드라도 새로운 신입 개발자가 바로 이해할 수 있을 정도의 명료하고 단순한 디자인, 새로운 기능을 추가 및 수정하는 일이 처음 개발할 때와 비슷한 수준의 개발 공수로 완료될 수 있는 소프트웨어를 말한다. 즉, 예측가능하고 유지보수할 수 있는 소프트웨어인 것이다. 산드로는 이런 소프트웨어를 작성하기 위해 필요한 개념으로 단위 테스트, 페어 프로그래밍, 지속적인 통합 등을 제시하고 있다. 하지만 저자가 제시한 방법들을 전부 실행하지 않더라도 사실 코드 리뷰만 잘 되어도 어플리케이션의 코드가 진짜 막장으로 가는 최악의 사태는 어느 정도 방어할 수 있긴 하다. 작동하는 코드를 작성하는 것은 개발자로서 당연히 해야하는 것이고, 프로페셔널한 개발자는 거기에서 더 나아가서 정교하며 솜씨있는 코드를 작성하는 것이다. 시간이 없어서 어쩔 수 없었다필자는 개인적으로 이 주제에 대해서 많은 반성을 했는데, 사실 필자가 회사에서 자주 했던 말이기 때문이다. 프로젝트의 막바지에 다가갈수록 더 이런 얘기를 하는 경향이 있었던 것 같다. 여기서 필자가 반성했던 것 중 하나는 단위 테스트 작성과 비즈니스 로직 작성을 완전 별개의 업무로 생각했다는 것이다. 그리고 유닛 테스트보다 비즈니스의 개발을 끝내는 것이 중요하다고 생각했다. 그렇기 때문에 시간이 없어서 테스트 작성은 다음에 한다라는 핑계도 댈 수 있었다. 저자인 산드로는 유닛 테스트 작성을 굉장히 강조하는 편인데, 처음에는 직접 손으로 테스트를 해도 할만하지만 나중에 어플리케이션이 거대해지면 거대해질수록 이 테스트에 소요되는 시간도 함께 늘어나기 때문이다. 상용 환경에서 갑작스럽게 발생하여 원인을 쉽게 파악하기 힘든 버그의 상당 수는 유닛 테스트를 작성함으로써 간단하게 개발 중에 잡아낼 수 있다. 산드로는 이렇게 유닛 테스트를 통해 소모적이고 반복적인 작업을 줄일 수 있고, 이는 곧 생산성의 증대로 이어진다고 이야기한다. 그러면 여기서 이런 의문이 들 수도 있다. 단위 테스트를 작성하게되면 절대적으로 작성해야 할 코드의 양도 늘어나고, 명료한 기능의 정의를 해야 하니까 이에 대해서 고민하는 시간도 늘어날테고… 그럼 결국 개발 기간은 더 길어질텐데? 이 이야기도 맞는 말이다. 그냥 짜는 것보다는 확실히 오래 걸릴 것이다. 그러니까 애초에 일정 산정을 테스트를 작성하는 것까지 모두 포함해서 해야한다. 비즈니스 로직과 테스트 작성은 별개의 업무가 아니라 그냥 기능 개발하는 과정 중 하나이다. 그리고 유닛 테스트말고도 시간이 없어서라는 변명을 또 하는 경우가 있는데, 바로 구조적이지 않은 코드를 작성할 때이다. 쉬운 말로 날림 코딩이다. 솔직히 말해서 일단 이렇게 해놓고 나중에 고치자라는 이야기는 필자도 많이 했다. 버그가 발생할 것 같지는 않지만 가독성이 떨어지거나 구조가 명료하지 않은 상황에서 시간이 없다는 이유로 일단 넘어간 경우는 필자 뿐만 아니라 다른 개발자들도 겪어봤으리라 생각한다. 산드로는 이런 행위를 한다는 것은 개발자가 질 나쁜 코드를 아무런 죄책감 없이 어플리케이션에 끼워넣었다는 그 이상 그 이하도 아니다라고 비판했다. 이 형 가만보면 사람 명치를 되게 잘 때리는 스타일이다. 필자는 다른 주제들보다 이 주제가 머릿 속에 오래 남았던 것 같다. 나도 모르게 시간 없어서, 프로젝트를 끝내는 게 더 중요해라는 말로 자기 합리화를 하면서 질 나쁜 코드를 작성하고 있던 게 아닌가라는 생각이 들었기 때문이다. 다른 주제들은 뭔가 깨달음을 주는 주제였다면 이 주제는 필자를 굉장히 부끄럽게 만든 주제였다. 마치며소프트웨어 장인을 읽어 보신 분은 아시겠지만 이 책의 저자인 산드로 만쿠소 형은 좀 세게 말하는 스타일이다. 기존의 잘못됐다고 생각하는 면에 있어서는 강하게 비판하고 그에 따른 해결책을 제시하는 그런 느낌이다.(사람 뼈를 여러 번 때린다) 그리고 프로페셔널한 개발자가 되기 위해 갖춰야 할 하드스킬과 소프트스킬에 대해서도 균형있게 다루고 있어서 지루하지 않게 이야기를 풀어나간다. 특히 소프트스킬은 소프트웨어 장인 면접보기와 같은 다른 책에서 보기 힘든 주제도 담고 있기 때문에 재미도 있다. 또한 이 책은 개발자 뿐만 아니라 개발자와 협업하는 다른 직군에서 일하고 있는 사람에게 하는 이야기도 담고 있기 때문에 굳이 개발자가 아니더라도 한번 읽어보면 좋을 것 같다. 이상으로 나는 어떤 마음으로 소프트웨어를 만들어야 하는가 포스팅을 마친다.","link":"/2019/09/05/about-software-craftsmanship/"},{"title":"커밋 히스토리를 이쁘게 단장하자","text":"이번 포스팅에서는 Git의 머지 전략 중 대표적인 3가지인 Merge, Squash and merge, Rebase and merge의 차이에 대해서 한번 이야기해보려고 한다. 이 3가지 머지 전략 모두 브랜치를 머지한다는 목적은 같지만, 어떤 방식을 선택하냐에 따라 커밋 히스토리가 기록되는 방식이 달라지게 된다. 이 3가지 머지 전략은 Github 뿐만 아니라 Atlassian의 Bitbucket에서도 동일하게 지원하고 있는데, 그 만큼 머지를 할 때 커밋 히스토리를 어떤 방식으로 남길 것이냐를 선택할 수 있는 것이 중요하다고 말할 수 있다. Github에서는 Pull Request를 머지할 때 머지 전략을 선택할 수 있다. Bitbucket에서는 레파지토리 설정에서 기본 머지 전략을 선택할 수도 있다. Github과 Bitbucket의 머지 전략은 이름은 조금 다르지만 이것들이 의미하는 기능은 모두 같다. Github의 Create a merge commit은 Bitbucket의 Merge commit과 같은 전략이고 Squash and merge는 Squash와, Rebase and merge는 Fast forward와 같은 전략을 의미한다. 물론 이 머지 전략들은 각자 장단점이 있기 때문에 적재적소에 잘 사용하는 것이 중요하다. 예를 들어, Git Flow를 사용할 때는 기능 개발을 하는 feature 브랜치가 develop 브랜치로 머지될 때는 Squash and merge를, develop 브랜치가 master 브랜치로 머지될 때는 Merge을 사용하는 등 유연하게 사용하기도 한다. 하지만 적재적소에 잘 사용하려면 각각의 머지 전략이 어떤 방식으로 브랜치를 머지하는지 잘 알고있어야 가능한 법이다. 그래서 이번 포스팅에서는 이 3가지 머지 방식이 뭐가 어떻게 다른지 살펴보려고 한다. 커밋 히스토리가 왜 중요한가요?일단 머지 전략에 대한 설명에 들어가기에 앞서, Git의 커밋 히스토리가 왜 중요한지에 대해 간단히 이야기해보려고 한다. 서두에서 이야기한 3가지 머지 전략은 브랜치를 머지할 때 커밋 히스토리를 어떻게 남길 것이냐를 선택하는 것이나 마찬가지이기 때문에 개발자들이 왜 커밋 히스토리에 이렇게 목매는지에 대한 이해가 필요하다. 모두 알다시피 커밋(Commit)은 Git을 구성하는 중요한 요소 중 하나이며, 원칙적으로 하나의 커밋은 의미있는 하나의 변경사항을 의미한다. 그 말인 즉슨, 커밋 메세지만 보고도 어떤 사항이 어떤 이유로 변경되었는지 쉽게 파악할 수 있어야한다는 것이다. 많은 개발자들이 의미 있는 커밋 메세지에 대한 중요성을 언급하는 이유도 짧은 커밋 메세지만 보고도 언제, 어떻게 코드가 변경되었는가를 한번에 알고 싶기 때문이다. 이 커밋들이 모여서 시간 순으로 정렬된 것을 커밋 히스토리(Commit History)라고 부른다. 히스토리라는 단어에서 알 수 있듯이, 이건 말 그대로 이 프로그램의 역사와 같은 것이다. 많은 개발자들이 커밋 히스토리에 의미있는 역사를 기록하는 것이 굉장히 중요하다고 하는 이유에는 여러 가지가 있겠지만 대표적인 두 가지는 다음과 같다. 버그가 언제 터졌는지 파악하기가 쉽다우리가 Git을 사용하여 프로그램의 버전 관리를 할 때 혼자 개발을 진행하는 경우도 있지만 대부분의 경우 다른 여러 명의 개발자들과 함께 협업을 하게 된다. 이때 프로그램의 변경 사항이 많을 수록, 혹은 프로그램의 규모 자체가 큰 경우 협업에 참여하고 있는 개발자들은 사소한 실수로 인해서 버그를 발생시킬 가능성 또한 커지게 된다 이때 개발자들이 커밋 히스토리를 보고 어떤 이유로 어떤 코드가 수정되었는지 빠르게 파악할 수 있다면 해당 버그의 원인을 찾는 것이 더 빨라진다. 예를 들어 새로운 버전을 릴리즈한 후에 결제 관련 버그가 터졌다고 생각해보자. 이때 당연히 개발자들은 결제에 관련된 코드부터 뜯어보기 시작할 것이다. 하지만 대부분의 프로그램은 내부적으로 수많은 모듈 간의 디펜던시가 얽혀있는 경우가 많기 때문에 그걸 짧은 시간안에 전부 파악하고 버그의 원인을 찾아서 수정한다는 것은 쉬운 일이 아니다. 이때 잘 정리된 커밋 히스토리가 있다면 이번 버전에서 결제 관련된 부분을 수정한 커밋을 찾아서 어떤 코드가 수정되었는지 빠르게 확인할 수 있다. 만약 이전 버전에서는 문제가 없었고 이번에 배포한 버전에서 문제가 발생했다면 결제 관련 버그가 발생한 이유는 해당 커밋에서 수정한 코드 때문일 가능성이 높기 때문에 조금 더 빠른 대응이 가능하다. 레거시 코드를 수정해야할 때두번째 이유는 조금 슬픈 상황인데, 바로 레거시 코드를 고쳐야하는데 코드 짠 사람이 없을 때이다. 이 사람이 없는 이유는 퇴사라던가, 퇴사라던가, 퇴사같은 경우가 있다. 사실 레거시 코드가 무서운 이유는 코드 자체가 너무 복잡해서 파악하기 힘들다는 것 보다는 이걸 건드렸을 때 다른 부분에 문제가 없을 것이란 보장이 없기 때문이다. 게다가 이런 레거시 코드는 어느 회사에나 다 존재하기 때문에 레거시를 수정해야하는 상황이 그렇게 드문 상황도 아니다. 만약 그 코드가 딱 봐도 책임 분리가 잘 되어 있는 코드거나 간단한 코드라면 뭐 그냥 가벼운 마음으로 수정할 수도 있지만, 대부분 우리가 수정하기 망설여지는 코드는 그냥 레거시가 아니라 오랜 시간 숙성된 레거시인 경우가 많다. 특히 이 코드가 회사 창립 초창기에 작성된 코드일 경우에는 그냥 코드만 봐도 당시 개발자가 얼마나 정신없이 개발을 했는지 알 수 있을 정도인 것들도 있다. 호랭이는 죽어서 가죽을 남기고 개발자는 죽어서 레거시를 남… 이런 코드의 경우 섣불리 수정했다가 예상 못한 곳에서 도미노처럼 와장창나는 경우가 있기 때문에 이 와장창을 몇번 경험해본 개발자들은 레거시 코드를 수정함에 있어서 신중하게 접근할 수 밖에 없다. 그럼 이 상황에서 우리가 선택할 수 있는 방법은 대략 4가지 정도가 있다. 건드리기 무서우니까 그냥 냅둔다. 퇴사자한테 어떻게든 연락해서 물어본다. 주변에 있는 개발자를 붙잡고 물어본다. 그냥 내가 분석한다. 음, 일단 1번의 경우는 본인이 PO나 CTO를 설득할 말빨이 없다면 성공할 확률이 낮다고 본다. 그리고 아마 좋은 소리를 들을 것 같지도 않다. 일단 개발자로써 월급을 받고 있으니 월급 값은 해야하지 않는가? 그렇다고 이미 퇴사한 사람한테 카톡해서 코드를 작성한 의도를 물어보기에는 왠지 싸대기 맞을 것 같기도 하고 좀 그렇다. 3번 같은 경우는 그나마 나은 경우긴 하지만 다른 팀원들도 다 바쁜데 매번 붙잡고 물어볼 수도 없는 노릇이니 결국 직접 분석하는게 제일 깔끔한 방법이다. 하지만 이 분석이라는 것이 말이 쉽지, 실제로 거대한 어플리케이션에서 단 하나도 놓치지 않고 모든 의존 관계를 파악한다는 것은 사실 쉬운 일이 아니다. 게다가 이런 분석은 단순히 코드만 본다고 되는 것이 아니라 비즈니스와도 밀접한 관련이 있는 경우가 많기 때문에 해당 기능의 개발 당시 비즈니스 히스토리도 어느 정도 함께 파악하는 것이 좋다. 그나마 팀 내에 해당 기능을 개발하게 된 히스토리를 알고 있는 동료가 있다면 다행이지만, 그 마저도 없을 경우 우리가 의지할 것은 당시의 개발자가 어떤 의도로 코드를 고쳤는지 기록해놓은 커밋 히스토리 밖에 없는 것이다. 물론 정신없이 개발하는 와중에 커밋 메세지에 당시의 비즈니스적인 의도까지 담는 경우는 거의 없기 때문에 비즈니스 히스토리는 파악하기 힘들 수 있지만, 의미있는 단위로 커밋이 되어있다면 적어도 어떤 의도로 이 코드를 수정했는지 정도는 파악할 수 있다. 말 그대로 역사를 읽는 것이다. 하지만 이때 커밋 히스토리가 너무 쓸데 없이 복잡하거나 커밋 메세지가 개판이라면 아무래도 읽어나가는데 어려움이 있을 수 밖에 없다. 이렇게 커밋해버리면 뭘 고친건지 알 수가 없다. [출처] https://xkcd.com/ 그래서 개발자들이 의미 있는 단위의 커밋, 의미 있는 커밋 메세지를 강조하는 것이고 여기에 더해 적절한 머지 전략을 사용하여 가독성이 높고 의미도 있는 커밋 히스토리 그래프를 유지하려고 하는 것이다. 필자는 이 중 깔끔한 히스토리 그래프를 만드는 방법에 대해 설명하려고 하는 것이고, 이때 필요한 것이 적절한 브랜치 머지 전략의 선택인 것이다. 히스토리를 깔끔하게 만드는 3가지 머지 전략위에서 한번 설명했듯이 Merge, Squash and merge, Rebase는 두 개의 브랜치를 머지한다는 의미는 모두 같지만 머지하는 방법과 커밋 히스토리의 기록을 다르게 가져가는 머지 전략들이다. 한번 이 3가지 전략이 어떤 방식으로 브랜치를 머지하는 지, 커밋 히스토리는 어떻게 기록되는지 살펴보고 이에 따른 장단점을 알아보도록 하자. Create a merge commit 머지(Merge)는 우리가 알고 있는 일반적인 머지 전략이다. 머지의 장점은 기존 개발자들에게 익숙한 개념이라는 것과 머지된 브랜치가 삭제되어 사라졌다고 해도 히스토리 그래프 상에서는 그대로 다른 가지로 표기되기 때문에 어떤 브랜치에서 어떤 커밋이 진행되어 어떻게 머지가 되었군이라는 자세한 정보를 얻을 수 있다는 것이다. first-merge 브랜치가 master로 머지된 히스토리 추후 first-merge 브랜치를 삭제하더라도 히스토리와 브랜치 가지는 그대로 남아있다 반면에 단점은 너무 자세하게 히스토리가 남기 때문에 브랜치의 개수가 많아지거나 머지 횟수가 잦아질수록 히스토리 그래프의 가독성이 떨어진다는 것이다. 또한 원칙적으로 커밋은 의미있는 변경 사항의 최소 단위라고는 하지만 사실 실무에서 일을 하다보면 오타 수정과 같은 자잘한 커밋을 하는 경우도 많다. 사실 이런 자잘한 커밋의 경우 별로 정보성이 없기 때문에 이런 커밋들이 많아지면 오히려 히스토리의 가독성을 저해하는 원인이 된다. 규모가 큰 어플리케이션일수록 이런 복잡한 히스토리가 그려지기 쉽다 위 그림에서 볼 수 있듯이 머지가 수행되었을 때 생기는 머지 커밋(Merge commit)은 어느 순간에 어떤 브랜치의 변경사항이 머지되었다라는 소중한 정보를 주는 커밋이지만 개발이 진행되고 있는 브랜치가 많아진 상황에서는 이 머지 커밋들과 해당 브랜치에서 발생한 커밋들이 전부 기록되기 때문에 그래프가 너무 복잡해져서 오히려 히스토리를 추적하기 힘들 수도 있다. 위 예시의 그래프는 조금 오래된 히스토리라 헤드가 앞으로 나아가면서 해당 시점의 master 브랜치가 최신 버전인 상황이기 때문에 반드시 맨 아래 쪽에 master 브랜치가 위치한다. 그 덕분에 master 브랜치를 기준으로 읽어나간다면 어느 정도 흐름을 읽을 수 있지만, 한창 개발이 진행되는 중이라 master의 헤드가 뒤로 밀리기도 하는 상황이면 그래프의 맨 아래 쪽에 master 브랜치가 위치하지 않고 중간 어딘가 쯤에 끼어있기도 하기 때문에 히스토리 그래프를 읽으면서 추적하다가 놓칠 때도 있다.(해보신 분은 알겠지만 진짜 눈알 빠질 것 같다) Squash and merge Squash and merge에서 Squash는 여러 개의 커밋을 하나로 합치는 기능을 말한다. 즉, 이 기능은 머지할 브랜치의 커밋을 전부 하나의 커밋으로 합친 뒤 타겟 브랜치에 커밋하는 방식으로 머지를 진행한다. 즉 Squash and merge에서 발생하는 머지 커밋은 실질적인 머지로 인해서 생성된 머지 커밋이라기보다는 그냥 다른 브랜치의 변경 사항을 하나로 뭉쳐놓은 커밋인 것이다. 그래도 Squash and merge 전략은 일단 머지 커밋이 남긴 하기 때문에 머지가 되었다는 사실을 히스토리 상에서 한번에 알아볼 수 있고 버전 별로 어떤 것이 변경 되었는지 한 눈에 알수 있다는 것이 장점이다. 또한 머지된 브랜치의 자잘한 커밋 사항이 남지 않기 때문에 머지가 되었다라는 사실 자체에만 집중한 기록이 남게되고, 그로 인해 이 프로그램의 변경 사항을 읽기가 한결 수월해진다. 단점은 일반적인 머지 커밋보다는 아무래도 정보력이 떨어진다는 것이다. 일반 머지는 해당 브랜치에서 누가 어떤 커밋을 통해 어떤 라인을 수정 했는지 까지 알려주지만 Squash and merge 전략은 머지 대상 브랜치의 모든 커밋을 하나로 통합해버리기 때문에 그 정도의 자세한 정보는 알 수가 없다. 머지하기 전 히스토리 그래프 지금 상황은 update-a-txt 브랜치의 헤드가 master의 헤드보다 하나 더 앞으로 나아간 상황이다. update-a-txt 브랜치의 가지를 보면 update a txt, Add b txt 총 2개의 커밋이 있고 최근에 master에서 최신 변경 사항을 받아왔다. 이때 Squash and merge 전략을 사용하여 master에 머지를 하게 되면 이 브랜치에 있는 모든 커밋은 하나의 커밋으로 합쳐져서 마스터에 커밋된다. Squash and merge를 사용하여 update-a-txt 브랜치를 master에 머지한 모습 위 그림에서 볼 수 있듯이 일반적인 머지와는 다르게 update-a-txt 브랜치의 가지가 master로 들어가는 형태가 아니라 master 브랜치에 update a txt(#1)이라는 새로운 커밋이 하나 추가된 것을 볼 수 있다. 이때 master에 추가된 커밋은 update-a-txt 브랜치의 모든 커밋, 즉 변경 사항을 하나로 합친 커밋이다. 이후 쓸모 없어진 update-a-txt 브랜치를 삭제하면 master에는 Squash된 커밋이 남지만 update-a-txt 브랜치에서 커밋되었던 자세한 내용을 볼 수는 없다. 즉, Squash and merge을 사용하여 브랜치를 머지하게 되면 머지된 사실 자체는 알 수 있으나 어떤 상황에서 어떤 코드를 변경 했는지까지는 알 수가 없다. Rebase and merge Rebase and merge 전략은 Git의 리베이스(Rebase) 기능을 사용하여 브랜치를 머지하는 것이다. 이때 리베이스는 말 그대로 브랜치 히스토리들의 베이스를 변경하는 기능이다. 베이스를 변경한다는 의미를 좀 더 쉽게 말하자면 a 브랜치의 변경 사항이 마치 b 브랜치에서 변경된 것처럼 바꿀 수 있다는 것이다. 리베이스는 머지된 브랜치의 커밋을 모두 살려놓기 때문에 누가 언제, 어떤 부분을 수정했다는 정보는 전부 알 수 있지만 해당 브랜치가 어느 시점에 머지되었는지는 알 수 없다. 그래서 리베이스를 사용하는 경우 다른 방법보다 더 태깅에 신경써줘야한다. 위 그래프는 rebase-test-1 브랜치에서 총 4번의 커밋을 진행하고 이제 master 브랜치로 머지해야하는 상황이다. 이때 리베이스를 사용하여 브랜치를 머지하게 되면 rebase-test-1 브랜치에서 발생한 모든 변경 사항이 마치 master에서 직접 커밋한 것 처럼 변경할 수 있다. 쨘, 리베이스를 진행하고 난 이후의 상황이다. rebase-test-1 브랜치의 모든 커밋들이 master 브랜치로 그대로 옮겨진 것을 볼 수 있다. 이제 쓸모 없어진 rebase-test-1 브랜치를 삭제하게 되면 처음부터 master에서 개발을 진행한 것과 같은 깔끔한 히스토리 그래프를 얻을 수 있다. 위 그림에서 볼 수 있듯이 리베이스를 사용하여 브랜치를 머지하게되면 머지 커밋이 생성되지 않기 때문에 어느 시점에 어떤 브랜치가 머지된 것인지 알 수가 없다. 그래서 위에서 말했듯이 필자는 tag 기능을 사용하여 해당 브랜치가 머지된 시점에 태그를 달아주는 것을 추천한다.(시멘틱 버저닝을 합시다) 그리고 리베이스의 치명적인 단점 중 하나는 바로 머지 충돌(Merge Conflict)이 발생했을 경우다. 이건 머지할 브랜치의 히스토리 자체를 그대로 복사해서 대상 브랜치의 히스토리에 박아버리는 방법이기 때문에 충돌이 발생하게 되면 Merge commit이나 Squash and merge처럼 충돌이 한번 발생하는 것이 아니라 각각의 커밋에 하나씩 충돌이 발생한다. 이게 머지할 브랜치의 커밋이 몇개 안되는 상황에서는 할만할지 몰라도 커밋이 몇 백개씩 되는 큰 기능의 브랜치를 리베이스로 머지했다가 충돌이 나면 그냥 죽었다 생각하고 커피를 타오도록 하자. 마치며사실 커밋 히스토리를 잘 남기는 것은 미래의 나 자신을 위한 것일수도 있지만, 그보다는 내가 작성하는 코드를 언젠가 고쳐야할 누군가를 위해 신경써야 하는 것이 더 크기는 하다. 쭉 읽어보면 알겠지만 이 3가지 머지 전략은 각각 장단점이 명확하기 때문에 머지 전략 간의 우위는 없다. 그냥 상황에 따라서, 혹은 팀의 전략에 따라서 알맞은 머지 전략을 선택하면 된다는 것이다. 혹자는 Squash and merge나 Rebase와 같은 기능이 필요없고 그냥 일반적인 머지만으로도 충분히 버전 관리가 가능하다고 말하기도 한다. 그래도 이 3가지 머지 전략이 어떤 원리로 브랜치를 병합하는지 제대로 파악하고 히스토리가 어떻게 기록되는지 알고 있다면 복잡한 협업을 통해 개발이 진행되는 상황에서도 가독성 높은 히스토리 그래프를 만들어 낼 수 있고, 깔끔한 히스토리가 가져다주는 장점들은 분명히 있기 때문에, 아직 일반적인 머지만을 사용하여 히스토리를 관리하고 있었다면 한번 여러가지 전략을 사용해보는 것을 추천한다. 이상으로 커밋 히스토리를 이쁘게 단장하자 포스팅을 마친다.","link":"/2019/08/30/commit-history-merge-strategy/"},{"title":"최소 값과 최대 값을 빠르게 찾을 수 있게 도와주는 힙(Heap)","text":"이번 포스팅에서는 대표적인 자료 구조 중 하나인 힙(Heap)에 대한 설명과 구현을 한번 해보려고 한다. 이전의 포스팅에서 몇 번 언급한 적이 있지만 필자는 지금 백수다. 이제 프라하에서 한 달간의 힐링도 끝났으니 슬슬 면접을 보러 다녀야 하는데, 모두들 알다시피 면접에서는 기초 알고리즘이나 자료 구조에 대한 질문이 들어올 확률이 굉장히 높다. 하지만 필자는 최근 1년 정도 기초 공부를 게을리 했기 때문에 다시 공부를 해야하는 상황이다. 그래서 일단은 자료 구조부터 다시 살펴볼 생각인데, 그 중 제일 기억이 잘 나지 않는 힙(Heap)부터 한번 부셔볼까 한다. 힙(Heap)이란?힙은 기본적으로 완전 이진 트리(Complete Binary Tree)를 기본으로 한 자료 구조이며, 부모 노드와 자식 노드 간의 대소관계가 성립하는 자료 구조이다. 그렇기 때문에 힙의 루트 노드는 힙 내의 데이터들 중 가장 큰 값이거나 가장 작은 값이라고 할 수 있다. 루트에 최대 값이 오게 되는 Max Heap의 모습 생긴 건 완전 이진 트리랑 똑같이 생겼다 즉, 힙 내의 가장 큰 값이나 가장 작은 값에 접근하고 싶을 때 비교 연산없이 한번에 접근할 수 있다는 의미이며, 이 접근 연산의 시간 복잡도는 당연히 $O(1)$이다. 이런 힙의 성격 때문에 힙은 여러 개의 데이터 중에서 가장 크거나 작은 값을 빠르게 찾아야 하는 영역에서 유용하게 사용된다. 사실 단순히 어떤 데이터 뭉치 안에서 최대 값이나 최소 값에 $O(1)$의 시간 복잡도로 접근하고 싶다면 그냥 링크드 리스트(Linked List)나 배열(Array)을 정렬해서 사용해도 무방하긴 하다. 한번 정렬만 해놓으면 그 다음부터는 그냥 헤드에서 값을 쏙쏙 뽑아다 쓰면 되기 때문이다. 그러나 이렇게 정렬되어 있는 데이터 뭉치에 새로운 데이터를 추가할 때는 전체 데이터 뭉치를 싹 다 뒤져서 다시 최대, 최소 값이 무엇인지 찾아내고 재정렬하는 과정을 거쳐야한다. 배열과 링크드 리스트과 같은 선형 자료 구조는 이 과정에 $O(n)$의 시간 복잡도가 소요되는 반면, 부모가 자식보다 크거나 작게 정렬된 이진 트리의 경우는 새로운 추가된 노드의 부모 노드들과만 비교해도 정렬 상태를 유지할 수 있기 때문에 $O(\\log n)$의 시간 복잡도만 소요된다. 즉, 정렬하고 싶은 데이터가 많을 수록 더 유리하다는 소리다. 전체 데이터를 비교할 필요없이 이 부분만 비교하면 다시 재정렬할 수 있다 힙은 완전 이진 트리를 기초로 하기 때문에 구현하는 방법 또한 완전 이진 트리와 흡사하다. 그렇기 때문에 힙을 구현해보기에 앞서 완전 이진 트리의 특징을 먼저 이야기 해보려고 한다. 완전 이진 트리(Complete Binary Tree)먼저, 힙의 기본이 되는 완전 이진 트리의 특징을 한번 살펴보자. 이진 트리(Binary Tree)란 어떤 하나의 노드가 자식 노드를 최대 2개까지만 가질 수 있는 트리를 말한다. 그 말인 즉슨, 한 레벨에 최대로 들어설 수 있는 노드의 개수가 정해져있다는 뜻이고, 노드들에게 고유한 인덱스를 부여할 수 있다는 것을 의미한다. 부모 노드는 반드시 2개 이하의 자식 노드를 가져야 한다. 완전 이진 트리(Complete Binary Tree)는 이진 트리의 노드를 생성할 때 트리의 왼쪽부터 차곡차곡 채워 나가는 트리를 의미한다. 이때 완전 이진 트리의 한 레벨이 꽉 차기 전에는 다음 레벨에 노드를 생성할 수 없다. 즉, 마지막 레벨을 제외한 모든 레벨에는 노드가 꽉 차 있어야 한다는 뜻이다. 트리는 보통 링크드 리스트(Linked List)나 배열(Array)과 같은 선형 자료 구조를 사용하여 구현하는데, 이때 링크드 리스트와 배열의 특징에 따라 장단점이 갈리기 때문에 선택이 필요하다.(링크드 리스트와 배열의 차이에 대한 내용은 이 포스팅의 주제가 아니므로 따로 설명하지는 않겠다.) 일반적으로 트리는 링크드 리스트를 사용하여 구현하지만 완전 이진 트리의 경우에는 배열로 구현하는 것이 더 효율적이다. 배열이 더 효율적인 이유사실 트리를 구현할 때 원소에 한방에 접근이 가능하다는 배열의 최대 장점을 뒤로 미뤄두고서라도 링크드 리스트를 사용하는 이유는, 배열을 사용하여 트리를 구현하면 메모리 고정 할당 방식이라는 배열의 특성 상 불편한 점이 너무 많기 때문이다. 그러나 완전 이진 트리에서는 이 불편한 점들이 사라지기 때문에 배열의 장점을 오롯이 가져갈 수 있다. 원하는 노드로의 접근이 쉽다이건 배열의 최대 장점인 인덱스만 알면 해당 원소에 바로 접근이 가능하다는 장점을 그대로 살린 것이다. 이진 트리는 각 레벨에 들어설 수 있는 노드의 최대 개수가 정해져 있기 때문에 간단한 수식만으로 특정 노드의 인덱스를 알아내어 $O(1)$의 시간 복잡도로 노드에 접근할 수 있다. 먼저, 특정 노드를 기반으로 부모나 자식 노드에 접근하고 싶다면 다음과 같이 인덱스를 계산해볼 수 있다. 루트 노드가 0번 인덱스를 가지고, 현재 노드가 $i$번 인덱스를 가지고 있을 때 [부모 노드] $(i-1) / 2$[왼쪽 자식 노드] $2i+1$[오른쪽 자식 노드] $2i+2$ 위와 같이 특정 노드의 인덱스를 기반으로 부모와 자식들의 인덱스를 알아내는 방법도 있지만, 배열로 만든 완전 이진 트리의 진짜 장점은 간단한 수식을 통해서 원하는 레벨, 원하는 순번의 노드로 바로 접근할 수도 있다는 것이다. 완전 이진 트리의 성질을 이용하면 특정 레벨까지의 최대 노드 개수인 $c$를 간단히 계산할 수 있기 때문에 가능한 일이다. c=2level−1\\begin{aligned} c = 2^{level} - 1 \\end{aligned}​c=2​level​​−1​​ 12345function getAllNodeCountByLevel (level = 1) { return 2**level - 1;}console.log(getAllNodeCountByLevel(3)); 17 // 3 레벨로 구성된 완전 이진 트리 내 노드의 최대 개수 이 식을 사용하면 $n$번째 레벨의 $m$번째 노드의 인덱스도 간단하게 알아낼 수 있다. 예를 들어, 완전 이진 트리의 n 레벨에 있는 m번째 노드의 인덱스인 $i$를 구하고 싶다면, 내가 접근하고 싶은 레벨의 바로 위 레벨인 $n - 1$ 레벨까지의 모든 노드의 개수 $c$을 구하고, 거기에 해당 레벨 내에서 내가 $m$번째 노드에 접근하고 싶은지만 더해주면 된다. i=c+m−1\\begin{aligned} i = c + m - 1 \\end{aligned}​i=c+m−1​​ 12345function getNodeIndex (level = 1, count = 1) { return getAllNodeCountByLevel(level - 1) + count - 1;}console.log(getNodeIndex(3, 3)); 15 // 3 레벨의 3 번째로 위치한 노드의 인덱스 마지막에 1을 빼주는 이유는 우리가 구하고 싶은 것이 노드의 순번이 아닌 인덱스이기 때문이다. 배열의 인덱스는 0부터 시작한다는 사실을 잊지 말자. 이처럼 배열로 구현한 완전 이진 트리는 간단한 계산만으로 원하는 노드에 접근하기가 용이하지만, 링크드 리스트로 구현하게되면 원하는 노드에 바로 접근할 수 없고, 트리를 순회하여 접근해야한다. 배열의 원소를 뒤로 밀어줘야 할 일이 없다배열은 메모리에 연속적인 공간을 할당하여 사용하기 때문에, 배열 중간에 원소를 끼워넣으려면 새로운 메모리 공간을 확보하기 위해 원소를 한 칸씩 뒤로 밀어줘야하는 슬픈 상황이 발생하지만, 링크드 리스트는 그냥 prev 값과 next 값만 변경함으로써 중간에 새로운 원소를 끼워넣기가 편하다. 그러나 이진 트리의 경우, 자식 노드를 최대 2개까지만 가질 수 있다는 제약이 있기 때문에 높이가 $h$인 트리는 최대 노드 개수가 $2^{h} -1$개로 정해져있다. 즉, 이 크기 만큼만 배열을 메모리에 할당하고 나면 중간에 노드를 새로 삽입하기 위해 배열의 원소를 뒤로 밀어야하는 경우가 발생하지 않는다는 것이다. 트리가 기울어지지 않는다일반적으로 배열로 구현한 트리가 기울어지게 되면 메모리 공간에 심한 낭비가 생기게 되는데, 그 이유는 그림으로 보면 이해하기가 한결 편하다. 위 그림은 이상적으로 균형이 잡힌 밸런스드 이진 트리(Balanced Binary Tree)의 모습이다. 만약 이 트리를 배열로 구현한다면 메모리에는 이렇게 값들이 담길 것이다. Location 0 1 2 3 4 5 Value A B C D E F 배열 중간에 빈 공간이 없이 차곡차곡 메모리에 담겼다. 노드의 인덱스는 트리의 왼쪽부터 순차적으로 부여되기 때문에 자식 노드를 왼쪽부터 생성한다면 메모리에 빈 공간을 만들지 않을 수 있다. 물론 자식 노드를 생성할 때 왼쪽을 건너 뛰고 오른쪽부터 생성하면 빈 공간이 생기긴 하지만, 더 큰 문제는 트리가 한 쪽으로 크게 기울어지게 되게 되는 편향 트리가 되는 경우이다. Location 0 1 2 3 4 5 6 7 Value A B C D - - - H 위 그림처럼 한 쪽으로 크게 기운 편향 트리는 중간 인덱스를 건너뛰고 다음 레벨에 노드를 생성한 경우이기 때문에 메모리에 빈 공간이 생길 수 밖에 없다. 그렇기 때문에 메모리 공간이 낭비된다고 하는 것이다. 물론 오른쪽으로 기울었다면 건너뛰어야 하는 인덱스도 더 크기 때문에 메모리 공간의 낭비가 더 심해진다. 그러나 완전 이진 트리는 노드를 왼쪽부터 차곡차곡 채워나가고, 한 레벨의 노드가 다 채워지기 전까지는 다음 레벨에 노드를 채울 수 없다는 제약들이 걸려있기 때문에 애초에 트리가 기울어질 일 자체가 없으니 메모리에 빈 공간이 생길 일도 없다. 이런 이유들로 인해 완전 이진 트리는 원소에 바로 접근이 가능한 배열의 장점을 살려서 구현하는 경우가 많고, 완전 이진 트리를 기반으로 하는 힙 또한 마찬가지 이유로 인해 주로 배열로 구현하게된다. 완전 이진 트리와 힙의 차이힙(Heap)은 완전 이진 트리를 기초로 하기 때문에 기본적인 노드의 삽입 및 삭제 알고리즘은 일반적인 완전 이진 트리와 동일하다. 노드의 삽입은 반드시 배열의 끝에만 가능하며, 노드를 삭제하고 나면 빈 공간이 남지 않도록 남은 노드들을 다시 당겨서 빈 공간을 채워줘야한다. 다만 힙은 부모와 자식 노드 간의 대소관계가 성립되어야 한다는 조건이 있기 때문에, 삽입 및 삭제 후 노드를 다시 정렬해주는 기능이 추가로 필요하다. 또한 힙은 사실 트리 내의 최대 값이나 최소 값을 쉽게 찾고자 하는 자료 구조이기 때문에, 트리 중간에 위치한 노드에 바로 접근할 일도 거의 없다. 항상 Array[0]에 위치한 루트 노드를 뽑아다 쓰면 되기 때문이다. 루트 노드를 뽑아온다는 것은 곧 루트 노드의 삭제를 의미하기 때문에 마찬가지로 힙을 다시 정렬해줘야한다. 힙은 최대 힙(Max Heap)과 최소 힙(Min Heap)으로 나누어지는데, 최대 힙은 항상 부모의 값이 자식보다 커야하고, 최소 힙은 반대로 부모의 값이 자식보다 작아야 한다. 즉, 최대 힙의 루트는 힙 내에서 가장 큰 값, 최소 힙의 루트는 힙 내에서 가장 작은 값을 의미한다는 것이다. 즉, 힙을 구현한다는 것은 완전 이진 트리를 구현하고, 최대 힙과 최소 힙에 맞는 정렬 기능을 추가하면 된다는 것이다. 힙을 구현해보자일단 최대 힙과 최소 힙의 차이는 사실 정렬할 때 조건 밖에 없으니, 필자는 최대 힙만 구현해보려고 한다. 위에서 이야기 했듯이 완전 이진 트리의 경우는 링크드 리스트를 이용하는 것보다 배열을 이용하는 것이 더 효율적이기 때문에 배열을 이용하여 구현할 것이다. 일단 작고 귀여운 MaxHeap 클래스를 하나 만들어주자. 12345class MaxHeap { constructor () { this.nodes = []; }} 만약 링크드 리스트로 구현하고자 한다면 Node 클래스를 별도로 선언해서 사용하겠지만 필자가 사용할 자료 구조는 배열이기 때문에 단촐하기 그지 없다. 그럼 이제 힙에 값을 삽입하는 메소드부터 한번 만들어보도록 하자. 새로운 값의 삽입. 버블 업!힙에 새로운 값을 삽입할 때는 완전 이진 트리의 규칙대로 무조건 트리의 왼쪽부터 채워나간다. 그 말인 즉슨, 그냥 push 메소드를 사용하여 배열의 꼬리로 값을 하나씩 쑤셔넣어주면 된다는 것이다. 잘 이해가 되지 않는다면 위의 완전 이진 트리 설명 부분을 다시 읽어보도록 하자. 123insert (value) { this.nodes.push(value);} 최대 힙은 항상 부모가 자식보다 큰 값을 가져야한다는 제약이 있으므로, 만약 우리가 힙에 삽입한 노드가 부모 노드보다 큰 값을 가지고 있다면 두 노드의 위치를 바꿔줘야한다. 그리고 이 작업을 현재 삽입한 노드가 루트까지 올라가거나, 부모보다 작은 값을 가지게 되거나, 두 조건 중 하나를 만족할 때까지 반복한다. 이때 새로 추가한 값이 부모 노드와 스왑되면서 점점 트리의 위로 올라가는 모양새가 거품이 뽀글뽀글 올라오는 모양새랑 비슷하다고 해서 버블 업(Bubble Up)이라고 부른다. 부모와 자리를 바꾸면서 트리의 위쪽으로 점점 올라간다 1234567891011121314151617181920212223insert (value) { this.nodes.push(value); this.bubbleUp();}bubbleUp (index = this.nodes.length - 1) { if (index < 1) { return; } const currentNode = this.nodes[index]; const parentIndex = Math.floor((index - 1) / 2); const parentNode = this.nodes[parentIndex]; if (parentNode >= currentNode) { return; } this.nodes[index] = parentNode; this.nodes[parentIndex] = currentNode; index = parentIndex; this.bubbleUp(index);} bubbleUp 메소드는 인자로 받은 인덱스의 노드의 값과 부모 노드의 값을 비교하여, 해당 노드가 부모 노드의 값보다 큰 값을 가지고 있다면 두 노드의 위치를 스왑하는 역할을 한다. 트리의 특성 상 이 작업은 분할 정복이 가능한 부분이므로 재귀 호출로 구현하였다. 여기까지 작성하고 간단히 테스트를 해보면 힙 내의 가장 큰 값이 배열의 헤드, 즉 트리의 루트에 위치하게 된다는 것을 알 수 있다. 12345678910const heap = new MaxHeap();heap.insert(1);heap.insert(3);heap.insert(23);heap.insert(2);heap.insert(10);heap.insert(32);heap.insert(9);console.log(heap.nodes); 1[ 32, 10, 23, 1, 2, 3, 9 ] 부모보다 큰 값이 추가되면 부모와 자리를 바꾼다 루트에서 값을 빼오자. 트릭클 다운!값의 삽입과 버블 업을 구현해보았으면 이번에는 루트에서 값을 빼오는 메소드를 만들어보자. 즉, 삭제 연산이다. 이 경우 루트 노드를 뽑아버리면 루트 노드의 자리가 비게 되니 힙을 재정렬해서 다시 루트 노드를 채워줘야한다. 이때 기존 루트 노드의 자식 노드들을 루트 노드로 올리는 것이 아니라 트리의 가장 마지막에 위치한 노드를 가져와서 루트 노드로 삽입하게 되는데, 이는 수정될 힙에서 빈 공간이 생기지 않게 함으로써 연산량을 줄이기 위해서이다. 바로 루트의 바로 밑에 있는 자식들을 끌어올려서 루트로 사용해버리면 자연스럽게 그 노드가 있던 곳은 공백이 생기게 되고, 그럼 또 다음 레벨에서 어떤 노드를 끌여올려야 할지 비교 연산이 필요하게 되기 때문이다. 그리고 힙은 반드시 완전 이진 트리의 형태를 유지해야하기 때문에 이 과정에서 트리가 한 쪽으로 기울게 되버리면 또 트리의 균형을 맞춰줘야하는 번거로움도 생길 수 있다. 그래서 가장 마지막에 위치한 노드를 새로운 루트 노드로 사용하고 자식들과 값을 비교해나가면서 자리를 바꿔나가는데, 이때 새로운 루트 노드가 트리의 아래 쪽으로 점점 이동하는 모양새가 물방울이 떨어지는 모양새랑 비슷하다고 해서 트릭클 다운(Trickle Down)이라고 부른다.(컴퓨터 용어 주제에 왠지 갬성적이다…) 123456789101112131415161718192021222324252627extract () { const max = this.nodes[0]; this.nodes[0] = this.nodes.pop(); this.trickleDown(); return max;}trickleDown (index = 0) { const leftChildIndex = 2 * index + 1; const rightChildIndex = 2 * index + 2; const length = this.nodes.length; let largest = index; if (leftChildIndex this.nodes[largest]) { largest = leftChildIndex; } if (rightChildIndex this.nodes[largest]) { largest = rightChildIndex; } if (largest !== index) { [this.nodes[largest], this.nodes[index]] = [this.nodes[index], this.nodes[largest]]; this.trickleDown(largest); }} 트릭클 다운 또한 버블 업과 마찬가지로 재귀 호출을 통한 분할 정복으로 해결할 수 있다. trickleDown 메소드는 부모 노드와 왼쪽 자식 노드, 오른쪽 자식 노드의 값을 비교한 후 자식 노드가 부모 노드보다 큰 값을 가지고 있다면 부모 노드와 해당 자식 노드의 위치를 변경한다. 쉽게 말하면 세 개의 노드 중 가장 큰 값을 가지고 있는 놈이 부모 자리를 먹는 것인데, 만약 이 힘싸움에서 밀린 것이 부모 노드라면 자신의 자식이 있던 곳으로 좌천되는 것이라고 보면 된다. 썩씌딩하는 꼬라지가 왠지 이 놈을 닮았다 만약 이 과정에서 부모 노드와 자식 노드의 위치가 변경되었다면 변경된 부모 노드의 인덱스를 다시 tickleDown 메소드의 인자로 넘겨서 이 과정을 계속 반복한다. 여기까지 작성했으면 버블 업과 마찬가지로 간단한 테스트를 한번 해보자. 힙에 들어가는 값은 이전에 버블 업에서 사용했던 값과 동일한 값들을 사용했다. 1234567891011121314const heap = new MaxHeap();heap.insert(1);heap.insert(3);heap.insert(23);heap.insert(2);heap.insert(10);heap.insert(32);heap.insert(9);const length = heap.nodes.length;for (let i = 0; i < length; i++) { console.log('MAX_VALUE = ', heap.extract()); console.log('HEAP = ', heap.nodes);} 123456789MAX_VALUE = 32HEAP = [ 23, 10, 9, 1, 2, 3 ]MAX_VALUE = 23HEAP = [ 10, 3, 9, 1, 2 ]MAX_VALUE = 10HEAP = [ 9, 3, 2, 1 ]... 트리의 맨 끝에서 노드를 빼와서 루트 노드로 사용한다는 것에 주의하자 마치며힙 자체는 단순히 데이터들이 느슨하게 정렬되어있는 완전 이진 트리이지만, 루트에는 항상 힙 내부에 있는 데이터들의 최대 값과 최소 값이 위치한다는 특징 때문에 다양하게 응용하여 사용할 수 있다. 그 중 대표적인 것은 선형 자료 구조를 정렬할 때 사용하는 방법인데, 이 정렬 알고리즘을 힙 정렬(Heap Sort)라고 한다. 현재 V8의 Array.prototype.sort 메소드는 퀵 정렬(Quick Sort)을 사용하고 있지만, 초반에는 힙을 사용하는 정렬 알고리즘인 힙 정렬을 잠깐 사용하기도 했었다. 어쨌든 그 동안 잊고 있었던 기초적인 자료 구조를 한번 다시 보니 왠지 기분이 좋다. 힙은 굉장히 다양한 곳에서 사용되고 있는 자료 구조이니 만큼, 알고 있어서 나쁠 게 없기도 하고 면접 때도 꽤 자주 물어봤던 것 같다. 이제 필자는 회사의 비즈니스 로직과 전혀 관련 없는 백수 개발랭이 신분이 되었으니, 그 동안 소홀했던 기초 이론 부분을 이 기회에 조금 더 자세히 공부해봐야겠다. 이상으로 최소 값과 최대 값을 빠르게 찾을 수 있게 도와주는 힙(Heap) 포스팅을 마친다.","link":"/2019/10/12/introduction-data-structure-heap/"},{"title":"[JS 프로토타입] 프로토타입을 사용하여 상속하기","text":"이번 포스팅에서는 이전 포스팅에 이어, 프로토타입을 사용한 다양한 상속 패턴에 대한 이야기를 해볼까 한다. 사실 자바스크립트에는 상속이나 캡슐화와 같은 개념이 명시적으로 존재하지는 않기 때문에 자바나 C++ 같은 클래스 기반 언어를 사용하던 개발자들은 자바스크립트에 클래스가 없다는 사실에 혼란스러워한다. 즉, 자바스크립트에서의 상속이나 캡슐화 등은 OOP(객체지향프로그래밍)에 익숙한 개발자들이 자바스크립트에서도 이런 개념들을 가져다 사용하기 위해 프토토타입을 사용하여 이를 유사하게 구현한 일종의 디자인 패턴이라고 할 수 있다. 자바스크립트에서의 상속은 프로토타입 체인을 사용하여 구현하고, 캡슐화는 클로저를 사용해서 구현하게 되는데, 이번 포스팅에서는 이 중 프로토타입을 사용한 상속 패턴에 집중해서 설명해볼까 한다. 프로퍼티와 메소드는 원본 객체를 통해 공유될 수 있다객체의 상속을 알아보기 전에 객체를 생성할 때 프로퍼티와 메소드를 부여하는 방법에 대해서 알아보도록 하자. 이전 포스팅에서 필자는 자바스크립트는 클래스가 아닌 함수를 사용하여 객체를 생성한다고 이야기 했었다. 12function User () {}const evan = new User(); 이때 User 함수를 생성자로 호출하면서 생성된 evan 객체는 User.prototype 객체를 원본 객체로 하여 복제된 객체이다. 이때 두 가지 방법을 사용하여 새롭게 생성되는 객체들에게 프로퍼티나 메소드를 부여할 수 있는데, 첫 번째는 생성자 함수 내에서 this를 사용하여 선언하는 방법, 두 번째는 새롭게 생성되는 객체들이 복사할 원본 객체인 프로토타입 객체에 선언하는 방법이다. 먼저, this를 사용하여 프로퍼티나 메소드를 정의하는 방법에 대해서 살펴보자. 생성자 함수 내에서 this를 사용하는 방법자바스크립트도 생성자 역할을 하는 함수 내에서 this를 사용하여 다른 언어와 비슷한 느낌으로 객체들에게 프로퍼티나 메소드를 부여할 수 있다. 123456789function User (name) { 'use strict'; this.say = function () { console.log('Hello, World!'); };}const evan = new User();console.log(evan.say()); 1Hello, World! 참고로 생성자 함수 내에서 strict 모드를 사용한 이유는, 해당 생성자 함수가 실수로 new 예약어 없이 호출되어 this가 전역 객체로 평가되는 불상사를 방어하기 위해서이다.(이 내용은 프로토타입과는 관련이 없기 때문에 자세히 다루지는 않겠다) 이 방법은 일반적인 생성자의 사용 방법과 비슷해서 직관적으로 이해가 되는 편이다. 이때 생성자 함수 안의 this는 새롭게 생성된 객체를 의미하기 때문에, 함수 내에서 this를 통해 정의한 프로퍼티나 메소드는 이 생성자 함수를 사용하여 객체가 생성될 때마다 새롭게 정의된다. 무슨 말인지 조금 더 쉽게 알아보기 위해 생성자 함수를 통해 두 개의 새로운 객체를 생성하고, 이 객체들의 메소드를 비교해도록 하자. 1234const evan = new User();const john = new User();console.log(evan.say === john.say); 1false 생성자 함수가 호출될 때 this는 각각 evan 객체와 john 객체를 의미했을 것이고, say 메소드 또한 이 객체들에게 직접 할당되었을 것이다. 자바스크립트의 완전할당연산자(===)는 다른 메모리에 적재된 객체는 다르다고 평가하므로 이 두 객체의 메소드들은 각자 다른 메모리에 담긴, 전혀 다른 함수라고 할 수 있다. 이때 evan 객체나 john 객체를 출력해보면, 객체 내부에 say 메소드가 정의되어 있는 모습 또한 확인해볼 수 있다. 1console.log(evan); 1User {say: function} 이 당연한 이야기를 하는 이유는 바로 밑에서 후술할 프로토타입 객체에 정의하는 방법과 차이점을 분명히 하기 위해서이다. 프로토타입 객체를 사용해서 프로퍼티나 메소드를 정의하게되면 지금과는 전혀 다른 결과가 나온다. 프로토타입 객체에 정의하는 방법이번에는 User 생성자 함수의 프로토타입 객체인 User.prototype을 사용하여 메소드를 한번 정의해보도록 하자. this를 통해서 정의하는 방법과 어떤 차이가 있을까? 1234567function User (name) {}User.prototype.say = function () { console.log('Hello, World!');}const evan = new User();console.log(evan.say()); 1Hello, World! 일단 this를 사용하여 정의했던 메소드와 동일한 느낌으로 작동하고 있다. 그래서 동일한 동작이라고 생각할 수도 있지만, 사실 두 방법들 사이에는 중요한 차이가 존재한다. 바로 생성자 함수를 통해 생성된 모든 객체들이 해당 메소드를 공유하고 있냐, 없냐의 차이이다. 이전과 마찬가지로 두 개의 객체를 생성하고, 두 객체의 메소드를 비교해보자. 1234const evan = new User();const john = new User();console.log(evan.say === john.say); 1true 음? 이번에는 아까와는 다르게 두 객체의 메소드가 같다고 한다. 방금 전과는 다르게 이번에는 evan.say와 john.say가 객체에 따로따로 정의된 메소드가 아닌, 원본 객체의 메소드를 공유하고 있는 상황이기 때문이다. 생성된 evan 객체를 한번 콘솔에 출력해보면, 원본 객체의 프로퍼티나 메소드를 공유하고 있다는 말이 무엇인지 알 수 있다. 1console.log(evan); 1User {} evan 객체를 출력해보니, 이 객체는 아무 메소드나 프로퍼티도 가지고 있지 않고 텅 비어있는 친구다. 즉, 생성자 함수 내에서 this를 사용하지 않고, 원본 객체에 메소드나 프로퍼티를 정의하게 되면 객체들에게는 해당 프로퍼티가 없고, 원본 객체의 프로퍼티나 메소드를 참조한다는 것이다. 이 특징을 제대로 인지하지 못하면 이런 상황도 발생할 수 있다. 1234User.prototype.name = 'Evan';console.log(evan.name);console.log(john.name); 12EvanEvan 그렇기 때문에 각 객체마다 고유한 프로퍼티를 부여하고 싶다면 원본 객체에 정의하는 것이 아니라, 생성자 함수 내에서 this를 사용하여 정의해야한다. 다시 말하지만 원본 객체에 정의한 프로퍼티나 메소드는 생성된 객체들 끼리 공유된다. 한 가지 이상한 점은, 분명히 evan 객체에는 아무런 프로퍼티나 메소드도 없었는데, 필자는 분명히 evan.say를 통해 해당 메소드에 접근할 수 있었다는 것이다. 어떻게 이런 일이 가능한 것일까? 프로토타입 룩업그 질문에 대한 해답은 바로 자바스크립트가 객체 내에서 프로퍼티를 찾는 방법 중 하나인 프로토타입 룩업(Prototype Lookup)에서 알아볼 수 있다. 방금 전 자바스크립트가 evan 객체에서 say 메소드를 찾아냈던 과정은 다음과 같다. evan.say로 접근 시도 어, say 프로퍼티가 없네? __proto__를 통해 원본 객체로 올라가보자! User.prototype객체야, 너는 say 프로퍼티 가지고 있니? 있네? Profit! 이런 식으로 우리가 어떤 객체의 프로퍼티에 접근을 시도했을 때, 자바스크립트는 먼저 그 객체가 해당 프로퍼티를 가지고 있는지를 확인하고, 해당 프로퍼티가 없다면 그 객체의 원본 객체로 거슬러 올라가서 다시 확인하게 된다. 이 집요한 확인 과정은 모든 객체의 조상인 Object.prototype에 다다를 때까지 계속되고, 만약 여기에도 존재하지 않는 프로퍼티라면 그때서야 undefined를 반환하게 된다. 이 말인 즉슨, 모든 객체는 자신의 프로토타입 체인 내에 있는 모든 원본 객체들의 프로퍼티나 메소드에 접근할 수 있다는 뜻이다. 쉽게 말해, 방금 생성한 evan 객체는 아무 프로퍼티나 메소드도 가지고 있지 않지만, 자신의 원본 객체인 User.prorotype에 정의된 say 메소드도 사용할 수 있고, Object.prototype에 있는 toString이나 hasOwnProperty와 같은 메소드도 사용할 수 있다는 것이다. evan 객체는 프토토타입 체인 내에 있는 모든 원본 객체의 프로퍼티를 공유받는다 이 프로토타입 룩업 과정은 객체의 프로퍼티나 메소드에 접근하는 그 순간마다 수행되기 때문에, 클래스가 정의될 때 모든 상속관계가 함께 평가되는 클래스 기반 언어의 상속과는 조금 다른 느낌이다. 그러나 추상적으로 생각해보면 원본 객체(부모)의 속성을 물려받고 있다는 점에서 착안하여, 프로토타입 룩업을 토대로 상속을 구현할 수 있다. 프로토타입을 사용한 상속자바스크립트에서 프로토타입을 사용하여 상속을 구현하는 방법은 크게 Object.create 메소드를 사용하는 방법과 이 메소드를 사용하지않는 (더러운) 방법, 두 가지로 나누어질 수 있다. 사실 Object.create만 사용해도 프로토타입을 사용한 상속은 충분히 구현이 가능하다. 하지만 굳이 두 가지를 나눠서 이야기한 이유는, Object.create 메소드가 Internet Explorer 9부터 지원이 되기 때문이다. 하지만 필자는 필자의 행복을 위해 쓰는 포스팅에서 IE 8 이하 환경에 대한 자세한 이야기는 별로 하고 싶지 않으므로 Object.create를 사용하지 않는 방법에 대한 코드를 간단하게 필자의 Github Gist 링크로 첨부하겠다. Object.create를 사용하자Object.create 메소드는 첫 번째 인자로 생성할 객체의 원본 객체가 될 객체, 두 번째 인자로 새로 생성할 객체에 추가할 프로퍼티를 객체 타입으로 받는다. 1Object.create(proto: Object, properties?: Object); 이때 두 번째 인자는 선택사항이며, 단순하게 { test: 1 }처럼 넘기는 것이 아니라, Object.defineProperties 메소드를 사용할 때 처럼 데이터 서술자와 접근 서술자를 지정해줘야한다. 1234567Object.create(User.prototype, { foo: { configurable: false, enumerable: true, value: 'I am Foo!', }}); 자세한 프로퍼티들의 의미는 MDN Web Docs: Object.defineProperties에서 확인해보도록 하자. 이 메소드에서 중요한 포인트는 객체의 프로토타입 객체를 지정할 수 있다는 것이며, 이 말인 즉슨 객체의 프로토타입 체인을 내 맘대로 만져줄 수 있다는 것이다. 심지어 동적으로 변경도 가능하다.(사실 이게 JS의 변태적인 면…) 그럼 이제 Object.create 메소드와 프로토타입을 사용하여 상속을 한번 구현해보도록 하자. 123456function SuperClass (name) { this.name = name;}SuperClass.prototype.say = function () { console.log(`I am ${this.name}`);} 우선 부모 클래스 역할을 할 SuperClass 생성자 함수를 생성하고, 이 함수의 프로토타입 객체에 say 메소드를 정의했다. 그럼 이제 자식 클래스 역할을 할 생성자 함수를 구현하고, 이 두 개의 함수의 상속 관계도 함께 정의해보자. 12345678function SubClass (name) { SuperClass.call(this, name);}SubClass.prototype = Object.create(SuperClass.prototype);SubClass.prototype.constructor = SubClass;SubClass.prototype.run = function () { console.log(`${this.name} is running`);} 뭘 이것저것 많이 만진 것 같지만, 막상 하나하나 뜯어보면 별 거 없다. SuperClass.call(this)Function.prototype.call 메소드는 호출된 함수의 실행 컨텍스트를 첫 번째 인자로 받은 녀석으로 변경한다. 즉, this의 타겟을 변경하는 것이다. 즉, SuperClass.call(this, name)의 의미는 부모 생성자 함수의 생성자를 호출하되, 실행 컨텍스트를 자식 생성자 함수로 변경하라는 의미이다. 자바로 치면 super 메소드를 호출하는 것과 비슷한 느낌이랄까. 필자는 이때 call 메소드를 사용했지만, 뭐가 됐든 부모 생성자 함수의 실행 컨텍스트만 변경해주면 장땡이기 때문에 apply나 bind 메소드를 사용해도 상관없다. SubClass.prototype 변경그 후 Object.create 메소드를 사용하여 SuperClass.prototype 객체를 원본 객체로 하는 새로운 객체를 생성하고, 이 객체를 SubClass의 프로토타입 객체로 할당해준다. 자식 생성자 함수의 프로토타입 객체와 부모 생성자 함수의 프로토타입 객체 간의 프로토타입 체인, 쉽게 말해 부모 자식 관계를 만들어 주는 것이다. SubClass.prorotype.constructor 변경우리는 부모 생성자 함수의 프로토타입 객체를 토씨 하나 안바꾸고 그대로 복제했기 때문에, 새롭게 생성한 자식 생성자 함수의 프로토타입 객체의 constructor 프로퍼티는 여전히 부모 생성자 함수인 SuperClass를 참조하고 있다. 하지만 자식 생성자 함수인 SubClass를 통해 생성된 객체가 SuperClass를 사용하여 생성된 것처럼 처리되면 안되므로, 다시 constructor 프로퍼티를 SubClass로 변경해줘야한다. 이런 과정들을 거치면 다음과 같은 관계가 성립된다. 이제 한번 SubClass 생성자 함수를 사용하여 객체를 생성해보고, 제대로 부모 생성자 함수의 속성들을 물려받았는지 확인해보자. 1234const evan = new SubClass('Evan');console.log(evan);console.log(evan.__proto__);console.log(evan.__proto__.__proto__) 123SubClass { name: 'Evan' } // 에반 객체SubClass { constructor: [Function: SubClass], run: [Function] } // 에반 객체의 원본 객체SuperClass { say: [Function] } // 에반 객체의 원본 객체의 원본 객체 evan 객체는 SubClass의 프로토타입 객체를 복제해서 정상적으로 생성되었고, evan 객체의 원본 객체와 원본 객체의 원본 객체도 잘 체이닝되어있다. 즉, evan -> SubClass.prototype -> SuperClass.prototype으로 이어지는 프로토타입 체인이 완성된 것이다. 이때 evan 객체의 run이나 say 메소드를 호출하면, 위에서 언급한 프로토타입 룩업을 통해 원본 객체의 메소드를 호출할 수 있다. 마치며이전 포스팅에 이어 이번에는 자바스크립트에서 프로토타입을 활용한 상속 패턴에 대한 내용을 한번 다뤄보았다. 솔직히 말해서, 필자가 실무에서 이러한 패턴을 사용해서 상속을 구현해본 경험은 거의 없다. 필자가 개발자로 일을 시작하고 얼마 되지 않아 ES6가 나오기도 했었고, 필자는 당시 자바가 더 익숙했기 때문에 새로 추가된 class 키워드에 흠뻑 빠져있었다. 하지만 일을 시작하고 몇 년이 지나면서 레거시 코드에서 이 상속 패턴을 꽤 마주치기도 했고, 면접에서 이런 패턴에 대해서 물어보는 경우도 있었기 때문에 확실히 공부할 필요는 있는 것 같다. 아무리 요즘 ES5를 거의 사용하지 않는다고 하지만, 사실 이런 상속 패턴이 자바스크립트를 사용한 프로그램 아키텍처의 근간이기도 하니 말이다. 이상으로 프로토타입을 사용하여 상속하기 포스팅을 마친다.","link":"/2019/10/27/inheritance-with-prototype/"},{"title":"패킷의 흐름과 오류를 제어하는 TCP","text":"TCP(Transmission Control Protocol)은 원활한 통신을 위해 전송하는 데이터 흐름을 제어하고 네트워크의 혼잡 상태를 파악해서 대처하는 기능을 프로토콜 자체에 포함하고 있다. 만약 TCP가 이런 기능들을 제공해주지 않는다면 개발자가 일일히 데이터를 어떤 단위로 보낼 것인지 정의해야하고, 패킷이 유실되면 어떤 예외처리를 해야하는 지까지 신경써야하기 때문에 TCP가 제공해주는 이러한 기능들 덕분에 우리는 온전히 상위 레이어의 동작에만 집중할 수 있는 것이다. 보통 TCP의 전송 제어 방법은 전송되는 데이터의 양을 조절하는 흐름 제어, 통신 도중에 데이터가 유실되거나 잘못된 데이터가 수신되었을 경우 대처하는 방법인 오류 제어, 네트워크 혼잡에 대처하는 혼잡 제어로 나누어진다. 물론 TCP 같은 전송 계층의 프로토콜을 어플리케이션 레이어에서 활동하는 개발자가 건드릴 일은 많이 없다. 그러나 혹시라도 이 부분에서 뭔가 문제가 발생했을 경우, TCP가 어떤 식으로 작동하는지 모른다면 고치는 건 둘째치고 원인 파악조차 하지 못하는 슬픈 상황이 발생할 수 있으므로 여러모로 알아두는 것이 좋다고 생각한다. (더불어 야근도 따라올 것이다) 그런 의미에서 이번 포스팅에서는 TCP의 흐름 제어 기법들과 오류 제어 기법들에 대한 이야기를 한번 해보려고 한다. TCP의 흐름 제어송신 측과 수신 측이 서로 데이터를 주고 받을 때, 여러가지 요인에 따라 이 두 친구들의 처리 속도가 달라질 수 있다. 이때 데이터를 받는 수신 측의 처리 속도가 송신 측보다 빠른 경우는 사실 별 문제가 없다. 주는 족족 빠르게 처리해주니 딱히 문제될 것이 없는 것이다. 그러나 수신 측의 처리 속도보다 송신 측이 더 빠른 경우 문제가 생긴다. 송신 측과 수신 측은 모두 데이터를 저장할 수 있는 버퍼를 가지고 있다. 이때 수신 측이 자신의 버퍼 안에 있는 데이터를 처리하는 속도보다 송신 측이 데이터를 전송하는 속도가 더 빠르다면, 당연히 수신 측의 버퍼는 언젠가 꽉 차버릴 것이기 때문이다. 마치 꽉 찬 물컵에 물을 계속 붓는 상황이랄까 수신 측의 버퍼가 꽉 찬 상태에서 도착한 데이터는 더 이상 담아둘 공간이 없기 때문에 폐기 처분된다. 물론 이런 상황에서는 송신 측이 다시 데이터를 보내주기는 하겠지만, 데이터 전송이라는게 네트워크 환경에 따라 변수가 워낙 많은 작업이기 때문에 사실 이 작업을 줄일 수 있으면 줄이는 것이 가장 좋다. 그래서 송신 측은 수신 측의 데이터 처리 속도를 파악하고 자신이 얼마나 빠르게, 많은 데이터를 전송할 지 결정해야한다. 이것이 바로 TCP의 흐름 제어인 것이다. 수신 측은 자신이 처리할 수 있는 데이터의 양을 의미하는 윈도우 크기(Window Size)를 자신의 응답 헤더에 담아서 송신 측에게 전해주게 되고, 송신 측은 상대방에게 데이터를 보낼 때 이 윈도우 크기와 네트워크의 현재 상황을 참고해서 알맞은 양의 데이터를 보냄으로써 전체적인 데이터의 흐름을 제어하게 된다. Stop and WaitStop and Wait 방식은 이름 그대로 상대방에게 데이터를 보낸 후 잘 받았다는 응답이 올 때까지 기다리는 모든 방식을 통칭하는 말이다. 이때 데이터를 받는 수신 측은 잘 받았어!와 못 받았어... 등의 대답을 해주게 되는데, 수신 측이 어떤 대답을 해주냐에 따라 사용할 수 있는 오류 제어 방법이 나눠지기도 한다. Stop and Wait로 흐름 제어를 할 경우의 대원칙은 단순히 상대방이 응답을 하면 데이터를 보낸다이기 때문에 구현 자체도 간단하고 프로그래머가 어플리케이션의 작동 원리를 파악하기도 쉬운 편이다. 기본적인 ARQ(Automatic Repeat Request)를 구현한다고 생각해보면, 수신 측의 윈도우 크기를 1 byte로 설정하고 처리 가능 = 1, 처리 불가능 = 0과 같은 식으로 대충 구현해도 돌아가기는 하기 때문이다. 하지만 서로 처리 가능, 처리 불가능 정도의 의미만 주고받는 방식은 간단한만큼 비효율적이라고 할 수도 있다. 왜냐하면 송신 측은 자신이 직접 데이터를 보내봐야 이 데이터를 수신 측이 처리할 수 있는지 알 수 있기 때문이다. 쉽게 말해서 이런 기초적인 Stop and Wait 방식은 그냥 될 때까지 주구장창 보내는 방식이라고 봐도 무방하다. 그런 이유로 Stop and Wait 방식을 사용하여 흐름 제어를 할 경우에는, 이런 비효율성을 커버하기 위해 이런 단순한 구현이 아닌 여러가지 오류 제어 방식을 함께 도입해서 사용한다. Sliding Window방금 알아본 바와 같이 Stop and Wait를 사용하여 흐름 제어를 하게 되면 비효율적인 부분이 있기 때문에, 오늘날의 TCP는 특별한 경우가 아닌 이상 대부분 슬라이딩 윈도우(Sliding Window) 방식을 사용한다. 슬라이딩 윈도우는 수신 측이 한 번에 처리할 수 있는 데이터를 정해놓고 그때그때 수신 측의 데이터 처리 상황을 송신 측에 알려줘서 데이터의 흐름을 제어하는 방식이다. Stop and Wait과 여러 가지 차이점이 있겠지만, 사실 가장 큰 차이점은 송신 측이 수신 측이 처리할 수 있는 데이터의 양을 알고 있다는 점이다. 이 정보를 알고 있기 때문에 굳이 수신 측이 처리 가능이라는 대답을 일일히 해주지 않아도 데이터를 보내기 전에 이게 처리될 지 어떨지 어느 정도 예측이 가능하다는 말이다. 송신 측과 수신 측은 각각 데이터를 담을 수 있는 버퍼를 가지고 있고, 별도로 윈도우라는 일종의 마스킹 도구를 가지고 있다. 이때 송신 측은 이 윈도우에 들어있는 데이터를 수신 측의 응답이 없어도 연속적으로 보낼 수 있다. 윈도우 안에 들어있는 프레임은 수신 측의 응답이 없이도 연속으로 보낼 수 있다 송신 측의 윈도우 크기는 맨 처음 TCP의 연결을 생성하는 과정인 3 Way Handshake 때 결정된다. 이때 송신 측과 수신 측은 자신의 현재 버퍼 크기를 서로에게 알려주게 되고, 송신 측은 수신 측이 보내준 버퍼 크기를 사용하여 음, 대충 이 정도 처리 가능하겠군이라는 과정을 통해 자신의 윈도우 크기를 정하게 된다. 123localhost.initiator > localhost.receiver: Flags [S], seq 1487079775, win 65535localhost.receiver > localhost.initiator: Flags [S.], seq 3886578796, ack 1487079776, win 65535localhost.initiator > localhost.receiver: Flags [.], ack 1, win 6379 tcpdump를 통해 3 Way Handshake를 관찰해보면 처음의 SYN과 SYN+ACK 패킷에는 각자 자신의 버퍼를 알려준 후 마지막 ACK 패킷 때 송신 측이 자신이 정한 윈도우 사이즈를 상대방에게 통보하는 것을 볼 수 있다. 이때 송신 측과 수신 측 모두 자신의 버퍼 크기라 65535라고 이야기했지만 최종적으로 송신 측이 정한 자신의 윈도우 크기는 6379이다. 왜 송신 측은 수신 측 버퍼 크기의 10분의 1로 자신의 윈도우 크기를 정한 것일까? 사실 송신 측의 윈도우 크기는 수신 측의 버퍼 크기로만 정하는 것이 아니라 다른 여러가지 요인들을 함께 고려해서 결정된다. 상대방이 보낸 버퍼 크기만 믿고 자신의 윈도우 크기를 정하기에는 네트워크는 너무나도 험난한 환경이기 때문이다. 이때 사용하는 대표적인 값이 바로 패킷의 왕복 시간을 의미하는 RTT(Round Trip Time)이다. 송신 측은 자신이 처음 SYN 패킷을 보내고, 다시 수신 측이 SYN+ACK 패킷으로 응답하는 시간을 재고, 이 값을 통해 현재 네트워크 상황을 유추한다. 이때 이 값이 너무 크다면 왕복 시간이 느리다는 것이므로 네트워크 상태가 좋지 않다고 생각하고 윈도우 크기를 조금 더 줄이게 되는 것이다. 그리고 이때 정해진 윈도우 크기는 고정이 아니라 통신을 하는 과정 중간에도 계속 네트워크의 혼잡 환경과 수신 측이 보내주는 윈도우 크기를 통해 동적으로 변경될 수 있다. 윈도우의 크기, 즉 연속적으로 보낼 데이터의 양을 변경해가면서 유연하게 흐름 제어를 할 수 있다는 말이다. 윈도우에 대해 대략적으로 이해를 했다면 이제 이 기법을 왜 슬라이딩 윈도우라고 하는 지 한번 살펴보도록 하자. 먼저, 송신 측이 0 ~ 6번의 시퀀스 번호를 가진 데이터를 상대방에게 전송하고 싶어하는 상황을 상상해보자. 이때 송신 측의 버퍼에는 전송해야할 데이터들이 이렇게 담겨져 있을 것이다. 이때 송신 측은 수신 측에게 받은 윈도우 크기와 현재 네트워크 상황을 고려하여 윈도우 크기를 3으로 잡았고, 윈도우 안에 있는 데이터를 우선 주르륵 전송한다. 이때 윈도우 안에 들어있는 데이터는 어떤 상태일까? 일단 데이터를 전송하기는 했지만 아직 수신 측으로부터 잘 받았다는 응답을 받지 못한 상태일 것이다. 즉, 윈도우에 들어있는 데이터들은 항상 전송은 했지만, 상대방이 처리했는지는 모르는 상태라고 할 수 있다. 물론 데이터를 윈도우에 넣고 나서 블록킹이 걸려 데이터를 처리하지 못하는 상태도 존재할 수 있지만, 그런 것까지 다 고려하면 너무 복잡하니까 간단하게 생각하도록 하자. 이후 수신 측은 자신의 처리 속도에 맞게 데이터를 처리한 후 응답으로 현재 자신의 버퍼에 남아있는 공간의 크기를 알려준다. 만약 수신 측이 응답으로 Window Size: 1을 보냈다면 “내 버퍼 공간이 1 byte만큼 남았으니까 그 만큼만 더 보내봐”라는 의미가 된다. 이제 송신 측은 자신이 데이터 한 개를 더 보낼 수 있다는 사실을 알았으니, 자신의 윈도우를 한 칸 옆으로 밀고 새롭게 윈도우에 들어온 3번 데이터를 수신 측에게 전송한다. 이때 윈도우를 옆으로 이동시키며 새로 들어온 데이터를 전송하기 때문에 슬라이딩 윈도우라고 하는 것이다. 만약 수신 측이 윈도우 크기를 1이 아니라 더 큰 수를 보냈다면, 송신 측은 그 만큼 윈도우를 옆으로 밀고 더 많은 데이터를 연속적으로 전송할 수 있을 것이다. 단, 이 경우 송신 측의 윈도우 크기가 3이기 때문에 수신 측이 4를 보냈다고 해서 4칸을 밀지는 않고, 자신의 윈도우 크기인 3만큼만 밀 수 있다. 그러나 이 경우에는 송신 측이 수신 측의 퍼포먼스가 더 좋아졌다는 것을 알았으니 자신의 윈도우 크기를 늘리는 방법으로 대처할 수 있을 것이다. 이렇게 데이터를 전송하는 송신 측의 버퍼는 대략 3가지 상태로 나눠질 수 있다. 즉 슬라이딩 윈도우 방식은 보내고 -> 응답받고 -> 윈도우 밀고를 반복하면서, 현재 자신이 보낼 수 있는 데이터를 최대한 연속적으로 보내는 방법이라고 할 수 있다. 이게 지금 0 ~ 6 밖에 안되는 단순화된 그림으로 봐서 잘 와닿지 않을 수도 있지만, 아무런 옵션도 적용하지 않은 TCP의 최대 윈도우 크기는 65,535 bytes이고, WSCALE 옵션을 최대로 적용하면 1GB로 설정하는 것도 가능하다. 게다가 연속적으로 한번에 보내는 데이터도 이렇게 한 개, 두 개 정도가 아니라 몇 백 바이트 단위로 보내는 경우가 많기 때문에 실제 환경에서는 Stop and Wait로 흐름 제어를 하는 것과 비교해봤을때 상당히 좋은 효율을 뽑아낼 수 있다. 즉, 이론적으로는 수신 측의 ACK 응답 없이도 최대 1GB를 연속적으로 전송할 수 있다는 말이다. 이렇게 슬라이딩 윈도우 방식은 일일히 하나 보내고, 응답 받고 하는 Stop and Wait보다 확실히 전송 속도 측면에서 빠르기도 하고, 송신 측과 수신 측의 지속적인 커뮤니케이션을 통해 윈도우 크기 또한 유연하게 조절할 수 있기 때문에 최근의 TCP에서는 기본적으로 슬라이딩 윈도우를 사용하여 흐름 제어를 하고 있다. TCP의 오류 제어TCP는 기본적으로 ARQ(Automatic Repeat Request), 재전송 기반 오류 제어를 사용한다. 말 그대로 통신 중에 뭔가 오류가 발생하면 송신 측이 수신 측에게 해당 데이터를 다시 전송해야한다는 말이다. 하지만 이 재전송이라는 작업 자체가 했던 일을 또 해야하는 비효율적인 작업이기 때문에, 이 재전송 과정을 최대한 줄일 수 있는 여러가지 방법을 사용하게 된다. 오류가 발생했다는 것은 어떻게 알 수 있나요?TCP를 사용하는 송수신 측이 오류를 파악하는 방법은 크게 두 가지로 나누어진다. 수신 측이 송신 측에게 명시적으로 NACK(부정응답)을 보내는 방법, 그리고 송신 측에게 ACK(긍정응답)가 오지 않거나, 중복된 ACK가 계속 해서 오면 오류가 발생했다고 추정하는 방법이다. 간단히 생각해보면 왠지 NACK를 사용하는 방법이 더 명확하고 간단할 것 같지만, NACK를 사용하게되면 수신 측이 상대방에게 ACK를 보낼 지 NACK를 보낼 지 선택해야하는 로직이 추가적으로 필요하기 때문에, 일반적으로는 ACK만을 사용해서 오류를 추정하는 방법이 주로 사용되고 있다. 이때 타임아웃은 말 그대로 송신 측이 보낸 데이터가 중간에 유실되어, 수신 측이 아예 데이터를 받지 못해 ACK를 보내지도 않았거나, 수신 측은 제대로 응답했지만 해당 ACK 패킷이 유실되는 경우에 발생하게 된다. 어쨌든 두 경우 모두 송신 측은 데이터를 전송했는데 수신 측이 응답하지 않고 일정 시간이 경과한 경우라고 생각하면된다. 그리고 두 번째 방법인 송신 측이 중복된 ACK를 받는 경우 오류라고 판별하는 방법은 대략 이런 느낌이다. 이 상황을 조금 더 쉽게 풀어보자면, 송신 측은 이미 SEQ 2 데이터를 보낸 상황인데 수신 측이 계속 야, 이번에 2번 보내줄 차례야라고 말하는 상황인 것이다. 그럼 송신 측은 자신이 보낸 2번 데이터에 뭔가 문제가 발생했음을 알 수 있다. 단, 패킷 기반 전송을 하는 TCP의 특성 상 각 패킷의 도착 순서가 무조건 보장되는 것이 아니기 때문에 위 예시처럼 중복된 ACK를 한 두번 받았다고 해서 바로 에러라고 판별하지는 않고, 보통 3회 정도 받았을 때 에러라고 판별하게 된다. Stop and WaitStop and Wait는 흐름 제어 때 한번 살펴보았던, 한번 데이터를 보내면 제대로 받았다라는 응답이 올 때까지 대기하고 있다가 다음 데이터를 보내는 방식이다. 이 친구가 오류 제어에서 다시 나오는 이유는 그냥 이렇게만 해도 기본적인 오류 제어가 가능하기 때문이다. 일석이조랄까. 애초에 제대로 받았다는 응답이 오지 않는다면 제대로 받을 때까지 계속 데이터를 재전송하는 방법이니까 흐름 제어도 되지만 오류 제어도 가능하다. 그러나 위에서 살펴본 슬라이딩 윈도우를 사용하여 흐름 제어를 하는 경우에는 윈도우 안에 있는 데이터를 연속적으로 보내야 하기 때문에, 오류 제어에 Stop and Wait를 사용해버리면 슬라이딩 윈도우를 쓰는 이점을 잃어버린다. 그런 이유로 일반적으로는 이런 단순한 방법보다 조금 더 효율적이고 똑똑한 ARQ를 사용하게 된다. Go Back NGo Bank N 방법은 데이터를 연속적으로 보내다가 그 중 어느 데이터부터 오류가 발생했는지를 검사하는 방식이다. 위에서 이야기했듯이 오류를 판별하는 방법에는 ACK의 이상 징후를 파악하는 방법을 더 많이 사용하기는 하지만, NACK를 사용하고 있다고 가정하는 것이 다이어그램을 이해하기가 편하므로 오류 제어 기법을 설명할 때는 수신 측이 NACK를 사용하고 있다고 가정할 것이다. 이 섹션에서는 오류 제어 기법을 설명하는 것이 목적이니, 오류를 어떻게 판별하는지보다는 오류를 어떻게 제어하는지에 대해서만 집중해보도록 하자. Go Back N 방식을 사용하면 데이터를 연속적으로 보낸 후 한 개의 ACK나 NACK만을 사용하여 수신 측의 처리 상황을 파악할 수 있으므로, 연속적으로 데이터를 보낼 수 있는 흐름 제어 방식인 슬라이딩 윈도우와 아주 잘 들어맞는다고 할 수 있다. 위 그림을 보면 수신 측이 4번 데이터부터 에러가 발생함을 감지하고 송신 측에게 4번부터 다시 보내줘라고 하고 있다. Go back N 방식에서 수신 측이 4번 데이터에서 에러가 발생했음을 감지하면, 4번 데이터 이후 자신이 받았던 모든 데이터를 폐기하고 송신 측에게 NACK를 보내게 된다. 에러가 발생하면 그 뒤에 있던 멀쩡한 데이터도 같이 날려버리는 쏘쿨한 방식이다 즉, 송신 측은 수신 측으로 NACK를 받고나면 오류가 발생한 4번 데이터와 그 이후 전송했던 모든 데이터를 다시 전송해줘야 한다는 말이 된다. 이때 송신 측은 비록 5번까지 전송했지만 오류가 발생하면, 오류가 발생한 4번 데이터로 되돌아가서 다시 전송해야하므로 Go Back N이라고 부르는 것이다. Selective RepeatSelective Repeat은 말 그대로 선택적인 재전송을 의미한다. Go Back N 방법도 Stop and Wait에 비하면 많이 효율적인 방법이지만, 에러가 발생하면 그 이후에 정상적으로 전송되었던 데이터까지 모두 폐기 처분되어 다시 전송해야한다는 비효율이 아직 존재한다. 그래서 나온 방식이 에러난 데이터만 재전송해줘 방식인 것이다. 얼핏 보면 이 방식이 굉장히 효율적이고 좋기만 한 것 같지만 Stop and Wait와 Go Back N 방식과 다르게, 이 방식을 사용하는 수신 측의 버퍼에 쌓인 데이터가 연속적이지 않다는 단점이 존재한다. 위 예시만 봐도 수신 측의 버퍼에는 0, 1, 2, 3, 4, 5가 순차적으로 들어있는 것이 아니라, 중간에 폐기 처분된 4를 제외한 0, 1, 2, 3, 5만 버퍼에 존재할 것이기 때문이다. 이때 송신 측이 4를 재전송하게되면 수신 측은 이 데이터를 버퍼 중간 어딘가에 끼워넣어서 데이터를 정렬해야한다. 이때 같은 버퍼 안에서 데이터를 정렬할 수는 없으니, 별도의 버퍼가 필요하게 된다. 결국 재전송이라는 과정이 빠진 대신 재정렬이라는 과정이 추가된 것인데, 이 둘 중에 재전송이 좀 더 이득인 상황에서는 Go Bank N 방식을, 재정렬이 좀 더 이득인 상황에서는 Selective Repeat 방식을 사용하면된다. 만약 TCP 통신에서 Selective Repeat 방식을 사용하고 싶다면, TCP의 옵션 중 SACK 옵션을 1로 설정하면 된다…만 사실 기본적으로 켜져 있는 경우가 많다. 12$ sysctl net.inet.tcp | grep sack:net.inet.tcp.sack: 1 OSX 같은 경우, sysctl 명령어를 사용하여 TCP와 관련된 커널 변수들을 확인해보면 그 중 net.inet.tcp.sack 값이 1로 잡혀있는 것을 확인할 수 있다. 아무래도 대부분의 경우에는 정글이나 다름 없는 네트워크를 다시 사용하는 쪽보다는 그냥 수신 측이 재정렬을 하는 것이 이득인 경우가 많다보니 기본적으로 Selective Repeat을 사용하는 것이 아닌가싶다. 마치며필자가 최근에 TCP에 대해 공부하면서 가장 어려운 점은, TCP라는 주제 안에 굉장히 다양한 개념들이 서로 얶혀있다는 점이다. 아무래도 개발된 지 50년이 다 되어가는 할배 프로토콜이다보니 초반에 개발된 버전에 비해서 개선사항도 굉장히 많고 옵션 또한 다양하다. 그래서 TCP의 어떤 동작 하나를 파헤쳐보려고해도 무슨 줄줄이 소세지처럼 이것저것 전부 엮여나온다. 예를 들어 슬라이딩 윈도우 기법을 설명할 때 나왔던 윈도우 크기의 경우, 단순히 수신 측이 보내준 윈도우 크기를 사용하는 것이 아니라 RTT, MTU, MSS 등 네트워크의 상황과 관련된 다양한 변수를 종합적으로 고려하여 결정된다. 정확히 말하면 수신 측이 보내준 윈도우 크기와 송신 측이 네트워크 혼잡도를 고려해서 정한 윈도우 크기 중 더 작은 값이 송신 측의 최종 윈도우 크기가 되는 식이다. 또한 오류 제어 섹션에서 이야기했던 오류 판별 기법은 사실 혼잡 제어에서도 중요한 부분으로 설명되며, 이후 방금 말한 로직을 통해 송신 윈도우 크기를 재설정하고 송수신 측이 합의본 오류 제어 기법을 사용하여 패킷을 재전송하는 과정으로 진행된다. 결국 이 포스팅에서는 다루지 않았지만 흐름 제어라는 주제 안에 혼잡 제어와 관련된 내용도 섞여있는 것이다. 개인적으로 흐름, 혼잡, 오류와 같이 주제를 나눠놓아서 오히려 더 헷갈리는 것도 있는 것 같다. 그래서 이런 구분없이 따로 정리해서 포스팅을 작성하려고도 했었지만, 아무래도 이런 구분이 모두에게 익숙한 상황이니 그냥 필자도 거기에 편승하기로 했다. 사실 이런 점 때문에 혼잡 제어도 함께 포스팅에 작성하려고 했지만 포스팅 라인 수가 500줄이 넘어가는 것을 보고 그냥 분리했다. 글이 너무 길어지면 읽는 사람도 힘들다. 다음 포스팅에서는 마지막으로 TCP의 혼잡제어에 대한 이야기를 한 번 해보려고 한다. 이상으로 패킷의 흐름과 오류를 제어하는 TCP 포스팅을 마친다.","link":"/2019/11/22/tcp-flow-control-error-control/"},{"title":"솔직한 피드백으로 좋은 팀워크를 만들 수 있을까? - 파워풀을 읽고","text":"최근 4일 동안 심한 몸살에 걸려 침대에만 누워있으면서, 컴퓨터를 멀리 하고 오랜만에 책을 읽었다. 마침 입사 예정인 회사에서 “린 스타트업”과 “파워풀” 두 권의 책을 보내주었기 때문에 무엇을 읽어볼까하는 고민을 할 시간을 줄일 수 있었다. 그런 이유로 넷플릭스의 기업 문화를 주도했던 패티 맥코드의 “파워풀”이라는 책을 먼저 읽어보았는데, 평소에 필자가 회사를 다니면서 고민했던 많은 문제들에 대해서 나름 명쾌한 해답을 제시해주는 것 같았다. 이 책은 총 8개의 챕터를 통해 넷플릭스가 지향하는 기업 문화인 “자유와 책임”에 대해서 이야기하고 있는데, 그 중에서 필자에게 많은 생각이 들게 했던 부분은 바로 3장의 “극도로 솔직해져라”라는 챕터였다. 극도로 솔직해져라패티 맥코드는 넷플릭스의 핵심 기업 문화 중 한 가지로 “솔직한 진심을 나눠야한다”라고 이야기한다. 그것이 같은 직원이던 경영진이던 동일하게 말이다. 넷플릭스의 솔직함에는 “상대방에게 직접 말할 것”, “피드백을 환영할 것”, “실수를 인정할 것” 등의 몇 가지 키워드가 있다. 최근 국내에서도 다양한 회사들이 이런 문화를 적극적으로 받아들여 기업의 자산 현황을 직원들에게 공개하거나 직급 여하와 상관없이 자신의 의견을 자유롭게 말할 수 있는 분위기를 조성하고 있다. 파워풀에서 이야기하고 있는 솔직함에 대한 내용 중에서 필자가 생각이 많아진 부분은 바로 “솔직한 피드백”이었다. 물론 서로 가감없이 솔직한 피드백을 주고 받을 수 있는 문화는 굉장히 좋은 문화라고 생각한다. 상대방의 발전을 기원하며 서로 건네는 피드백이 건강하게 작용한다면 구성원의 학습의 질을 높은 수준으로 끌어올릴 수 있기 때문이다. 필자도 전 직장에서 팀원들 간에 이런 피드백을 주고 받으며, 서로에게 조언도 해주고 책이나 아티클도 공유했던 좋은 기억이 있다. 심지어 필자가 이번 이직 때 직장을 선택하는 기준 중에 하나도 이런 피드백을 통해 정하게 되었을 정도이다. 그렇다면 이런 건강한 피드백 문화를 만들기 위해 필요한 것이 정말 “솔직함” 뿐일까? 그러나 필자는 개인적으로 이러한 피드백 주고받기 문화를 “솔직함”이라는 키워드만 바라보고 그대로 현실에 적용하기에는 조금 애매하다고 생각한다. 불가능하지는 않지만 이런 문화가 조직에 성공적으로 정착하기까지 넘어야 할 장애물이 몇 가지 있다. 사람은 감정을 100% 배제하고 생각할 수 없다패티 맥코드는 피드백에 관련된 내용을 이야기할 때 주로 “피드백을 주는 쪽”의 입장을 많이 이야기했다. 상대방이 너의 피드백을 무시하거나 상처받을 것이라고 생각하지말고 “너의 팀원을 믿어라”라는 식으로 말이다. 필자가 이 책을 읽으며 아쉬웠던 점은, 피드백을 받는 사람의 현재 감정 상태를 딱히 고려하지 않는다는 점이었다. 오히려 패티 맥코드는 “감정없이 대화하는 법”에 대한 내용을 언급한다. 물론 업무적인 이야기를 할 때 불필요한 감정은 배제하기 위해 노력하는 것이 프로페셔널한 자세인 것은 맞지만 사람이 감정을 완전히 배제하고 생활한다는 것이 가능한 일일까? 책의 흐름을 보았을 때, 패티 맥코드가 말하는 감정은 부정적인 감정을 의미하는 것일테다. 그러나 책에서는 이에 대해 직접적으로 이야기하지 않았고, 특정 상황을 예시로 들며 상대방을 배려하는 말투로 피드백을 주라고 간접적으로 이야기하고 있다. 음, 필자는 감정이 팀워크에 미치는 영향에 대해서 꽤나 중요하게 생각하는 사람이기 때문에 이에 대한 이야기를 조금 더 해보려고 한다. 지난 6월에 작성한 좋은 프로그래머란 무엇일까? 포스팅에서도 한 차례 언급한 바 있지만, 감정이라는 것은 기본적으로 두뇌의 신호를 받아 호르몬이 분비된 결과물이다. 그 말인 즉슨, 사람이 의도적으로 감정을 컨트롤한다는 것이 결코 쉬운 일이 아니라는 것이다. 아무리 회사고 공적인 조직이라고 하지만, 회사도 결국은 사람들이 모여서 함께 뭔가를 이루기위해 노력하는 커뮤니티이다. 사람들이 모여서 서로 소통하며 뭔가를 하는 이상 그 속에서 감정을 100% 배제하는 것은 불가능하다. 그래서 필자는 상호 간의 커뮤니케이션에서 중요한 점을 하나 뽑으라고 한다면 제일 먼저 “존중”이라는 키워드를 뽑는다. 그리고 이 존중이라는 단어 안에는 상대방의 직무, 감정에 대한 배려가 숨어있다. 문제는 피드백이라는 것이 대부분 부정적인 의견을 전달하는 용도의 커뮤니케이션이라는 것이다. 물론 건강한 피드백은 상대방이 어떠한 단점을 보완하여 더욱 더 발전했으면 하는 마음에서 나오는 것이나, 피드백을 받아들이는 사람이 어떻게 생각할 지는 며느리도 모르는 일이다. 아무리 채용 과정에서 피드백에 대한 부정적인 관점을 가지지 않는 사람이라는 것이 어느 정도 입증되었다고 해도, 사람 마음이라는 것이 늘 한결 같은 것도 아니고, 그 날의 상황에 따라서 기분이 좋지 않을 수도 있지 않은가? 예를 들어 평소 회사에서 일을 하면서 하루에 100명 정도에게 피드백을 듣는다고 생각해보자. 사실 필자는 이 정도 피드백은 아무렇지 않다. 평소의 필자는 피드백을 들었을 때 “이 단점을 보완하자”와 같은 이펙트를 내는 타입이기 때문이다. 하지만 갑자기 가족이 투자 사기를 당해서 집이 하루 아침에 파산했다면 어떤가? 그래도 다음 날 아무렇지 않게 100명의 피드백을 받아들이고 평소처럼 “단점을 보완해야지”라는 긍정적인 생각을 할 수 있을까? 물론 가능한 사람도 있겠지만, 필자는 아닐 것 같다. 머릿 속에 설상가상(雪上加霜)이라는 단어가 떠오르면서 “요즘 일이 잘 안풀리네”라는 부정적인 생각이 들 것만 같다. 이렇듯 사람 마음이라는 것이 외부 환경에 독립적인 요소가 아닌 만큼, 상황에 따라서 같은 자극에도 다른 반응이 나타날 수 있는 것이다. 또한 필자가 이런 상황을 다른 사람들에게 말하지 않는 이상 다른 사람은 필자의 현재 심리 상태를 알기 힘들다. 사실 회사 뿐 아니라 모든 조직의 커뮤니케이션에서 가장 어려운 문제가 이처럼 “상대방의 마음”을 도통 알 수가 없다는 것이다. 우리는 프로토스처럼 칼라를 통해 소통하는 종족도 아니지 않은가? 프로토스는 칼라를 통해 감정과 생각을 공유하지만 인간은 아니다 다른 사람과의 커뮤니케이션은 언제나 이런 불확실성 속에서 진행되는 것이기 때문에 어려운 것이고, 그 불확실성에는 그 사람의 감정이 크게 기여한다고 생각한다. 이런 이유로 전 직장에서는 아침마다 “체크인”이라는 활동을 통해 자신의 현재 컨디션을 1과 10 사이의 숫자로 표현하는 방법을 사용하기도 했다. 건강한 피드백 문화라는 것은 단순히 솔직함이라는 키워드 만으로 만들어지는 것이 아니다. 여기에는 “솔직하게” 그리고 “따뜻하게” 상대방을 배려하고 존중하면서 피드백을 줄 수 있는 문화가 함께 동반되어야한다. 부드러운 어투로 피드백을 주면 된다?여기까지 이야기를 하고나면 많은 분들이 “그럼 부드러운 어투로 피드백을 주면 되는 것 아닌가?”라고 하실텐데, 사실 이것도 굉장히 애매한 부분이다. “부드럽게 말하다”의 기준 또한 사람마다 주관적이기 때문이다. 필자는 전 직장에서 딱 한번 말투로 인해서 피드백을 받았던 적이 있었다. 필자는 당시 모든 팀이 이슈 관리용으로 사용하는 JIRA라는 솔루션을 관리하고 있었는데, 어느 날 살펴보니 팀이나 개인 단위로 무분별하게 보드를 생성해서 어떤 것이 진짜 사용하는 보드이고 어떤 것이 사용하지 않는 보드인지 한 눈에 파악하기가 힘들었다. 그래서 필자는 각 팀에게 현재 이 보드를 사용하고 있는지에 대한 여부를 물어보고 더 이상 사용하지않는 보드는 백업 후 삭제하는 작업을 하게되었는데, 문제는 CX(고객경험)팀에 필자가 질문을 하러 갔을 때 발생했다. - 똑똑똑 -에반: 저기 죄송한데, 혹시 CX팀에서 SCX 보드 현재 사용하시나요?CX 팀원: 네…? (놀람) 아뇨 저희 지금 안 쓰고 있어요.에반: 넵 감사함다. 필자는 확인도 받았으니 해당 보드를 백업하고 삭제했는데, 조금 시간이 지난 뒤에 CX 팀원으로부터 “저희가 혹시 뭐 잘못 했나요…?”라는 슬랙 메세지를 받았다. 메세지를 받고 순간 “왜…?”라는 생각이 들었는데, 조금 생각해보니 그럴만 했다는 결론을 내렸다. CX팀 입장에서는 갑자기 개발자가 자기네 부서에 오더니 아무런 앞 뒤 설명없이 “이 보드 쓰세요?”라고 물어보고 간 상황이었던 것이다. 게다가 보통 개발자가 다른 팀에 이런 질문을 하는 경우는 다른 팀에서 뭔가 만졌는데 데이터가 꼬였다던가 하는 경우 였음을 생각해보면 필자에게 저런 질문을 하는 것이 이상한 일도 아니였다. 필자 입장에서는 저 작업 외에도 일이 많고 바쁘기 때문에 용건만 간단히 물어본 것이지만, 듣는 사람 입장에서는 당황스러웠을 것이다. (지금 생각해보니 표정도 딱딱했던 것 같다) 물론 이후에 자초지종을 설명하고 오해가 풀렸지만, 이 사건은 필자가 커뮤니케이션에 대해서 깊은 고민을 하게 만드는 기폭제가 되기도 했다. 사실 필자는 스스로 말투가 그렇게 부드러운 편이 아니라는 사실을 알고 있었다. 그래서 필자에 대해서 잘 모르는 사람들과 이야기할때는 나름 말투에 신경을 쓰는 편이고, CX팀에 가서 저 질문을 했을때도 나름대로 신경쓴 편이었다. 하지만 CX팀에서는 필자의 말투가 너무 딱딱해서 무섭다고 했다. 결국은 “내가 받아들이는 것”과 “상대방이 받아들이는 것”이 다를 수 있다는 것이며, 원활한 커뮤니케이션을 위해서는 이 사실을 인지하고 있어야한다는 것이다. 그리고 이런 경우, 필자가 아무리 신경을 썼다고 해도 상대방이 딱딱한 말투라고 느꼈다면 필자가 더 부드럽게 말하거나 표정같은 비언어적인 커뮤니케이션을 사용하여 보완하는 것이 맞다. “부드러운 말투로 피드백을 준다”라는 말도 결국 마찬가지다. 내 입장에서는 아무리 부드럽게 말한다고 해도 상대방이 그렇게 받아들이지 않을 확률은 늘 존재하며, 솔직한 피드백을 주고 받는 문화가 성공적으로 정착하려면 이런 이슈들에 대한 구성원들의 성숙한 고민과 발전이 끊임없이 계속 되어야한다. 물론 패티 맥코드도 “적대적이거나 거들먹거리는 말투”로 피드백을 주지 말라는 이야기는 간단히 언급했으나 필자는 개인적으로 이런 상대방에 배려와 존중 또한 솔직함이라는 키워드와 동일하게 중요하다고 생각하는 편이라서, 이 책에서는 너무 “솔직함”에만 집중한 것이 아쉽다는 생각이 들었다. 경영진과 직원 사이의 피드백패티 맥코드는 직급의 고하와 상관없이 피드백을 주고 받을 수 있어야 한다고 말하며, 이런 피드백 매커니즘은 위에서 아래로 내려오는 탑다운(Top-Down)으로 전파되어야 한다고 한다. 음, 하지만 현실적으로 이게 제일 어려운 부분이 아닐까? 아무리 의사결정을 각 실무자에게 위임하고 수평적인 의사소통을 지향한다고 해도 경영진이라는 자리는 회사 전체를 리딩하는 자리인 만큼 가지는 책임이나 권한이 일반적인 직원보다 크기도 하고, 20년 넘게 학교나 알바, 군대 등 온갖 수직적인 구조를 겪으며 살아온 우리에게는 아직 “사장님과 수평적인 관계”라는 사실 자체가 어색하니 말이다. (이건 특별히 한국이라서 그런게 아니라 만국 공통이다) 그런 이유로 직원의 입장에서 경영진에게 피드백을 건넨다는 것은 솔직히 부담스러울 수 밖에 없다. 물론 많은 기업들이 수평적인 문화를 지향하고 또 실제로 그런 문화를 실천하고 있기는 하지만, 경영진이라는 타이틀을 가진 사람을 내 동료처럼 편하게 대한다는 것은 꽤나 어려운 일이다. 직원이 경영진을 “동료”로 느끼게 만드는 일은 생각보다 쉬운 일이 아니다 그래서 패티 맥코드 또한 이런 피드백 매커니즘을 정착시키려면 탑다운으로 전파되어야한다고 했던 것이다. 여기서 말하는 탑다운은 “명령”이 아니라, 경영진이 먼저 다가서라는 의미이다. 당연히 수평적인 문화를 지향하는 많은 회사의 대표들도 이런 사실을 알고 있기에 꾸준하게 “저한테 편하게 대해주세요”라는 제스처를 취한다. 하지만 이상하게도 직원들 마음은 그렇게 쉽게 열리지 않는 경우가 많다. 그 이유는 무엇일까? 이건 마치 연애와 비슷한 느낌인데, 말로는 사랑한다고 하지만 상대방에게 무심한 행동만 보여준다면 상대방이 진짜 이 사람이 자신을 사랑한다고 믿을 수 있을까? 오히려 말로는 사랑한다고 하지 않아도 늘 따뜻하고 배려있는 행동을 보여준다면 더 믿을 수 있을 것 같다. (오히려 이런 츤데레가 더 매력적일 수도 있다) 마찬가지로 필자는 경영진이 정말 수평적인 커뮤니케이션과 피드백을 원한다면 “편하게 대해달라”라는 말보다는, 경영진이 먼저 직원들에게 편하게 다가오고 주기적인 교육을 통해 우리는 수평적인 관계라는 사실을 이야기하며, 실제로 피드백을 받았을 때도 긍정적인 반응을 보이는 등의 행동을 실천함으로써 직원들에게 꾸준한 신뢰를 줘야한다고 생각한다. 그 신뢰란, 내가 이 사람에게 피드백을 줘도 나에게 불이익이 없을 것이라는 신뢰, 그리고 내 피드백이 이 사람에게 받아들여질 수 있다는 신뢰, 이 피드백으로 인해 우리 조직이 전체적으로 발전할 수 있을 것이라는 신뢰다. 그래서 직급과 관계없이 자유롭게 피드백을 주고 받는 문화를 가지고 있는 기업들을 보면 주기적으로 타운홀 미팅을 하면서 공개적으로 직원들에게 피드백을 받거나, 대표가 먼저 피드백에 대해서 물어보고, 피드백에 대한 교육을 실시하는 등 지속적으로 노력하고 있는 모습을 볼 수 있다. 즉, 직원과 경영진 간 투명한 피드백을 주고받는 문화는 “솔직함” 뿐만 아니라 경영진이 직접 노력하는 모습을 보여줌으로써 직원들과 신뢰를 쌓는 과정 또한 중요하다고 할 수 있다. 마치며패티 맥코드의 파워풀은 넷플릭스의 핵심 기업 문화인 “자유와 책임”에 대한 전반적인 내용을 재미있게 설명하고 있는 책이다. 필자 또한 지금까지 스타트업에서 일을 해오면서 자연스럽게 자유와 책임을 함께 가져야하는 문화를 몸에 익혔기 때문에 여러모로 공감이 갔다. 그리고 패티 맥코드가 이야기한 “솔직함”에 대한 내용 또한 많은 공감이 갔다. 뒤에서 이야기하지말고 당사자에게 직접 이야기하라는 것이나, 서로의 발전을 위해 솔직한 피드백을 주고 받으라는 것, 조직 내의 모든 사람은 비즈니스에 대해 뭐든지 질문하고 대답할 수 있어야한다는 것들 말이다. 넷플릭스는 이러한 자유와 책임이라는 기업 문화를 이끌어나가기 위해 직원들을 프로페셔널리즘을 가진 “어른”으로 대하고, 또 그런 사람을 채용하려고 많은 노력을 기울인다. 그러나 어른이라고 한들 감정이 없는 로봇은 아니라는 점을 잊어서는 안된다. 사실 프로페셔널한 어른이라면 자기 감정 정도는 스스로 컨트롤하고 건설적인 마인드로 일을 한다. 하지만 위에서 언급했듯이 감정을 도저히 숨기기 힘든 상황도 있을 것이다. 그럴때는 솔직한 피드백보다는 커피 한잔과 함께 건네는 따뜻한 위로가 팀워크를 더 향상시켜줄 수도 있지 않을까? 우리는 사회에서 많은 사람과 일을 하고 경험을 쌓으며 무의식적으로 상대방의 감정을 건드리는 선을 넘지 않는 법을 배운다. 그러나 무의식적으로 선을 넘지 않는 것과 의식적으로 동료를 배려하고 존중하는 마음은 엄연히 다른 것이다. 함께 일을 하면서 서로에게 자극이 되는 동료, 어려운 문제를 함께 해결할 수 있는 동료도 물론 좋지만, 단순히 그것만으로 팀워크가 돈독해지지는 않는다. 튼튼한 팀워크를 만들기 위해서는 훌륭한 동료라는 조건 저변에 팀원에 대한 인간적인 신뢰도 함께 동반되어야한다고 생각한다. 이상으로 솔직한 피드백으로 좋은 팀워크를 만들 수 있을까? 포스팅을 마친다.","link":"/2019/12/04/about-honestly-feedback/"},{"title":"수학에서 기원한 프로그래밍 패러다임, 순수 함수","text":"이전에 작성했던 기존의 사고 방식을 깨부수는 함수형 사고 포스팅에 이어, 이번 포스팅에서는 함수형 프로그래밍이 지향하는 관점을 실제 프로그램에 구현하기 위해 알고 있어야하는 필수적인 개념 중 하나인 순수 함수(Pure functions)에 대한 이야기를 해볼까 한다. 2017년 쯤, 함수형 프로그래밍이라는 패러다임이 떠오르면서 순수 함수라는 개념 또한 함께 주목받기 시작했고, 지금도 구글에 순수 함수라고 검색하면 많은 개발자 분들이 순수 함수의 특징에 대한 포스팅을 작성해놓은 것을 볼 수 있다. 일반적으로 우리가 순수 함수에 대해서 공부하려고 하면 다음과 같은 두 가지 특징을 가지는 함수라고 정의하는 경우를 많이 볼 수 있다. 동일한 인풋(인자)에는 항상 동일한 결과를 내야한다. 함수 외부의 상태를 변경하거나, 외부의 상태에 영향을 받아서는 안된다. 그러나 이렇게 공부하게 되면 “순수 함수는 이런저런 특징을 가지고 있는 함수”라고 외우게 되기 쉬운데, 사실 순수 함수는 이렇게 접근할 필요가 없는, 더 심플한 개념이다. 뭐 그냥 이렇게만 외워놔도 순수 함수가 어떤 것인지 이해하고 사용하는 데는 전혀 무리가 없지만, 필자는 순수 함수의 이러한 특징이 어디서 나온 것인지, 순수 함수라는 것이 정확하게 무엇을 의미하는지에 대해 조금 더 근본적인 이야기를 해보려고 한다. 순수 함수는 그냥 수학적 함수다우리가 순수 함수라고 이름을 붙히고 순수 함수의 특징은 이러이러한 것들이 있다고 공부하기 때문에 뭔가 특별한 함수인 것 같지만, 사실 순수 함수는 그냥 수학에서 사용하는 함수를 프로그래밍의 세계에 똑같이 구현해놓은 것에 불과하다. 위키 백과의 함수형 프로그래밍의 정의를 보면 이 개념에 대해 조금 더 자세하게 작성된 설명을 볼 수 있다. 함수형 프로그래밍(functional programming)은 자료 처리를 수학적 함수의 계산으로 취급하고 상태와 가변 데이터를 멀리하는 프로그래밍 패러다임의 하나이다. 명령형 프로그래밍에서는 상태를 바꾸는 것을 강조하는 것과는 달리, 함수형 프로그래밍은 함수의 응용을 강조한다. 함수형 프로그래밍 - 위키 백과원문 링크 이 설명에서 가장 중요한 키워드는 바로 수학적 함수라는 단어이다. 우리가 이런저런 특징을 외우며 공부하는 순수 함수라는 녀석은 말 그대로 순수한 함수, 즉 수학에서 사용하는 함수를 의미하는 것이다. 우리가 수학의 세계와 프로그래밍의 세계에서 동일하게 함수라는 개념을 사용하고 있기 때문에 간혹 잊어버리긴 하지만, 사실 프로그래밍에서의 함수는 수학의 그것과는 다른 점이 상당히 많다. 그럼 수학적인 함수와 프로그래밍의 함수 간 차이점을 알아보기 위해, 수학적인 함수의 정의부터 다시 한번 확실하게 짚고 넘어가도록 하자. 수학에서의 함수우리가 중학생 때 배웠던 함수라는 녀석은 대략 다음과 같은 정의를 가지는 개념이다. 임의의 $x \\in X$에 대하여 그에 대응하는 $y \\in Y$가 유일하게 존재하는 대응 관계 수학이라는 학문 특유의 어려워 보이는 문법을 사용하긴 했지만 뜯어보면 별 거 없다. 이 정의에서 등장하는 $X$는 정의역, $Y$는 치역이라고 하며, 각각 정의역은 함수의 input, 치역은 함수의 output에 사용될 수 있는 값의 집합이라고 생각하면 된다. 즉, 정의역은 함수의 인자로 사용되는 값들, 치역은 함수의 결과물로 사용되는 값들이라는 뜻이라고 봐도 무방하다. 하지만 이 정의에서 가장 중요한 것은 정의역이니 치역이니 하는 개념이 아니라, 함수의 인자로 사용되는 값 하나에 대응하는 함수의 결과 값이 유일하게 존재한다라는 개념이다. 어떤 값을 함수에 던지면 반드시 하나의 값을 반환하는 것, 이것이 본래 함수의 정의다. 오른쪽 그림처럼 정의역의 원소에 대응하는 치역의 원소가 없거나 2개 이상인 경우는함수의 정의에서 벗어나게 된다 조금 더 편한 이해를 위해 인자로 받은 값에 2를 곱하는 간단한 함수를 생각해보자. 우리는 이런 함수를 정의할 때 $f(x) = 2x$와 같은 식으로 나타낸다. 이제 이 함수의 인자인 $x$를 1이라고 생각해보면 우리는 $f(1) = 2 \\times 1 = 2$라는 결과를 얻을 수 있다. 만약 어느 날 갑자기 $f(1) = 3$이 되어버린다면, 이 함수는 특정한 정의역의 원소에 맞대응되는 치역의 원소가 유일하지 않으므로 더 이상 함수라고 부를 수 없는 것이다. 일반적으로 이야기하는 순수 함수의 특징들은 바로 이러한 수학적 함수의 성질에서 기원한다. 프로그래밍에서의 함수그러나 프로그래밍에서의 함수에는 이러한 제약이 전혀 없다. 이런 저런 예시를 들 것도 없이, 어떤 값도 반환하지 않는 void형 함수가 있지 않은가? 12345function foo (x): void { const y = x * 2;}console.log(foo(1)); 1undefined 수학적인 함수의 정의로 비춰볼 때 이러한 void형 함수는 함수가 아니다. 정의역의 원소인 x와 맞대응하는 치역의 원소가 없기 때문이다. 그래서 프로그래밍에서의 함수라는 개념이 수학의 함수와 약간 다르다고 이야기하는 것이다. 사실 프로그래밍의 함수는 수학의 함수에서 “어떤 값을 던져주면 뭔가를 계산한다”라는 개념만 들고 온 것에 불과하며, 수학적인 관점에서 바라보면 프로그래밍의 함수는 사실 함수가 아닌 경우가 더 많다. 무엇보다 수학의 함수와 프로그래밍에서의 함수가 가장 큰 차이를 보이는 점은 바로 함수의 동작이 일관되지 않을 수 있다는 것이다. 아까 예시로 들었던 $f(x) = 2x$라는 수학의 함수는 내부 구현이 어떻게 되어있던 항상 $x$로 1을 받으면 2를 뱉어내는 것이 보장되어 있지만, 프로그래밍에서는 그렇지 않은 함수도 얼마든지 만들어 낼 수 있다. 예를 들면 Math.random이라던가, Date.prototype.getTime과 같은 메소드들을 사용한 함수 같은 것들 말이다. 이 메소드들은 함수의 동작과 전혀 상관없는 값을 만들어내기 때문에, 함수의 연산이 이러한 값들에 종속되어 버린다면 개발자는 이 함수가 어떤 값을 뱉어낼 지 절대 예측할 수가 없다. 1234567function sum (x: number): number { return x + Math.random();}sum(1);sum(1);sum(1); 1235 // ?4 // ?9 // ? 이런 개념은 특정한 의미를 가지는 값들을 저장, 할당, 호출할 수 있는 프로그래밍의 세계에서만 존재하는 것들이며, 수학의 세계에서는 이런 개념 자체가 없다. 이렇게 특정한 의미를 가지는 값들을 우리는 상태(State)라고 부른다. 상태는 프로그램의 현재 상황을 보여주는 좋은 역할도 하지만, 여기저기서 무분별하게 이 상태를 참조하거나 변경하는 경우, 개발자조차 현재 프로그램이 어떻게 돌아가는지 파악하기 힘든 슬픈 상황이 발생할 수도 있다. 그래서 개발자들은 상태를 변경하는 행위에 특정한 규칙과 제약을 정해서 무분별한 상태 변화를 최대한 피하고, 이런 변화를 추적할 수 있는 상황을 선호한다. 문제는 프로그래밍에서의 함수는 이런 상태들, 더 정확히 이야기하자면 함수 외부의 상태들과 뭔가 썸씽이 생기는 경우가 많다는 것이다. 여기 인자로 받은 수를 함수 외부에 선언된 변수와 더한 후 반환하는 addState라는 간단한 함수가 있다. 123456let state = 3;function addState (x: number): number { return state + x;}addState(1); 14 addState 함수는 자신 외부에 있는 state라는 값을 참조하여 자신이 인자로 받은 수를 더해주는 간단한 일을 한다. 즉, 이 함수의 결과 값은 함수의 외부 상태인 state 변수에 종속되어 있다는 것이며, 이런 상황은 프로그래머가 함수의 동작을 예측할 수 없게 만드는 위험 요소로 작용할 수 있다. 만약 다른 곳에서 state 변수의 값을 변경이라도 하면 상황은 더욱 꼬이기 시작할 것이다. 12state = 10;addState(1); 111 이전과 같은 함수에 같은 인자를 사용했지만, 결과값은 전혀 다르게 나왔다. 이 함수는 외부 상태의 변화에 따라 자신의 결과 값도 변경되기 때문에, 개발자는 이 함수의 동작을 전혀 예측할 수 없는 것이다. 이렇게 함수가 함수 외부 상태에 영향을 받거나, 함수 외부 상태를 직접 변경하는 행위를 사이드 이펙트(Side Effect)라고 하며, 사이드 이펙트를 발생시키는 함수는 프로그래머가 예측하지 못한 버그를 발생시키는 위험 요소 중 하나이다. 그런 이유로 자바스크립트와 같은 언어에서는 전역 변수의 선언 및 할당을 최대한 지양하는 컨벤션을 내놓기도 하며, React Hooks에서는 사이드 이펙트를 발생시키는 동작을 따로 구분하기 위해 useEffect라는 훅을 제공하기도 한다. 12345678910function TestComponent () { useEffect(() => { localStorage.setItem('greeting', 'Hi'); return () => { localStorage.removeItme('greeting'); }; }); return TestComponent;} 지금 이게 간단한 함수인데다가 의도적으로 연출한 상황이라 부자연스러워 보일 수도 있지만, 실제 어플리케이션에는 이거보다 훨씬 복잡하고 이상한 짓들을 하는 함수가 수두룩하다. 예를 들면 API 서버와 통신한 결과물을 뱉어내는 간단한 함수 또한 순수하지 않은 함수의 일종이다. 123456789async function getUsers () { try { const response = await fetch('/api/users'); return response.json(); } catch (e) { throw e; }} 딱 봐도 getUsers는 호출할 때마다 항상 같은 값을 반환하는 함수는 아니다. 현재 데이터베이스의 상태에 따라 유저 리스트는 매번 달라질 수 있기 때문이다. 이렇게 순수하지 않은 함수는 개발자가 함수의 결과를 예측하는 것이 불가능하기 때문에, 함수의 동작을 검사하는 테스트를 작성하는 것 또한 불가능하다. 애초에 아웃풋으로 뭘 내보낼 지도 감이 안오는 변덕스러운 녀석을 어떤 기준으로 검사한단 말인가? 이렇듯 프로그래밍의 세계에서 이야기하는 함수는 수학의 함수보다 더 변수가 많고, 결과를 예측하기가 힘든 개념이다. 순수한 수학적 함수로 회귀하자자, 이제 수학의 세계에서 말하는 함수와 프로그래밍의 세계에서 말하는 함수의 차이를 살펴보았으니, 다시 순수 함수의 정의를 가져와보자. 동일한 인풋(인자)에는 항상 동일한 결과를 내야한다. 함수 외부의 상태를 변경하거나, 외부의 상태에 영향을 받아서는 안된다. 앞서 이야기 했듯이, 수학의 세계에서 함수는 단순히 인풋을 받으면 뭔가 계산을 해서 단 하나의 결과를 내는 개념이다. 그리고 수학의 세계에는 뭔가 값을 저장해놓고 할당도 하고 호출할 수도 있는 상태라는 개념이 없으니, 함수가 함수 외부 상태에 영향을 주고 받는 사이드 이펙트라는 것도 당연히 존재할 수가 없다. 즉, 수학에서의 함수를 프로그래밍에 그대로 적용하면 순수한 함수의 특성인 “함수의 결과는 함수의 인자에만 영향을 받는다”라는 조건과 “함수 외부의 상태를 변경하거나 영향을 받아선 안된다”라는 조건이 자연스럽게 충족되는 것이다. 그리고 함수형 프로그래밍에서 이야기하는 불변성(immutable) 또한 수학과 맞닿아 있는 지점인데, 애초에 상태라는 개념이 존재하지 않는 수학의 함수를 프로그래밍으로 구현한 순수 함수를 사용하고 있으니, 상태를 변경한다는 개념 또한 없어야 하는 것이다. 하지만 프로그래밍의 세계에는 엄연히 상태라는 개념이 존재하기 때문에, “함수의 인자를 직접 수정해서는 안된다”와 같은 제약들을 스스로 정의하고 지켜나갈 수 있도록 저런 개념을 명시적으로 이야기하는 것이다. 또한 순수 함수를 사용함으로써 따라오는 장점들인 “테스트가 쉬워진다”, “참조 투명성이 보장된다”와 같은 이야기들도 수학적인 개념에서의 함수를 생각하면 사실 당연하기 그지 없는 이야기들이다. 앞서 잠깐 이야기 했지만, 매번 다른 값이 나오는 함수에 대한 유닛 테스트를 짠다고 생각해보면 진짜 답이 없다. 애초에 개발자가 함수의 동작을 예측할 수 없으니 함수의 동작에 대한 모법 답안을 제시할 수도 없을 것이고, 당연히 테스트 작성도 불가능 하다. 또한 순수 함수를 사용하면 참조 투명성이 보장된다는 말도 결국 우리가 수학에서 사용하고 있는 = 기호의 의미를 생각해보면 그렇게 특별한 말이 아니다. f(x)=2xf(1)=2∴f(1)+1=3\\begin{aligned} f(x) = 2x\\\\ \\\\ f(1) = 2\\\\ \\\\ \\therefore f(1) + 1 = 3 \\end{aligned}​f(x)=2x​​f(1)=2​​∴f(1)+1=3​​ 참조 투명성이라는 것은 $f(1)$(함수의 실행부)를 $2$(함수의 결과물)로 치환해도계산 결과가 변하지 않는다는 것을 의미하는데, 애초에 우리는 예전부터 수학에서 그 개념을 사용하고 있었다 이렇듯 순수 함수는 어떤 인자를 사용했을 때 어떤 결과 값이 나올 지 동작을 예측할 수 있고, 상태라는 것을 아예 없애버린 개념이기 때문에, 개발자가 예측 가능한 어플리케이션을 개발하기 쉽게 만들어준다. 또한 함수 자체가 함수 외부의 상태와 관계 없이 순수하게 단일한 연산에만 집중하고 있으니, 한 어플리케이션에서 선언한 순수 함수는 다른 어플리케이션에다가 가져다 붙혀도 반드시 동일한 동작을 한다는 것이 보장된다. 즉, 좋은 모듈화의 조건 중 하나인 높은 응집도에도 부합한다. 이렇게 순수 함수를 사용하여 작성된 어플리케이션은 개발자가 구조와 동작을 쉽게 이해할 수 있기 때문에, 굳이 함수형 프로그래밍 패러다임이 아니더라도 전반적인 어플리케이션 설계에 꽤나 도움이 되는 개념이라고 할 수 있다. 마치며필자는 처음 순수 함수라는 개념을 접했을 때 구글링과 다른 분들이 작성해주신 포스팅들을 통해 순수 함수의 특징, 장점, 단점 등을 먼저 접하게 되었는데, 당시에는 “또 새롭게 공부할게 나왔구만”이라는 생각이었다. 또 새로운 공부거리가 생겨버렸네… 사실 순수 함수와 같은 패러다임을 처음 접하게 되면 습관처럼 구글링을 통해 정보를 습득하고 공부를 하게 되는데, 이때 일반적으로 다른 사람들이 정리해놓은 포스팅을 보고 공부하게 되는 경우가 많았다. 그러나 이렇게 공부를 하는 경우, 해당 패러다임의 근본적인 발생 이유나 원리에 대해서 깊이 파악하기 보다는 몇 가지 특징이나 장단점을 먼저 학습하게 되는 경우가 많았던 것 같다. 그래서 순수 함수도 “새롭게 공부해야하는 것”이라는 느낌으로 받아들였었지만, 나중에 곰곰히 생각해보니 그냥 어릴 때 배웠던 수학적인 함수의 개념을 그대로 프로그래밍으로 구현한 것이라는 개념이라는 것을 깨닿고 꽤나 허무했던 기억이 있다. 그래서 필자는 이 포스팅에서 “순수 함수는 이런저런 특징을 가진 함수”라고 설명하지 않았던 것이다. 개인적인 생각이기는 하지만, 대부분의 사람들은 어릴 때 이미 학교에서 함수에 대한 정의와 개념을 학습했기 때문에, “수학적인 함수”라는 키워드로 접근하는 것이 오히려 이해가 빠를 것이라고 생각했다. 어쨌든 필자는 이 포스팅을 통해 순수 함수는 전혀 새로운 개념이 아니라는 이야기를 하고 싶었고, 대한민국 의무 교육을 받은 사람이라면 누구든지 다 익숙하게 받아들일 수 있는 개념이라는 것을 이야기하고 싶었다. 물론 순수 함수를 사용하여 어떤 식으로 프로그램을 설계하는 것이 훌륭한 설계인지와 같은 이야기는 의무 교육과정에 없기 때문에 별도로 공부를 해야겠지만, 적어도 함수형 프로그래밍에서 중요한 키워드로 이야기하고 있는 순수 함수와 불변성에 대한 이해 정도는 그렇게 어려운 것은 아닐 것이라고 생각한다. 다음 포스팅에서는 순수 함수와 함께 함수형 프로그래밍에서 중요한 개념 중 하나인 불변성에 대한 이야기를 해보려고 한다. 이상으로 수학에서 기원한 프로그래밍 패러다임, 순수 함수 포스팅을 마친다.","link":"/2019/12/29/about-pure-functions/"},{"title":"20대의 마지막, 2019년을 돌아보며","text":"이제 필자의 마지막 20대를 보내는 2019년도 어느덧 10일 정도 밖에 남지 않았다. 물론 서른이 된다고 해서 크게 달라지는 것은 없지만, 스무살이 되었을 때 이후 처음으로 나이 앞 자리가 바뀌는 만큼 기분이 싱숭생숭 하기도 하다. 작년에 비해서 올해는 필자에게 많은 의미를 가지는 해였다. 이것 저것 도전해보았고, 또 도전으로 인한 결실을 얻기도 했던 해였다. 그 도전들 중에서는 개발과 관련된 것도 있고, 그냥 필자가 해보고 싶어서 했던 것들도 있는데 뭐가 되었든 작년에 비해 도전 자체를 많이 해봤다는 게 의미있는 것 같다. 그래서 이번 포스팅에서는 필자가 올해 겪었던 대표적인 일들을 시간 순서대로 나열하고 돌아보는 회고를 한 번 작성해보려고 한다. 1년 동안 집필했던 책 출간먼저 첫 번째로, 작년부터 친구와 함께 집필했었던 커피 한 잔 마시며 끝내는 Vue.JS책이 2019년 7월 말 출간되었다. 제목이 조금 특이하다보니 많은 분들이 “진짜 커피 한 잔만에 끝낼 수 있냐”라고 물어보시는데, 사실 제목은 출판사에서 정해준거라 필자도 잘 모르겠다. (커피 한 잔이 몇 리터라고는 말 안했다) 처음 집필 제의를 받은 것이 2018년 8월 쯤이고, 책 출간이 2019년 7월 31일에 되었으니 총 1년 정도의 기간이 소요된 셈이다. 물론 처음 한 두달 정도는 책의 내용을 기획하고 공동 집필하는 친구와 의견 일치를 보는 기간이었으니 대략 10개월 정도 집필을 했다. 사실 처음에는 내용이 많은 블로그 포스팅 정도로 가볍게 보고 시작했는데, 생각보다 블로그와 다른 점이 굉장히 많았던 것 같다. 블로그 포스팅은 기껏해야 200줄에서 300줄 정도의 짧은 내용을 담고 있지만, 400페이지에 달하는 내용이 한 호흡에 이어져야하는 글쓰기는 굉장히 낯선 경험이었다. 게다가 블로그 포스팅과는 다르게 명확한 기한이 정해져있는 프로젝트였기 때문에, 평일에 퇴근하고 아무리 피곤해도 책을 집필해야해서 집중력을 유지하기가 꽤나 힘들었다. 그리고 책의 내용 뿐 아니라, 책에 들어가는 예제인 SPA 어플리케이션 2개와 API 서버 어플리케이션까지 직접 작성하고 테스트해야하다보니, 아무리 두 명이 함께 한다고 해도 힘든 작업이었다. 작성한 API는 Postman을 통해서 서로 공유했다 그런 이유로 필자와 친구는 최대한의 효율을 내기 위해 여러가지 방법을 모색했는데, 그 중 하나가 Git과 마크다운을 사용하여 책을 집필하자는 아이디어였다. 원래 출판사에서는 MS 워드로 작성해서 보내달라고 했으나, MS 워드는 헤딩이나 각주 등의 스타일 포맷을 지정할 때 일일히 마우스로 클릭해주거나 단축키를 눌러야 해야한다는 문제가 있었다. 그러나 마크다운은 나중에 css를 사용하여 통일된 스타일을 일괄 적용할 수 있기 때문에 훨씬 효율이 좋았다. 그리고 최종 집필을 마친 후에는 Pandoc이라는 라이브러리를 사용하여 *.md 파일을 *.docx 파일로 변경해서 출판사로 보냈다. 또한 둘 다 직장이든 사이드 프로젝트든 항상 Git을 사용하고 있어 익숙하기 때문에, 예제 어플리케이션과 책 집필 모두 Git을 사용하여 버전 관리를 진행할 수 있었고, 덕분에 많은 작업 시간을 단축하고 버그나 놓친 부분을 잡아낼 수 있었다. 또한 책의 모든 내용은 마크다운으로 작성되고, 코드 리뷰를 통과한 이후에만 마스터 브랜치로 머지될 수 있기 때문에 그 과정에 서로의 의견이 맞지 않는 부분을 합의하고 넘어가기도 좋았던 것 같다. SSR 서버가 프론트엔드의 영역이냐 아니냐로 치열하게 의견 교환 중 함께 공동 집필했던 친구는 필자와 굉장히 친한 친구인데, 평소에는 사이드 프로젝트 정도만 함께 하고 이렇게 공적인 업무를 함께 하는 것은 처음이었기 때문에 책을 집필하는 과정에서 서로 일하는 스타일이 안 맞아서 약간의 갈등이 있기도 했다. 그러나 오히려 이런 시간을 통해 서로의 업무 스타일을 제대로 파악할 수 있었고, 서로 안 맞는 부분을 맞춰나갔던 과정 또한 좋은 추억으로 남을 수 있었다. 책 집필 과정에 대한 자세한 내용과 회고는 흔한 개발랭이의 작가 입문기라는 포스팅으로 따로 작성했으니 여기서 확인해볼 수 있다. 블로그를 본격적으로 시작하다사실 필자는 블로그를 열심히 쓰는 사람은 아니었다. 필자의 아카이브를 보면 2019년 6월부터 급격하게 포스팅 양이 늘기 시작한 것을 볼 수 있는데, 이때가 딱 책 집필이 거의 마무리되고 리뷰어들에게 책에 대한 리뷰를 받고 있던 시점이었다. 또한 2019년 6월 이전 포스팅들은 행성의 궤도 계산하기, 간단한 인공신경망 만들기와 같이 필자가 공부했던 내용들을 남겨놓기 위한 아카이브의 느낌이 강했다면, 2019년 6월 이후 작성된 포스팅들은 자바스크립트, Git과 같이 조금 더 일반적인 내용들에 대해 작성하기 시작한 것을 볼 수 있다. 이때 블로그 포스팅의 양이 늘고 주제가 바뀌기 시작한 이유에는 책을 집필하며 필자의 글쓰기 스킬이 성장했다는 것도 있지만, 더 큰 이유는 이 시점 이후로 필자가 블로그 포스팅을 대하는 마음 가짐에 약간의 변화가 있었다는 점이다. 필자는 기본적으로 자신의 스킬에 대한 자신감이 많은 사람은 아니다. 그래서 필자는 “내가 알고 있는 것 정도는 남들도 다 알고 있을거야”라는 생각을 가지고 있었지만, 책을 집필하면서 진행했던 스터디로 인해 생각이 변하게 되었다. 필자와 친구는 책을 집필하며 중간 중간 집필한 내용을 가지고 다른 분들과 함께 VueJS에 대한 스터디 겸 책에 대한 리뷰를 진행했었는데, 그때 함께 스터디를 했던 분들이 책의 내용이 공부에 많은 도움이 되었다는 이야기를 해주셔서 “내가 알고 있는 지식이 다른 개발자들에게 도움이 되는구나”라는 생각을 처음으로 하게 되었던 것 같다. 그 이후 지금까지 필자의 공부만을 위해 작성했던 블로그 포스팅의 주제를 “필자가 알고 있는 지식의 정리와 공유” 쪽으로 변경하게 되었고, 그렇게 작성한 블로그 포스팅을 다른 사람들에게 공유하게 되었다. 그렇게 대략 5개월 정도 블로그 포스팅을 공유하다보니, 필자의 포스팅을 읽은 분들이 페이스북 메세지나 링크드인, 이메일과 같은 채널을 통해 “잘 읽었다”, “도움이 되었다”와 같은 피드백을 보내주시는 경우도 생기게 되었는데, 이런 메세지들을 받으면서 블로그 포스팅 작성에 대한 동기부여가 되었던 것 같다. 나름 성장하고 있는 Organic 유입 그래프 그리고 필자는 기술과 관련된 내용만 작성하는 것이 아니라 가끔 에세이 포스팅도 작성하곤 하는데, 이 과정에서 생각을 정리하거나 싱숭생숭한 마음을 정리할 수도 있어서 멘탈 관리에도 꽤나 도움이 되는 것 같다. 이제 필자에게 블로그 포스팅은 단순히 공부한 내용을 정리하거나 공유하거나 하는 느낌이라기보다 “글쓰기”라는 취미 생활에 가까워진 것 같은 느낌이다. 앞으로도 한 주에 한 개정도의 포스팅을 꾸준히 작성할 예정이고, 이런 식으로 몇 년동안 글이 쌓여나가면 나중에는 좋은 추억으로 남을 것 같다는 생각도 든다. 2년 반 동안 정들었던 직장을 떠나다 2019년 8월을 마지막으로 지난 2년 반동안 즐겁게 일했던 정든 브레이브모바일을 떠나게 되었다. 직원 수 고작 10명 남짓이었던 작은 회사가 Series A, B를 투자 받으며 성장하는 과정을 경험하면서 많은 것들을 배웠고, 좋은 동료들도 만날 수 있었다. 기억을 되돌아보면 필자는 이 회사에 입사하기 전, 1년도 채 안되는 짧은 스타트업 근무 경험과 한 번의 창업 경험만을 가지고 있던 햇병아리 개발자였고, 사이드 프로젝트로 특이한 것을 많이 만들기는 했지만 그렇다고 코딩을 남들보다 잘하는 편은 또 아니였다. 프로그램을 잘 만드는 것보다 만든다는 것 자체에 흥미를 느끼던 시절이랄까. 그러나 대학을 졸업하고 필자가 경험했던 모든 근무 환경들은 누군가 시키는 일을 하는 환경이 아닌 스스로 문제를 찾아내고 해결해가는 과정의 연속이었기 때문에, 자연스럽게 어떤 문제에 대한 의사결정을 내리는 것에 익숙해질 수 있었고, 그에 따른 책임을 지는 것 또한 두려워하지 않을 수 있었다. 초기 스타트업은 늘 인적 자원도 부족하고 누군가 의사결정을 일일히 내려주는 시스템이 아니기 때문에 각자의 책임 범위 안에서 스스로 리더가 되어야하는 경우가 많은데, 필자의 이런 경험들이 당시 작은 규모의 스타트업이었던 브레이브모바일에 잘 맞아들어갔기 때문에 더 재밌게 일할 수 있었던 것 같다. 그래서 필자는 입사 초반부터 프론트엔드 어플리케이션에 대한 전반적인 의사결정권을 가지고 수 없이 많은 의사결정을 거치며 일을 했었다. 뭐 그 중에는 잘한 선택도 있고 못한 선택도 있지만, 결과가 어찌되었든 필자같은 햇병아리 개발자에게 이런 경험은 굉장히 좋은 성장의 양분이 될 수 있었다. 처음 필자가 입사했던 2017년에 숨고라는 서비스는 프론트엔드 프레임워크도 붙어있지 않은 전통적인 MPA 어플리케이션이었다. 입사를 하고 처음 열어본 소스 코드에는 React를 붙히려다가 실패했던 흔적과 보일러 플레이트로 기본적인 스캐폴딩만 되어있는 Vue가 덩그러니 있었던 기억이 난다. 즉 입사 후 필자에게 주어진 첫 미션은 Django로 작성된 MPA 어플리케이션에 Vue를 어떻게든 적용하는 것이었는데, 당시에 이런 방법으로 Vue를 사용하는 레퍼런스도 많지 않아서 고생했던 기억이 난다. (억울하게도 지금은 구글링하면 레퍼런스가 꽤 많이 나오는 편이다) 이후 시간이 지나고 조금씩 여유가 생기며, Express를 사용하여 직접 SSR 서버도 만들고 타입스크립트도 도입하면서 점점 어플리케이션을 단단하게 만들어 갔는데, 이런 과정 또한 아무것도 만들어져있지 않은 초기 스타트업에서 겪을 수 있는 꽤나 값진 경험 중 하나라고 생각한다. 또한 프론트엔드 어플리케이션에 대한 의사결정권이 필자에게 있었던 만큼, 백엔드 개발자 뿐 아니라 디자이너, PO, 마케터 등 다양한 직군과의 잦은 커뮤니케이션도 필수적으로 발생했는데, 이런 경험 또한 나름 재미있었다. 덕분에 개발 뿐만 아니라 다른 직군들이 하는 일에 대해서도 알 수 있었고 팀워크에 대한 생각도 많이 해볼 수 있었던 기회였던 것 같다. 그리고 브레이브모바일은 효율적인 애자일 프로세스에 대해서 깊은 고민을 하는 조직이었기 때문에, 비싼 돈 들여서 유명한 애자일 코치님에게 코칭을 받는 호사도 누릴 수 있었고, 이때 애자일 코칭을 받으며 배웠던 것들을 애자일이 도대체 뭐길래? 라는 포스팅으로 정리했었다. 하지만 시간이 갈수록 익숙한 사람들과 함께 반복되는 업무를 처리하는 일상이 이어지며 새로운 경험을 할 수 있는 횟수가 점점 줄어들게 되었고, 성장에 대한 갈증이 더 커지게 되었다. 또한 필자의 위시리스트 중 “20대가 끝나기 전에 혼자 해외여행을 가보자”라는 조건을 만족시킬 수 있는 기한이 딱 올해까지였기 때문에 오랜 고민 끝에 퇴사를 결정하게 되었다. 필자가 퇴사할 당시 작성한 2년 동안 근무했던 회사를 떠나며 포스팅에 더 자세한 내용이 있으니, 궁금하신 분들은 이 포스팅을 읽어보도록 하자. 프라하에서 한 달 살기브레이브모바일을 퇴사한 후 20살 때부터 줄곧 꿈꿔왔던 “혼자 해외 여행해보기”를 실행에 옮겼다. 사실 필자는 해외여행을 그렇게 많이 가본 사람도 아니고, 해외여행을 가더라도 항상 친구나 가족들과 함께 다녔기 때문에 막상 혼자 한국을 벗어날 생각을 하니 조금 무섭기도 했지만, 막상 비행기표를 예매하고나니까 갑자기 마음이 굳어져서 일사천리로 하루 만에 모든 예약을 진행해버렸다. 그리고 이번에는 휴가를 내는 것이 아니라 아예 퇴사하고 자유로운 상태로 여행을 떠나는 것이기 때문에 굳이 4박 5일과 같은 빡빡한 일정을 잡을 필요도 없었고, 개인적으로도 급한 일정 속에 관광 스팟을 찍으면서 돌아다니는 스타일의 여행을 선호하지 않기 때문에 이번 여행의 컨셉을 “한 달 살기”로 잡았다. 필자는 살면서 유럽이라고는 모스크바밖에 가본 적이 거의 없기 때문에 이번에는 조금 더 유럽 냄새를 맡아보고 싶었고, 퇴사하면서 브레이브모바일과 2개월 간의 외주 계약을 체결한 상태였기 때문에, “유럽이지만 인터넷이 빵빵해야한다”라는 조건을 토대로 여행지를 알아보고 있었다. 그렇게 리서치를 해보던 중 프라하와 부다페스트가 도시도 아름답고 유럽치고 인터넷도 빵빵하다는 정보를 입수하게 되었는데, 최종적으로는 프라하에서 한 달 동안 지내는 것으로 결론을 내렸다. 문제는 필자가 프라하에 대해서 알고 있는 정보가 체코 필하모닉 오케스트라와 프라하의 연인 정도 밖에 없었다는 것이다. 당시 필자의 생각은 이랬다. 어차피 한 달동안 살 건데, 가서 알아보면 되겠지 뭐 ㅎㅎ (행복회로 가동 중) 행복회로 돌리지 말고 제대로 알아보고 갑시다 필자가 크게 놓쳤던 것 중 한 가지는 바로 2015년형 15인치 맥북 프로의 기내 반입 금지 사항이었다. 당시 미국에서 배터리 폭발 사고가 발생해서 각 국가 정부의 지침이나 항공사의 안전관리규정에 따라 맥북 프로를 비행기에 반입하지 못하는 이슈가 있었는데, 필자가 이걸 놓치고 만 것이다. 필자는 이 사실을 출발 하루 전 저녁에 우연히 알게되어 체코 항공에 메일로 문의를 했지만, 이때는 주말이었기 때문에 당연히 비행기를 탈 때까지 아무런 답변도 받을 수 없었다. 유럽항공안전청 홈페이지를 확인해보니 EU는 비행기 내에서 전원만 끄면 반입할 수 있다는 입장이었지만, 그래도 항공사마다 안전관리규정이 다른데다가 혹시 한국으로 돌아올 때 지침이 더 강화되면 맥북 프로를 체코에 놓고 와야할 수도 있다는 생각에 결국 2009년형 골동품 맥북 프로를 가져갔다. (덕분에 프라하에서 맥둥이가 고장나서 수리도 했다) 그리고 또 한 가지는 생각보다 영어가 잘 통하지 않았다는 것이다. 체코는 러시아어랑 비슷한 느낌인 체코어를 사용하기는 하지만, 프라하가 워낙 관광지로 유명한 만큼 다들 영어를 잘 할줄 알고 방심했다. 하지만 필자의 숙소는 관광지 근처가 아니었기 때문에 영어를 잘 하는 사람이 생각보다 많이 없었고, 덕분에 의사소통할 때 꽤나 어려움을 겪었다. 그래서 다음부터는 영어권이 아닌 다른 국가를 여행할 때 기본적인 단어나 문법 정도는 그 나라의 언어로 말할 수 있을 정도로 공부를 하고 갈 계획이다. 이때 필자가 프라하에서 고생하며 얻어낸 정보는 프라하에서 디지털 노마드로 살아남기라는 포스팅으로 정리했다. 개발자들과의 의미있는 스몰 토크 모임 2019년 11월에는 대학생때부터 친구들과 함께 운영하고 있는 토이 프로젝트팀인 루비콘에서 Chit Chat이라는 스몰 토크 모임을 개최했다. 개발자들은 다른 직군에 비해 컨퍼런스나 네트워킹과 같은 모임이 많은 편이긴 하지만 이런 모임들의 주제는 대부분 기술 그 자체에 집중되어 있고, 공식적으로 각자의 생각이나 가치관을 나눠볼 수 있는 기회는 그리 많지 않기 때문에 기술 외적인 주제에 대해서 이야기를 나눠볼 수 있는 자리가 있었으면 좋겠다고 생각했다. 그런 이유로 소프트 스킬에 대해서 서로의 이야기를 나눠볼 수 있는 스몰 토크 모임을 기획하게 되었고, 다양한 경력과 다양한 직군의 개발자들이 모여서 두 시간동안 소프트 스킬에 대한 각자의 생각을 나누는 좋은 경험을 해볼 수 있었다. 하지만 역시 태어나서 처음 진행해보는 행사이다보니, 걱정되는 부분도 있었다. 사실 필자는 낯을 조금 가리는 성격이라 평소에 네트워킹이나 컨퍼런스에 자주 참여하는 편이 아닌데, 이번에는 직접 토론의 사회자 역할까지 맡아야 한다고 생각하니 부담스럽기도 했다. 그러나 막상 사람들이 모이고 토론을 시작하자 다들 열정적으로 자신의 의견을 이야기해주셔서 생각보다 사회자로써 할 일은 많지 않았고, 그냥 필자도 토론 자체를 즐기며 재밌는 시간을 보냈던 것 같다. Chit Chat에서 어떤 주제로 어떤 이야기를 나누었는지 궁금하신 분들은 루비콘 팀 블로그에 올려놓은 Lubycon 1st Chit Chat 후기 포스팅에서 확인해볼 수 있다. 새로운 시작 블로그를 통해 공식적으로 이야기한 적은 없지만, 필자는 지난 12월 9일부터 토스라는 금융 서비스를 만드는 비바리퍼블리카에 입사하게 되었다. 이 회사를 선택한 이유에는 여러가지가 있겠지만, 역시 가장 큰 이유는 토스팀은 “재미있게 일하는 조직”이라는 생각이 들어서였다. 필자는 아직 이 회사에서 많은 것을 경험하지 못 했고 이제 하나하나 배워가는 단계이지만, 확실히 토스팀 자체가 활력적인 에너지가 느껴지는 팀이라는 것이 느껴지기는 한다. 게다가 일이든 개발이든 잘하시는 분들이 워낙 많다보니 자극도 받을 수 있어서 좋다. 최근 필자가 올렸던 Pull Request에는 피드백이 20개 정도 달렸는데, 이렇게 누군가가 내 코드에 대해 상세하게 읽어보고 피드백을 주는 경험이 오랫만이라서 기분도 좋았고, 이런 사람들과 같은 프로덕트를 만든다는 것이 든든하기도 했다. 물론 아직 3 Month Review라는 난관이 하나 남아있기는 하지만, 딱히 신경 안 쓰고 열심히 자기 할 일만 잘 해보려고 한다. 혹여나 떨어지더라도 최선을 다 했다면 후회는 없을 것이라는 생각이 든다. 20대를 마치며2019년은 필자의 마지막 20대였고, 내년부터는 이제 30대의 길로 접어든다. 어릴 적 필자에게 서른이라는 나이는 뭔가 “어른”이라는 이미지였지만, 막상 서른을 코 앞에 둔 지금 나 자신을 바라보면 딱히 어른이 되었다는 생각은 들지 않는다. 그냥 나이만 한 살 더 먹은 느낌이랄까. 올해 필자는 이런 저런 도전을 많이 했고 나름의 결실도 이뤄냈다. 책도 내고 유럽에서 한 달 동안 살아도 봤으며 새로운 직장으로 이직도 했다. 중요한 것은 이런 것들이 뭔가 예전부터 계획된 일들이 아니였고, 그때 그때 하고 싶은 대로 했다는 점이다. 사실 17살 때부터 필자는 스스로에게 다짐한 한 가지 원칙을 가지고 있었는데, 바로 “하고 싶은 것만 하면서 살기”이다. 이런 이야기를 하면 철이 없다고 말씀하시는 분도 있고, 현실과 타협할 줄 알아야 어른이라고 말씀하시기도 하지만, 그런 의미의 어른이라면 딱히 되지 않아도 괜찮을 것 같다는 생각이 든다. 하고 싶은 것만 하면서 산다는 것이 그냥 내 맘대로 산다는 의미는 아니다. 현실 속에서 하고 싶은 것만 하면서 산다는 것이 쉬운 일이 아니라는 것은 다들 알고 있을 것이라 생각한다. 당연히 그에 따른 많은 노력이 동반되어야 한다. 코딩이 너무 재밌어서 개발자로 일하고 싶었기 때문에, 공부를 열심히 해서 개발자가 되었다. 내가 알고 있는 지식을 사용하여 다른 사람들에게 도움이 되고 싶었기 때문에 블로그 포스팅을 꾸준히 작성하고 있다. 필자는 이런 작은 도전과 노력들이 모여서 “하고 싶은 건 반드시 한다”라는 현실을 만들어 낼 수 있다고 생각한다. 뭐, 앞으로는 어떤 어려운 일이 또 닥칠지 모르기 때문에 장담할 수는 없겠지만 되도록이면 이 가치관을 관짝에 들어갈 때까지 가져가보려고 한다. 그런 이유로 2020년에는 “발표”나 “강의”와 관련된 도전을 한 번 해볼까 생각 중이다. 발표나 강의하시는 분들을 보면 참 대단하다고 생각이 들면서도, 남들 앞에 서서 뭔가를 이야기한다는 게 조금 무섭기도 해서 매번 미루고 있었는데 올해에는 한 번 용기를 내보려고 한다. 이상으로 20대의 마지막, 2019년을 돌아보며 포스팅을 마친다.","link":"/2019/12/22/2019-retrospective/"},{"title":"Question Driven Thinking - 스스로 질문하며 학습하기","text":"처음 프로그래밍을 배우던 시절을 떠올려보자. 아닌 사람도 있겠지만 대부분의 사람들은 처음 프로그래밍을 배울 때 학교나 학원에서 누군가에게 가르침을 받으며 공부했을 것이다. 프로그래밍을 처음 접하는 입문자에게 독학의 길은 너무 멀고 험난하기 때문이다. 그러나 입문자 시절이 끝나고 개발자로써 취업도 하고나면 이야기는 달라진다. 이제는 나를 가르쳐줄 선생님이 없기 때문에 스스로 나의 실력을 키워나가야하는 상황이 되어버린 것이다. 이런 환경 속에 갑자기 놓인 사람들은 다시 프로그래밍을 처음 시작할 때처럼 막막한 기분이 든다. 주변 사람을 붙잡고 물어보기는 하지만 그것도 하루이틀이지, 비슷한 수준의 질문을 계속 하게된다면 돌아오는 것은 RTFM(Read the fucking menual)이나 구글링을 해보라는 냉랭한 대답 뿐일 것이다. 그렇다고 구글링을 하자니 어떤 키워드로 검색을 해야하는지 조차 막막하다. 이런 경험은 개발자라면 누구든지 겪어보았을 것이라고 생각한다. 공부해야할 것은 산더미이지만 뭐부터 공부해야할지, 어떻게 공부해야할지도 감이 안 오는 상황말이다. 누군가 친절하게 커리큘럼을 짜주는 것도, 알려주는 것도 아니다. 결국 나 스스로 정보를 찾아내고 골라내어 씹어먹어야하는 것이다. 현실부터 한번 보자본격적인 이야기에 들어가기에 앞서 도대체 이 “막막한 기분”이 정확히 뭔지 한 번 짚고 넘어가자. 이 기분을 만드는 것은 그렇게 어렵지 않을 것 같다. 필자는 프론트엔드 개발자이니, 간단한 웹 프론트엔드 어플리케이션을 만들어야하는 상황을 한번 가정해보도록 하겠다. 우선 뷰와 모델을 연동하여 UI를 그려줄 라이브러리나 프레임워크를 선택해야한다. 이 역할을 하는 도구 중 유명한 것은 React, Vue, Angular 정도가 있겠다. 일반적으로는 생태계가 가장 두터운 리액트를 주로 선택하지만, 사실 뭘로 만들든 원하는 기능을 구현하는데는 아무런 문제가 없다. 그리고 최근의 웹 어플리케이션에서는 클라이언트에게 복잡한 상태 관리 수준을 요구하기 때문에 상태를 수월하게 관리할 수 있는 라이브러리도 선택해야한다. 이 쪽 계열에서 유명한건 Redux, Mobx 정도가 있으니 입맛대로 골라보자. 만약 UI프레임워크로 Vue를 선택했다면 그냥 Vuex를 사용하는 것이 정신 건강에 좋다. 그리고 복잡한 객체 상태를 관리하려면 불변 상태를 유지하게 도와주는 라이브러리인 ImmutableJS나 Immer도 사용하면 좋을 것 같다. 만약 상태 관리 라이브러리로 Redux를 사용한다면 비동기 처리를 도와주는 redux-thunk, redux-saga 등의 미들웨어도 선택해야한다. 아니면 최근 많이 사용하는 RxJS는 어떨까? Redux와 편하게 함께 사용할 수 있도록 redux-observable과 같은 미들웨어로도 제공하고 있다. 패키지 관리자는 npm이나 yarn 중에 선택해서 사용하면 되고, 유닛 테스트 도구는 Mocha, Jest 중에 고르면 될 것 같다. eslint의 룰은 airbnb나 standard 중에 입맛에 맞는 걸로 고르고 적당히 커스터마이징해서 쓰자. 음, 그리고 생각해보니 자바스크립트보다는 정적 타이핑 언어인 타입스크립트를 사용하는 것도 나쁘지 않을 것 같다. 이왕 만드는 거 조금 더 단단하게 만들면 좋으니까. 그렇다면 위에서 이야기했던 라이브러리들이 타입스크립트를 지원하는지 알아봐야한다. 간단한 어플리케이션 만든다매…? 방금 이야기한 수많은 도구들은 필자가 잘난척하려고 줄줄 읊어댄 것이 아니라, 실제로 회사에서 일을 하거나 사이드 프로젝트를 진행할 때 프론트엔드 개발자들이 매번 고민하는 것들이다. 물론 저 도구들은 전부 사용해도 되고 안해도 되는 도구들이지만, 사용 방법만 익힌다면 생산성이 훨씬 높아지기 때문에 대부분의 프론트엔드 개발자들은 저 정도는 차려놓고 사용한다. 그리고 사실 저런 도구들을 제대로 사용하기 위해서는 클로저나 이벤트 루프와 같은 자바스크립트의 디테일한 개념, 가상돔, 데이터 바인딩, MVVM 패턴, Flux 패턴, 함수형 프로그래밍 등에 대한 지식도 필요하다. 최근 들어 프론트엔드 분야에 유독 새로운 게 많이 나오기는 하지만, 다른 직군의 개발자들도 알아야하는 지식의 양은 대부분 비슷비슷하다. 다만 각 직군에 맞게 특화되어 있을 뿐이다. 사실 이 길고 긴 이야기를 통해 필자가 하고 싶은 말은 딱 이거 하나다. “이 많은 걸 친절하게 다 알려줄 수 있는 사람은 없다.” 결국 정보는 스스로 찾아야한다저렇게 많은 것을 다 알려줄 수 있는 사람이 과연 존재할까? 개인적으로 필자는 없을 것이라고 본다. 만약 수학처럼 100년 전에 누군가 만든 공식이 100년 후에도 그대로 사용되는 학문이라면 누군가 체계적인 커리큘럼을 만들어서 저런 것들을 모두 가르쳐줄 수 있을지 모르지만, 필자가 이야기했던 저 도구들 중 많은 수는 당장 내년이 되면 사라질 수도 있는 녀석들이다. 즉, 지식의 생명이 굉장히 짧다는 것이다. 그리고 같은 역할을 하는 저 많은 도구들 중에서 뭐가 더 좋고 나쁘고를 가리기도 애매하다. 당장 근처에 있는 프론트엔드 개발자에게 React가 좋아? Vue가 좋아?라고 물어본다면 뭐라고 대답할까? 아마 둘 다 좋다거나 둘 다 구리다거나, 아니면 자기 맘에 드는 것 하나를 골라서 이야기할 것이다. 그런 이유들 때문에 프로그래밍은 특정한 커리큘럼을 만들기가 꽤나 어려운 분야이다. 물론 처음 프로그래밍을 배우는 단계에서는 커리큘럼을 어느 정도 만들어줄 수 있지만, 나중에 가면 사실 커리큘럼이라는 것 자체가 별로 의미가 없다. 기껏 학원에서 3개월동안 열심히 Vue를 학습했는데 막상 이직한 직장은 React를 쓰고 있다면 어떻게 할 것인가? 그렇다고 회사에 양해를 구하고 학원에 가서 React를 3개월 동안 다시 배울 수는 없는 노릇이다. 결국 개발자들은 속해있는 조직의 상황이나 배우고 싶은 기술 등에 맞춰서 자신에게 필요한 정보들을 스스로 찾아서 학습해나가는데 익숙해질 수 밖에 없고, 그래서 개발자들에게 뭔 질문만 했다하면 구글링을 해보라는 대답이 돌아오는 것이다. 하지만 구글이나 스택오버플로우에서 정보를 스스로 찾아 주워먹는 식의 능동적인 학습을 하기 위해서는 적절한 키워드를 선택하여 구글링을 할 수 있는 능력과 좋은 정보와 나쁜 정보를 가릴 수 있는 식별력까지 겸비해야 올바른 정보를 주워먹어가며 학습할 수 있기 때문에 생각보다 쉽지 않다. (영어도 잘 해야 한다) 게다가 이건 누가 알려줄 수 있는 것도 아니라서 스스로 연습하고 연구해야한다. 하지만 당장 구글에 뭘 검색해야할지, 뭘 공부해야 좋을 지도 모르겠는데 무작정 구글링을 하라고 할 수는 없는 법이니 필자가 자주 사용하는 아주 간단한 방법을 한번 공유해보려고 한다. 모든 키워드는 연결되어 있다우리는 보통 공부를 할 때 “암기”를 한다. 암기를 통한 학습은 좁은 폭의 정보를 빠르게 습득할 수 있게 해주기 때문에 늘 새로운 트렌드에 쫓기며 사는 개발자들에게는 꽤나 잘 맞는 학습 방법이라고 할 수 있다. 뭐, 순수 함수의 특징 4가지를 외운다던가, TCP나 UDP의 특징과 장단점을 외운다던가 하는 것들 말이다. 또는 새로운 라이브러리나 프레임워크가 나오면 공식 문서를 한번 쭉 흝어보고 코딩을 해보며 연습하는 것도 일종의 암기라고 할 수 있다. 그러나 이런 단순 암기를 통한 학습 방법은 빠르다는 장점도 있는 반면, 학습 키워드의 습득이 단편적이라는 단점도 가지고 있다. 키워드의 습득이 단편적이라는 말은 어떠한 키워드를 습득했을 때 자연스럽게 다음 레벨의 키워드로 연결되기가 어렵다는 말이다. 아마 이렇게 공부를 하다보면 어느 순간 이런 생각이 들 때가 있었을 것이다. 좋아… 여기까진 완벽해… 근데 다음엔 뭘 공부해야하지…? 분명히 머리로는 내가 공부한 것들이 아직 한참 모자라다는 것을 알고는 있지만 그 다음에 뭘 공부해야하는지도 모르겠고, 그렇다고 전혀 다른 키워드를 찾아서 공부하기에는 괜히 여기저기 찔러보기만 하는 것 같은 기분 말이다. 이는 단순한 암기를 통해서 학습한 지식들이 서로 “연결”되지 않았기 때문에 나타나는 현상이다. 사실 지식이라는 것은 일종의 키워드 마인드맵이라고 할 수 있으며, 마인드맵 안에 있는 모든 키워드는 밀접하게 관련이 있는 다른 키워드들과 연결되어 있다. 결국 능동적인 학습은 이렇게 연결된 키워드를 찾아나서는 탐구 과정의 연속이라고 할 수 있다. 단순 암기를 통한 학습은 이 마인드맵의 키워드 하나하나를 직접 찾아내어 학습하는 것이다. 그러나 저 많은 키워드들을 스스로 전부 찾아내어 공부한다는 건 꽤나 어려운 일이다. 그래서 우리는 하나의 키워드를 공부하며 그 속에서 자연스럽게 다음 키워드를 찾아낼 수 있는 방법을 생각해야한다. 그리고 이렇게 하나의 키워드에 대해서 학습하며 다음 키워드를 찾는 행위를 반복하다보면 단순 암기를 통해 저장했던 파편적인 정보들 간의 연결고리도 자연스럽게 생성되기 때문에 더 기억이 오래가는 효과도 누릴 수 있으므로 일석이조의 효과를 누릴 수도 있다. 그렇다면 어떤 과정을 통해 연결된 키워드를 찾아낼 수 있을까? 스스로에게 질문을 던져보자우리가 어떤 주제에 대해서 공부하면서 연관된 키워드를 찾아내지 못하는 이유 중 하나는 그저 암기를 통해 정리되어있는 지식을 받아들이기만 한다는 것에 있다. “A는 B다”라고 외우기만 한다면 그 속에 숨어있는 다른 키워드들을 찾을 기회조차 놓치게 되어버리는 것이다. 필자가 제시하는 방법은 바로 “질문”이다. 단, 이 질문은 남들에게 질문하는 것이 아니라 스스로에게 하는 질문이다. (남들에게 질문하는 것은 가장 최후의 보루로 남겨놓도록 하자) 스스로에게 “왜 이렇게 되는데?”, “어떻게 이게 가능한데?”와 같은 질문을 던지며 공부하고 있는 키워드에 대해서 자신이 모르는 것이 무엇인지 하나씩 찾아가는 것이다. 그리고 자신이 모르는 것이 무엇인지 알았다면 이제 구글링을 통해 그 질문에 대한 답을 찾아보면 된다. 이렇게 몇 번 반복하다보면 아마 그 질문에 대한 답을 쥐고 있는 새로운 키워드를 찾아낼 수 있을 것이다. 리액트 훅의 useState를 한번 예로 들어보겠다. 리액트의 useState는 함수형 컴포넌트에서 상태를 저장하고 기억하고 싶을 때 사용하는 훅이다. 123456789function Foo () { const [count, setCount] = useState(0); return ( {count} setCount(count++)}>+ );} 사실 useState라는 훅을 사용하면 함수형 컴포넌트 내에서 상태를 관리할 수 있다는 사실만 알고 있어도 코딩을 하고 어플리케이션을 만드는 데에는 아무 지장이 없고, 시간에 쫓기는 학습을 해야하는 상황이라면 이쯤에서 학습을 접고 코딩을 해도 된다. 그러나 단지 useState 훅을 사용할 수 있다는 사실만으로 만족할 수 없다면 이쯤에서 새로운 질문을 하나 던져봐야한다. 필자는 처음 리액트의 함수형 컴포넌트를 접하고 이 훅을 알게 되었을 때 이런 원초적인 궁금증이 들었었다. useState 훅은 어떻게 상태를 저장할 수 있는걸까? 사실 이런 질문은 그렇게 대단한 질문도 아니다. “useState 훅을 사용하면 함수형 컴포넌트 안에서도 상태를 저장할 수 있다”는 단순한 암기에서 조금만 더 호기심을 가지고 앞으로 나아갔을 뿐이다. 물론 보자마자 useState 훅의 원리를 바로 알 수 있을 정도로 자바스크립트에 대한 지식이 많은 분이라면 질문의 난이도도 더 높았을테지만, 당시 필자는 이 훅의 원리에 대해 전혀 감을 잡지 못했었다. 자, 어쨌든 이렇게 스스로 질문을 했으니 이제 답을 찾아야한다. 당연히 이 질문에 답해줄 수 있는 선생님은 내 곁에 없으니 갓 구글에 이 질문을 그대로 검색해보도록 하자. 필자가 선택한 검색 키워드는 useState 원리이다. 검색 키워드를 바꿔서 재시도한다고 과금되는 것도 아니니 그냥 손 가는대로 마음 가는대로 때려넣어보자. 사실 구글링에 무슨 대단한 스킬이 필요한 것도 아니다. 역시 구글신께서는 모든 걸 알고 계신다 구글에 해당 키워드를 검색해보니 이미 다른 분들이 리액트 훅에 대해 자세히 정리해놓은 포스팅들이 보인다. 만약 한글로 구글링을 진행했는데 원하는 자료가 나오지 않는다면 Principle of useState와 같은 영어 키워드로 다시 한번 검색해보면 대부분 원하는 자료를 찾아낼 수 있다. 사실 한글로 검색했을 때와 영어로 검색했을 때 찾을 수 있는 정보량의 차이는 어마무시하기 때문에 되도록이면 영어로 검색하는 습관을 들이는 것이 좋기는 하다. 사실 리액트의 useState 훅은 자바스크립트 클로저의 성질을 이용하여 함수 내부에 상태를 저장할 수 있도록 만든 것이기 때문에 조금만 검색해보면 여러분은 클로저(Closure)라는 키워드를 얻어낼 수 있다. 이렇게 클로저라는 새 키워드를 얻었을 때 여러분이 클로저가 무엇인지 안다면 useState 훅을 직접 만들어 보며 공부해봐도 좋고, 아니면 다른 질문을 던져서 다시 새로운 키워드를 찾으면 된다. 만약 클로저가 무엇인지 잘 모른다면 자바스크립트 클로저와 같은 검색 키워드로 다시 구글링해서 해당 내용에 대한 학습을 진행하면 된다. 아마 클로저를 공부하다보면 또 다른 질문을 던져볼 수 있을 것이고 그로 인해 더 깊숙한 곳에 있는 새로운 키워드를 또 찾을 수 있을 것이다. 이 글을 읽으신 독자분들께서는 “당장 공부하고 사용하기만 해도 벅찬데 더 어려운 원리를 언제 다 이해해?”라고 생각할 수도 있겠다. 그러나 “키워드를 이해할 수 있는 능력”과 “키워드를 찾을 수 있는 능력”은 엄연히 다르다는 것을 알아야한다. 애초에 키워드를 이해할 수 있는 능력은 사실 몇 개의 팁만으로 단기간 안에 키울 수 있는 능력이 아니다. 어떤 분야의 키워드를 빠르게 이해할 수 있는 능력은 일정 수준 이상의 해당 분야 지식을 기반으로 하기 때문이다. 하지만 “키워드를 찾을 수 있는 능력”은 다르다. 딱히 일정 수준 이상의 지식이 필요한 것도 아니고, 그저 구글에 검색어를 칠 수 있는 손가락만 있으면 된다. 그리고 만약 새로운 키워드를 획득했는데 이해하기가 너무 어렵다고 느껴진다면 굳이 바로 학습하지 않아도 된다. 키워드를 획득한다는 것이 가지는 가장 작은 의미는, 적어도 “세상에 이런 것도 존재하는구나”라는 정도의 지식은 얻을 수 있다는 것이다. 이런 키워드가 존재한다는 것을 알고 있다면 나중에 자신이 학습한 지식의 깊이가 더 깊어지고나서 그 키워드를 다시 꺼내보면 되는 것이고, 그렇게 학습을 다시 진행하며 다음 키워드를 또 찾아내는 과정을 반복하면 된다. (개발자로 일하다 보면 분명히 언젠가 그 키워드에 대해서 학습해야할 날이 온다) 마치며사실 제목을 Question Driven Thinking이라고 거창하게 쓰기는 했지만, 앞서 말했듯이 이렇게 질문을 통해 지식을 습득하는 학습 방법은 필자가 처음 이야기하는 것이 아니라 꽤나 유명한 학습 방법 중 하나이다. 이렇게 질문을 통해 조금씩 답을 찾아나가는 방법은 고대 그리스 시절부터 계속 해서 사용해오던 굉장히 유명한 교수법 중 하나이며, 스스로에게 질문을 함으로써 자신이 모르고 있는 것을 계속 해서 찾아나가고 해답을 찾아가는 과정은 과거 소크라테스 형님도 많이 사용하던 방법이었다. 내가 아무것도 모른다는 것을 안다는 것이 제일 현명한 것이라는 띵언을 남기신 형님 사실 우리는 어릴 때부터 교과서에 나오는 수학 공식, 영어 단어, 시험문제 유형 같은 것들을 달달 외우는 학습을 거의 10년 동안 받아왔기 때문에 단순 암기하는 학습에 더 익숙하다. 오히려 학교에서는 질문을 하면 괜히 수업시간 더 길어지게 만드는 이상한 놈 취급을 받기 일쑤였던 것 같다. (나대는 놈이라는 칭호도 함께 얻을 수 있다) 하지만 단순히 “왜?”라는 질문을 던짐으로써 “A는 B다”라고 기억되던 평면적인 지식을 “A는 ~기 때문에 B다”와 같은 심층적인 지식으로 바꿀 수 있기 때문에 우리는 이제 질문에 익숙해져야한다. 질문을 던지며 내가 모르는 것이 무엇인지 찾아내고 그 질문에 대한 해답을 찾아내어 다시 대답하는 과정을 반복하며 키워드를 계속 만들어나가는 것이다. 그리고 스스로에게 질문을 하게 되면 생각에 생각이 계속 해서 꼬리를 물며 깊어질 수 있기 때문에, 공부할 때 뿐만 아니라 생각을 정리할 때도 좋은 방법이므로 한번쯤 도전해보는 것을 권한다. 이상으로 Question Driven Thinking 포스팅을 마친다.","link":"/2020/02/11/question-driven-thinking/"},{"title":"나는 프론트엔드를 안다고 말할 수 있을까?","text":"이번 포스팅에서는 개발보다는 약간 철학적인 고민 이야기를 해보려고 한다. 사실 이 고민은 필자가 처음 개발을 시작할 때부터 가지고 있던 고민인데, 아직도 해답을 찾지 못했던 질문이기에, 이번 포스팅은 필자의 생각을 제시하는 것이 아니라 질문을 던지는 느낌으로 끄적여볼까한다. 필자가 개발을 시작하고 나서 6년 동안이나 하고 있는 이 고민은 바로 “안다는 것은 무엇인가?”이다. 사실 필자가 이런 고민을 하게 된 이유는 주변 개발자들의 평가로부터 출발한다. 약 5년 정도 여러 조직에서 프론트엔드 개발자로 일을 하게 되면서 다양한 동료들의 평가나 피드백을 받을 수 있었는데, 그 중 감사하게도 필자를 높게 사주시는 몇몇 분들이 “에반은 프론트엔드를 잘 한다”라는 평가를 해주셨을 때 도저히 자신있게 “아유 그럼요”라는 말을 할 수가 없었기 때문이다. 그 이유는 단순하게도 필자가 스스로 프론트엔드 개발을 잘 못한다고 생각하기 때문이다. 그리고 다른 사람들이 결정하는 주관적 가치에 대한 신뢰가 없기 때문이기도 하다. 필자는 누가 봐도 인정할 수 있을 정도의 객관적인 가치를 가진 “잘 하는 개발자”가 되고 싶었고, 지금도 바라고 있다. 물론 사람마다 기준은 다르겠지만, 객관적인 기준의 “잘 한다”라는 것은 어떤 지식에 대해 통달했다는 의미라고 생각한다. 하지만 필자가 프론트엔드 개발에 통달했냐고 묻는다면 대답은 당연히 “No”다. 아직도 모르는 것이 너무 많고, 매일 새로운 문제에 쩔쩔매며 구글신에게 기도드리는 일상을 보내고 있기 때문이다. 그래서 필자는 6년 동안이나 “진정한 앎이란 무엇인가?”에 대한 고민을 하고 있다. 나는 잘 하는 개발자”였다”사실 필자도 처음부터 이런 고민을 했던 것은 아니였다. 지금 생각해보면 상당히 우습고 거만하지만 처음 개발을 시작할 때는 스스로 실력이 좋고 학습 속도가 굉장히 빠른 편이라고 생각하고 있었다. 2015년 당시 필자는 사운드 엔지니어로 어떤 연예기획사에서 일을 하고 있었는데, 그때 회사에서 만난 동료와 함께 어떤 제품을 만들고 싶다는 이야기를 나누게 되었다. 하지만 당시 필자와 그 친구는 웹 개발에 대한 지식이 너무나도 부족했기에 가진 인맥을 총동원해서 이 제품을 함께 만들 친구들을 구하게 되었고, 그게 바로 필자가 지금 6년째 활동하고 있는 토이 프로젝트 팀인 루비콘의 시작이다. 당시 이 팀에서 필자가 맡게 된 포지션이 바로 프론트엔드 개발자였고, 그때 처음으로 프로그래밍을 독학으로 공부하기 시작하며 제품을 만들기 시작했다. 이후 회사를 그만두고 학교에도 다시 복학을 하며 전공 수업을 통해 부족한 CS(Computer Science) 지식을 보충하면서 실력이 쑥쑥 자라던 시기였다. 어깨를 으쓱거리던 시절2015년 당시 Angular가 조금씩 떠오르고 있긴 했지만 지금의 React나 Vue처럼 대부분의 개발자들이 알 정도로 유명하지는 않았기 때문에 필자는 자연스럽게 jQuery를 사용했었다. 비록 요즘 jQuery가 퇴물 취급받기는 하지만, 당시에는 CSS 셀렉터와 동일한 문법으로 간단하게 DOM 요소를 선택하고 조작할 수 있는 라이브러리였기 때문에 나름 혁신적인 평가를 받는 라이브러리였고, 필자는 이 친구를 사용하여 뷰를 만드는 것에 대해 생각보다 빠른 속도로 익숙해질 수 있었다. (document.getElementById('foo')가 $('#foo')로 줄어드는 매직…) 당시 루비콘이 서비스에는 웹 상에 obj나 fbx 같은 3D 모델 파일을 업로드하고 자유롭게 이리저리 돌려볼 수 있는 뷰어 기능과 마테리얼이나 배경 스카이박스도 설정할 수 있는 간단한 모델 에디터 기능을 구현해야 했었는데, 필자는 이 과정에서 WebGL이라는 기술을 처음 접하고 사용하게 되었다. 추억의 OBJ Parser 개발하던 시절… 사실 WebGL은 일반적인 서비스 개발자들이 크게 사용할 일이 없는 기술인데다가, 자유자재로 사용하려면 컴퓨터 그래픽스, 선형대수학에 대한 기본적인 이해가 필요하기 때문에 러닝커브가 완만한 편은 아니다. 그래서 그런지 다른 개발자들을 만나서 “저 WebGL로 이런 거 만들고 있어요”라고 이야기하면 대부분 “호에? 그거 겁나 어렵지 않아요?”라는 반응을 보였었다. 이런 반응을 자주 듣다보니 필자의 어깨는 저절로 으쓱거렸고, 점차 “나는 잘 하는 개발자다”라는 건방진 생각을 하게 된 것이다. 그러나 이런 귀여운 생각을 하루 아침에 개박살내는 사건이 있었는데, 이 사건이 지금까지 계속된 필자의 고민의 시작이었다. 사실 나는 아는 게 없었다어느 날 필자는 친구의 소개로 Code For Seoul이라는 밋업에 참여하게 되었는데, 막상 가봤더니 필자같이 꼬꼬마 개발자는 별로 없었고 경력이 지긋하신 시니어 개발자 분들이 대다수였다. (분위기는 진짜 가족같은 따뜻한 분위기라서 좋았다) 그렇게 모여 앉아 맛있는 다과를 먹으며 기술에 대한 이야기를 도란도란 나누던 중에 늘 듣던 그 질문이 다시 나오게 되었다. 동욱님은 토이 프로젝트 하신다던데, 어떤 걸 만드시는 거에요? 이 질문을 들은 필자는 평소대로 “WebGL을 사용해서 간단한 웹 에디터랑 뷰어를 만들고 있어요”라고 대답했지만, 그 이후에 그 분들이 보여주신 반응은 평소에 필자가 겪던 상황과 많이 달랐다. 필자의 대답을 들은 아저씨들은 “호에?” 같은 반응 대신 WebGL이 가지고 있는 퍼포먼스 이슈나 캔버스 렌더링 시 발생하는 메모리 릭과 같은 문제점을 어떻게 해결하고 있는지에 대한 질문을 쏟아내기 시작했지만, 필자는 애초에 그런 이슈들이 있는 줄도 몰랐기 때문에 어벙벙하고 있을 수 밖에 없었다. 게다가 저 질문들을 던지신 분이 개발자도 아니고 DA(Data Analyst)였다는 점도 충격이었다. 이때 필자는 한 가지 중요한 사실을 깨달았다. 세상은 넓고 굇수는 많구나…난 우물 안 개구리도 아니고 그냥 올챙이 새끼였구나… 그도 그럴 것이 대학교에 다니는 학생들과 실무에서 10년 넘게 경력을 쌓아온 개발자의 짬밥 차이는 어마무시하다. 필자는 학교 안에서는 “꽤 잘하는 학생”이었지만, 학교 밖 정글에서는 그저 한 마리 올챙이에 불과했던 것이다. 물론 질문을 하셨던 아저씨는 “모를 수도 있지 허허허” 하면서 넘어가셨지만, 필자는 이 상황이 너무 부끄러워서 어디 쥐구멍에라도…아니 쥐구멍을 만들어서라도 숨고 싶었다. 필자는 러닝커브가 가파른 WebGL이라는 기술을 사용하고 있다는 것만으로 거들먹거렸지만, 사실은 단지 WebGL API를 사용할 수 있었을 뿐이지 이 기술들에 대해서 제대로 알고 있지 않았던 것이다. 이 사건 이후 벌써 6년이 지났지만, 아직도 필자는 어디 가서 뭘 잘 한다는 이야기를 하지 못하고 있다. 누군가 필자에게 “에반 잘 하잖아요!”라고 하면 “잘 하는 것과 할 줄 아는 것은 다르죠”라고 대답하는 것이 거의 습관이 되어버릴 정도다. 그리고 이때 처음으로 스스로에게 이런 질문을 던지게 되었다. 내가 뭘 잘 한다는 이야기를 하려면 얼마나 알고 있어야 하는 걸까…? 나는 프론트엔드 개발자인가?이처럼 내가 스스로 잘 알고 있다고 생각했던 것들이 사실은 제대로 모르고 있다는 것임을 깨닿는 경험을 비단 필자만 겪었던 것은 아닐 것이라고 생각한다. 사실 이 고민의 시작은 “얼마나 알고 있어야 한다는 것일까?”였지만, 이러한 고민을 계속 반복하다보니 결국은 “그래서 안다는 게 뭔데…?”라는 고민까지 오게 된 것이다. 자, 필자와 함께 앎에 대한 깊은 고민을 해보기 전에 이런 경험이 없으신 분들은 앎이라는 것이 얼마나 모호한 개념인지 공감이 잘 되지 않을 수도 있으니 간단하게 예시를 하나 들어볼까 한다. 필자가 여러분에게 던지고 싶은 질문은 바로 이것이다. 프론트엔드 개발자와 퍼블리셔는 같은 포지션인가? 아마 이 업계에서 일을 하시는 분들이라면 이 포지션들이 굳이 무엇인지 설명하지 않아도 다들 아실 것이라고 생각한다. 그리고 이 포지션들에 대한 의견들도, 이 질문에 대한 대답들도 각자 가지고 있을 것이다. 물론 필자가 실제로 개발자들을 모아놓고 이런 질문을 던져보지는 않았지만, 채용 사이트에 올라와 있는 포지션들이나 구글링을 통해 읽어본 정보들을 보면 실제로 많은 사람들이 프론트엔드 개발자와 퍼블리셔를 구분하고 있다는 사실을 알아내는 것은 그리 어려운 일이 아니다. 그럼 여기서 다시 질문을 던져보겠다. 여러분은 이 두 개의 포지션을 구분할 수 있는가?만약 그렇다면 어떠한 기준으로 이 두 개의 포지션을 구분하는가? 이 질문을 던짐으로써 여러분에게 묻고 싶은 것은 말 그대로 프론트엔드 개발자와 퍼블리셔를 구분하는 명확한 기준이 무엇인지에 대한 것이다. 사실 이 두 포지션은 모두 웹 클라이언트를 만드는 포지션이다. 그러나 방금 이야기했듯이 사람들은 꽤 철저하게 이 두 포지션을 구분하고 있다. 즉, 구분하는 방법을 알고 있다는 것이다. 일반적으로 프론트엔드 개발자는 웹 브라우저에서 작동하는 클라이언트 프로그램을 작성하는 개발자를 의미한다. HTML을 사용하여 DOM 구조를 정의하고, 자바스크립트를 사용하여 백엔드 시스템과 데이터를 주고 받거나 스탠드얼론 클라이언트 프로그램을 만들기도 하고, 때로는 CSS를 사용하여 사용자에게 유려한 UI/UX를 제공하기도 하는 개발자말이다. 하지만 이렇게 놓고 보자니 왠지 퍼블리셔와도 비슷한 부분이 많은 것 같다. 아니, 실제 직장에서 하는 일을 생각해보면 비슷한 정도가 아니라 똑같다. 혹자는 퍼블리셔는 React나 Vue같은 기술을 모르기 때문에 프론트엔드 개발자가 아니라고도 한다. 그렇다면 예전에 jQuery를 사용하여 복잡한 웹 클라이언트를 개발하던 사람들은 퍼블리셔였을까? 하지만 당시 업계에는 분명히 “프론트엔드 개발자”라는 포지션이 인식되고 있었고, 실제로 필자도 그렇게 불렸었다. 또 어떤 사람은 퍼블리셔는 자바스크립트나 컴퓨터 공학에 대한 기본 지식이 프론트엔드 개발자보다 부족하다고도 한다. 음… 그렇다면 이런 질문을 한번 던져볼 수 있겠다. 퍼블리셔가 자바스크립트를 어디까지 알아야 프론트엔드 개발자라고 할 수 있는가?퍼블리셔가 단순히 CS에 대한 지식을 얻기만 하면 바로 그 사람은 프론트엔드 개발자가 되는 것인가? 이런 질문을 던지면 이제 사람들마다 답이 갈리게 된다. 어떤 사람은 클로저 정도는 알아야한다고 할 수도 있고, 어떤 사람은 클로저는 몰라도 되는데 CORS 정책 위반 정도는 스스로 해결할 수 있어야 한다고 대답할 수도 있다. 한 발 물러서서 클로저를 능숙하게 사용할 수 있으면 프론트엔드 개발자라고 치자. 그렇다면 클로저의 대략적인 원리를 알고 사용만 할 수 있어도 되는 것인가? 아니면 자바스크립트 엔진 내에서 함수 스코프와 클로저가 어떤 방식으로 생성되는 정도까지는 알아야 되는 것인가? 애매모호하다. 여러분은 어떤 기준으로 이 두 개의 포지션을 나누고 있었는가? 여러분은 정확히 그 기준을 알고 나누고 있었던 것이 맞는가? 아니면 이 포지션을 나누는 것 자체가 처음부터 의미가 없었던 일인가? 소피스트가 아닌 소크라테스가 되어야 한다만약 이 질문들에 제대로 대답하지 못했다면 사실 여러분은 퍼블리셔와 프론트엔드의 차이를 정확히 알지 못하고 있는 것이다. 아니 어쩌면 사실은 이러한 차이 자체가 존재하지 않았을지도 모르는 일이다. 필자는 이 예시를 통해 우리가 사실은 잘 알고 있었다고 생각한 것들이 정작 자세히 들여다보려고 하면 정확하게 알고 있는 것이 아닐 수 있다는 이야기를 하고 싶었다. 마치 우리가 프론트엔드, 퍼블리셔, 풀스택 개발자와 같은 포지션이 명확히 나누어져 있는 것처럼 인식하고 있지만, 막상 이것들이 정확히 어떤 기준으로 나눠지는지 고찰해보면 쉽게 대답하지 못하거나 사람마다 다른 답변이 나오는 등 정확한 기준이 없는 것처럼 말이다. 사실 우리의 삶이나 우리가 사랑하는 프로그래밍 속에는 이런 문제들이 굉장히 많이 널려있다. 단지 우리가 “알고 있다”라고 생각하기 때문에 고찰해보지 않았을 뿐이다. 고대 그리스의 소크라테스는 이런 문제에 대해 깊게 고민했던 사람 중 하나인데, 이 아저씨는 진정한 앎이 무엇인지, 현명하다는 것이 무엇인지 알기 위해 당대 똑똑하다고 소문났던 소피스트들을 찾아가 질문을 던지는 짓을 했던 것으로 유명한 아저씨다. 소피스트는 나름 똑똑한 사람들이었다소크라테스가 활동하던 당시의 아테네는 진짜 말 그대로 “말빨”로 먹고 사는 사회였다. 아테네 한 가운데 있는 아고라에서는 늘 토론이 활발하게 일어났으며, 연설을 통해 시민들의 지지를 얻음으로써 정치적 기반 또한 만들 수 있었다. 게다가 아테네에서는 시민들 간의 소송도 활발하게 이루어졌었는데, 변호사같이 전문가가 대리 변호를 해준다는 개념도 없었기 때문에 법정에서도 스스로 자신의 변호를 해야했다. 결국 지식이 부족하고 말을 잘 못하는 것만으로도 실질적인 손해를 입을 수 있는 사회였던 것이다. 본격 입 털어서 먹고 사는 시대였던 고대 아테네 그래서 아테네 시민들은 논쟁에 필요한 지식과 말빨을 중요하게 생각할 수 밖에 없었고, 이때 등장하는 사람들이 바로 아테네의 고액 과외 선생님들인 “소피스트”다. 소피스트들은 시민들에게 수사학, 변론술, 웅변술 등을 가르쳐 주면서 돈을 받았는데, 남을 가르치려면 당연히 많은 지식을 가지고 있어야 했으니 대부분의 소피스트들은 당대 유명한 웅변가나 철학가들이었다. 사실 소크라테스도 달변가로 유명했기 때문에 충분히 소피스트로 활동할 수 있는 지식 수준이 되었지만 소크라테스는 자신이 사람들을 가르치는 것이 아니라 그저 사람들이 본래 알고 있던 것을 질문을 통해 끌어내기만 하는 것이라는 신념을 가지고 있었기에 공짜로 사람들을 가르쳐 주었고, 이 때문에 소크라테스는 다른 소피스트들에게 약간 밉상같은 존재였다. 그러던 어느 날 소크라테스는 델포이 신전에서 우연히 한 가지 신탁을 받게 되는데, 이 신탁이 소크라테스가 앎에 대한 탐구를 본격적으로 시작하게 된 계기가 되었다. 나는 내가 모른다는 것을 알고 있다소크라테스는 델포이 신전에서 “아테네에서 가장 현명한 사람은 소크라테스다”라는 신탁을 받게된다. 이 시대는 아직 신화적인 사고를 하던 시대였기 때문에 신탁은 거의 법이나 마찬가지였지만, 소크라테스는 절대 그럴 리가 없다며 직접 자신보다 현명한 사람을 찾아내겠다는 다짐을 하게 된다. 이때부터 소크라테스는 당대 용하다는 고액 과외 선생님들인 소피스트들을 찾아다니며 질문을 하며 그 사람이 얼마나 현명한 사람인지 알아보기 시작했는데, 소크라테스가 소피스트들에게 했던 질문의 흐름을 앞서 이야기했던 퍼블리셔와 프론트엔드에 대한 주제에 대입해보면 이런 느낌이다. Q: 프론트엔드 개발자는 어떤 사람인가?A: 웹 클라이언트를 만드는데 필요한 지식을 알고, 웹 클라이언트를 실제로 구현할 수 있는 사람입니다. Q: 그럼 퍼블리셔도 프론트엔드 개발자 아닌가?A: 아닙니다. Q: 그렇다면 프론트엔드와 퍼블리셔의 차이는 무엇인가?A: 퍼블리셔는 프론트엔드보다 컴퓨터 공학 지식이 적다고 생각합니다. Q: 그럼 퍼블리셔가 컴퓨터 공학 지식을 공부하기만 하면 프론트엔드 개발자가 되는 것인가?A: 프론트엔드 개발자로 이직까지 해야 프론트엔드 개발자라고 할 수 있을 것 같습니다. Q: 그렇다면 회사에서 정해주는 포지션이 영향을 준다는 것인가?A: …그런 것 같습니다. Q: 그럼 유명한 IT기업에서 일하던 프론트엔드 개발자가 웹 에이전시의 퍼블리셔로 이직하면 그 사람은 프론트엔드 개발자인가 퍼블리셔인가?A: …. (모순 발생) 이렇게 소크라테스는 소피스트들에게 원론적인 질문들을 던지고 그 사람이 답변을 하면 다시 의문이 드는 부분에 대해서 질문을 던지는 식의 대화법을 사용했는데, 이렇게 소크라테스의 질문을 몇 번 받은 소피스트들은 어느 순간 답변이 막히게 되며 자신이 알고 있는 것이 사실 오류가 있는 개념임을 깨닿고 당황하거나 화를 내었다고 한다. 이렇게 배가 좌초된 것처럼 모순과 난제에 빠진 상태를 아포리아라고 한다 결국 이렇게 소피스트들을 찾아다니며 질문을 해도 자신의 질문에 제대로 끝까지 대답하는 사람이 없자, 소크라테스는 “이들이 제대로 아는 것은 없다”라는 결론을 내리게 된다. 여러분은 어떤가? 소크라테스가 여러분에게 프로그래밍에 대한 질문이나 프론트엔드와 퍼블리셔를 구분하는 기준에 대한 질문을 던지면 끝까지 대답할 수 있을까? 만약 대답할 수 있다면, 여러분은 적어도 그 지식에 대해서는 소크라테스를 이긴 사람이다. 하지만 만약 소크라테스가 필자에게 저런 질문을 건네온다면 필자는 열심히 대답하다가 결국 아포리아 상태에 빠지고 말 것 같다는 생각이 든다. 그리고 이러한 질문법은 비단 소피스트들 뿐 아니라 소크라테스가 스스로에게 던지는 질문이기도 했다. 결국 소크라테스는 이 짓을 계속 반복하다가 한 가지를 깨닿게 된다. 내가 알고 있는 단 한 가지는 내가 아무것도 모르고 있다는 사실이다 이 말은 소크라테스 자신에게 하는 말이기도 했으며, 다른 소피스트들에게 하는 말이기도 했다. 너나 나나 제대로 아는 것은 하나도 없지만, 나는 적어도 “내가 모른다는 것”은 알고 있다는 것이다. 근데 소크라테스의 이런 결론이 딱히 틀린 것도 아닌게, 자신이 모른다는 것을 인정해야 더 깊은 진리를 탐구할 수도 있기 때문이다. 애초에 “난 알고 있다”라고 생각하고 있는 사람이 그 주제에 대해서 더 깊은 탐구를 하지는 않을테니 말이다. 당연히 소크라테스의 이런 태도는 소피스트들에게 곱게 보일리가 없었고 결국 소크라테스는 “아테네의 전통을 해치고 젊은이들을 타락시켰다”라는 죄목으로 기소되었다. 뭐 사실 법원에서도 눈치 잘 보고 입만 잘 털면 살아남을 수 있었지만 소크라테스가 스스로의 신념을 끝까지 굽히지 않았고, 배심원들을 자극하는 변론을 하며 자충수를 두는 바람에 사형당해버렸다. (다른 도시국가로 망명 신청도 할 수 있었는데, 그냥 독약을 마신 상남자…) 결국 이 아저씨는 독약을 먹게 된다 이렇게 똑똑했던 아저씨도 결국 그 긴 고민 끝에 내린 결론이 “나는 아무것도 모른다”라니, 약간은 허무하기 그지 없는 결론이다. 마치며이렇게 긴 이야기를 풀어내다보니 필자의 질문은 “앎이란 무엇인가?”에서 “우리가 알고 있는 것이 정말로 알고 있다는 것인가?”로 바뀌게 되었을 뿐 결국 어떠한 답을 찾아내지는 못했다. 뭐 4대 성인으로 불리는 소크라테스 아저씨도 결국 저런 허무한 결론을 낸 질문이니 필자가 이 질문에 대해 쉬이 대답하지는 못할 것 같다는 생각도 들지만, 필자가 궁극적으로 목표하는 “객관적인 기준으로 잘하는 개발자”가 되기 위해서는 반드시 이 질문에 대한 나름의 해답을 정의해야하기에, 앞으로도 이 고민은 계속 될 것이다. 애초에 이 포스팅은 답변을 제시하기 위한 포스팅이 아니다. 이 포스팅을 쓰기 전에도 필자는 6년 정도 이런 고민을 계속 해왔지만 점점 질문에 질문이 꼬리를 물고 깊어져 갈 뿐, 어떠한 해답의 실마리 조차 잡지 못했기 때문이다. 물론 필자도 늘 바쁘게 일하고 공부하다보면 이런 철학적인 고민은 쉽사리 잊혀지기 마련이라는 것은 알고 있다. 이런 것보다 당장 내일 출근해서 신경써야하는 것들이 많다는 것도 알고 있다. (필자도 일 오지게 밀려있다…) 그러나 가끔은 프로그래밍에 대한 직접적인 고민보다 이렇게 한 걸음 물러선 시선에서 바라보는 고민이 오히려 더 도움이 될 때도 있는 것 같기에 여러분도 필자와 같은 고민을 한번 쯤은 해봤으면 하는 마음으로 필자의 고민을 공유하려고 한다. 이상으로 “나는 프론트엔드를 안다고 말할 수 있을까?” 포스팅을 마친다.","link":"/2020/03/02/what-is-knowing/"},{"title":"Paypal - Express Checkout Restful API 사용하기","text":"이번 포스팅에서는 Paypal의 RESTful API인 Express Checkout을 사용하는 방법에 대해서 포스팅 하려고 한다. 진행하기에 앞서 먼저, 페이팔 샌드박스 홈페이지에 접속해서 sandbox용 계정을 만들어야 한다. 이 계정으로 테스트를 진행하고 실제 운영 계정은 페이팔 홈페이지에서 회원가입하면 된다. Express Checkout이란?Paypal에서 제공해주는 결제 플로우 방식 중 하나이며, 유저가 페이팔로 구매하기버튼을 클릭했을 때 페이팔 로그인 Modal window가 렌더되고, 이를 통해 결제를 진행하는 플로우이다.모든 국가를 지원하며, 브라우저 지원은 다음과 같다. Internet Explorer Chrome Mozila Firefox Safari Opera IE 9 이상 ver 27 이상 ver 30 이상 ver 5.1 이상 ver 23 이상 Paypal에서는 총 3가지의 결제플로우를 제공하고 있으며, 한국에서는 이 중 All countries로 제공되는 다음 2가지 방식 중 선택이 가능하다. Paypal StandardPaypal에서 제공해주는 HTML코드를 기반으로 생성된 버튼을 웹 클라이언트 소스에 직접 삽입하는 방식이다.Paypal사이트에서 직접 상품 이름, 상품 ID, 가격 등을 입력하면 그에 맞는 버튼의 코드를 자동으로 생성해준다. 이후 그 코드를 클라이언트 소스에 삽입하면 된다. Express CheckoutPaypal의 JavaScript SDK인 checkout.js를 사용하여 버튼을 동적으로 렌더하는 방식이다.상품 데이터를 본인 서비스의 서버로 전달한 후 Server to Server방식으로 Paypal서버에서 인증을 받는 방식으로 진행된다. 필자는 이 방법을 선택하였다. Express Checkout의 흐름먼저 이 포스팅은 Paypal Developer 페이지의 Express Checkout항목을 참고하여 작성하였다. https://developer.paypal.com/docs/classic/express-checkout/ 쉽게 생각하면 Facebook이나 Instagram계정으로 로그인하는 플로우와 UX가 비슷하다고 보면 된다.유저가 구매버튼을 클릭하면 데스크탑에서는 팝업, 모바일에서는 새 탭의 형태로 페이팔 로그인 창이 열리고, 유저는 그 창에서 인증과 결제를 순차적으로 진행하게 된다.이후 모든 플로우가 끝나면 유저는 결제 진행 결과에 따라 페이팔에 redirect_url파라미터로 보내졌던 URL로 이동을 하게되며 최종적으로 성공, 실패, 취소에 따른 페이지와 결제 정보 결과를 보게 된다.Paypal서버와 통신하는 방법으로는 RESTful API와 NVP/SOAP API로 2가지 방법이 있으나 본 문서에서는 RESTful API만 기술하겠다. Paypal Express Checkout의 플로우는 다음과 같다. 유저가 구매버튼을 클릭 Express Checkout의 client SDK인 checkout.js를 통해 로그인 모달창이 열리거나 API서버가 Paypal API를 호출하여 payment이벤트를 초기화한다. payment이벤트 초기화 후, Express Checkout 플로우가 모달창 내부에서 시작된다. 유저가 페이팔 로그인을 진행 유저가 결제 정보를 확인 후 Contunue버튼을 클릭 유저가 Continue버튼을 클릭하면 다시 서비스의 리다이렉트페이지로 이동하며 최종 결제 정보가 표시된다.결제 정보를 생성할 때 보냈던 상품 정보 파라미터는 이 단계에서 노출시키는 것이 좋다. 최종적으로 execute API가 호출되고 결제가 마무리된다. 그리고 사용자는 서비스 상의 결제완료 페이지로 리다이렉트된다. Client만으로 진행하기 Paypal 공식문서에서는 Express Checkout과의 버전 호환성을 최대한 보장하기 위해 CDN을 이용한 동적로딩을 추천하고 있다. 직접 checkout.js파일을 다운받아 클라이언트 소스에 넣는 것은 추천하지 않는다.이 문서에서의 checkout.js의 버전은 4.0으로 진행한다. 지금 작성하는 플로우는 필자 서비스의 API서버를 통하지 않고 checkout.js를 이용하여 바로 페이팔 서버로부터 인증과 결제까지 한번에 마치는 방법이다.클라이언트에서는 데이터를 정의하고 checkout.js를 사용하여 Paypal결제 버튼을 렌더한다. 이 버튼을 클릭하면 checkout.js의 내장 메소드를 호출하여 자동으로 인증과 결제토큰생성, 결제 완료까지 한큐에 진행하게 된다. 먼저, checkout.js SDK의 동적로딩을 위해 index.html파일에 script태그를 열고 다음 스크립트를 작성하였다. 123456(function() { var _DOM = document.createElement('script'); _DOM.src = 'https://www.paypalobjects.com/api/checkout.js'; var element = document.getElementsByTagName('script')[0]; element.parentNode.insertBefore(_DOM, element);}); 이후 Paypal버튼을 DOM을 작성해주고 checkout.js를 사용해 버튼을 렌더한다. 1Test to payment 12345678910111213141516171819202122232425262728paypal.Button.render({ env: 'sandbox', // 테스트용은 'sandbox'를, 운영은 'production'을 입력 client: { sandbox: 'paypal-sandbox-key', // paypal에서 발급 받은 client_key를 입력 production: 'paypal-production-key' }, payment: function() { var env = this.props.env; var client = this.props.client; return paypal.rest.payment.create(env, client, { transactions: [{ amount: { total: '10.00', currency: 'USD' } }], }); }, commit: true, // false일 시 모달의 마지막 버튼의 문구가 Continue로 변하고 바로 결제가 진행되는 것이 아니라 리다이렉트 페이지로 이동한다. // true일 시 Pay Now로 변하고 바로 결제가 진행된다 onAuthorize: function(data, actions) { return actions.payment.execute().then(function(res) { // 결제 성공 시 콜백파라미터인 res에 데이터가 담겨온다 console.log(res); }); }}, '#pay-test-btn'); 그러면 아래와 같이 아까 작성한 #pay-test-btn DOM엘리먼트에 자동으로 Paypal버튼이 렌더된다. 버튼 모양이 조금 이상해졌지만 신경쓰지 말자… 정상적으로 버튼이 렌더되었다면 이제 테스트 결제를 진행해보자.저 버튼을 클릭하면 아래와 같은 창이 하나 뜰 것이다. 이 창은 사용자의 이용환경이 데스크탑이냐, 모바일이냐에 따라 팝업창, iframe또는 새 탭으로 열릴 수 있다. 이 창을 Paypal에서는 light window라고 부르지만 이 포스팅에서는 편의상 그냥 로그인 모달이라고 하겠다.이 후 사용자는 로그인 모달의 안내에 따라 결제를 진행하게 되고, 최종 결제가 완료되면 onAuthorize메소드에 있는 Promise Callback함수의 res파라메터로 결과값을 전달받게 된다. Client와 Server의 통신으로 진행하기이 플로우는 Paypal에서 발급해주는 client_key와 secret을 이용하여 페이팔 인증부터 차례대로 진행하는 방법이다.이 방법의 장점으로는 본인의 서비스의 UX플로우를 최대한 지키며 결제를 진행시킬 수 있고, 첫번째 방법보다 플로우가 유연하며 버튼 디자인또한 css로 커스터마이징이 자유롭다.그런 이유로 Paypal에서도 이 방법을 권장하고 있다. 결제플로우는 크게 3가지 단계로 나눠진다. client_key와 secret을 사용하여 페이팔 서버로부터 액세스토큰을 받는 인증 과정 결제를 create하는 과정 결제를 execute하는 과정 먼저 클라이언트는 버튼을 렌더하고 본 서비스의 API를 통해 create요청을 보내야한다.이때 전 플로우와의 차이점은 딱히 checkout.js를 사용하지않아도 딱히 상관이 없다는 점이다.먼저 버튼을 렌더한다. 참고로 필자는 AngularJS와 md-material을 사용하였다. 1234567 TEST 이후 Paypal결제를 다른 페이지에서 사용하게 될때 중복로직을 작성해야하므로 따로 팩토리를 정의했다.따로 팩토리를 정의하지 않고 컨트롤러 내부에 구현해도 상관은 없다.이 팩토리에서는 필자의 API서버와 통신을 한 후 Promise를 사용하여 다시 컨트롤러로 값을 전달한다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384(function() { 'use strict'; angular .module('services') .factory('PaypalService', [ '$rootScope', 'Restangular', 'SNS_KEYS', '$q', PaypalService ]); function PaypalService($rootScope, Restangular, SNS_KEYS, $q) { //페이팔에서 발급해준 키를 전역으로 사용하기 위해 app.constants에 미리 담아놓았다. var clientKey = SNS_KEYS.paypal; /** * @public * @name create * @description create paypal payment request * @param { Object } data * @return { Promise } */ function create(data) { var defer = $q.defer(); data.clientKey = clientKey; data.redirect_urls = { return_url: location.origin + '/paypal/result', // 결제가 완료되었을 때 리다이렉트될 페이지 cancel_url: location.origin + '/paypal/result' // 결제가 취소되었을 때 리다이렉트 될 페이지, return_url과 같아도 상관없다 }; // API서버의 API endpoint Restangular.all('paypal/payments/create') .customPOST(data, undefined, undefined, undefined, { 'Content-Type': 'application/json' }).then(function(res) { defer.resolve(res); }, function(err) { defer.reject(err); }); return defer.promise; } /** * @name execute * @description execute paypal payment * @param { Object } data * @return { Promise} */ function execute(data) { var defer = $q.defer(); // API서버의 API endpoint Restangular.all('/paypal/payments/execute').customPOST(data, undefined, undefined, { 'Content-Type': 'application/json' }).then(function(res) { defer.resolve(res); }, function(err) { defer.reject(err); }); return defer.promise; } /** * @name getPaymentInfo * @description getting created paypal payment information * @param { Object } data * @return { Promise } */ function getPaymentInfo(data) { var defer = $q.defer(); // API서버의 API endpoint Restangular.all('/paypal/payments/detail').customGET('', data).then(function(res) { defer.resolve(res); }, function(err) { defer.reject(err); }); } return { create: create, execute: execute, getPaymentInfo: getPaymentInfo }; }})(); 이후 버튼을 렌더한 html파일에 물려있는 Controller에서 위에서 정의한 Paypal서비스를 호출한다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849(function() { 'use strict'; angular .module('app') .controller('PaypalCreateController', [ '$rootScope', '$scope', 'PaypalService', PaypalCreateController ]); function PaypalCreateController() { var vm = this; //테스트를 위한 더미데이터를 정의한다. 실서비스에는 이 데이터들이 폼에 바인딩 될 것이다. vm.paymentData = { transactions: [{ amount: { total: '1.00', currency: 'USD' }, description: 'This is the description', item_list: { items: [{ name: 'test', description: 'This is test product', quantity: '1', price: '1.00', sku: '1', currency: 'USD' }] } }] }; vm.postData = postData; function postData() { var data = angular.copy(vm.paymentData); // Deep Copy를 하는 이유는 데이터를 전송하기전에 변조해야할 경우가 생길 수 있기 때문이며 // Object타입은 기본적으로 Call by reference이기때문에 여기서 변조를 해버리면 원본 데이터도 함께 변조되기 때문이다. PaypalService.create(data).then(function(res) { // Paypal에서는 총 3개의 url을 리턴해주는데 // links[0] = 방금 생성된 결제정보의 자세한 값을 받을 수 있는 GET메소드 요청 URL // links[1] = 생성된 결제페이지의 리다이렉트 URL // links[2] = 결제 실행 URL // 순으로 나열된다. window.location = res.result.links[1].href; }); } }}); 이때 API서버는 클라이언트에서 보내준 값들을 가지고 Paypal서버와 통신하여 인증을 진행하는 로직을 가지고 있어야한다.필자의 API서버는 Laravel 5.x로 되어있다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869namespace App\\Http\\Controllers;use Log;use Illuminate\\Http\\Request;use App\\Http\\Requests;use App\\Http\\Controllers\\Controller;use GuzzleHttp\\Client;class PaypalPaymentController extends Controller { public $client; public $paymentUrl; public $accessToken; public function __construct() { $this--->client = new Client(); // 페이팔에서 발급받은 access token값을 env에서 가져온다 $this->accessToken = env('PAYPAL_ACCESS_TOKEN_SANDBOX'); // 지금은 테스트 중이라 sandbox url로 요청을 날리고있다. 본 서비스는 sandbox를 제거하고 api.paypal.com으로 날리면 된다. $this->paymentUrl = \"https://api.sandbox.paypal.com/v1/payments/payment\"; } public function detail(Request $request){ $query = $request->query(); $response = $this->client->request('GET', $this->paymentUrl.'/'.$query['paymentId'] , [ 'headers' => [ \"Content-Type\" => \"application/json\", \"Authorization\" => $this->accessToken, ], ])->getBody()->getContents(); $decodeResult = json_decode($response); return response()->success($decodeResult); } public function payment(Request $request) { $response = $this->client->request('POST', $this->paymentUrl, [ 'headers' => [ \"Content-Type\" => \"application.json\", \"Authorization\" => $this->accessToken, ], 'json' => [ // Paypal서버에 날릴 요청의 Body, 페이팔 공식사이트에 적혀있는 파라미터와 동일해야한다. \"intent\" => \"sale\", \"redirect_urls\" => $request->redirect_urls, \"payer\" => [ \"payment_method\" => \"paypal\", // 한국은 paypal메소드밖에 지원이 안된다 ], \"transactions\" => $request->transactions, // client에서 보내준 결제 정보 ] ])->getBody()->getContents(); $decodeResult = json_decode($response); return response()->success($decodeResult); } public function execute(Request $request){ $response = $this->client->request('POST', $this->paymentUrl.'/'.$request->paymentId.'/execute' , [ 'headers' => [ \"Content-Type\" => \"application/json\", \"Authorization\" => $this->accessToken, ], 'json' => [ \"payer_id\" => $request->PayerID, ] ])->getBody()->getContents(); $decodeResult = json_decode($response); return response()->success($decodeResult); }} 이제 클라이언트에서 테스트 버튼을 클릭하면 미리 필자가 정의한 API인 /paypal/payment/create를 통해 필자의 API서버로 결제 데이터가 전송되고, API서버는 다시 그 데이터를 가지고 Paypal서버와 통신 후 클라이언트로 결과를 반환해 줄 것이다.그 후 클라이언트는 리턴된 데이터의 link[1]에 담긴 리다이렉트페이지를 그냥 열기만 하면 결제가 시작된다!왜 link[1]인지는 아래에서 다시 설명하겠다. 이후 사용자는 Paypal로그인 상태에 따라 로그인페이지 또는 결제 확인 페이지로 이동하게되며, Paypal의 결제 확인 창에서 Continue버튼을 누르면 아까 우리가 create API를 통해 보내놓았던 redirect_url페이지로 랜딩된다.이때 Paypal은 사용자가 최종 결제 확인을 했고, 이제 결제를 시작해도 된다는 데이터를 redirect_url뒤에 쿼리스트링의 형태로 붙혀서 보내준다. 이제 클라이언트와 서버의 기본적인 로직이 모두 준비되었다! 전체적인 플로우를 다시 설명하면 다음과 같다. 클라이언트에서 API서버로 결제 생성 요청을 보낸다. API서버는 받은 데이터를 가지고 다시 Paypal서버로 결제 생성 요청을 보내고 이후 클라이언트로 값을 반환해준다. 클라이언트는 반환받은 데이터에 있는 redirect_url로 사용자를 리다이렉트 시킨다. 이후 사용자는 이동한 Paypal페이지에서 로그인 및 간략한 결제정보 확인을 마치고 Continue버튼을 클릭한다. Continue버튼이 클릭되면 Paypal은 결제 생성 요청에 담겨있던 redirect_url로 사용자를 리다이렉트 시킨다. 이후 사용자는 리다이렉트된 페이지에서 최종 결제전 상세 결제 정보를 확인한다. 사용자가 “최종결제”버튼을 클릭하면 클라이언트는 결제 실행 요청을 API서버로 보낸다. API서버는 다시 Paypal서버로 이 요청을 전달하고, 클라이언트로 값을 반환해준다. 결제의 실행결과에 따라 사용자는 해당 페이지로 다시 리다이렉트된다. 뭔가 굉장히 복잡해 보이지만 잘 보면 API 서버는 거의 그냥 통신 셔틀이라고 보면 된다.그러면 그냥 Paypal서버와 다이렉트로 통신하면 안될까? 라고 생각할 수 있지만 이는 보안과 직결되는 문제이다.Paypal 서버와 통신을 하기 위해서는 client_key와 secret이 필요한데, API서 버를 중개하지 않고 Paypal서버와 바로 통신을 하려면 클라이언트가 이 2개의 값을 다 가지고 있어야 한다는 뜻이 된다.하지만 알다시피 웹 상에서 클라이언트 소스는 공개되기가 쉽고 난독화를 한다고 해도 Object의 key같은 String변수는 난독화되지 않기 때문에 악의를 가진 사용자가 손쉽게 client_key와 secret을 탈취할 수 있다.그래서 상대적으로 안전한 서버에 secret을 저장하고 클라이언트에는 client_key만 저장하는 식으로 2개의 값을 한번에 볼 수 없도록 나눠 놓는다. 실행결과먼저, API서버를 통해 create요청을 진행한 결과, 필자는 다음과 같은 response를 받을 수 있었다. links라는 배열을 하나 받았는데, 각 인덱스의 의미는 이렇다. 0 - 결제의 상세 정보를 GET메소드로 확인해볼 수 있는 API의 URI1 - Paypal의 결제페이지로 리다이렉트 시키는 URL2 - 결제를 실행시킬 수 있는 URL 아까 위에서 설명한 links[1]의 의미를 이제 알 수 있을 것이다.그리고 보내진 url의 host를 보면 전부 sandbox.*로 시작하는데 이는 현재 sandbox 계정을 사용하여 테스트를 진행하고 있다는 뜻이다. 실제 운영 API에는 저 sandbox 부분이 빠져있다.그리고 이제 links[1]의 url로 리다이렉트를 시키면 당연히(…) 저 값들은 먼지가 되어 사라지게 된다. 쿠키에 담든 어떻게 사용은 할 수 있겠지만 어차피 또 보내주니까 미련없이 버리자. 필자도 리다이렉트를 안시키고 팝업으로 저 페이지를 열어보았는데 401 Unauthrized를 뱉으며 결제정보가 장렬히 산화하는 모습을 볼 수 있었다….하지만 어차피 Paypal에 create요청을 날린 시점에서 Paypal서버에는 필자가 처음 보낸 redirect_url값이 저장되어있기 때문에 사용자가 Paypal페이지에서 결제확인을 완료하게 되면 다시 주도권은 필자 서비스로 돌아오게 된다. 자 이제 리다이렉트를 시켜보자. 그러면 이런 화면이 하나 뜬다. 만약 Paypal에 로그인이 안되어있다면 먼저 로그인페이지로 이동 후 로그인이 완료되면 이 페이지로 이동하게 될 것이다.이 페이지에서 유저는 자신이 결제할 금액을 확인할 수 있으며 오른쪽 상단에 있는 카트모양 아이콘을 누르면 조금 더 상세한 정보가 나오긴 한다. 근데 알아보기 힘들 정도로 간략하게 적혀있어서 딱히 의미는 없을 것 같았다.그리고 필자가 요청을 날린 통화단위는 EUR, 즉 유로화로 등록이 되어있던 상품이었지만 Paypal에서는 자동으로 사용자의 국가의 환율로 자동 환산을 해준다.이제 사용자가 모든 항목을 확인하고 Continue버튼을 누르면 필자가 처음 create시 보냈던 redirect_url로 이동이 시작된다. 이 페이지는 필자가 테스트를 위해 임시로 만들어 놓은 페이지이고 실제 운영서버였다면 이 페이지는 최종결제 전 마지막으로 정보를 확인할 수 있는 페이지가 될 것이다.이때 주목해야할 것은 이 페이지의 URL이다. 쿼리스트링으로 페이팔이 보내준 paymentId와 token, PayerID가 들어있다. 그럼 이제 저 값들을 사용해서 다음 플로우를 진행하면 된다.이 페이지의 Controller는 저 값들을 사용하여 디테일한 결제정보를 받아오는 로직과 결제를 실행하는 로직을 가진 메소드로 이루어져있다. 혹시 아까 위에서 정의한 Paypal서비스의 로직이 기억나지 않는다면 위에서 다시 보고 오자. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546(function() { 'use strict'; angular .module('app.pages.product') .controller('PaypalRedirectController', [ '$rootScope', 'Restangular', 'PaypalService', '$location', PaypalRedirectController ]); /** @ngInject */ function PaypalRedirectController( $rootScope, Restangular, PaypalService, $location ) { var vm = this; var queryString = $location.search(); // 페이팔에서 보내준 쿼리스트링을 가져온다 vm.init = (init)(); function init() { getPaymentInfo(); } /** * @public * @method executePaypal */ vm.executePaypal = executePaypal; function executePaypal() { // 실제 결제 실행 메소드 PaypalService.execute(queryString).then(function(res) { console.log('EXECUTE RESULT -> ', res); }); } /** * @private * @method getPaymentInfo */ function getPaymentInfo() { // 결제 정보 받아오는 메소드 PaypalService.getPaymentInfo(queryString).then(function(res) { console.log('GET PAYMENT INFO -> ',res); }); } }})(); 필자는 해당 페이지에 있는 버튼에 excutePaypal메소드를 클릭 이벤트로 걸어놓았다.이제 유저가 저 버튼을 클릭하면 최종 결제가 실행되고 결과를 response로 받아볼 수 있다. 정상적으로 결제가 진행되고 Paypal서버가 보내준 결과이다. payer.status가 approved라면 정상적으로 결제가 승인된 것이다. 이제 실제로 결제가 승인되었고 돈이 제대로 들어왔는지 확인해볼 차례이다. 페이팔의 샌드박스 페이지에 접속해서 구매자 계정으로 확인해본 상태이다.구매내역에 정상적으로 12유로가 출금 되었다고 나와있다. 판매자 계정에도 정상적으로 12유로가 들어왔다고 적혀있다. 샌드박스의 구매 테스트 계정. 12유로가 출금 되었다. 샌드박스의 판매 테스트 계정. 최상단에 필자 이름과 함께 12유로가 들어와있다. 하지만 아직 완전한 처리가 이루어지지 않은 상태라 Payment status는 Unclaimed로 되어있다. 아직은 저게 현금화 된게 아니라 그냥 Paypal서버에 들어가있는 데이터 쪼가리일 뿐이다.이제 Accept버튼을 눌러 판매자의 계좌로 입금을 진행하면 상태가 Completed로 전환되며 계좌에 12EUR가 USD로 환전되며 입금된다. 이상으로 Express Checkout에 대한 포스팅을 마친다.","link":"/2017/05/14/paypal-express-checkout/"},{"title":"Vue Server Side Rendering","text":"이번 포스팅에서는 Universal Server Side Rendering에 이어서 VueJS의 공식 라이브러리인 vue-server-renderer와 Express를 사용하여 SSR(Server Side Rendering) 어플리케이션을 개발한 과정과 운영 환경에서 생겼던 문제, 그리고 그 문제를 어떻게 해결했는지 적어보려고 한다. 필자는 Frontend 개발자로 일하면서 Backend 프레임워크를 건드릴 일이 사실 거의 없었다. 그러나 필자의 현 직장에 SSR 서버를 필자가 도입하자고 주장하였고, 따라서 오너쉽도 필자에게 있었기 때문에 클라이언트 환경과 전혀 다른 서버의 작동방식과 여러 문제점에 대해서 상세하게 알고 있어야 할 필요가 있었다.보통 Frontend 개발자는 클라이언트에서 작동하는 어플리케이션을 개발하기 때문에 서버에서 작동하는 어플리케이션에서 발생할 수 있는 (조금만 생각해보면 당연한)문제에 대해서 의외로 쉽게 놓치고 지나갈 수 있다고 생각한다.그래서 두번 다시 이런 실수를 반복하지 않도록 문서로 정리를 하고 회고하려고 한다. 먼저 Vue SSR의 렌더링 과정을 전체적으로 살펴본 후, 서버단 렌더링과 클라이언트단 렌더링을 나눠서 다시 살펴본다. 이 포스팅에 예제로 나와있는 코드는 현 직장의 비즈니스 로직 때문에 생략된 부분이 있기 때문에 코드를 복사붙혀넣기해도 작동하지않을 수 있습니다. Vue Server Side Rendering의 구조필자는 Nuxt.js를 사용하지 않고 보일러플레이트를 사용해서 약간 개선해서 구현했다. 처음에는 ‘그냥 Nuxt쓸걸…’이라고 후회하기도 했지만 그래도 덕분에 Universal SSR의 실행 과정을 더 깊게 알아볼 수 있는 좋은 기회였다고 생각한다.(라고 삽질을 포장해본다)해당 포스팅에서는 필자가 작성했던 SSR 어플리케이션의 초기화 과정에 대해서 함수단위까지 자세하게 기재하려고 한다. 먼저 어플리케이션의 렌더링 과정은 다음과 같다. 이후 각 과정에 대한 자세한 설명을 후술하도록 하겠다. 클라이언트가 서버에 리소스 요청 nginx가 Express가 띄워져있는 포트로 요청을 서빙 Express 라우팅 시작 server-entry.js 실행 서버의 vue-router 라우팅 진행 vue-server-renderer를 사용하여 HTML 렌더링 서버가 클라이언트로 응답 client-entry.js 실행 클라이언트 어플리케이션 초기화 함수 실행 클라이언트의 vue-router 라우팅 진행 app.$mount 1번 요청과 7번 응답을 제외한 2~6번 까지는 서버에서 일어나는 과정이고 8~10번 까지는 클라이언트에서 일어나는 과정이다. 특이한 점은 서버와 클라이언트의 엔트리 포인트가 다르다는 것이다. 그리고 후술하겠지만 이 엔트리 포인트들은 몇가지 같은 함수를 공유하며 사용한다. router.onReady나 createApp같은 함수들이 그렇다. 애초에 Universal SSR은 기본적으로 첫 요청만 서버 사이드 렌더링하고 이후는 SPA처럼 작동하게 하자. 그리고 코드는 서버랑 클라이언트에서 재사용가능하게 하자!라는 개념이다. 그래서 편한 면도 있지만 실행 타이밍이나 환경이 같은 함수라도 완전 달라질 수 있기 때문에 별도의 예외처리를 해줘야 하는 등 헷갈리는 부분도 많았다.그리고 이 두개의 엔트리포인트가 서버와 클라이언트에서 실행될 때 서로 다른 초기화과정을 거치는데, 서버에서 초기화를 하고 클라이언트에서 싹 다 처음부터 다시 초기화를 진행하게 되면 비효율적이므로 몇가지 방법을 사용하여 최대한 효율적으로 렌더를 수행한다. 먼저 서버사이드렌더링부터 살펴보자. Server Side Rendering클라이언트가 서버에 리소스 요청클라이언트에서 서버로 요청을 보낸다. nginx가 Express가 띄워져있는 포트로 요청을 서빙보통 nodeJS를 사용하여 서버를 개발할 때 node server.js와 같은 명령어로 바로 서버를 띄우는 경우는 드물고 보통 nginx나 apache와 같은 서버 엔진을 같이 사용한다.그 이유는 다음과 같다. 서버 엔진 소프트웨어의 특성 상 nodeJS보다 더 빠른 Static file serving이 가능하다. 그리고 그런 요청을 nodeJS까지 보내지 않고 엔진단에서 처리되므로 백엔드의 부하가 분산된다. Node.js의 창시자인 Ryan Dahl이 “You just may be hacked when some yet-unknown buffer overflow is discovered. Not that that couldn’t happen behind nginx, but somehow having a proxy in front makes me happy” 라는 말을 한 적이 있음. 즉, 아직 발견되지 않은 취약점에 의한 공격을 어느 정도 방지할 수 있다는 뜻이다. 그래서 대략 다음과 같은 nginx config를 작성하였다. 123456789101112131415161718192021server { listen 80; server_name example.com; location / { proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-NginX-Proxy true; proxy_pass http://127.0.0.1:3000/; proxy_redirect off; } gzip on; gzip_comp_level 2; gzip_proxied any; gzip_min_length 1000; gzip_disable \"MSIE [1-6]\\.\" gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript;} 요청이 80포트로 들어오면 node server.js로 실행된 nodeJS 서버가 대기하고 있는 3000포트로 포워딩 해준다.그리고 사실 nodeJS 서버를 실행시킬 때 pm2나 forever와 같은 프로세스 관리자를 사용해서 실행시키는 편이 좋은데, 이 내용은 다음 포스팅에 언젠가 작성하겠다. 필자는 pm2를 사용 중. Express 라우팅 시작이렇게 들어온 요청은 nodeJS서버인 server.js에서 처리하게 된다.server.js에는 Vue 코드는 없고 nodeJS로 작성된 Express 프레임워크의 코드가 작성되어있다. 12345678910111213141516171819202122232425262728293031323334353637383940const fs = require('fs');const express = require('express');const createRenderer = (bundle, template) => { return require('vue-server-renderer').createBundleRenderer(bundle, { template, runInNewContext: 'once', });};const bundle = require('./dist/vue-ssr-bundle.json');const template = fs.readFileSync(resolve('./dist/index.html'), 'utf-8');const renderer = createRenderer(bundle, template);const app = express();app.set('views', './src/express/views');app.set('view engine', 'ejs');app.get('/ping', (req, res) => { debug(`health check from ELB`); res.render('healthCheck');});const bundle = require('./dist/vue-ssr-bundle.json');const template = fs.readFileSync(resolve('./dist/index.html'), 'utf-8');const renderer = createRenderer(bundle, template);app.get('*', (req, res) => { if (!renderer) { return res.end('렌더링 중 입니다 뿜뿜'); } res.setHeader('Content-Type', 'text/html'); const context = { url: req.url, cookie: req.cookies }; if (!context.url) { errorLog('[ERR] context url is not exist!!', context); } // 렌더 스트림 진행 const stream = renderer.renderToStream(context);}); 필자는 Express를 사용했기 때문에 당연히 Express의 라우터를 사용했다. 그러나 실질적인 라우팅은 Vue가 진행하기 때문에 Express에서는 app.get('*')과 같이 와일드카드를 사용하여 모든 요청에 대한 콜백 함수를 실행하도록 한다. 중간에 보면 app.get('/ping')이라는 코드도 있는데 저건 AWS의 Elastic Beanstalk의 Health Check 때문에 별도로 작성한 라우터이다. ELB에서는 현재 환경이 제대로 작동하고 있는지를 체크하려고 그 환경에 속한 인스턴스들의 특정 URL로 주기적으로 ping을 날린다. 이 URL은 ELB의 설정에서 바꿔줄 수 있고, 필자는 /ping이라는 URL로 설정했다.굳이 이 라우터를 따로 나눈 이유는 vue-ssr-renderer의 render 함수가 많이 실행될 수록 메모리에 올라가는 HTML 템플릿이 많아질 것이고 그렇게 됨으로써 결국 렌더 과정에 병목이 발생하기 때문에 vue-ssr-renderer를 실행시키지 않고 Express만으로 간단한 페이지를 응답으로 보내주게 해놓은 것이다. Express라우팅과 밑에서 설명할 vue-router의 라우팅이랑 헷갈릴 수 있는데, 방금 설명한 대로 실질적인 라우팅은 Vue에서 진행하게 되지만 요청을 Express에서 먼저 받아 처리한 후에 Vue로 넘겨주는 순서이기 때문에 Express에서도 라우팅을 해줘야한다. 라우팅 후 마지막 줄의 renderToStream 메소드가 실행되고나면 Vue에서 진행되는 라우팅과 렌더링을 시작하게 된다. 이제 app.get('*') 라우터 내부를 자세하게 설명한다. 1234567891011121314151617181920212223242526272829303132333435363738app.get('*', (req, res) => { if (!renderer) { return res.end('렌더링 중 입니다 뿜뿜'); } res.setHeader('Content-Type', 'text/html'); const context = { url: req.url, cookie: req.cookies }; if (!context.url) { errorLog('[ERR] context url is not exist!!', context); } const stream = renderer.renderToStream(context); stream.on('data', () => { /* @desc * vue-meta 플러그인을 사용하면 컴포넌트에 선언되어있는 metaInfo 메소드에서 반환한 값을 받아올 수 있다. * https://github.com/declandewet/vue-meta 참고할 것 */ const { title, link, style, script, noscript, meta, } = context.meta.inject(); context.head = ` ${title.text()} ${meta.text()} ${link.text()} ${style.text()} ${script.text()} ${noscript.text()} `; }) .on('error', err => { debug(`렌더 중 에러 발생`); // 에러 페이지를 보여주는 등의 에러 핸들링 로직이 위치한다. }) .on('end', () => { debug(`렌더링 종료`); }) .pipe(res);}); 이 라우팅에서 가장 중요한 부분은 renderToStream 메소드의 역할이다.vue-ssr-renderer는 renderToString과 renderToStream이라는 2가지 렌더 함수를 가지고 있다.renderToString은 모든 렌더가 끝나면 렌더된 HTML을 string의 형태로 반환하고 그 이후 클라이언트로 HTML을 한번에 반환한다. 때문에 렌더 속도가 오래 걸리게 되면 유저는 빈 화면을 보고 있을 수 밖에 없다. 또한 데이터를 한번에 내려주기 때문에 HTML 렌더를 진행할 때 내용을 전부 다 메모리에 올려야한다는 단점이 있다. HTML의 크기가 작으면 문제가 되지 않겠지만 파일의 크기가 커질 수록 매 렌더링 시 메모리 공간을 많이 잡아먹는다. renderToStream은 한 이벤트가 끝날때마다 nodeJS의 ReadableStream객체를 반환한다. stream은 데이터를 일정한 chunk단위로 불러오고 on메소드를 사용한 이벤트 콜백 호출로 stream을 관리할 수 있는 nodeJS의 기능이다. data이벤트는 각 chunk가 readable상태가 될때마다 호출되며 모든 데이터를 불러왔다면 end이벤트가 호출된다.이 stream에 관한 내용은 추후 다른 포스트에서 언젠가 설명하도록 하겠다. server-entry.js 실행renderToStream함수가 실행되면 vue-server-renderer는 서버 쪽 엔트리 파일인 server-entry.js파일을 찾게된다. 이 파일에서는 app.js에 있는 팩토리 함수를 사용하여 app 객체를 생성하고 몇가지 초기화 과정을 거친 뒤 라우팅을 한다. 12345678import { createApp } from './app'; // 팩토리 함수 importexport default context => { return new Promise(async (resolve, reject) => { // 해당 프로미스에서 resolve되면 router.push가 호출된 후에도 stream이 계속 진행되고 // 해당 프로미스에서 reject되면 stream의 error이벤트가 호출된다. });}; 이 파일의 코드를 설명하기 위해서는 상단에 import된 createApp 함수에서 반환된 app, store, router가 뭔지 알고 있는 게 좋으므로, 자세히 살펴보기 전에 맨 위에서 import된 createApp 팩토리 함수를 먼저 살펴보자.createApp 함수는 Vue 인스턴스, vue-router의 VueRouter 인스턴스, Vuex의 Store 인스턴스를 반환하는 팩토리 함수이다.이후 이 팩토리 함수는 client-entry에도 재사용되어 초기화를 진행하게 된다. 12345678910111213import Vue from 'vue';import App from './App.vue';import Store from './stores';import { Router } from './router';export function createApp() { const store = Store(); const router = Router(); const app = new Vue({ router, store, render: h => h(App) }); return { app, router, store, }} 일반적으로 클라이언트에서 Vue를 초기화하는 코드와 비슷하지만 다른 부분이 하나 있는데, store와 router 인스턴스를 팩토리 함수를 사용해서 생성한다는 점이다. 보통 SPA 어플리케이션에서는 1234567export default new Vue({ el: '#app', components: { App }, template: '', router: new VueRouter({ ... }), store: new Vuex.Store({ ... }),}); 이런 식으로 Vue인스턴스를 생성한다. 하지만 이 로직을 그대로 서버에서 사용하기엔 문제가 하나 있다. 문제는 export default로 call by reference 평가전략을 사용하는 자료형을 반환하게 될 때 발생한다. new Vue()에서 호출하는 Vue는 인스턴스를 반환하는 클래스같이 작동하기 때문에 해당 코드는 최종적으로 Vue 인스턴스가 올라간 메모리 주소를 반환하게 되는데, 이 로직은 클라이언트에서는 딱히 문제가 없지만 서버에서는 문제가 발생할 수 있다. 클라이언트와 다르게 서버는 한번 올라가면 오랜 시간동안 계속 돌아가는 프로그램 이라는 것이다. 현재 서버에 접속해있는 유저들이 Store의 상태를 공유하면 안되기 때문에 서버는 각 요청에 대해서 새로운 Store와 Vue 인스턴스를 생성해야한다.하지만 위의 코드에서 export하는 것은 결과적으로 Vue인스턴스의 메모리 포인터이고 위 모듈이 import 될때 이 모듈은 처음 한번만 Vue인스턴스를 생성하고 이후는 참조해야하는 메모리 포인터, 즉 같은 인스턴스 를 반환하게 된다.그렇기 때문에 서버 사이드 렌더링 때는 상태오염을 피하기 위해, 인스턴스의 메모리 포인터가 아닌 팩토리 함수를 노출시키고 매번 새로운 인스턴스를 생성해 반환하는 방법으로 작성하여야 한다. 필자는 이 사실을 놓쳐서 유저들이 Store내부의 세션을 공유하게 되서 내 계정으로 로그인했지만 다른 사람 계정으로 로그인되버리는 버그를 생성한 적이 있다. 지금 생각해도 아찔한 순간이다. A Node.js server is a long-running process. When our code is required into the process, it will be evaluated once and stays in memory.…So, instead of directly creating an app instance, we should expose a factory function that can be repeatedly executed to create fresh app instances for each request. Avoid Stateful Singletons - Vue SSR Guide 심지어 이렇게 공식 문서에도 버젓히 적혀있는 걸 놓쳐서 엄청난 버그를 내고 말았다. 공식 문서를 반드시 읽읍시다! 두번세번 읽읍시다! 자, 이제 createApp를 살펴보았으니 다시 server-entry.js로 돌아와서 해당 파일에 대한 설명을 계속 이어가겠다. 1234567891011121314151617181920212223242526272829303132import { createApp } from './app'; // 팩토리 함수 importimport { TOKEN_KEY } from 'src/constants';import { SET_TOKEN, DESTROY_TOKEN } from 'src/stores/auth/config';import APIAuth from 'src/api/auth';export default context => { return new Promise(async (resolve, reject) => { const { router, store } = createApp(); // 새로운 앱 생성 const cookies = context.cookie; const authToken = cookies[TOKEN_KEY]; // 요청을 보낸 클라이언트의 쿠키에 있는 토큰 const next = () => { router.push(context.url); }; if (authToken) { try { await APIAuth.isValidToken(authToken); // valid하면 200, invalid하면 400 store.dispatch(SET_TOKEN, authToken); } catch (e) { console.error(e); // throw하면 렌더 실패로 간주된다. 하지만 토큰이 invalid하다고 렌더 자체를 실패시키면 안된다. store.dispatch(DESTROY_TOKEN); } } router.onReady(() => { // 라우팅 로직이 위치 }, reject); next(); });}; 이 파일의 메인 로직은 크게 2가지로 나누어 진다. 요청을 보낸 클라이언트의 쿠키에 토큰이 저장되어있을 경우 store.dispatch(SET_TOKEN, authToken)로 Store에 인증상태를 저장 router.onReady로 선언된 서버 측 라우팅 로직 및 예외처리 먼저 1번부터 살펴보자. 왜 굳이 인증된 토큰을 Store에 담아야 할까? 먼저 이 서버는 렌더링만을 수행하는 렌더서버이기 때문에 세션의 유효성 검사는 외부에 있는 API서버와 통신을 해서 수행해야한다. 인증상태는 서버에서도 필요할 수 있고 클라이언트에서도 필요할 수도 있는데, 그럼 서버에서 한번 통신해서 토큰을 검사하고 클라이언트에서도 또 통신을 해서 토큰을 검사해야한다. 하지만 이런 방식은 비효율적이기 때문에 보통 이런 유니버셜 SSR을 지원하는 프레임워크에서는 서버의 상태를 클라이언트로 반환해주는 방법으로 window객체에 서버의 상태를 직렬화해서 렌더 시 태그 안에 선언해주는 방식을 사용한다.vue-server-renderer에서는 클라이언트에 반환할 서버의 상태를 Vue의 Flux아키텍처 라이브러리인 Vuex를 사용하여 선언한다.그렇게 서버의 상태는 렌더 시 JSON.stringify를 사용하여 직렬화되어 window.__INITIAL_STATE__라는 프로퍼티에 담기게 되고, 이후 클라이언트 초기화 시 해당 프로퍼티에 접근해 JSON.parse를 사용하여 Object형으로 형변환 후 Vuex Store의 replaceState메소드를 사용해 Store를 업데이트하게 된다. 브라우저 콘솔에서 이렇게 확인해볼 수 있다 서버의 vue-router 라우팅 진행다음 2번이었던 라우팅 로직을 살펴보자. Universal SSR 어플리케이션은 맨 처음 사용자가 페이지를 열었을 때는 서버 쪽에서 라우팅을 진행하고 그 이후 사용자가 페이지를 이동할때는 클라이언트에서 라우팅을 진행하게된다.즉 server-entry.js 내부의 라우팅 로직은 맨 처음 사용자가 어플리케이션을 초기 실행시킬 때 딱 한번 실행되는 로직이라는 의미이다. 필자는 서버에서는 이 라우터에 연결된 컴포넌트가 있는지에 대한 검사만 진행하고 클라이언트에 라우터 인증 관련 로직을 작성했기 때문에 서버 쪽 엔트리의 라우팅 로직은 간단하게 작성했다. 이 파일에서 사용된 router객체는 createApp 팩토리 함수에서 생성되어 반환된 vue-router 라이브러리 내 VueRouter클래스의 인스턴스이다. 필자는 이 클래스의 getMatchedComponents 메소드를 사용해서 현재 라우트가 유효한 라우트인지만 검사하기로 했다.VueRouter의 멤버변수와 메소드의 의미는 vue-router의 공식 문서에도 나와있지만 가끔씩 라이브러리는 업데이트가 되었으나 공식 문서는 업데이트가 늦는 경우도 있으므로 필자는 직접 vue-router의 코드를 살펴봤다.node_modules/vue-router/types/router.d.ts 파일을 살펴보면 VueRouter 클래스의 멤버 변수와 메소드를 확인할 수 있다. 123456789101112131415161718192021222324252627282930declare class VueRouter { constructor (options?: RouterOptions); app: Vue; mode: RouterMode; currentRoute: Route; beforeEach (guard: NavigationGuard): Function; beforeResolve (guard: NavigationGuard): Function; afterEach (hook: (to: Route, from: Route) => any): Function; push (location: RawLocation, onComplete?: Function, onAbort?: Function): void; replace (location: RawLocation, onComplete?: Function, onAbort?: Function): void; go (n: number): void; back (): void; forward (): void; getMatchedComponents (to?: RawLocation | Route): Component[]; onReady (cb: Function, errorCb?: Function): void; onError (cb: Function): void; addRoutes (routes: RouteConfig[]): void; resolve (to: RawLocation, current?: Route, append?: boolean): { location: Location; route: Route; href: string; // backwards compat normalizedTo: Location; resolved: Route; }; static install: PluginFunction;} getMatchedComponent메소드는 RawLocation타입이나 Route타입을 인자로 받아서 Component 리스트를 반환해주는 메소드라는 것을 확인할 수 있다. 그럼 이제 node_modules/vue-router/dist/vue-router.common.js파일에서 getMatchedComponent이 어떻게 구현되어있는지 확인해보자. 123456789101112131415VueRouter.prototype.getMatchedComponents = function getMatchedComponents (to) { var route = to ? to.matched ? to : this.resolve(to).route : this.currentRoute; if (!route) { return [] } return [].concat.apply([], route.matched.map(function (m) { return Object.keys(m.components).map(function (key) { return m.components[key] }) }))}; VueRouter클래스의 getMatchedComponent라는 메소드는 to 인자를 받으면 해당 라우트와 매치된 컴포넌트를 반환하고, 인자가 주어지지 않는다면 현재 라우트에 매치된 컴포넌트를 반환하도록 되어있다. VueRouter 클래스의 타입 선언부에서 확인한 대로 to인자에는 optional을 의미하는 ?가 붙어있었기 때문에 필요한 경우가 아니면 굳이 인자를 넘겨줄 필요는 없을 것 같다. 이제 router.onReady이벤트훅 내부를 한번 작성해보자. 12345678910111213141516router.onReady(() => { /** * @desc 현재 라우터에 연결되어 있는 컴포넌트가 없다면 reject함으로써 * nodeJS stream의 error이벤트가 호출되고 별도로 작성해놓은 errorHandler가 404페이지가 렌더 될 것이다. */ const matchedComponents = router.getMatchedComponents(); if (!matchedComponents.length) { return reject({ code: 404, msg: `${router.currentRoute.fullPath} is not found`, }); } else { resolve(app); }}, reject); 얼추 된 것 같다. 하지만 필자의 어플리케이션은 asyncData라는 프로퍼티를 사용하여 라우팅을 진행하기 전에 비동기로직을 기다릴 수 있도록 작성이 되어있다. Vue의 SSR라이브러리인 Nuxt에서도 비슷한 방식을 사용했던 것 같은데 이 부분은 잘 기억이 나지않는다.어쨌든 현재 라우트에 매치된 컴포넌트리스트 중 asyncData를 가지고 있는 컴포넌트가 있다면 Promise를 사용해서 기다리도록 만들어주면 되는 간단한 로직이기 때문에 Promise.all을 사용하여 다음과 같이 작성하였다. 12345678910111213141516171819202122232425router.onReady(() => { /** * @desc 현재 라우터에 연결되어 있는 컴포넌트가 없다면 404페이지를 렌더한다. */ const matchedComponents = router.getMatchedComponents(); if (!matchedComponents.length) { return reject({ code: 404, msg: `${router.currentRoute.fullPath} is not found`, }); } // start: 추가된 부분 Promise.all(matchedComponents.map(Component => { if (Component.asyncData) { return Component.asyncData({ route: router.currentRoute, store, }); } })).then(() => { /** @desc * context에 state를 넘겨주고 렌더러에`template` 옵션을 사용하면 context.state를 직렬화하여 `window .__ INITIAL_STATE__`로 HTML에 주입해준다. */ context.state = store.state; resolve(app); }).catch(reject); // end: 추가된 부분}, reject); 그리고 모든 라우팅이 완료되었을 때 context.state = store.state처럼 context.state에 store상태를 담아주면 vue-server-renderer가 알아서 window.__INITIAL_STATE__에 상태를 주입해준다. vue-server-renderer를 사용하여 HTML 렌더링12345678910stream.on('error', err => { return errorHandler(req, res, err, bugsnag);}).on('end', () => { debug(`render stream end ==============================`); debug(`${Date.now() - s}ms`); debug('================================================');}).pipe(res); 이렇게 server-entry.js에서 Promise.resolve가 호출되어 초기화가 끝나면 아까 선언해놓았던 server.js의 stream의 end이벤트가 실행되고나서 체이닝되어있는 pipe메소드가 실행된다. 서버가 클라이언트로 응답위 과정을 거친 후 렌더가 끝난 HTML을 클라이언트로 전송한다. Client Renderingclient-entry.js 실행클라이언트에서 서버 렌더링이 완료된 HTML과 entry.js를 받아온 후 클라이언트 렌더링이 시작된다. 이때 웹팩이 컴파일할때 클라이언트단 엔트리 포인트로 잡는 파일은 client-entry.js이다.먼저 client-entry.js파일의 init 함수를 살펴보자. 클라이언트 어플리케이션 초기화 함수 실행123456789101112131415161718192021222324import { createApp } from './app';import { LOGIN } from 'src/stores/auth/config';const { app, router, store } = createApp();const init = async function () { /** @desc 서버의 스토어의 클라이언트 스토어의 동기화 */ if (window.__INITIAL_STATE__) { store.replaceState(window.__INITIAL_STATE__); } /** @desc 토큰 존재 여부 확인 후 로그인 처리 */ const hasToken = store.state.auth.authToken; if (hasToken) { try { await store.dispatch(LOGIN); } catch (e) { // 쿠키 내 토큰을 삭제하는 등의 별도 예외처리 } } return Promise.resolve();}; 현 직장의 코드다 보니까 전체를 적지는 못했지만 init함수가 수행하는 로직은 서버의 스토어 상태를 클라이언트에 반영과 사용자 인증처리이다. 서버의 스토어 상태를 받아오는 원리는 4. server-entry.js 실행에서 설명했으니, 이번에는 왜 로그인처리를 서버에서 하지않고 클라이언트에서 하는가?에 대해서 설명해보려고 한다. 그 이유는 이 서버가 별도의 인증 로직을 가지고 있지 않은 렌더 서버이기 때문에 유저 정보를 가져오거나 인증 여부를 확인하거나 하는 작업은 모두 외부의 API 서버에 의존하고 있기 때문이다. 처음에 이 사실을 간과하고 서버 렌더링 시 API 통신을 한 후 유저데이터를 클라이언트로 내려주는 방법을 택했는데, 다음과 같은 문제가 발생했다. HTML 템플릿 렌더시간에 API 통신시간이 포함되었다. (렌더가 끝나면 서버렌더링의 라이프사이클도 같이 끝나기 때문에 렌더 중간에 await를 사용하여 API 통신을 동기처리할 수 밖에 없다. 심지어 인증된 유저 정보 GET API가 꽤 느린 편) vue-ssr-renderer의 render 메소드의 수행시간이 늘어났다. render메소드의 수행시간이 늘어나면서 한번에 메모리에 올라가는 템플릿이 많아졌다. 메모리가 꽉 차서 더 이상 렌더링을 수행하지 못한다. 서버가 응답을 하지 못한다. Fail 그래서 이 렌더 서버를 구축할 때 가장 집중했던 부분은 render 메소드의 수행시간 단축이었고, 그 결과 유저 데이터를 받아오는 로직을 클라이언트로 내리게 되었다. 나중에 생각해보니 현재 인증된 사용자의 데이터가 필요한 뷰는 SEO가 필요없는 부분이라서 굳이 서버에서 할 필요가 없었다. 이제 마지막으로 클라이언트의 라우팅을 살펴보자. 클라이언트의 vue-router 라우팅 진행client-entry.js에는 클라이언트 사이드의 전역 라우터도 같이 선언이 되어있다. 12345678910111213141516171819202122232425262728293031import { createApp } from './app';import { LOGIN } from 'src/stores/auth/config';const { app, router, store } = createApp();const init = async function () {...};router.onReady(async () => { await init(); router.beforeEach((to, from, next) => { const matched = router.getMatchedComponents(to); const prevMatched = router.getMatchedComponents(from); let diffed = false; const activated = matched.filter((c, i) => { return diffed || (diffed = (prevMatched[i] !== c)); }); if (!activated.length) { next(); } Promise.all(activated.map(c => { if (c.asyncData) { return c.asyncData({ store, route: to }); } })).then(() => { /* LOADING INDICATOR */ next(); }).catch(next); }); app.$mount('#app');}); 사실 라우터 부분은 sever-entry.js에 있던 라우팅 로직 부분과 별로 다르지 않다.하나 차이점이 있다면 client-entry.js의 라우팅에서는 현재 라우터의 컴포넌트와 이전 라우터의 컴포넌트를 비교하는 로직이 있다는 것이다.첫 요청 시 라우팅이 단 한번 일어나는 서버 렌더링과 다르게 클라이언트의 라우팅은 사용자의 액션에 따라서 여러 번 일어나게된다. 클라이언트 렌더링은 라우터가 변경되었을 때 컴포넌트가 변경된 부분만 새로 렌더하고 나머지는 그대로 유지하기 때문에 다음 라우터에는 현재 라우터에 있던 컴포넌트를 그대로 사용하고 있을 수 있다.중요한 점은 클라이언트에서도 asyncData를 서버와 마찬가지로 라우팅이 완료되기 전에 데이터를 fetch해오는 용도로 사용되고 있다는 점이다.즉, 현재 라우터에 존재하는 컴포넌트가 다음 라우터에도 존재한다면 굳이 그 컴포넌트의 asyncData에서 중복되는 로직을 수행할 필요가 없기 때문에 라우터가 변경될 때 컴포넌트를 비교하는 로직을 수행한 후, 달라진 컴포넌트의 asyncData만 수행하도록 로직을 작성해야한다. app.$mount -> 렌더 종료. Vue 라이프사이클 시작그 후 마지막에 app을 #app DOM에 직접 마운트하면 클라이언트 사이드의 Vue 라이프사이클이 시작된다.마지막으로 해당 프로젝트 보일러 플레이트의 Github 링크를 첨부한다. 이상으로 Vue SSR 포스팅을 마친다.","link":"/2018/09/25/vue-ssr/"},{"title":"JavaScript와 함께 해시테이블을 파헤쳐보자","text":"이번 포스팅에서는 많이 사용되는 자료구조 중 하나인 해시 테이블(Hash Table)에 대해서 정리하려고 한다. 먼저 해시 테이블이 무엇인지, 왜 사용하는지 알아보자! 해시 테이블(Hash Table)이 뭔가요?해시 테이블은 어떤 특정 값을 받으면 그 값을 해시 함수에 통과시켜 나온 인덱스(index)에 저장하는 자료구조이다. 보통 배열을 사용해서 구현하는 경우가 많은 것 같다. 일단 해시 함수가 뭐길래 사용한다는 건지 해시가 뭔지 설명하기 전에 해시 테이블이라는 개념이 어디서부터 출발한 것인지 알아보자. 직접 주소 테이블(Direct Address Table)해시 테이블의 아이디어는 직접 주소 테이블이라는 자료구조에서 부터 출발한다. 직접 주소 테이블은 입력받은 value가 곧 key가 되는 데이터 매핑 방식이다. 코드로 보면 더 이해가 쉽다. 123456789101112131415161718192021222324class DirectAddressTable { constructor () { this.table = []; } setValue (value = -1) { this.table[value] = value; } getValue (value = -1) { return this.table[value]; } getTable () { return this.table; }}const myTable = new DirectAddressTable();myTable.setValue(3);myTable.setValue(10);myTable.setValue(90);console.log(myTable.getTable()); 만약 데스크톱으로 이 포스팅을 보고 있다면 이 코드를 복붙한 후 브라우저 콘솔이나 NodeJS로 실행해보자.(물론 IE에서는 안돌아간다)그러면 콘솔에 우리의 이쁜 테이블이 출력된다. 1[ , 3, , 10, , 90 ] 우리가 3을 테이블에 넣으면 이 값은 배열의 3번 인덱스의 요소가 되고 90을 넣으면 90번 인덱스의 요소가 된다. 그야말로 초 심플하다.이렇게 직접 주소 테이블을 사용할때는 들어오는 값이 뭔지 알면 이 값이 저장된 인덱스도 함께 알 수 있기 때문에 저장된 데이터에 바로 접근해서 값을 가져올 수 있다. 1myTable.getValue(3); // 3 찾고자 하는 값과 테이블의 인덱스가 동일하므로 테이블을 뒤적거릴 필요없이 값이 저장된 공간에 바로 접근해서 값을 가져올 수 있으므로 시간복잡도는 $O(1)$이다. 마찬가지로 테이블에 있는 값을 삽입, 수정, 삭제하는 행위도 값이 어디 있는지만 알고있으면 모두 한방에 해결할 수 있으므로 역시 $O(1)$의 시간복잡도로 해결할 수 있다. 보통 이런 단순한 자료구조에서 값을 탐색, 삽입, 수정, 삭제하는 알고리즘이 시간을 잡아먹게 되는 이유는 대부분 비슷비슷하다. 내가 찾고 싶은 값이 어디 있는지 모른다. 일단 효율적으로 뒤져보자.(이진트리탐색 같은 경우) 내가 이 값을 삽입하거나 삭제하면 다른 값이 영향을 받는다.(링크드 리스트 같은 경우) 이렇게 직접 주소 테이블은 내가 보고 싶은 값이 어디 있는지 알고 있기 때문에 바로 접근해서 이후 작업을 수행할 수 있다는 점에서 굉장히 편리하다고 할 수 있다. 하지만 직접 주소 테이블도 당연히 단점이 있다. 바로 공간의 효율성이 좋지 않다는 것이다.방금 선언했던 myTable의 테이블 상태를 한번 보면 이해가 바로 된다. 이 테이블에는 3, 10, 90의 값을 넣었고 이 값들은 크기 차이가 꽤나 큰 편이다. 그 결과 우리의 myTable은 이렇게 듬성듬성한 구조로 데이터를 저장하게 된 것이다. 123456Array(91) [ 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 90] 윌리를 찾아라 Array 버전. 값이 어디에 저장되었을까요? 위에서도 볼 수 있듯이 저장된 데이터를 제외하고 0으로 채워진 나머지 공간은 값은 없지만 메모리 공간은 할당되어 있는 상태인 것이다. 즉, 사용하지 않는 아까운 공간이다. 즉 테이블에 넣고자 하는 데이터의 값의 범위보다 값의 개수가 작다면 공간적인 효율이 떨어지는 것이다. 이런 상황을 적재율이 낮다라고 표현하는데, 적재율은 값의 개수/테이블의 크기로 나타내게 된다. 필자가 방금 만든 이 테이블의 현재 적재율은 3/91 = 0.03296...으로 약 3% 정도이므로 높은 적재율은 아니라고 볼 수 있다. 만약 1000과 같이 큰 값이 하나만 더 테이블에 들어온다고 해도 테이블의 크기는 1001이 되고 적재율은 0.003996...으로 약 0.4%가 된다. 즉, 직접 주소 테이블이 큰 힘을 발휘할 수 있는 순간은 1, 2, 3과 같이 연속적인 값을 저장하거나 혹은 값들의 범위 차이가 크지 않은 데이터라고 할 수 있다. 직접 주소 테이블의 단점을 해시 함수로 보완하자!이렇게 직접 주소 테이블은 값에 접근하기는 편하지만 공간 효율이 좋지 않다는 단점이 있다. 그래서 이 단점을 보완한 게 바로 해시 테이블인 것이다. 해시 테이블은 직접 주소 테이블처럼 값을 바로 테이블의 인덱스로 사용하는 것이 아니라 해시 함수(Hash Function)이라는 것에 한번 통과시켜서 사용한다. 해시 함수는 임의의 길이를 가지는 임의의 데이터를 고정된 길이의 데이터로 매핑하는 함수이다. 이때 이 함수가 뱉어내는 결과물을 해시(Hash)라고 부른다. 해시(Hash)의 사전적 정의는 사실 “잘게 썬 요리” 같은 뉘앙스로 사용된다. 롯데리아에서 파는 해시브라운을 생각해보자.해시 함수도 어떤 값이 들어오든 간에 다 뭉개서 내가 원하는 길이의 값으로 만든다는 점에서 일맥상통한다. 그럼 한번 이해를 돕기위해 간단한 해시 함수를 만들어보겠다. 12345678function hashFunction (key) { return key % 10;}console.log(hashFunction(102948)); // 8console.log(hashFunction(191919191)); // 1console.log(hashFunction(13)); // 3console.log(hashFunction(997)); // 7 이게 끝이다. 물론 허접한 해시 함수이지만 그래도 해싱이라는 본연의 역할을 잘 수행하는데는 별로 문제가 없다.필자의 허접한 해시 함수는 무조건 들어온 값을 10으로 나눈 후의 나머지를 반환하는 일을 할 뿐이지만 어떤 값이 들어오든 그 값의 1의 자리만 반환될 것이기 때문에 반환되는 값은 무조건 0~9 사이의 값이라는 것이 보장된다. 그리고 해시 함수의 특성 중 하나가 해싱만 보고는 인자로 어떤 값을 받았는지 추측하기 힘들다는 것이다. 1의 자리만 보고 원래 수가 얼마였는지 맞출 수는 없을 것이다. 이러한 해시 함수의 특징은 암호학에서도 아주 잘 사용하고 있다. 자, 하지만 우리에게 중요한건 암호학이 아니라 바로 이것이다. 어떤 값이 해시 함수로 들어오든 무조건 0~9사이의 값이 반환된다! 직접 주소 테이블의 단점이 바로 10000이라는 값이 하나만 들어오더라도 10000번 인덱스에 값을 저장하기 위해 10000의 크기를 가진 테이블을 생성해야하기 때문에 나머지 9999개의 버리는 공간이 생기는 것이다. 그러나 필자의 해시 함수를 사용하면 100이 들어오면 0을 반환할 것이고 10001이 들어오면 1을 반환, 심지어 8982174981274가 들어와도 4를 반환한다. 즉, 고정된 테이블의 길이를 정해둘 수 있고 그 안에만 데이터를 저장할 수 있게 된 것이다. 해시 함수의 이러한 성질을 이용해서 아주 간단한 해시테이블을 한번 작성해보겠다. 필자는 해시 테이블 크기를 5로 설정하고 어떤 값이 들어와도 이 테이블 안에 저장될 수 있도록 해시 함수를 작성할 것이다. 12345678910111213const myTableSize = 5;const myHashTable = new Array(myTableSize);function hashFunction (key) { // 들어온 값을 테이블의 크기로 나눠주고 나머지를 반환하면 된다. return key % myTableSize;}myHashTable[hashFunction(1991)] = 1991;myHashTable[hashFunction(1234)] = 1234;myHashTable[hashFunction(5678)] = 5678;console.log(myHashTable); // [empty, 1991, empty, 5678, 1234] 들어온 값들은 1991, 1234, 5678로, 해시 테이블의 사이즈인 5보다 훨씬 큰 값이지만 해시 함수를 거친 결과 0~4 사이의 값만 반환되기 때문에 필자의 작고 귀여운 해시 테이블 안에 값이 차곡차곡 저장될 수 있다. 내 해시 테이블…자그마해 귀여워 이로써 직접 주소 테이블의 단점이었던 밑도 끝도 없이 낭비되는 공간을 줄일 수 있게 되었다! 해시의 충돌(Collision)이렇게 해피 엔딩으로 끝나면 좋겠지만 해시 테이블의 단점도 있다. 그 단점은 바로 해시의 충돌이다. 충돌이 뭔지 설명하기 전에 필자의 해시 함수를 가지고와서 한번 충돌을 일으켜보고 직접 확인해보자. 12hashFunction(1991) // 1hashFunction(6) // 1 다른 값을 해시 함수에 넣었지만 같은 값이 튀어나오는 것이 바로 충돌(Collision)이다. 사실 상식적으로 생각해보면 직접 주소 테이블은 동적으로 테이블을 늘려나갔지만 해시 테이블은 처음부터 고정적인 공간을 할당하고 값을 계속 우겨넣는 방식이다.그렇다면 테이블의 크기를 100으로 잡고 그 테이블에 200개의 데이터를 넣는다면 100개만 저장되고 100개는 남을텐데 얘네는 어떻게 되는걸까? 걱정하지 말자. 애초에 해시 테이블은 담고자 하는 데이터의 개수보다 테이블의 크기를 작게 하고 싶다는 의지에서 나온 자료구조이기 때문에 충돌을 해결할 수 있는 방법 또한 같이 고안되었다. 충돌 해결하기이처럼 해시 테이블에는 해시의 충돌이라는 단점이 있기 때문에 해시 테이블을 운용할 때 가장 중요한 것은 사실 해시 함수가 얼마나 균일하게 값을 퍼트릴 수 있느냐이다. 어떤 값을 넣어도 같은 인덱스만 주구장창 나올 확률이 높다면 좋은 해시 함수가 아니라는 것이다.그러나 해시 함수를 아무리 잘 짜더라도 근본적으로 충돌을 완전히 방지한다는 것은 힘든 일이다. 그렇기 때문에 어느 정도는 충돌을 감안하되 최소화하기 위해 해시 함수의 알고리즘을 개발하거나, 혹은 충돌이 발생하더라도 우회해서 해결할 수 있는 방법을 사용한다. 그 중 이 포스팅에서는 충돌이 발생하더라도 우회해서 해결하는 방법 중 몇가지를 설명하려고 한다. 개방 주소법(Open Address)개방 주소법은 해시 충돌이 발생하면 테이블 내의 새로운 주소를 탐사(Probe)한 후, 비어있는 곳에 충돌된 데이터를 입력하는 방식이다. 해시 함수를 통해서 얻은 인덱스가 아니라 다른 인덱스를 허용한다는 의미로 개방 주소(Open Address)라고 한다. 개방 주소법은 어떤 방식으로 비어있는 공간을 탐사할 것이냐에 따라 4가지로 나누어 진다. 1. 선형 탐사법(Linear Probing)선형 탐사법(Linear Probing)은 말 그대로 선형으로 순차적으로 탐사하는 방법이다. 위에서 해시 충돌의 예로 들었던 1991과 6의 상황을 한번 예시로 알아보자. 0 11991 2 3 4 5 6 처음에 1991을 해시 함수에 통과 시킨 후 해시 테이블에 넣었을 때에는 테이블의 1번 인덱스에 위치했을 것이다. 그 이후 6을 해시 함수에 통과시켰더니 또 1이 나왔다. 하지만 이미 1번 인덱스에는 1991이 들어가 있기 때문에 6은 더 이상 해시 테이블에 들어갈 자리가 없게 되었다. 충돌이 발생한 것이다! 선형 탐사법은 이렇게 충돌이 났을 때 정해진 $n$ 칸만큼의 옆 방을 주는 방법이다. 만약에 $n = 1$이라면 2번 인덱스를, $n = 3$이라면 4번 인덱스에 6을 저장할 것이다. 0 11991 26 3 4 5 6 아쉬운대로 옆 방이라도 들어가야지... 이런 식으로 충돌이 났을 때 순차적으로 정해진 만큼의 옆 방을 주는 것이 바로 선형 탐사법이다. 만약 여기서 또 충돌이 발생한다면 이번에는 그 값을 3번 인덱스에 저장할 것이다. 이런 방식으로 빈 공간이 나타날 때까지 순차적으로 탐사를 한다.선형 탐사법의 단점은 특정 해시 값의 주변이 모두 채워져있는 일차 군집화(Primary Clustering)문제에 취약하다는 것이다. 0 11991 26 313 421 5 6 같은 해시가 여러 번 나오는 경우 선형 탐사법을 사용하면 데이터가 연속되게 저장될 가능성이 높아진다. 즉, 데이터의 밀집도가 높아진다는 것이다. 이런 경우 해시의 값이 1이 나왔을 때 뿐만 아니라 2나 3이 나왔을 때도 충돌이 발생한다. 이게 진짜 악순환의 반복인데, 이런 식으로 충돌이 계속 될 수록 데이터가 연속되게 저장되기 때문에 나중에 가면 데이터가 밀집되어 있는 거대한 덩어리가 생긴다. 그럼 해시로 어떤 값이 나오더라도 그 덩어리가 차지한 인덱스와 충돌이 날 확률이 올라가고, 충돌난 값은 또 그 덩어리 뒤에 저장되게 되므로 데이터 덩어리가 더 커진다. 충돌! -> 데이터 덩어리 뒤에 충돌난 값 저장 -> 충돌 발생 확률 증가 -> 충돌! -> 또 저장. 덩어리 더 커짐 -> 충돌 발생 확률 증가 -> 충돌… 이런 식으로 눈물나는 무한 반복 사이클이 발생한다. 이것이 Primary Clustering이다. 이제 이 눈물나는 문제점을 그나마 보완한 다른 방법을 알아보자. 2. 제곱 탐사법(Quadratic Probing)제곱 탐사법(Quadratic Probing)은 선형 탐사법과 동일하지만 탐사하는 폭이 고정폭아닌 제곱으로 늘어난다는 것이 다르다.첫번째 충돌이 발생했을 때는 충돌난 지점으로 부터 $1^2$만큼, 두번째 충돌이 발생했을 때는 $2^2$, 세번째는 $3^2$과 같은 식으로 탐사하는 스텝이 빠르게 커진다.선형 탐사법때와 동일한 상황에서 제곱 탐사법을 사용하면 해시 테이블은 아래와 같은 모양이 된다. 0 11991 26 3 4 513 6 ... 1021 첫번째 충돌이 났을 때 6은 충돌이 난 1번 인덱스로 부터 $1^2 = 1$만큼의 옆 방에 들어간다.두번째 충돌이 났을 때 13은 충돌이 난 1번 인덱스로 부터 $2^2 = 4$만큼의 옆 방에 들어간다.세번째 충돌이 났을 때 21은 충돌이 난 1번 인덱스로 부터 $3^3 = 9$만큼의 옆 방에 들어간다. 이렇게 제곱 탐사법을 사용하면 충돌이 발생하더라도 데이터의 밀집도가 선형 탐사법보다 많이 낮기 때문에 다른 해시값까지 영향을 받아서 연쇄적으로 충돌이 발생할 확률이 많이 줄어든다.그래도 결국 해시로 1이 여러번 나오면 계속 충돌이 나는 것은 피할 수 없다. 결국 데이터의 군집은 피할 수 없는 숙명이므로 이 현상을 이차 군집화(Secondary Clustering)이라고 부른다. 그럼 여기서 더 개선할 수는 없을까? 3. 이중해싱(Double Hashing)그래서 나온 방법이 바로 이중해싱이다. 말 그대로 해시 함수를 이중으로 사용하는 것이다.하나는 기존과 마찬가지로 최초 해시를 얻을 때 사용하고, 다른 하나는 충돌이 났을 경우 탐사 이동폭을 얻기 위해 사용한다. 이렇게 하면 최초 해시로 같은 값이 나오더라도 다시 다른 해시 함수를 거치면서 다른 탐사 이동폭이 나올 확률이 높기 때문에 매번 다른 공간에 값이 골고루 저장될 확률도 높아진다. 1234567891011121314151617181920212223242526272829const myTableSize = 23; // 테이블 사이즈가 소수여야 효과가 좋다const myHashTable = [];const getSaveHash = value => value % myTableSize;// 스텝 해시에 사용되는 수는 테이블 사이즈보다 약간 작은 소수를 사용한다.const getStepHash = value => 17 - (value % 17);const setValue = value => { let index = getSaveHash(value); let targetValue = myHashTable[index]; while (true) { if (!targetValue) { myHashTable[index] = value; console.log(`${index}번 인덱스에 ${value} 저장! `); return; } else if (myHashTable.length >= myTableSize) { console.log('풀방입니다'); return; } else { console.log(`${index}번 인덱스에 ${value} 저장하려다 충돌 발생!ㅜㅜ`); index += getStepHash(value); index = index > myTableSize ? index - myTableSize : index; targetValue = myHashTable[index]; } }} 이때 테이블 사이즈와 두번째 해시함수에 사용될 수는 둘 다 소수를 사용하는 것이 좋다. 둘 중에 하나가 소수가 아니라면 결국 언젠가 같은 해싱이 반복되기 때문이다. 딱 보기에 뭔가 좀 복잡해보이지만 하나하나 뜯어보면 별 거 없다. 한번 순서대로 살펴보자. 저장할 인덱스를 getSaveHash 해시 함수로 얻는다. 반복문 시작 거기 비었니?3-1. 비었어? 오케이 저장!3-2. 사람 있어? 다음 인덱스 내놔! 다시 3으로…3-3. 풀방이야? 종료합시다. 위 코드를 브라우저 콘솔이나 NodeJS로 실행시켜보면 출력되는 문자열을 통해 이 과정이 어떤 방식으로 흘러가는 지 대략적으로 알 수 있다. 12345678910console.log(setValue(1991));console.log(setValue(6));console.log(setValue(13));console.log(setValue(21));// 13번 인덱스에 1991 저장!// 6번 인덱스에 6 저장!// 13번 인덱스에 13 저장하려다 충돌 발생!ㅜㅜ// 17번 인덱스에 13 저장!// 21번 인덱스에 21 저장! 아까 선형 탐사법과 제곱 탐사법을 사용했다면 모두 해시의 결과가 1이어서 연쇄적으로 충돌이 발생해야 할 값들이지만 이중 해싱을 사용함으로써 한번의 충돌만으로 모든 값을 저장할 수 있게 되었다.위에 코드를 복붙해서 함수를 선언해놨다면 저 콘솔을 여러 번 돌려보고 저장할 값도 바꿔보면서 어떤 식으로 해쉬테이블에 값들이 저장되는지 살펴볼 수 있다. 테이블이 꽉 차면 풀방입니다가 출력되니까 그때까진 신나게 돌려봐도 된다. 분리 연결법(Separate Chaining)분리 연결법은 개방 주소법과는 다른 개념으로 접근하는 충돌 우회 방법이다. 분리 연결법은 해쉬 테이블의 버킷에 하나의 값이 아니라 링크드 리스트(Linked List)나 트리(Tree)를 사용한다.사실 트리를 쓰던 링크드 리스트를 쓰던 개념은 동일하니 링크드 리스트로 설명을 진행하겠다. 위에서 계속 사용했던 예시를 또 가져와보자. 테이블 크기가 5일때 해시로 1을 반환하는 1991, 6, 13, 21을 저장할 때 분리 연결법을 사용하면 이렇게 된다. 0 1[21, 13, 6, 1991] 2 3 4 간단하다. 근데 자세히 보면 [1991, 6, 13, 21]의 순서가 아니라 [21, 13, 6, 1991]의 순서로 뒤집혀 있다. 이는 데이터를 삽입할 때 조금이라도 수행 시간을 줄이기 위해서 사용하는 방법이다. 왜 그런지 설명하기 위해서 우리 테이블의 1번 인덱스에 저장된 링크드 리스트가 [1991, 6, 13, 21] 순서일때를 살펴보자. 1{ 값: 1991, 다음 노드: 2 } 2{ 값: 6, 다음 노드: 3 } 3{ 값: 13, 다음 노드: 4 } 4{ 값: 21, 다음 노드: null } 왠지 해쉬테이블이랑 비슷하게 생겼지만 이번엔 링크드 리스트다 만약 이번에 추가할 값이 11이라고 해보자. 일단 메모리 주소가 99인 곳이 남길래 여기에 { 값: 11, 다음 노드: null }을 저장했다.그 후 이 새로운 노드를 리스트에 붙혀야 하니까 해당 리스트의 마지막 노드인 메모리 4에 저장된 노드까지 찾아가야 한다.그 다음에 메모리 4에 저장된 값을 { 값: 21, 다음 노드: 99 }로 바꿔주면 끝! 1{ 값: 1991, 다음 노드: 2 } 2{ 값: 6, 다음 노드: 3 } 3{ 값: 13, 다음 노드: 4 } 4{ 값: 21, 다음 노드: 99 } 99{ 값: 11, 다음 노드: null } 문제는 새 노드를 리스트에 붙히기 위해서 메모리 4에 저장된 노드를 찾는 과정이다. 먼저 리스트의 머리인 메모리 1부터 찾은 다음에 다음 메모리 주소 값을 확인하고 2로 이동해서 또 다음 메모리 주소를 확인하고…이게 지금 4번이니까 할만하지, 리스트의 길이가 길어질수록 수행 시간도 비례해서 늘어나기 때문에 확실히 좋은 느낌은 아니다. 하지만 순서를 반대로 뒤집으면 데이터 삽입이 한결 쉬워진다. 99{ 값: 11, 다음 노드: 1 } 1{ 값: 1991, 다음 노드: 2 } 2{ 값: 6, 다음 노드: 3 } 3{ 값: 13, 다음 노드: 4 } 4{ 값: 21, 다음 노드: null } 맨 앞에 노드를 추가하는 것이기 때문에 다른 노드를 탐색할 필요없이 그냥 메모리에 밀어넣고 { 값: 11, 다음 노드: 1 }이라고 저장해주면 되기 때문이다. 그래서 해시 테이블에 저장할 때도 리스트의 꼬리(Tail)로 데이터를 붙히기보다는 머리(Head)에 붙히는 방법을 보통 많이 사용한다. 대신 이렇게 분리 연결법을 사용하려면 해시 함수의 역할이 굉장히 중요하다. 결국 균일하지 못한 해시를 사용해서 특정 인덱스에 데이터가 몰리게 된다면 다른 곳은 텅텅 비어있는데 한 버킷에 저장된 리스트의 길이만 계속 길어지기 때문이다. 0 1[21, 13, 6, 1991, 7, 11, 25, ...] 2 3 4 극단적인 예시긴 하지만... 이건 그냥 링크드 리스트를 쓰는 것과 다를 게 없다 결국 내가 찾고자 하는 값이 리스트의 맨 마지막에 위치하고 있다면 링크드 리스트를 처음부터 끝까지 다 탐색해야하기 때문에 $O(n)$의 시간복잡도를 가지게 된다. 그렇기 때문에 최대한 저장하고 하는 데이터를 균일하게 퍼트려서 리스트의 길이를 어느 정도로 유지해주는 해시 함수의 역할이 중요한 것이다. 테이블 크기 재할당(Resizing)해시 테이블은 고정적인 공간을 할당해서 많은 데이터를 담기 위한 자료구조인 만큼 언젠가 데이터가 넘치기 마련이다. 개방 주소법을 사용하는 경우에는 위에서 예시로 작성했던 코드에서 풀방입니다가 출력되는 상황, 즉 테이블이 실제로 꽉 차서 더이상 저장을 못하는 상황이 발생할 것이고,분리 연결법을 사용하는 경우에는 테이블에 빈 공간이 적어지면서 충돌이 발생할 수록 각각의 버킷에 저장된 리스트가 점점 더 길어져서 리스트를 탐색하는 리소스가 너무 늘어난 상황이 발생할 것이다. 그렇기 때문에 해시 테이블은 꽉꽉 아낌없이 채우기보다는 어느 정도 비워져 있는 것이 성능 상 더 좋으며, 해시 테이블을 운용할 때는 어느 정도 데이터가 차면 테이블의 크기를 늘려줘야한다.이건 특별한 알고리즘이라기보다는 그냥 기존 크기의 두 배정도로 새로운 테이블을 선언해서 기존 테이블의 데이터를 그대로 옮겨 담는 방법을 사용한다. 분리 연결법을 사용한 해시 테이블의 경우 재해싱(Rehashing)을 통해 너무 길어진 리스트의 길이를 나누어서 다시 저장하는 방법을 사용하기도 한다. 이상으로 JavaScript와 함께 해시테이블을 파헤쳐보자 포스팅을 마친다. ul.hash-table-dummy { width: 100%; margin: 0 auto; display: flex; border: 1px solid #ddd; } ul.hash-table-dummy > li { flex-grow: 1; list-style: none; border-right: 1px solid #ddd; margin: 0; text-align: center; font-size: 14px; } ul.hash-table-dummy dt { border-bottom: 1px solid #ddd; background-color: #f0f0f0; } ul.hash-table-dummy dd { margin: 0; }","link":"/2019/06/25/hashtable-with-js/"},{"title":"V8 엔진은 어떻게 내 코드를 실행하는 걸까?","text":"이번 포스팅에서는 구글의 V8 엔진이 어떤 방식으로 자바스크립트를 해석하고 실행하는지 살펴 보는지에 대해 포스팅하려고 한다. V8은 C++로 작성되었지만 필자의 메인 언어가 C++이 아니기도 하고, 워낙 소스가 방대하기 때문에 자세한 분석까지는 아니라도 최대한 웹 상에 있는 정보들과 필자가 분석한 V8의 소스코드를 비교해가면서 살펴보려고 한다. V8 엔진이란?V8 엔진은 구글이 주도하여 C++로 작성된 고성능의 자바스크립트 & 웹 어셈블리 엔진이다. 또한 V8은 오픈 소스이기 때문에 V8 엔진 깃허브 레파지토리에서 클론받을 수 있다. 현재 Google Chrome과 NodeJS에서 사용되고 있으며 ECMAScript와 Web Assembly를 표준에 맞게 구현하였다. Kangax Table에서 확인해보면, V8을 사용하고 있는 CH(Google Chrome)과 Node는 거의 대부분의 ES2016+(ES7+)의 기능을 구현해놓은 반면 MicroSoft의 Chakra나 Mozila의 SpiderMonkey의 경우 붉은 색으로 표시된 부분이 꽤 존재한다는 것을 볼 수 있다. 다른 플랫폼과의 호환과 서로 간의 리스크를 줄이기 위해, 마이크로소프트 Edge는 이제 이 변경 사항의 일부분으로 인해 V8 엔진을 사용할 것 입니다. 우리가 배워야할 것들은 아직 많지만 그래도 우리는 V8 커뮤니티의 일원이 된 것과 이 프로젝트에 기여하게 된 것이 매우 기대됩니다. ChakraCore teamChakraCore Github Issue 하지만 Chakra 엔진을 사용하던 Microsoft Edge의 경우, 이제 Chromium 오픈소스 프로젝트에 동참하면서 V8을 사용할 예정이라고 한다. 음… 좋은 건지 나쁜 건지 아직은 잘 모르겠다. 만약 V8을 빌드해서 실제로 디버깅까지 해보고 싶다면 단순히 git을 사용하여 클론 받는 것으로는 빌드를 할 수 없다. 빌드까지 해보고 싶은 분은 V8 공식 홈페이지의 Checking out the V8 source code를 읽어보자. Chromium이 워낙 대형 프로젝트다보니 빌드 한번 하려면 설치 과정부터 까다롭다 필자는 예전에 한번 셋업해본 적이 있는데, 당연하게도 순순히 잘 설치되어주지는 않으니 시간 많은 주말에 시도하는 것을 추천한다. 이번에는 단순히 소스 코드만 분석할 예정이므로 필자는 git을 이용하여 직접 클론받았다. V8 엔진의 작동원리를 살펴보자.일반적으로 우리가 자바스크립트를 사용할 때 엔진의 작동 원리와 같은 로우 레벨(Low Level)의 내용까지 깊게 신경쓸 필요는 없다. 사실 개발자들이 그런 것까지 일일히 신경쓰지 말라고 엔진을 사용하는 것이기 때문이다. 그러나 정말 자바스크립트로 뽑아낼 수 있는 최적의 성능을 사용하고 싶다면 내 코드가 어떤 식으로 실행되는 지에 대한 이해는 어느 정도 있어야한다. 그럼 먼저 간단한 그림으로 V8이 우리의 자바스크립트 소스 코드를 어떻게 해석하고 실행하는 지 살펴보자. 자세한 설명은 밑에서 다시 할 예정이므로 그냥 한번 흝고 넘어간다는 느낌으로 보면 된다. JSConf EU 2017에서 발표한 Franziska Hinkelmann님의 자료 V8은 우리의 소스 코드를 가져와서 가장 먼저 파서(Parser)에게 넘긴다. 이 친구는 소스 코드를 분석한 후 AST(Abstract Syntax Tree), 추상 구문 트리로 변환하게 된다.그 다음에 이 AST를 그림에 나와있는 Ignition에게 넘기는데 이 친구는 자바스크립트를 바이트 코드(Bytecode)로 변환하는 인터프리터이다. 원본 소스 코드보다 컴퓨터가 해석하기 쉬운 바이트 코드로 변환함으로써 원본 코드를 다시 파싱(Parsing)해야하는 수고를 덜고 코드의 양도 줄이면서 코드 실행 때 차지하는 메모리 공간을 아끼려는 것이다. 이후 이 바이트 코드를 실행함으로써 우리의 소스 코드가 실제로 작동하게 되고, 그 중 자주 사용되는 코드는 TurboFan으로 보내져서 Optimized Machine Code, 즉 최적화된 코드로 다시 컴파일된다. 그러다가 다시 사용이 덜 된다 싶으면 Deoptimizing 하기도 한다. 여기서 사용되는 용어들이 굉장히 재미있는데, V8은 원래 8기통 엔진의 종류를 의미하는 단어다. 제네시스 G90이나 기아 K9같은 차에 들어간다고 한다.그럼 Ignition은 뭐냐. 엔진에 시동걸 때 사용하는 점화기이다. 내 소스 코드가 부릉부릉 실행되는 것이다. 그러다가 너무 많이 호출되서 내 코드가 뜨거워지면 TurboFan으로 최적화해서 너무 과열되지 않게 식혀주는 그런 느낌적인 느낌…? 분명히 다 노리고 네이밍한거다. V8 분석 포스팅에서 컴파일 파이프라인을 설명할 때 빠지지 않던 Full-codegen과 Crankshaft는 어디로 갔냐 하면… V8 v5.9부터 처음으로 Ignition과 TurboFan은 자바스크립트 실행을 위해 전체적으로 사용됩니다. 또한 V8의 v5.9부터 V8을 잘 지탱해준 기술이었던 Full-codegen과 Crankshaft는 새로운 자바스크립트의 기능과 이러한 기능들이 요구하는 최적화 기능을 더 이상 따라갈 수 없기 때문에 V8에서 더 이상 사용되지 않습니다. 우리는 이것들을 곧 완전히 제거할 계획이며, 이는 V8이 앞으로 훨씬 더 단순하고 유지 보수 가능한 아키텍처를 갖게 된다는 것을 의미합니다. V8 teamLaunching Ignition and TurboFan 네. V8의 v5.9부터 세대 교체 당했습니다. v5.9 이전까지는 Full-codegen과 Crankshaft도 공존하고 있었지만 이건 V8 팀이 원했던 것이 아니라 초창기의 Ignition과 TurboFan의 성능이 생각만큼 잘 나와주지 않았던 것도 있고 Optimizing된 코드를 다시 Deoptimizing할 때 바이트 코드로 바로 변환할 수 없던 이슈들이 있어서 어쩔 수 없이 중간에 Full-codegen과 Crankshaft를 살려둔 것이다. V8 팀의 원래 목적은 처음부터 Ignition과 TurboFan만 사용하여 바이트 코드 최적화된 코드 사이를 왔다갔다 하는 것이었다. 이 영상은 BlinkOn 2016에서 Chrome Mobile Performance London Team팀의 Ross McIlroy이 Ignition을 소개하는 영상인데, 9:47 ~ 11:14 구간에서 레거시 코드인 Full-codegen과 Crankshaft를 삭제하지 못한 슬픈 사정을 설명해준다. 본인도 말하면서 웃긴듯. 그럼 이제 V8이 자바스크립트를 어떤 식으로 파싱하고 실행시키는 지 간략하게 한번 알아보자. Parsing, 코드의 의미 파악하기파싱(Parsing)이란, 소스코드를 불러온 후 AST(Abstract Syntax Tree), 추상 구문 트리로 변환하는 과정이다.AST는 컴파일러에서 널리 사용되는 자료 구조인데, 우리가 일반적으로 작성한 소스 코드를 컴퓨터가 알아먹기 쉽게 구조화한다고 생각하면 된다. 예를 들어, 자바스크립트로 자바스크립트를 파싱한다고 하면 이런 느낌이다. 1234567891011121314151617function hello (name) { return 'Hello,' + name;}// 위 코드는 대략 이렇게 구조화 할 수 있다.{ type: 'FunctionDeclaration', name: 'hello' arguments: [ { type: 'Variable', name: 'name' } ] // ...} 이렇게 놓고 보니 생각보다 심플하다. 다만 이것은 예시 중 하나일 뿐이고 컴파일러는 for, if, a = 1 + 2, function () {}과 같은 문법도 모두 해석하여 파싱해야 하다보니 파서(Parser)의 내부는 생각보다 거대하다. 당장 V8의 parser.cc 파일도 3000줄이 넘는다. 어쨌든 파싱이라는 개념 자체는 컴퓨터가 분석하기 쉬운 형태인 추상 구문 트리로 변경하는 작업이라는 것만 기억하자. V8 엔진은 방금 예시에서 자바스크립트로 표현했던 것을 C++을 사용하여 그대로 할 뿐이다. 그럼 V8 엔진의 파싱 코드 중 1 + 2와 같이 리터럴로 선언된 수식을 담당하는 메소드를 한번 살펴보자. v8/src/parsing/parser.cc123456789101112131415161718bool Parser::ShortcutNumericLiteralBinaryExpression(Expression** x, Expression* y, Token::Value op, int pos) { if ((*x)->IsNumberLiteral() && y->IsNumberLiteral()) { double x_val = (*x)->AsLiteral()->AsNumber(); double y_val = y->AsLiteral()->AsNumber(); switch (op) { case Token::ADD: *x = factory()->NewNumberLiteral(x_val + y_val, pos); return true; case Token::SUB: *x = factory()->NewNumberLiteral(x_val - y_val, pos); return true; // ... default: break; } } return false;} 이 코드는 V8 엔진의 parser.cc에 선언된 Parser 클래스의 ShortcutNumericLiteralBinaryExpression(이름이 더럽게 길다…) 스태틱 메소드이다. 인자를 한번 살펴보면 Expression 클래스의 객체인 x와 y는 표현식에 사용된 값들 의미한다. op는 이 표현식이 의미하는 것이 x + y인지 x - y인지와 같은 실제 구문 내용과 그 타입을 의미하고 pos는 전체 소스 코드 중 현재 파싱하는 소스 코드의 위치를 의미한다. 이 메소드는 위에서 설명했듯이 1 + 2와 같은 소스 코드를 만났을 경우 호출되며, 인자로 받은 표현을 Token::ADD나 Token::SUB와 같은 조건으로 검사하여 조건에 맞게 파싱하고 있는 모습을 볼 수 있다. 여기서 말하는 토큰은 소스 코드를 자바스크립트의 문법 규칙에 따라 어휘 분석하여 나온 문자열 조각들이다. 12// 토큰은 대충 이런 느낌['const', 'a', '=', '1', '+', '2'] 이후 알맞게 계산되어 나온 값을 AstNodeFactory 클래스의 NewNumberLiteral 스태틱 메소드를 사용하여 추상 구문 트리의 노드로 만드는 모습을 볼 수 있다. v8/src/ast/ast.cc1234567Literal* AstNodeFactory::NewNumberLiteral(double number, int pos) { int int_value; if (DoubleToSmiInteger(number, &int_value)) { return NewSmiLiteral(int_value, pos); } return new (zone_) Literal(number, pos);} V8은 이 과정에서 변수, 함수, 조건문과 같은 코드의 의미를 파악하며, 우리에게 익숙한 자바스크립트의 스코프 또한 이 과정에서 설정된다.이 중 변수 선언에 관한 자세한 내용은 JavaScript의 let과 const, 그리고 TDZ을 참고하자. Ignition으로 바이트 코드(Bytecode) 생성하기 바이트 코드(Bytecode)는 고오급 언어로 작성된 소스 코드를 가상머신이 한결 편하게 이해할 수 있도록 중간 코드로 한번 컴파일 한 것을 의미한다. V8에서는 Ignition이 이 역할을 수행하고 있다. Ignition이란?Ignition은 기존의 Full-codegen을 완벽히 대체하는 인터프리터이다. 기존에 사용하고 있던 Full-codegen은 전체 소스 코드를 한번에 컴파일했는데, 위에서 설명했듯 V8팀은 기존의 Full-codegen이 모든 소스 코드를 한번에 컴파일할때 메모리 점유를 굉장히 많이 한다는 사실을 인지하고 있었다. 또 자바스크립트는 C++과 같은 정적 타이핑 언어가 아닌 동적 타이핑 언어라서 소스 코드가 실행되기 전에는 알 수 없는 값들이 너무 많았기 때문에 이런 접근 방법으로는 최적화를 하기도 힘들었다고 한다. 그래서 Ignition을 개발할 때는 모든 소스를 한번에 해석하는 컴파일 방식이 아닌 코드 한줄 한줄이 실행될 때마다 해석하는 인터프리트 방식을 채택하여 다음 세가지 이점을 가져가고자 하였다. 메모리 사용량 감소. 자바스크립트 코드에서 기계어로 컴파일하는 것보다 바이트 코드로 컴파일하는 것이 더 편하다. 파싱 시 오버헤드 감소. 바이트 코드는 간결하기 때문에 다시 파싱하기도 편하다. 컴파일 파이프 라인의 복잡성 감소. Optimizing이든 Deoptimizing이든 바이트 코드 하나만 생각하면 되기 때문에 편하다. Ross McIlroyIgnition - an interpreter for V8 이렇게 Ignition은 코드가 한줄한줄 실행될 때마다 코드를 바이트 코드로 바꿔주는 친구라는 정도만 알아두면 된다.그럼 바이트 코드라는 게 도대체 어떻게 생겨먹었길래 컴퓨터가 해석하기 더 편하다는 걸까? 아까 위에서 사용했던 hello 함수를 가져와서 한번 어떤 바이트 코드가 생성되는 지 살펴보자. 바이트 코드를 직접 확인하기1234function hello (name) { return 'Hello,' + name;}console.log(hello('Evan')) // 함수를 호출해서 코드를 사용하지 않는다면 바이트 코드로 인터프리팅하지 않는다. 만약 NodeJS v8.3+을 사용하고 있다면 --print-bytecode 옵션을 주는 것만으로 내 소스 코드가 바이트 코드로 어떻게 인터프리팅되었는지 확인할 수 있다. 혹은 V8이 제공하고 있는 D8 디버깅 도구를 사용해도 되는데 이 친구는 V8을 빌드해야 사용할 수 있고, 위에서 설명했듯이 빌드 환경 세팅이 순탄하지는 않기 때문에 필자는 그냥 --print-bytecode를 사용했다. 123456789101112$ node --print-bytecode add.js...[generated bytecode for function: hello]Parameter count 2Frame size 8 15 E> 0x2ac4000d47b2 @ 0 : a0 StackCheck 30 S> 0x2ac4000d47b3 @ 1 : 12 00 LdaConstant [0] 0x2ac4000d47b5 @ 3 : 26 fb Star r0 0x2ac4000d47b7 @ 5 : 25 02 Ldar a0 46 E> 0x2ac4000d47b9 @ 7 : 32 fb 00 Add r0, [0] 53 S> 0x2ac4000d47bc @ 10 : a4 Return... 필자가 선언한 hello 함수가 바이트 코드로 변환된 모습이다. 어라? 근데 필자는 분명히 name 인자 한개만 사용했는데 Parameter count가 2라고 찍혀있다.이 중 하나는 암시적 리시버인 this이다. 함수 내부에서 this를 사용하면 함수 자신을 가리킬 수 있는 그 this 맞다. 이제 그 밑으로는 레지스터에 값들을 할당하는 모습을 볼 수 있는데, 간단하게만 설명하고 넘어가겠다. 혹시 모르실 분들을 위해 간단히 설명하자면, 레지스터(Register)는 CPU가 가지고 있는 고속 메모리이고 누산기(Accumulator)는 계산한 중간 결과를 저장하기 위한 레지스터이다. StackCheck: 스택 포인터의 상한값을 확인한 것이다. 이때 스택이 임계 값을 넘어가면 Stack Overflow가 발생하기 때문에 함수 실행을 중단해버린다. LdaConstant [0]: Ld는 Load의 약자이다. 말 그대로 어떠한 상수를 누산기(Accumulator)에 불러온 것이다. 이 상수는 Hello,이다. Star r0: 누산기에 들어있는 값을 레지스터 r0번으로 이동시킨다. r0은 지역 변수를 위한 레지스터이다. Ldar a0: 누산기에 레지스터 a0번에 있는 값을 담는다. 이 경우 a0 레지스터의 값은 인자 name이다. Add r0, [0]: r0에 있는 Hello,와 0을 더하고 누산기에 저장한다. 이때 상수 0은 코드가 실행될 때 인자 name으로 매핑된다. Return: 누산기에 있는 값을 반환한다. hello 함수는 평소에 자바스크립트를 사용할 때는 아무 생각 없이 선언할 수 있는 정도의 가벼운 함수였지만 내부적으로는 6단계를 거쳐서 값을 반환하고 있었다. 이렇게 일 많이 하니까 가끔 느리다고 너무 뭐라 하지 맙시다 바이트 코드는 직접 CPU 내의 레지스터와 누산기를 어떤 식으로 사용하라고 명령하는 명령문이나 마찬가지기 때문에 사람 입장에서는 머리 터지겠지만 컴퓨터 입장에서는 한결 이해하기가 편한 방식이다.V8 엔진은 우리가 작성한 자바스크립트 코드를 내부적으로는 이런 모습의 바이트 코드로 전부 변환해놓기 때문에 코드 라인이 처음 실행될 때는 조금 시간이 걸리겠지만 그 이후부터는 거의 컴파일 언어에 가까운 성능을 보일 수 있다. TurboFan으로 뜨거워진 코드 식히기 TurboFan은 V8의 v5.9부터 기존에 사용하던 Crankshaft 컴파일러를 완전히 대체한 최적화 담당 컴파일러이다. 그럼 Crankshaft는 왜 사라졌을까?처음 V8이 세상에 나온 이후로 새로운 컴퓨터 아키텍처도 나오고 자바스크립트도 계속 발전했기 때문에 V8도 계속해서 이런 것들을 지원해줘야 했다. V8 팀은 이런 새로운 사양에 맞춰서 V8을 계속 개량했어야 했는데, 어떻게든 계속 해서 땜빵치다가 결국 Crankshaft의 구조로는 지속적인 확장이 어렵다고 판단했고, 여러 레이어로 계층화되어 좀 더 유연하게 확장에 용이하도록 설계한 TurboFan을 만들어서 사용하고 있다. Crankshaft와 TurboFan을 동시에 굴릴 때 왠지 이런 느낌이었을 것 같다… 7개의 아키텍처를 지원할 때 Crankshaft로는 13,000 ~ 16,000라인의 코드로 작성했던 게 TurboFan에서는 3,000라인 미만의 코드로 커버가 가능하다고 한다. V8은 런타임 중에 Profiler라는 친구에게 함수나 변수들의 호출 빈도와 같은 데이터를 모으라고 시킨다. 이렇게 모인 데이터를 들고 TurboFan에게 가져가면 TurboFan은 자기 기준에 맞는 코드를 가져와서 최적화를 하는 것이다. 최적화 기법으로는 히든 클래스(Hidden Class)나 인라인 캐싱(Inline Caching) 등 여러가지 기법을 사용하지만 이 내용은 추후 다른 포스팅에서 더 자세히 다루도록 하겠다.간단히만 설명하자면 히든 클래스는 비슷한 놈들끼리 분류해놓고 가져다 쓰는 것, 인라인 캐싱은 자주 사용되는 코드가 만약 hello()와 같은 함수의 호출부라면 이걸 function hello () { ... }와 같이 함수의 내용으로 바꿔버리는 것이다. 말 그대로 캐싱(Caching)이다. 어떤 조건으로 최적화 하는 걸까?그렇다면 TurboFan은 정확히 어떤 조건으로 최적화될 코드를 구분하는 걸까?우선 V8 소스 내에서 함수를 최적화할지 말지를 판별하는 RuntimeProfiler의 ShouldOptimize 메소드를 예시로 한번 살펴보자. v8/src/execution/rumtime-profiler.cc12345678910111213141516OptimizationReason RuntimeProfiler::ShouldOptimize(JSFunction function, BytecodeArray bytecode) { // int ticks = 이 함수가 몇번 호출되었는지 int ticks = function.feedback_vector().profiler_ticks(); int ticks_for_optimization = kProfilerTicksBeforeOptimization + (bytecode.length() / kBytecodeSizeAllowancePerTick); if (ticks >= ticks_for_optimization) { // 함수가 호출된 수가 임계점인 ticks_for_optimization을 넘기면 뜨거워진 것으로 판단 return OptimizationReason::kHotAndStable; } else if (!any_ic_changed_ && bytecode.length() < kMaxBytecodeSizeForEarlyOpt) { // 이 코드가 인라인 캐싱되지 않았고 바이트 코드의 길이가 작다면 작은 함수로 판단 return OptimizationReason::kSmallFunction; } // 해당 사항 없다면 최적화 하지 않는다. return OptimizationReason::kDoNotOptimize;} 생각보다 조건이 별로 없어서 당황했다. 물론 이 메소드에서 판별 하지않는 좀 더 디테일한 조건들도 존재하지만 일단 큰 틀은 이 메소드가 거의 다 가지고 있다. kHotAndStable은 코드가 뜨겁고 안정적이라는 것인데, 쉽게 말하면 자주 호출되고(뜨겁고) 코드가 안 변함(안정적)이라는 것이다.매번 같은 행동을 수행하는 반복문 내에 있는 코드 같은 경우가 여기에 해당하기 쉽다. kSmallFunction은 말 그대로 인터프리팅된 바이트 코드의 길이를 보고 특정 임계점을 넘기지 않으면 작은 함수라고 판단해서 최적화를 진행하는 것이다. 작고 단순한 함수는 크고 복잡한 함수보다 동작이 매우 추상적이거나 제한적인 확률이 높기 때문에 안정적이라고 볼 수 있다. TurboFan이 일하는 모습 훔쳐보기그럼 간단한 코드 예제를 통해 최적화가 어떤 방식으로 진행되는 지 확인해보자. 필자는 작은 함수 하나를 선언하고 반복문을 통해서 계속 호출해줄 것이다.필자의 목표는 선언한 함수가 ticks >= ticks_for_optimization 조건에 걸려서 kHotAndStable 상태가 되는 것이다. 필자 생각으로는 대충 아무 함수나 선언해서 같은 타입의 인자를 사용하고 반복적으로 파바박! 호출하면 ShouldOptimize 메소드의 ticks >= ticks_for_optimization 조건에 걸릴 것이고, kHotAndStable 상태가 되어 최적화가 진행될 거라고 생각한다. NodeJS를 실행할 때 --trace-opt 옵션을 주면 런타임 때 코드가 최적화되는 것을 확인해볼 수 있다. 12345678function sample(a, b, c) { const d = c - 100; return a + d * b;}for (let i = 0; i < 100000; i++) { sample(i, 2, 100);} 123456789$ node --trace-opt test.js[marking 0x010e66b69c09 for optimized recompilation, reason: small function, ICs with typeinfo: 3/3 (100%), generic ICs: 0/3 (0%)][marking 0x010e66b6a001 for optimized recompilation, reason: small function, ICs with typeinfo: 3/3 (100%), generic ICs: 0/3 (0%)][compiling method 0x010e66b6a001 using TurboFan][compiling method 0x010e66b69c09 using TurboFan OSR][optimizing 0x010e66b69c09 - took 0.132, 0.453, 0.027 ms][optimizing 0x010e66b6a001 - took 0.850, 0.549, 0.012 ms][completed optimizing 0x010e66b6a001 ] 오 최적화가 되긴 했다. ICs with typeinfo: 3/3 (100%)라고 적혀있는걸 보니 인라인 캐싱을 했나보다. 근데 최적화를 한 이유를 보니 small function이라고 적혀있다. 필자가 원했던 조건은 kHotAndStable으로 빠지는 것이었기 때문에 코드를 조금 바꿔서 다시 해봐야겠다. 함수가 너무 간단하니까 TurboFan이 필자의 함수를 만만하게 봤나보다. 1234567891011121314function sample () { if (!arguments) { throw new Error('인자를 주시오'); } const array = Array.from(arguments); return array.map(el => el * el) .filter(el => el < 20) .reverse();}for (let i = 0; i < 100000; ++i) { sample(1, 2, 3, 4, 5);} 그냥 아무 의미 없지만 적당히 더 복잡하게 만들어 보았다. sample 함수는 그냥 인자를 받아서 변형하고 걸러내고 순서를 뒤집어서 반환하는 역할을 수행한다. 여기서 Turbofan이 감시하고 있는 최적화 대상은 sample, map, filter, reverse, Array.from 같은 친구들이 될 것이다. 감시 대상이 많으므로 로그도 어마무시하게 나오기 때문에 TurboFan이 함수를 최적화 대상으로 marking 하는 부분만 가져오겠다. 12345678910111213$ node --trace-opt test.js[marking 0x1a368a90cc51 for optimized recompilation, reason: small function, ICs with typeinfo: 3/3 (100%), generic ICs: 0/3 (0%)][marking 0x1a36bcfa9611 for optimized recompilation, reason: small function, ICs with typeinfo: 1/1 (100%), generic ICs: 0/1 (0%)][marking 0x1a36bcfa96a1 for optimized recompilation, reason: small function, ICs with typeinfo: 1/1 (100%), generic ICs: 0/1 (0%)][marking 0x1a368a90cc11 for optimized recompilation, reason: hot and stable, ICs with typeinfo: 10/11 (90%), generic ICs: 0/11 (0%)][marking 0x1a36e4785c01 for optimized recompilation, reason: small function, ICs with typeinfo: 1/5 (20%), generic ICs: 0/5 (0%)][marking 0x1a36e4786fc1 for optimized recompilation, reason: hot and stable, ICs with typeinfo: 4/5 (80%), generic ICs: 0/5 (0%)] 오 드디어 reason: hot and stable이 나왔다. 과 같은 포맷으로 함수 정보가 함께 출력되기 때문에 hot and stable의 이유로 최적화 대상을 찍힌 친구가 sample과 reverse 함수라는 것을 알 수 있다. 이와 같이 TurboFan은 한가지 데이터가 아니라 여러가지 데이터를 프로파일링하며 이 코드를 최적화할 것인지 구분한다. 마치며최근 잘 짜여진 자바스크립트는 C++에 근사하는 성능을 낼 수도 있다고 한다. 처음 이 이야기를 들었을 때는 “인터프리터 언어가 어떻게 컴파일 언어 성능을 내?”라고 생각했었지만 V8의 작동 원리를 살펴보니 생각보다 많은 최적화 기법이 들어가 있어서 놀랐다. 그리고 이렇게 자바스크립트 엔진을 하나하나 뜯어보면서 내가 좋아하는 언어에 대한 이해도를 높히는 과정은 굉장히 재밌었다.(학교 다닐 때 C++ 좀 많이 써볼걸…) 사실 V8 엔진 내부에는 “오…개쩌는데…?” 라는 말이 나올만한 최적화 기법이 많이 적용되어 있기 때문에 이것저것 다 소개하고 싶었지만, 포스팅을 쓰다보니 뭔가 내용이 길어지면서 점점 산으로 가고…그래서 이번에는 전체적인 흐름을 설명하는 선까지만 하고 다음 포스팅때 Ignition과 TurboFan의 작동에 대해서 좀 더 디테일하게 설명해보도록 하겠다.특히 TurboFan의 동작 흐름을 아는 것은 내 자바스크립트 코드를 최적화할 수 있는 지름길이기 때문에 알아두면 요긴하게 써먹을 수 있을 것 같다. 이상으로 V8 엔진은 어떻게 내 코드를 실행하는 걸까? 포스팅을 마친다.","link":"/2019/06/28/v8-analysis/"},{"title":"애자일이 도대체 뭐길래?","text":"이번 포스팅에서는 소프트웨어 개발 방법론 중 하나인 애자일 프로세스(Agile Process), 줄여서 애자일이라고 부르는 그것에 대해서 포스팅하려고 한다. 최근 많은 조직들이 애자일 프로세스를 사용하고 있고, 필자가 다니고 있는 현 직장도 마찬가지로 애자일 프로세스를 도입해서 사용하고 있다. 많은 조직이 애자일을 그냥 단순히 이슈마다 스토리 포인트를 매겨서 얼마나 걸릴 지 산출한다, 1~2주 단위의 스프린트를 돌린다, 매일 아침 데일리 스크럼 회의를 한다 등 표면적인 것들에 집중하고 있다. 필자의 직장도 애자일을 계속 해서 해왔지만 어떤게 진짜 애자일한 업무 방법인가?에 대해서 다들 의문만 가질 뿐 제대로 배워볼 기회가 없었다. 애자일의 본질은 모른 채 그냥 내가 이렇게 몇번 해봤는데 되게 좋았어! 혹은 다른 회사가 이렇게 하면서 좋아졌대! 정도로 시작했으니 잘 굴러갈리가 없었고 매번 빗나가는 에스티메이션과 망가지는 스프린트로 인한 팀원들의 의욕 저하 등 여러가지 문제점이 발생했다. 그래서 필자의 현 직장인 숨고에서는 꽤나 비싼 돈을 들여 애자일 코치를 초빙해서 약 한달간 애자일에 대한 교육을 시행했던 적이 있다.(그 분 한달 수입이 거의 한 사람 연봉이라 많은 이들이 애자일 코치로의 전업을 꿈꿨다…갓자일 코치…) 그 경험을 바탕으로 필자가 배웠던 애자일에 대한 내용과 직장에서 실제로 여러가지 시도를 해본 후 느꼈던 점에 대해서 한번 얘기해볼까 한다. 애자일을 왜 사용하는걸까?최근에는 많은 조직에서 이미 애자일 프로세스를 사용하고 있기 때문에 애자일이 무엇인지는 많이 알고 계실거라 생각한다.애자일(Agile)은 기민하다라는 뜻으로 너무 계획이 없는 개발 방법론과 너무 체계적인 계획이 있는 개발 방법론 사이의 균형을 잡아보자는 의도로 나온 개발 방법론이다. 전통적인 개발 방법론으로는 폭포수 모델(Waterfall)이 대표적인데, 이 개발 방법론의 특징은 바로 앞만 보고 달리되, 굉장히 긍정적인 마인드를 품은 개발 방법론이라는 것이다.폭포수 모델을 포함한 이런 전통적인 개발 방법론들은 대략 다음과 같은 큰 틀을 가진다 기획 > 디자인 > 개발 > 테스트 > 배포 > 유지보수 굉장히 익숙하지 않은가? 우리도 알게 모르게 여러 번 겪었을 프로세스다. 이 프로세스는 마감 기한을 딱! 정해놓고 그 마감 기한 안에 프로젝트를 끝내기 위해 모든 팀원이 자신이 맡은 일을 끝낸 후 다음 차례로 넘긴다. 이렇게 아무 문제 없이 끝난다면 모두가 행복한 해피엔딩이겠지만 현실은 그렇지 않다. M06B님의 “무리한 요구를 하는 클라이언트” 카톡 이모티콘 현실 세계에서는 갑인 클라이언트님께서 프로토타입을 보고 수정 사항을 요청할 수도 있고, 인하우스 프로덕트를 개발하는 회사라고 해도 열심히 개발해서 배포했더니 유저의 반응이 예상과는 전혀 다르게 안좋을 수도 있다. 이런 상황에서의 폭포수 모델은 너무 많은 리스크를 팀원들에게 강요한다. 기껏 한두달 열심히 개발했더니 그동안 고생한게 모두 물거품이 되버리기 쉽다는 얘기다. 또한 이런 경우도 생긴다. 기획 > 디자인 > 개발(어? 이건 지금 상황에선 불가능 한데…?) 개발자가 기획 회의에 참여한 경우 이런 경우를 어느 정도 방지할 수 있지만 그렇지 않은 경우 다시 기획자에게 찾아가서 현재 상황과 불가능한 이유를 설명하고 다시 기획 > 디자인 프로세스부터 시작해야한다. 이런 경우, 작게는 시간 낭비부터 시작해서 크게는 팀원간의 감정도 상할 수 있고 그로 인해 팀 사기에도 좋지 않은 영향을 끼치기 때문에 좋은 경험은 아니라고 생각한다.(필자는 팀원들의 감정 상태를 굉장히 중요하게 생각한다) 그럼 전통적인 방법들의 이런 단점을 극복하고자 고안된 애자일이 무엇인지 한번 알아보도록 하자. 애자일 선언에 대해서애자일 개발 방법론은 2001년 발표한 애자일 선언에서부터 시작한다. 우리는 소프트웨어를 개발하고, 또 다른 사람의 개발을 도와주면서 소프트웨어 개발의 더 나은 방법들을 찾아가고있다.이 작업을 통해 우리는 다음을 가치 있게 여기게 되었다. 공정과 도구보다 개인과 상호작용을 포괄적인 문서보다 작동하는 소프트웨어를 계약 협상보다 고객과의 협력을 계획을 따르기보다 변화에 대응하기를 가치 있게 여긴다. 이 말은 왼쪽에 있는 것들도 가치가 있지만, 우리는 오른쪽에 있는 것들에 더 높은 가치를 둔다는 것이다. 애자일 선언 한국어 원문agilemanifesto.org 우리는 이 선언에서 애자일이 어떤 것을 지향하는 지 알 수 있다. 이 얘기를 좀 더 쉽게 풀면 이런 식이다. 자, 우리는 팀이니까 따로 놀지말고 커뮤니케이션을 잘하고!쓸데없는 문서 작성에 너무 시간 뺏기지 말고 일단 작동하는 뭔가를 만들고!고객이 실제로 가치를 느낄 수 있는 프로덕트를 만들고!계획은 늘상 변하는 거니까 변화에 대응할 수 있게 유연한 작업 방식으로 일하자! 애자일은 굉장히 이상적이다. 오직 고객만을 위한 제품을 만들기 위해 유연하게 움직이는 조직이 되자는 의미이기 때문이다. 애자일 선언 이면의 원칙(12개 원칙)애자일은 애자일 선언 외에도 12개의 원칙을 세워놓고 그 원칙을 따르자고 한다. 우리는 다음 원칙을 따른다. 우리의 최우선 순위는 가치 있는 소프트웨어를 일찍 그리고 지속적으로 전달해서 고객을 만족시키는 것이다. 비록 개발의 후반부일지라도 요구사항 변경을 환영하라. 애자일 프로세스들은 변화를 활용해 고객의 경쟁력에 도움이 되게 한다. 작동하는 소프트웨어를 자주 전달하라. 두어 주에서 두어 개월의 간격으로 하되 더 짧은 기간을 선호하라. 비즈니스 쪽의 사람들과 개발자들은 프로젝트 전체에 걸쳐 날마다 함께 일해야 한다. 동기가 부여된 개인들 중심으로 프로젝트를 구성하라. 그들이 필요로 하는 환경과 지원을 주고 그들이 일을 끝내리라고 신뢰하라. 개발팀으로, 또 개발팀 내부에서 정보를 전하는 가장 효율적이고 효과적인 방법은 면대면 대화이다. 작동하는 소프트웨어가 진척의 주된 척도이다. 애자일 프로세스들은 지속 가능한 개발을 장려한다. 스폰서, 개발자, 사용자는 일정한 속도를 계속 유지할 수 있어야 한다. 기술적 탁월성과 좋은 설계에 대한 지속적 관심이 기민함을 높인다. 안 하는 일의 양을 최대화하는 기술이 필수적이다. 최고의 아키텍처, 요구사항, 설계는 자기 조직적인 팀에서 창발한다. 팀은 정기적으로 어떻게 더 효과적이 될지 숙고하고, 이에 따라 팀의 행동을 조율하고 조정한다. 애자일 선언 이면의 원칙agilemanifesto.org 우리가 애자일 프로세스하면 떠올리는 몇가지 일들은 이 12개의 원칙을 기반으로 생겨난 것이다.예를 들면 스프린트(Sprint)는 3번 원칙에 기초하며 매 스프린트가 끝날 때마다 행하는 회고(Retrospective)는 12번 원칙에 기초한다.또한 애자일에서는 거대한 팀보다는 일명 Task Force Team(TF Team)을 선호하는데, 이건 5번에서 기초한다. 그 이유는 거대한 팀보다 작은 팀이 더 유기적으로 움직이기 편하기 때문이다. 이 12개의 원칙은 어떻게 하면 더 애자일하게, 즉 기민하게 프로덕트를 생산하고 고객에게 전달할 수 있는지에 대한 일종의 가이드라인 역할을 하는 셈이다. 애자일의 기본적인 요소들 [출처] Differences Between Lean, Agile and Scrum 애자일은 12개의 원칙에 기초하는 여러 개의 요소를 가지고 있다. 이 포스팅에서는 구성 요소들 중 필자가 중요하다고 생각하는 User Story, Backlog, Estimation, Retrospective를 짚고 넘어가겠다. 한번 간단하게 하나하나 살펴보자. User Story애자일은 유저 스토리 기반으로 구성된다. 이 스토리라는 것이 상당히 중요한데, 이건 철저히 유저 중심으로 만들어져야 한다. 우리가 보통 스토리 혹은 이슈를 생성하면 이런 식으로 작성될 확률이 높다. SD-1234 요청서 페이지 리뉴얼 SD-1235 웹뷰로 되어있는 프로필 페이지를 네이티브로 변경 SD-1236 소셜 로그인을 위한 DB 스키마 설계 및 API 엔드포인트 작성 SD-1237 유저 채팅에 사진 여러 장 한번에 보내기 기능 추가 필자가 애자일 코칭을 받을 때 애자일 코치님이 가장 먼저 지적한 것이 바로 이것이었다. 애자일 코치님은 스토리에 되도록이면 누가(Who) 무엇을(What) 왜(Why)의 세가지 요소가 들어가면 좋다고 했었다.이렇게 작성된 스토리는 그 제목 자체만으로 팀이 왜 이 스토리를 해결되어야 하는지 설명해준다. 따라서 위의 스토리들은 이렇게 다시 작성되어야 한다. SD-1234 고객은 요청서를 쉽게 보내기 위해 필요한 정보를 한눈에 확인할 수 있다. SD-1235 고객은 자연스러운 UX를 위해 프로필 페이지를 웹뷰가 아닌 네이티브로 사용할 수 있다. SD-1236 클라이언트 개발자는 소셜 로그인 기능 개발을 위해 관련 API 엔드포인트를 사용할 수 있다. SD-1237 고객은 채팅 내에서 여러 장의 사진을 쉽게 보내기 위해 멀티 셀렉트 기능을 사용할 수 있다. 이렇게 작성된 유저 스토리는 이 스토리들과 전혀 관련없는 제 3의 팀원이 보더라도 별다른 부연 설명없이 원하는 정보를 얻을 수 있다. 그리고 유저(Who)는 반드시 고객이 아니어도 상관없다. SD-1236의 경우 유저는 클라이언트 개발자이다. 이런 경우에도 클라이언트 개발자(Who)는 소셜 로그인 기능 개발을 위해(Why) 관련 API 엔드포인트를 사용할 수 있다.(What)과 같은 정보를 모두 채워서 넣으면 된다. 팀원들은 프로덕트의 생산자이자 서로의 내부 고객이기도 하기 때문이다. Product BacklogProduct Backlog는 앞으로 처리해야할 여러 개의 유저 스토리로 구성되어 있다. 이 스토리들은 PO(Product Owner)가 정의한 우선 순위대로 정렬되어 있으며 이 우선 순위에 대한 권한은 PO의 직권이다. 일단 이 애자일은 팀원들이 PO가 내린 결정을 신뢰해야 한다는 게 기본 전제 조건이다.그렇다고 무조건 믿고 까라면 까라는 게 아니고, 당연히 의견이 있다면 자유롭게 토의할 수 있지만 최종 결정은 PO가 한다는 의미이다.그리고 백로그는 그냥 방치하면 계속 스토리들이 쌓이게 되는데 PO는 주기적으로 이 스토리들이 실행 여부를 판단하여 스토리들을 정리해주는 작업을 해줘야한다. 그렇지 않으면 스토리들이 계속 쌓여서 어떤 스토리가 의미가 있고 어떤 스토리가 의미가 없는 지 판단하는 데 걸리는 시간이 길어지게 된다. EstimationEstimation은 한국말로 직역하면 견적 정도의 의미이다. 근데 회사에서는 다들 그냥 에스티메이션이라고 하므로 필자도 거기에 익숙하기 때문에 그냥 에스티메이션이라고 하겠다.에스티메이션은 PO가 만들어놓은 여러 가지 스토리를 가져와서 실제 그 스토리를 해결할 팀원들이 이 이슈가 어느 정도의 크기를 가진 이슈인지 스토리 포인트를 매기는 것이다. 이때 많이 하는 방법이 1점 === 1시간으로 생각해서 스토리 포인트를 절대값으로 매기는 데, 필자의 직장도 처음에는 1점 === 1시간으로 생각하고 하루에 집중해서 일할 수 있는 시간을 4시간으로 정한 후 4점 === 이 이슈는 하루 정도는 걸리는 이슈다. 라고 에스티메이션하는 방법을 사용했다. 근데 문제는 이 예상이 거의 대부분 빗나간다는 것이다. 물론 자기가 하는 일이 정확히 언제 끝날 지 산정하실 수 있는 굇수분들도 있겠지만 필자와 필자의 팀원들은 그냥 일반인이기 때문에 그러지 못했다. 예상하지 못한 버그가 터지거나 예상하지 못한 회의가 생겼다거나 혹은 갑자기 아파서 병가를 썼다거나 이런 예외 상황은 생각보다 자주 발생했고 그럴때마다 스프린트도 펑펑 터져나갔다. 그래서 택한 방법은 바로 스토리 포인트를 상대값으로 매기는 것이다. 방법은 이렇다. 과거에 지나갔던 스토리 중에 기준이 될만한 스토리를 하나 가져와서 그 스토리에 대해서 팀원들이 그때의 경험을 토대로 에스티메이션을 한다. SD-1234 사용자는 요청서를 쉽게 보내기 위해 필요한 정보를 한눈에 확인할 수 있다. 철수: 이거 생각보다 일이 없었던 것 같은데? 저는 3.영희: 이거 디자인하느라 맨날 야근했어. 나는 5.민철: 난 영희를 좋아하니까 나도 5. 이런 식으로 어느 정도 팀원들의 의견이 수렴이 되면 요청서 페이지의 디자인을 바꿨던 스토리가 5정도 된다라는 기준을 잡을 수 있다. 이제 이 스토리를 기준으로 다른 스토리들의 크기를 상대적으로 측정해나가는 것이다. 측정하는 스토리 포인트는 XS, SM, M, L, XL와 같이 티셔츠 사이즈로 하는 방법도 있고 1, 2, 3, 4와 같은 자연수를 사용해도 상관없다. 어차피 상대적인 크기를 측정하는 것이므로 추상적인 크기만 설정할 수 있으면 된다. 참고로 필자의 회사는 1, 2, 3, 5, 8...의 피보나치 수열을 사용하고 있다. 처음에는 상대적인 크기를 측정한다는 것이 어렵게 느껴질 지 모르지만 필자가 경험해본 느낌으로는 시간을 측정하는 것보단 이 방법이 더 정확했다. 단, 팀이 한 스프린트에 어느 정도의 스토리 포인트를 쳐낼 수 있는지에 대한 정보가 먼저 필요하다. 이건 그냥 적당히 몇번 돌려보면 각이 나온다. 첫번째 스프린트에 40, 두번째 스프린트에는 20을 쳤다면 다음 스프린트에는 평균 값인 30만큼의 스토리만 넣는 것이다. 비록 첫번째 스프린트에 비해 두번째 스프린트는 달성률이 50%나 떨어졌지만 걱정하지말자!두번째 스프린트에 끝내지 못한 이슈는 세번째 스프린트까지 이어진 후 그 스프린트 안에 끝나기 때문에 다음 스프린트의 달성률은 원래 예상치보다 더 높아지므로 세 스프린트의 평균을 내보면 비슷한 값이 나온다. 그런 식으로 몇번 스프린트를 돌려보면 한번의 스프린트가 도는 동안 팀이 평균적으로 몇 만큼의 스토리 포인트를 쳐낼 수 있는지 각이 나온다. 저번 스프린트에 망한 달성률만큼 다음 스프린트에는 초과 달성될 확률이 높다! 그리고 중요한 건 스프린트 마감일은 데드라인이 아니다. 배포가 예상보다 늦어졌다면 늦어진 이유를 분석하고 다음에 또 늦지않으면 되는거니 너무 부담가지지 말자. 두번 실수하지만 않으면 된다. Retrospective회고(Retrospective)는 스프린트를 마무리하는 단계에서 행하는 것으로 필자는 다른 것들보다 이 회고가 가장 중요하다고 생각한다.회고는 팀이 발전하기 위한 방향을 논의하는 가벼운 회의이다. 절대 무거운 분위기로 진행하는 것이 아니다! 그래서 필자의 직장에서는 회고를 사무실 근처 카페에서 커피를 마시면서 진행하기도 한다. 회고 방법은 타임라인을 그려놓고 팀원들이 각자 따뜻하게 느껴졌던 일과 차갑게 느껴졌던 일을 포스트잇에 적어서 타임라인에 붙히고 투표를 통해 논의하는 방법도 있고 그냥 각자 미리 주제를 정해와서 자유롭게 토론하는 방법도 있다. 이렇게 회고를 통해서 다음 스프린트에 시행할 액션 아이템(Action Item)을 정한 다음 각 액션 아이템마다 담당자를 붙혀준다. 해당 액션을 관리할 담당자가 없으면 흐지부지 해지기 쉽기 때문이다.담당자가 된 팀원은 다른 팀원들에게 주기적으로 노티를 줘서 책임감을 가지고 그 스프린트 안에 해당 액션 아이템을 처리하는 것을 목표로 한다. 그럼 애자일은 좋은 건가?일단 필자는 개인적으로 데드라인에 쫓기는 폭포수 모델보다는 1~2주안에 작은 스토리를 처리하고 배포하는 애자일을 좋아하긴 한다. 짧은 시간안에 개발과 배포가 이루어지다보니 사용자의 피드백을 빠른 시간안에 받을 수도 있고 뭔가 배포할때마다 느끼는 상쾌함도 자주 돌아오기 때문에 좋다. 그러나 애자일이 모든 상황에서 항상 잘 듣는 만병 통치약은 아니다. 애매한 상황도 생긴다는 얘기다. 이건 애자일 하지 않아요!이 대사는 필자가 직장에서 데일리 스크럼 때 꽤나 많이 들었던 말이다. 상황은 대충 이렇다. 뭔가를 해야한다. -> 생각보다 이슈의 크기가 크다. -> 기능 간 디펜던시가 커서 따로 배포는 힘들 것 같다. -> 그냥 한번에 하고 배포하자! 이런 상황에서 누군가 이야기 한다. 흠 근데…이건 애자일 하지 않은 것 같은데요…? 그러면 그 공간에 있는 모두가 머리를 싸매기 시작한다. 애자일하게 이걸 처리하려면 어떻게 해야하지...? 라는 생각으로 머리가 복잡해지기 시작한다.그럼 애자일하게 하려면 어떻게 해야할까요?라고 누군가 서두를 던져보지만 아무도 답을 내지 못한다. 이건 애초에 잘게 쪼갤 수 없는 이슈였기 때문이다.하지만 애자일의 본질은 이런게 아니다. 어떤 규칙과 규정으로 정해져 있는게 아니란 뜻이다. 물론 애자일 선언과 12개의 법칙이 있긴 하지만 그건 그냥 가이드라인일 뿐이다. 각 팀마다 성격이 다른데 어떠한 방법이 전세계 모든 팀에게 모두 좋은 효과를 보인다는 건 말도 안된다.필자는 애자일의 기본 가이드 라인을 따라가되, 각 팀에 맞게 유연해서 변형해서 사용하는 것이 더 자연스럽고, 또 그게 맞는 방법이라고 생각한다.굳이 애자일 선언이나 뭐 저런 법칙이나 그런 걸 굳이 안 따라하더라도 그 팀에 맞는 어떠한 창의적인 방법이라도 괜찮다. 다 일 잘하자고 하는건데 굳이 어떤 규칙을 만들어서 지켜야할 이유는 없다. 일만 잘하면 됐지. 애자일에 너무 얶매이지는 말자필자는 사실 애자일 선언을 처음 들었을 때 마치 공산주의같다고 생각했다.마르크스가 처음 마르크스주의를 제창하고 자본론을 집필했을 때의 사회 분위기는 이러했다. 공산주의는 완벽한 체제이고 점점 발전해가며 자본주의는 언젠가 스스로 붕괴할 것이라는 분위기가 노동자들 사이에서 팽배했다. 그러나 현실에 도입된 공산주의는 현실의 벽 앞에 부딫히고 스탈린주의, 마오주의 등으로 점점 변질되어 대부분 독재로 마무리 되었다. 원래는 우리 모두 함께 잘살아보세!에서 시작했지만 현실에 강림한 공산주의의 실체는 우리 함께 못살자가 되버린 격이다. 마찬가지로 현실의 애자일도 생각보다 순탄하게만은 돌아가지 않는다. 방금 위에서 예를 든 상황도 굉장히 애매한 상황 중 하나이다. 이슈를 쪼갤 수 없어서 한 스프린트안에 배포가 힘들 것 같은데 어떻게 해야한단 말인가? 애자일 선언은 물론 굉장히 좋은 말이고 옳은 말이지만 저기에 너무 집착하게 되면 오히려 일을 위한 일을 만들게 되는 조직이 된다고 생각한다. 우리가 지금 누리고 있는 자본주의도 원래 애덤 스미스 형의 시장은 냅두면 알아서 잘 굴러간다 운운하던 예전 그대로라면 망테크였겠지만, 공산주의에서 제창하던 것들을 좋은 방향으로 잘 섞은 뉴딜정책으로 인한 국유화 사업과 복지 정책 등이 포텐터지면서 지금까지 발전하게 된 것이다. 뭐든지 절대적으로 완전한 건 없다. 마찬가지로 필자는 애자일이든 폭포수 모델이든 칸반이든 뭐가 됐든 그냥 일만 효율적으로 잘하면 그만이라고 생각하기 때문에 저 애자일 선언은 그냥 한번 듣고 흘렸다. 그냥 좋아보이는 걸 적당히 섞어서 써도 팀에만 잘 맞는다면 상관없다.애자일 선언은 너무나도 당연한 말들만 해서 오히려 더 담아두지 않았던 것 같다. 마치며사실 필자가 애자일 코칭을 받고 나서 느낀건 애자일에는 정답이 없구나와 동시에 아니 그럼 저 애자일 선언이고 나발이고 그냥 기민하게 제품을 배포만 할 수 있으면 아무 상관없는 거 아닌가?라는 아이러니한 생각이었다.지금도 필자는 애자일은 그냥 제품을 기민하게 고객에게 배송할 수만 있다면 스프린트를 1주로 하든 2주로 하든 에스티메이션을 어떻게 하든, 방법 따윈 뭐가 됐든 상관없다고 생각한다. 다만 만약 여러분의 팀에서 애자일을 도입해보고자 한다면 제대로 해보는 게 좋다고 생각하기 때문에 필자가 애자일 코칭을 하면서 배웠던 경험들을 공유한다.팀원들이 단합해서 이것 저것 다 시도해보고 회고를 통해 어떤 점이 좋았는지, 어떻게 하면 우리 팀이 좋은 방향으로 개선될지에 대해 논의하고 실제로 더 나아지는 경험은 쉽게 얻을 수 없는 좋은 경험이기 때문이다. 해보면 되게 재밌다 진짜로. 애자일은 제대로 하려면 생각보다 공부가 굉장히 많이 필요한 개발 방법론이다. 알아야 할 것도 많고 이것들을 각자의 팀에 맞게 응용하고 점검할 수 있는 능력도 있어야 한다.그러나 필자가 직접 다른 조직에서 경험하거나 근처 개발자들한테 주워들은 얘기를 빌자면, 애자일을 도입한 많은 조직들이 단순히 스프린트를 돌리고 데일리 스크럼 미팅을 하고 JIRA를 사용하기만 하면서 애자일을 한다고 하는 경우도 많이 있다. 우리 팀이 스프린트에 쳐낼 수 있는 스토리 포인트가 어느 정도인지, 또 우리가 애자일을 하면서 외부 고객과 내부 고객들이 만족하고 있는지, 이런 끊임없는 점검이 부재한 애자일은 그냥 1~2주 단위의 데드라인을 반복할 뿐이다. 이거 해봐서 아는데 진짜 죽을 맛이다. 매일 데드라인의 압박을 느낀다는 건…후 이 글을 읽는 여러분이 애자일에 관심을 가짐과 동시에 협업에 대한 열정도 뿜뿜하면서 팀에서 재밌고 즐겁게 일할 수 있기를 바란다. 이상으로 애자일이 도대체 뭐길래? 포스팅을 마친다.","link":"/2019/07/02/what-is-agile/"},{"title":"데이터 기반 의사결정, 과연 완벽한 걸까?","text":"이번 포스팅에서는 데이터 기반 의사결정에 대해서 이야기해볼까한다. 데이터 기반 의사결정은 2013년 쯤 빅데이터 열풍이 불면서 뜨기 시작했다. 이미 많은 기업들이 사용하고 있는 의사결정 방법이며 또한 여러가지 선례도 많기 때문에 나름 신뢰성을 가지는 의사결정 방법이다. 단, 데이터의 본질을 제대로 이해하고 사용했을 때만 말이다 데이터 기반 의사결정은 세상이 떠드는 것 만큼 신뢰를 가질 수 있는 의사결정 방법일까? 아니, 그 전에 데이터의 본질이라는 게 뭘까? 데이터의 본질이 뭐길래?우리는 일반적으로 데이터와 정보(Information)를 동일시 한다. 그러나 사실 데이터 그 자체는 정보가 될 수 없다.데이터란, 단순히 현실 세계를 관측해서 얻은 값이다. 그냥 값 그 이상 그 이하도 아니라는 것이다.반면에 정보란, 이 데이터들을 어떤 방식으로든 처리해서 의미를 가지는 단위로 만든 것이다. 이때 데이터를 처리해서 정보로 만드는 과정을 데이터 마이닝(Data mining)이라고 하는 것이다. 이렇게 얘기하면 뭔가 전공책에나 나올 것 같은 느낌이니까 쉽게 풀어보자.예를 들어서 내일 평균 기온이 섭씨 28도라고 한다. 이때 내일 평균 기온: 섭씨 28도는 데이터가 된다. 이 값은 기상청 슈퍼컴퓨터를 써서 측정했든 알파고를 썼든 뭘 썼든 간에 아직까지는 그냥 관측한 값일 뿐이다. 이제 이 데이터를 내일 집에만 있을 예정인 사람한테 건네주면 의미가 있을까?뭐 집에 에어컨이 없다면 큰 의미가 있겠지만, 2019년 대한민국에서는 에어컨은 몰라도 선풍기 정도는 집에 하나씩 다 있기 때문에 별 의미는 없다고 본다. 이러면 이 데이터는 별 의미없는 값이 되어버리는 것이다. 내일 나가지도 않을건데 밖이 얼마나 덥든 별 상관 없지 않은가? 집에서 에어컨 틀어놓고 수박이나 먹으면 된다. 밖이 섭씨 28도든 말든 이불 밖으로 안나가면 그만이다. 하지만 이 데이터를 내일 서울시 강서구에서 경기도 판교까지 출근해야 하는 사람한테 주면 어떻게 될까? 내일 평균 기온: 섭씨 28도라는 데이터는 이 사람의 생사를 가를 수도 있는 굉장히 의미있는 정보가 된다.(필자는 더위에 약하기 때문에 조금 오버했다)여기다가 내일 긴팔 입고 나가면 객사할 수도 있으니 조심하세요!^^라고 자연어로 표현된 귀여운 정보도 함께 보여주면 좋겠다. 즉, 데이터는 그 자체로는 별로 의미가 없지만 어떤 상황에서 사용되느냐, 어떤 이유로 사용되느냐에 따라 그 정보성의 가치는 천차만별이 되는 것이다. 이게 바로 데이터의 본질이다. 데이터는 어떻게 수집할까?그렇다면 데이터 기반 의사결정은 어떤 식으로 하는 걸까?별 거 없다. 그냥 데이터를 주구장창 모은다. 방대하게 비축된 데이터는 그것만으로도 회사의 자산이라고 할만큼 가치있다. 데이터가 있으면 어떻게 가공하든 그건 자유지만 데이터가 없으면 가공도 못하기 때문이다. 나도 모르게 스토킹 당하고 있다우리와 거의 물아일체인 스마트폰을 지배하는 애플과 구글 역시 알게모르게 데이터를 다 모은다. 아이폰 유저라면 연락처 앱 내에서 가장 상단에 위치한 자기 프로필을 눌러보자.쭉 내려보면 주소(Siri가 특별한 위치에서 찾음)이라는 제목으로 보통 여러분의 집이나 직장이 저장되어 있을 것이다. 필자 역시 시리한테 스토킹당하고 있는 운명이다. 필자도 결국 애플의 노예 중 한명이기 때문에, 시리에게 추적당하고 있다. 필자의 집은 실제로 서울시 강서구 양천로 어딘가이고 밑에 나온 주소는 필자의 직장 사무실 소재지이다. 이런 정보는 국가보안법 위반도 아니니 그냥 까발리기로 한다. 어쨌든 이 정보를 실제 애플 서버에 저장하는 지 어떤지는 잘 모르겠지만 어쨌든 이런 식으로 기업들은 여러분의 데이터를 지금 이 순간도 실시간으로 모으고 있다. 물론 애플이나 구글같은 대기업이 아니라 스타트업 규모의 회사라도 사용자의 행동 정보를 모두 추적하고 있다. 기업들은 Google Analytics, Amplitude, Mixpanel, Hotjar 등의 솔루션을 사용하여 여러분의 행동 정보를 쌓고 최대한 의사결정에 이용한다. 이쯤 되면 이런 생각이 들 것이다. 난 언제나 널 지켜보고 있어… 그렇다. 여러분은 여러분도 모르게 기업들한테 질 좋은 데이터들을 뿜뿜 제공해주고 있다. 조지 오웰의 1984에 등장하는 빅브라더가 생각난다. 물론 대한민국 정부가 이걸 그냥 가만 보고 있을 양반들은 아니라서 어찌어찌 힘쓰고 있긴 하다. 2016년에 행정자치부 및 관계부처가 합동 발표한 개인정보 비식별 조치 가이드라인에 따르면 행동 데이터를 포함한 개인정보 데이터는 개인을 식별할 수 없는 형태로 비식별조치하고 필요한 데이터만 남겨놓는 형태로 사용해야 한다고 권고하고 있긴 하다. (사전 검토) 개인정보에 해당하는지 여부를 검토 후, 개인정보가 아닌 것이 명백한 경우법적 규제 없이 자유롭게 활용 (비식별 조치) 정보집합물(데이터 셋)에서 개인을 식별할 수 있는 요소를 전부 또는 일부삭제하거나 대체하는 등의 방법을 활용, 개인을 알아볼 수 없도록 하는 조치 (적정성 평가) 다른 정보와 쉽게 결합하여 개인을 식별할 수 있는지를 「비식별 조치적정성 평가단」을 통해 평가 (사후관리) 비식별 정보 안전조치, 재식별 가능성 모니터링 등 비식별 정보 활용 과정에서재식별 방지를 위해 필요한 조치 수행 개인정보 비식별 조치 가이드라인행정자치부 근데 또 이 권고 사항이 겁나 까다롭기도 하고 말 그대로 권고라서 아마 대부분의 기업에서는 그냥 무시하고 하던대로 하고 있을 거라고 생각한다. 그래도 정상적인 회사라면 회원가입할때 띄워주는 개인정보 처리방침같은 문서에 여러분의 개인정보를 어떤 이유로, 어떤 방식으로 사용할 것인지 적어놨기 때문에 귀찮더라도 한번 읽어보길 추천한다. 이제 데이터 기반 의사결정이다!데이터 기반 의사결정이란 결국 이렇게 수집한 데이터를 분석해서 뭔가 그럴싸한 결론을 도출해내는 것이다.아무래도 그냥 밑도 끝도 없이 결정하는 것보다는 뭔가 관측한 값을 기반으로 의사결정을 하기 때문에 상대적으로 리스크가 적은 결정을 할 확률이 높다. 특히 이런 데이터 드리븐 방식은 마케팅에서 굉장히 잘 써먹는데, 바로 여러분이 관심을 가질만한 상품을 추천하는 맞춤 광고 시스템이다. 맞춤 광고 시스템은 여러분이 웹 상에서 이것저것 하고 다닐 동안 그런 행동 데이터를 수집해서 여러분이 어떤 것에 관심이 있는 지 분석한 후 거기에 맞는 광고를 보여주는 것이다. 이런 맞춤 광고 시스템도 일종의 데이터 기반 의사결정이라고 할 수 있다. 확실히 관심을 가질 확률이 높은 상품을 찔러보는 것이기 때문에 그냥 아무 상품이나 찔러보는 것보다 전환률이 확실히 높다. 그럼 먼저 데이터 기반 의사결정에는 어떤 장점이 있는지 한번 살펴보자. 데이터 기반 의사결정의 장점당연히 데이터 기반 의사결정의 장점은 많다. 하지만 하나하나 다 나열하면 끝이 없으므로 필자가 생각하는 대표적인 장점 몇가지만 적어보겠다. 불확실성을 최소화 할 수 있다먼저 제일 단순하고 대표적인 장점으로 의사결정의 후폭풍으로 몰려오는 리스크가 적다는 것이다. 위에서 말했듯이 그래도 관측을 통한 데이터를 가지고 분석해서 의사결정 하는 것이기 때문에 그냥 밑도 끝도 없이 직관만 믿고 결정하는 것 보다는 당연히 리스크가 적다. 타인을 설득하기 쉽다두번째로는 타인을 설득하기 쉽다는 것이 있다. 필자 생각에는 이건 조금 심리적인 이유도 있는 것 같다. 아무래도 그냥 말로만 얘기하는 것보다 그럴싸한 그래프와 표를 그려놓고 숫자를 기반으로 얘기하는 것이 더 신뢰가 가지 않겠는가? 이건 아무래도 논리와 숫자에 익숙한 개발자같은 직군보다는 직관을 잘 활용해야 하는 디자이너같은 직군들에게 더 해당이 되는 이유인 것 같다. a. UX상 이 버튼은 여기에 위치하는 것이 더 유저들이 발견하기 쉬울 거에요!b. 유저 테스트를 해본 결과 78%의 유저들이 페이지 상단 20% 내 공간에서만 행동하다가 이탈했습니다. 그러므로… 이 두개의 차이인 것 같다. 같은 디자이너끼리야 척하면 척이겠지만 다른 직군, 특히 PO나 CEO, 개발자처럼 디자이너가 아닌 사람들을 상대할 때는 확실히 후자가 더 설득하기 쉽다. 물론 저 숫자를 무조건 신뢰하는 것은 위험하다. 그러나 데이터 수집 방법이나 실험 환경이 신뢰가 갈만한 환경이었다면 충분히 들이대볼만한 방법이다. 기대와 성과가 명확하다데이터를 기반으로 기능을 기획하면 우리가 이 기능을 만들면 유저의 이 행동이 증가할거야라는 식의 기대를 가지고 기획을 하게 된다. 배포한 후에 실제로 해당 행동이 몇 퍼센트나 증가했는지 데이터를 분석해보면 바로 나오기 때문에 성과도 명확하게 가져갈 수 있다. A/B 테스트를 통해서 배포한 기능으로 인해 실제로 원하는 결과를 얻었는지 테스트하기도 한다 복잡한 문제를 단순화 시킬 수 있다.위에서 설명한 맞춤 광고같은 경우는 사실 굉장히 복잡한 문제이다. 애초에 사람의 관심사라는 추상적인 개념을 수치해석적으로 표현한다는 것은 쉬운 일은 아니다.그러나 쉬운 일이 아니라는 게 못한다는 말은 아니다. 예를 들어 필자는 요즘 CSV 전자담배에 관심을 가지고 실제로 구매를 해서 잘 사용하고 있다. 그 과정에서 필자는 블로그와 유튜브를 돌아다니면서 전자담배에 대한 리뷰를 살펴봤고 전자담배에 끼울 수 있는 액상통과 액상을 웹 상에서 구매할 수 있는지 알고 싶어서 전자담배용품 판매 사이트도 들락날락거렸다. 분명히 필자의 웹 히스토리에는 전자담배라는 키워드가 다수 검출될 것이다. 이 키워드를 수집한 후 필자에게 전자담배 키워드를 가진 광고를 노출시켜주는 것은 그렇게 어려운 기술은 아니다.이렇게 이 사람의 관심사를 알고 싶다라는 복잡한 문제를 단순한 키워드 매칭으로 해결할 수 있는 것도 데이터 기반 의사결정의 장점 중 하나이다. 데이터 기반 의사결정의 단점자, 여기까지만 보면 데이터 기반 의사결정은 리스크도 적고 성과도 명확히 측정할 수 있고 복잡한 문제도 쉽게 풀 수 있는 빈틈없는 의사결정 방법인 것 같다! 응. 아니야 물론 데이터 기반 의사결정은 장점이 많은 의사결정 방법이지만 위에서 이야기한 데이터의 본질을 제대로 알고 사용하지 않는다면 현실과 전혀 동떨어진 이상한 의사결정을 하게 될 수도 있다.필자가 생각하기에 주로 놓치게 되는 포인트들은 이렇다. 데이터는 그냥 값이다.위에서도 한번 얘기했듯이 데이터는 그냥 단순한 값에 불과하다. 즉, 데이터를 분석해서 정보성을 가지도록 만들어야 하는데 이 과정에서 사람의 주관이 들어갈 수 밖에 없다는 것이다.보통 조직에서 데이터를 분석하는 역할을 맡은 데이터 분석가(Data Analyst)의 직관(Insight)이 크게 작용한다. 사실 이 직관이 얼마나 정확하냐가 이 사람들의 능력이기도 하다. 중요한 것은 데이터를 기반으로 나온 결과는 결국 사람의 주관적인 평가가 들어간 정보라는 것이다. 데이터 기반 의사결정을 너무 신뢰하다보면 이렇게 생각하기 쉽다. 우리가 직접 관측한 값을 기반으로 나온 결과니까 결과도 정확할거야! 땡. 데이터가 정확하다고 해서 결과도 정확하리라는 법은 없다. 항상 비판적인 시각으로 결과를 바라봐야 한다.이 비판적인 시각이란 바로 왜 이 결과가 이렇게 나왔는지 따지는 습관을 의미한다. 데이터를 맹신하면 잘못된 결정을 내릴 수 있다.데이터는 여러분에게 단지 일어난 현상 그 자체를 설명할 뿐, 절대 여러분에게 이 현상이 발생한 이유에 대해서는 설명해주지 않는다. 그래서 왜 이런 데이터가 나왔을까라는 질문은 보통 전문가의 직관에게 맡기거나 좀 더 공을 들여서 정성적 데이터를 수집하는 방식으로 알아내기도 한다.그렇다면 이유를 생각하지 않고 현상만 보고 의사결정을 하는 것이 왜 위험할까? 인과관계와 상관관계인과관계와 상관관계는 통계학에서 기초 중의 기초이다. 이것도 파고들면 한도 끝도 없으므로 간단한 정의 정도만 설명하고 넘어가겠다. 먼저 상관관계는 어떤 통계적인 변인이 증가할 때 다른 변인도 증가하거나 감소하는 것을 의미한다. 변인 $x$가 감소할 때 변인 $y$도 함께 감소하거나 증가한다는 것이다. 이 두개의 변인이 완전 제멋대로 움직이면서 따로 놀고 있다면 이 두개의 변인은 상관관계가 없는 것이다.이때 두 변인이 얼마나 상관이 있냐를 나타내는 수를 상관계수라고 한다. 상관계수가 0에 가까울 수록 두 변인은 별로 상관관계가 없는 것이고, -1에 가까울 수록 음(-)의 상관관계, 1에 가까울 수록 양(+)의 상관관계가 크다고 할 수 있다. 그 다음 인과관계는 먼저 선행된 한 변인이 후행되는 다른 변인의 원인이라고 판명되는 관계이다. 예를 들면 이런 느낌이다.비가 많이 올 수록 한강의 수위가 올라간다 이 예시에서도 사실 다른 숨은 변수들이 있을 수는 있지만 대체적으로 비가 많이 올 수록 한강의 수위가 올라간다라는 관계에서 비가 많이 온다라는 변인은 한강의 수위가 올라간다라는 변인에 다른 변수보다 큰 영향을 끼치는 것이 거의 확실하므로 이런 경우 그냥 인과관계라고 친다. 사실 인과관계와 상관관계의 구분은 생각보다 명확하게 딱 떨어지는 그런 건 아니다. 대충 느낌으로 보면 이렇다.비가 많이 올 수록 한강의 수위가 올라간다는 인과관계 이지만 여름이 가까워질수록 한강의 수위가 올라간다는 상관관계이다. 여름에 한강의 수위가 올라간 이유는 대한민국의 전체 강수량 중 70%정도가 여름에 집중되어있기 때문이지 단순히 여름이라서 그런건 아니기 때문이다. 비즈니스에서 대부분의 변인 관계는 대부분 상관관계이기 때문에 항상 숨겨진 다른 변수가 없는 지 생각해야하고 이 결과가 진짜 믿을 만한 결과 인지 끊임없이 의심하는 태도를 가져야한다. 잘못된 결정의 예시데이터를 맹신하면 어떻게 되는지에 대한 이해를 돕기 위해 데이터의 위험성을 설명할 때의 대표적인 예시인 두 가지 사례를 이야기해보겠다. 한 연구자가 아이스크림 판매량의 연중 증감 추이를 확인하고 있었다.그리고 연중 익사 사망자의 증감 추이를 함께 놓고 두 데이터 간의 상관분석을 해 보았다. 결과는 놀라웠다. 무서울 정도로 명백한 상관관계가 나타나고 있었다. 아이스크림 판매량이 증가하는 동안, 익사 사망자 수도 함께 증가하고 있었으며, 아이스크림 판매량이 감소하는 동안 익사 사망자 수도 감소하고 있었던 것이었다.연구자는 몸서리를 치면서 다음과 같은 결론을 내렸다. 익사 사망자의 증감은 아이스크림이 그 원인이다. 이 예시에서 연구자는 아이스크림이 많이 팔릴 수록 물에 빠져죽는 사람도 많다라고 얘기하고 있다. 터무니없는 소리 같지만 실제로 아이스크림 판매량 증감과 익사자 수 증감은 어느 정도 일치한다. 왜 그럴까? 여기서 숨겨진 변수는 바로 기온이다. 여름이 되서 더워지면 아이스크림 판매량이 증가하고, 피서철이라서 바닷가나 계곡에 놀러가는 사람들이 많이지기 때문에 사고로 인한 익사자 수도 증가하는 것이다.만약 여기서 숨겨진 변수인 기온을 파악하지 못했다면 이런 의사결정을 내릴 수도 있겠다. 익사자 수를 줄이기 위해서 아이스크림 판매를 금지한다. 자, 이제 두번째 예시를 한번 보자. 이 예시는 TED 2017에서 발표한 수학자이자 데이터 과학자인 Cathy O'Neil님께서 발표한 내용에서 가져온 것이다. 필자는 개인적으로 이 강연, 엄청 재밌게 들었다. TED에서 발표 중인 스웩넘치는 캐시 누나 1996년에 설립된 폭스 뉴스사는 고용 절차를 효율적으로 진행하기 위해 머신러닝 알고리즘을 도입하기로 했습니다. 학습에 필요한 데이터셋은 지난 21년간 폭스 뉴스에 지원한 지원자들의 데이터를 사용하면 되겠네요.그리고 회사에서 성공적인 역할을 할 수 있는 사람을 가려내기 위해서 성공에 대한 정의도 해야겠군요.음, 폭스 뉴스에서 4년 정도 근무하면서 적어도 한 번쯤 승진한 사람 정도면 성공했다고 할 수 있겠네요. 자, 이제 이 알고리즘을 현재의 지원자들에게 적용하면 어떻게 될까요? 여성 지원자는 절대 합격하지 못할 겁니다. 필자는 개인적으로 이 예시를 듣고 굉장히 충격받았다. 처음 저 이야기를 시작할 때 지난 21년간 폭스 뉴스에 지원한 지원자들의 데이터와 폭스 뉴스에서 4년 정도 근무하면서 적어도 한 번쯤 승진한 사람이라는 정의를 들었을 땐 꽤 공평하고 객관적인 지표라고 생각했기 때문이다. 하지만 우리 사회에서 성별에 전혀 관계없이 능력만 보고 직원을 고용하게 된 것은 생각보다 그렇게 오래 되지 않았기 때문에 위에서 정의한 규칙대로라면 폭스 뉴스에서 성공한 사람의 절대적인 양 또한 남성이 더 많을 수 밖에 없는 것이다.게다가 폭스 뉴스는 보수적인 언론사이기 때문에 더더욱 그런 편향이 나타날 것이다. 이 예시에서는 숨겨진 변수로 사회적 분위기가 있던 것이다. 이렇게 아무리 객관적이라고 생각했던 지표라고 할지라도 왜 이런 결과가 나왔지?를 고민하지 않고 의사결정한다면 현실과 전혀 동떨어진 의사결정을 할 수도 있게 된다. 정성적인 데이터를 무시하지 말자대부분의 조직에서 데이터 기반 의사결정을 할 때, 정성적인 데이터보다는 정량적인 데이터에 집중한다.그 이유는 정량적인 데이터가 수집, 비교, 가공 등 정보를 뽑아내는 데이터 마이닝 작업하기가 더 쉽기 때문이다. 반면에 정성적인 데이터를 제대로 수집하려면 유저 테스트, 설문 조사 등 조금 더 공을 들이는 방법을 사용해야 한다.또한 수집한 데이터도 숫자가 아닌 자연어인 경우가 많기 때문에 이를 분석하는 것 역시 쉬운 작업이 아니다. 이런 이유로 정성적인 데이터가 자주 우선순위에서 밀리는 모습을 많이 봐왔는데, 딱히 좋은 태도는 아니다.왜냐면 정량적인 데이터는 현상을 설명해주지만 정성적인 데이터는 이유를 설명해주기 때문이다. 예를 들면 둘의 차이는 이런 거다. 사용자들이 우리의 검색 시스템을 사용한 후 결제를 하는 전환률이 30% 상승했다. (정량적인 데이터)설문 조사 결과 사람들은 검색을 해서 자신들에게 더 맞는 상품을 스스로 찾았기 때문이라고 대답했다. (정성적인 데이터) 물론 위의 정성적인 데이터로 든 예시도 정량적인 수치로 변경할 수는 있다. 설문지에 자유롭게 대답할 수 있는 빈 칸이 아니라, 오지선다형으로 답변을 고르도록 하면 정량화할 수 있다.그러나 그렇게 하면 결국 답변에 없는 이유는 수집할 수 없기 때문에 결국 기타(아무거나 적으세요)라는 칸을 하나씩은 만든다.그렇다면 기타에 적힌 내용들은 정성적인 데이터가 되는 것이다. 그냥 단순히 생각해도 이 기타란에는 사람들이 직접 하나하나 적은 자연어가 들어갈 것이기 때문에 이걸 또 분류하고 분석하는 작업은 아마 인공지능아니면 인간지능 둘 중에 하나를 갈아넣어야 할 수 있을 것이다.그렇기 때문에 보통 기업들은 좀 더 취급하기 쉬운 정량적인 데이터에 좀 더 집중하고 정성적인 데이터는 보조적인 용도로 많이 활용한다. 하지만 고객들이 진짜 무엇을 원하는지, 그들의 목소리를 들어보고자 하는 기업이라면 리소스가 많이 투입되더라도 지속적으로 고객들에게 설문조사나 유저 테스트를 통해 정성적인 데이터를 많이 수집하려고 노력한다.필자의 직장은 리소스가 부족해서 자주 하지는 못하지만 그래도 큰 기능을 배포할 때는 UI/UX 디자이너들이 선별한 페르소나에 맞는 사용자들을 회사로 초청해서 유저 테스트를 진행하고 있다. 마치며필자는 데이터 기반 의사결정은 양날의 검이라고 생각한다. 잘 사용하면 위에서 설명한 장점대로 적은 리스크에 성과 측정도 명확하고 복잡한 문제도 추상화할 수 있는 좋은 도구이지만 잘못 사용하면 뭐가 잘못되고 있는지도 모른채 계속 숫자만 믿고 잘못된 결정을 하게 될 위험도 크기 때문이다. 또한 기대와 성과가 명확하다는 장점은 반대로 단점이 될 수도 있다. 위에서 계속 말했듯이 결국 데이터 자체는 그냥 값이기 때문에 뽑아낸 결과에는 최종적으로 사람의 주관이 들어갈 수 밖에 없다. 그 말인 즉슨, 뽑고 싶은 결과를 미리 정해놓고 거기에 데이터를 어떻게든 끼워맞춰도 그럴싸한 결과를 만들어 낼 수 있다는 말이다. 이런 점들은 진짜 고객을 위한 서비스를 만드는 방향이 아닌 회사에서 중요하다고 생각하는 지표만 높히는 방향으로 의사결정을 하거나, 실제로 고객들은 불편함을 느껴서 전환률이 떨어지더라도 자신이 원하는 결과가 나올때까지 변인을 조작하여 결국은 원하는 결과를 뽑아내는 등의 윗 사람들에게 인정받기 위한 정치적인 목적으로 사용되기 쉽다. 이런 행위들은 데이터 기반 의사결정이 아닌, 데이터 기반 의사결정의 탈을 쓴 숫자놀이에 불과하다. 그렇기 때문에 항상 데이터를 기반으로 의사결정을 할때는 자신이 객관적이고 공정한 결론을 내렸는지, 혹은 이 데이터가 정말 오염되지 않았고 유의미한 결과를 가져다 줄 수 있는 데이터인지, 원하는 대로 결과가 나왔다면 왜 이런 결과가 나오게 되었는지 자기 자신에게 끊임없이 되물으며 비판적인 사고를 유지해야한다. 이상으로 데이터 기반 의사결정, 과연 완벽한 걸까? 포스팅을 마친다.","link":"/2019/07/04/danger-of-data-driven/"},{"title":"Git 뉴비를 위한 기초 사용법 - 시작하기","text":"이번 포스팅에서는 너도 쓰고 나도 쓰고 우리 모두 쓰고 있는 Git의 기초에 대해서 포스팅 하려고한다. 필자는 Git을 대학교 때 처음 접했는데 처음에는 왠 이상한 클라우드에 소스코드를 올려놓는다 정도로만 이해하고 사용했던 기억이 난다. 하지만 Git의 기능은 단순히 코드 공유에서 끝나지 않는 버전 관리 도구이므로 Git을 잘 쓰면 실무에서 펼쳐지는 다이나믹한 상황에 유연하게 대처할수도 있다. 하지만 Git의 모든 기능을 이 포스팅에서 전부 다루기에는 지면이 부족하니, 이번에는 Git을 사용하기 위한 기본적인 명령어인 add, commit, push, pull, fetch에 대한 설명만 진행하도록 하겠다. Git은 누가, 왜 만들었나요?Git은 2005년 리누스 토르발즈가 자기가 쓰려고 만든 분산 버전 관리 시스템이다. 일단 이 리누스 토르발즈라는 핀란드 형부터가 프로그래머라면 대부분 알고 있을 정도로 유명하다. 아실 만한 분들은 다 아시겠지만 이 형은 오픈소스 커널인 리눅스(Linux)를 만든 사람이다. 리눅스 커널로 만든 유명한 운영체제는 데비안 계열의 우분투(Ubuntu), 레드햇 계열의 센트OS(CentOS) 등이 있다. 이 운영체제들은 서버에서도 많이 사용되고, 터미널의 기본 기능도 튼실해서 개발 친화적이기 때문에 프로그래머라면 대부분 알고 있는 운영체제이다. 참고로 안드로이드 OS도 리눅스 커널을 기반으로 만든 운영체제다. 뭐 어쨌든 이 형은 이 쪽 업계에서는 상당히 유명한데, 뭐 프로그래밍을 잘하거나 리눅스 커널 만든 걸로도 유명하지만 이런 걸로 더 유명하다. 세계 그래픽 카드 점유율 1위 기업한테 엿을 먹이는 패기 이게 대략 2012년 쯤인데, 어떤 포럼 연설에서 실제로 이렇게 엔비디아 엿먹으라고 했다. 당시에 엔비디아 옵티머스 칩을 사용하는 컴퓨터에서 리눅스로 만든 OS가 제대로 안굴러가는 이슈가 있었는데, 엔비디아가 지적재산권을 이유로 오픈소스 드라이버를 공개를 안했었기 때문에 리눅스의 설치가 힘들었던 것이다. 당시의 대화는 대략 이랬다. [청중] 아 님… 엔비디아 옵티머스 칩 쓰는 노트북에 리눅스 까는게 너무 힘들어요ㅜㅜ 좋은 방법 없을까요? [리누스] … 난 우리가 하드웨어 제조업체들과 겪었던 최악의 문제 중 하나가 엔비디아라는 것을 이렇게 공개적으로 지적할 수 있어서 기쁘네요! 이건 조금 슬픈 일인데, 엔비디아는 안드로이드 시장에 많은 칩을 팔고 있지만 이 회사는 우리가 경험한 회사 중 진짜 최악입니다. 그러니까 엔비디아… 엿이나 먹어라 리누스는 이런 형이다. 참고로 리눅스 커널이 오픈소스라고 함부로 커밋했다가는 이 형한테 욕먹고 현타가 올 수도 있으니 마음 단단히 먹자. 이 형이 바로 이 업계의 고든 램지다. 근데 왜 이 형이 Git을 만들게 되었냐? 리누스는 리눅스 커널을 만들 당시에 BitKeeper라는 분산 버전 관리 시스템을 사용했는데, 이 Bitkeeper라는 서비스는 원래 유료였지만 리눅스 커뮤니티에는 무료로 제공해주고 있었다. 근데 이 커뮤니티의 개발자 한명이 BitKeeper의 통신 프로토콜을 리버스 엔지니어링해서 해킹하는 사건이 발생했고, Bitkeeper는 무료로 리눅스 커뮤니티에 서비스를 제공하던 것을 철회한 것이다. 근데 이걸 쓰지 말라고 막은 게 아니라 그냥 무료에서 유료로 돌린 거다. 근데 이 형이 돈내기는 싫었는지 그냥 자기가 분산 버전 관리 시스템을 2주 만에 뚝딱! 만들었는데 그게 바로 Git이다.(2달 아니고 2주 맞다…) 리누스가 Git을 처음 커밋했던 내용은 Github의 Git 미러 저장소에서 확인해볼 수 있는데 여기서도 리누스 형의 성격이 드러난다. GIT - the stupid content tracker “git” can mean anything, depending on your mood. - random three-letter combination that is pronounceable, and not actually used by any common UNIX command. The fact that it is a mispronounciation of “get” may or may not be relevant. - stupid. contemptible and despicable. simple. Take your pick from the dictionary of slang. - “global information tracker”: you’re in a good mood, and it actually works for you. Angels sing, and a light suddenly fills the room. - “goddamn idiotic truckload of sh*t”: when it breaks Linus Torvaldsgit/git/README.md 음, 이게 리누스가 한 첫번째 커밋의 README.md 파일의 일부를 가져온 것인데, Git은 그냥 아무 의미 없는 세글자 알파벳이라고 한다. 그냥 유닉스 명령어 중에 git이라는 명령어가 없어서 정했다고 한다. 기분이 좋으면 global information tracker라고 하고 기분이 구리면 goddamn idiotic truckload of sh*t이라고 하랜다.(진짜 이 세상 쿨함이 아니다.) 정리하자면 Git은 핀란드의 어떤 천재 형이 쓰던 버전 관리 도구가 갑자기 유료가 되서 2주만에 만들어낸 시스템이고, 지금은 전 세계적으로 널리 사용되고 있는 분산 버전 관리 시스템이다. 기초적인 개념 알아보기Git은 분산 버전 관리 시스템이기 때문에 리모트 서버에 있는 소스를 수정하려면 로컬 환경으로 소스를 클론(Clone)하는 과정이 필요하다. 말 그대로 모든 소스를 복사하여 사용자의 컴퓨터로 받아오는 것이다. 이후 Git은 로컬 환경의 파일을 추적하고 있다가 사용자가 소스를 수정하면 그 변경 사항을 감지한다. 그 후 사용자는 자신이 리모트 서버에 변경 사항을 반영하고 싶은 파일이나 소스 코드의 라인을 고른 뒤 리모트 서버에 업로드한다. 맨 위의 origin/master는 리모트 서버의 버전, 맨 밑의 master는 필자 컴퓨터의 버전을 의미한다. 자 일단 Git에 대한 기본적인 개념은 이게 끝이다. 리모트 서버에 있는 파일을 내 컴퓨터로 복붙한 다음 수정해서 다시 리모트 서버로 업데이트한다는 것. 이때 사용자가 자신이 변경한 로컬의 소스를 서버의 소스에 업로드하는, 즉 서버로 밀어올리는 행위를 Push라고 부르고 사용자가 서버의 소스를 자신의 클라이언트로 가져오는 행위를 Pull 또는 Fetch라고 하는 것이다. 쉽지 않은가? 하지만 처음 Git을 접하면 평소에 접해보지 못했던 remote, origin, repository와 같은 용어들이 튀어 나오기 때문에 당황할 수 있다. 그럼 이 용어들이 무엇을 뜻하는지부터 간단하게 알아보자. Remote / Origin우선 Remote는 말 그대로 리모트 서버 자체를 의미한다. 이 리모트 서버라는 개념이 잘 이해가 안되시는 분은 우리가 자주 사용하는 구글 드라이브나 N드라이브와 같은 클라우드 스토리지를 사용하는 것을 떠올리시면 된다. 전 세계 어딘가에 있는 서버에 우리의 소스를 저장하는 것이다. 이때 이 서버를 제공해주는 대표적인 업체가 Github, Bitbucket, GitLab과 같은 회사들이다. 이 회사들이 Git을 만든 게 아니라 Git이라는 시스템에 필요한 리모트 서버와 Git을 좀 더 편리하게 사용할 수 있는 기능들을 제공하는 것이다. Git을 사용할 때는 내가 어떤 리모트 서버에 변경 사항을 업로드 할 것인지 정해야하는데, 반드시 하나의 리모트 서버만 사용할 수 있는 것이 아니기 때문에 내가 사용하는 리모트 서버의 이름을 정해줘야한다. 이때 주로 사용하는 관례적인 이름이 바로 Origin이다. 보통은 한 개의 리모트 서버만 운용하는 경우가 대다수이기 때문에 많은 사람들이 Remote와 Origin을 혼용해서 부르곤 한다. Repository레파지토리(Repository, Repo)는 저장소라는 뜻으로, 리모트 서버 내에서 구분되는 프로젝트 단위라고 생각하면 된다. 우리가 구글 드라이브를 사용할 때도 하나의 디렉토리에 모든 파일을 다 때려넣지않고 몇 개의 디렉토리를 만들고 용도에 따라 파일을 나눠서 구분하는 것과 동일하다. 일반적으로 한 개의 레파지토리는 하나의 프로젝트를 의미하지만 경우에 따라서 레파지토리 하나에 여러 개의 프로젝트를 구성하기도 한다. 12https://github.com/user/repository.githttps://user@bitbucket.org/group-name/repository.git 레파지토리를 클론받을 때는 해당 레파지토리를 가리키는 URL이 필요한데, 레파지토리의 이름은 URL의 맨 마지막에 .git 확장자를 가지는 방식으로 표현된다. Branch브랜치는 일종의 독립된 작업을 진행하기 위한 작업 공간의 개념이다. 맨 처음 Git을 초기화했을 때 기본적으로 master라는 이름의 브랜치가 하나 생성된다. 그 후 개발하는 기능 또는 버그 픽스에 따라서 브랜치를 새로 생성하고 거기서 작업한 후에 나중에 다시 master로 합치는 것이다. master 브랜치에서 다른 브랜치를 분리한 모습 이 브랜치 개념은 Git에 익숙하지 않은 분들에게 잠깐 설명하고 넘어가기에는 직관적으로 이해가 잘 안될 수 있는 개념이기 때문에 추후 다른 포스팅에서 다시 설명하겠다. 일단 지금은 이 3가지 정도만 기억해두자. Git을 초기화하면 기본적으로 master 브랜치가 생긴다. 이 친구가 메인 브랜치 역할을 한다. 브랜치는 어떤 브랜치에서 분리시키는 것이고, 분리된 브랜치는 분리될 당시의 부모 브랜치 상태를 그대로 가지고 있다. 개발자는 각각의 브랜치에서 개발을 진행한 뒤 나중에 다시 master 브랜치로 변경 사항을 합친다. 필수 명령어들을 알아보자만약 여러분이 혼자서만 프로젝트의 버전 관리를 한다면 단순히 리모트 서버의 레파지토리에서 소스를 받아와서 변경한 후 다시 리모트 서버로 업로드하는 과정만으로도 프로젝트를 진행하는데는 사실 아무 문제가 없다. 하지만 Git은 애초에 혼자서 개발하는 상황보다는 여럿이서 함께 소스를 수정하며 개발하는 협업 상황을 상정하고 만들었기 때문에 협업에서 발생할 수 있는 여러가지 곤란한 상황들을 타파하기 위한 많은 기능을 가지고 있다. Git은 기본적으로 CLI(Command Line Tools)을 통해 사용하고 commit, fetch, branch와 같은 여러가지 명령어를 사용하여 이 기능들을 사용할 수 있게 해준다. 그럼 이번에는 Git을 사용하여 버전을 관리하기 위해 기본적으로 알아야 하는 몇 가지 명령어를 한번 살펴보자. 리모트 서버와 연동하기cloneclone은 말 그대로 리모트 서버의 레파지토리에서 클라이언트로 파일을 복붙하는 행위를 말한다. 이때 클론을 수행하기 위해서는 어떤 레파지토리에서 파일을 가져올 것인지에 대한 정보가 필요한데, 이 정보는 위에서 설명했듯이 URL로 표현한다. HTTPS 프로토콜이나 SSH 프로토콜을 사용하여 소스를 클론할 수 있는데, 보통 HTTPS를 많이 사용한다. 보통 Github과 같은 리모트 서버 제공업체들은 레파지토리를 쉽게 클론할 수 있도록 눈에 잘 띄는 버튼을 만들어 놓고 해당 레파지토리의 URL을 제공하는 경우가 많다. 사용자는 단지 저 URL을 복사한 다음 Git의 clone 명령어를 사용해서 레파지토리를 클론하기만 하면 된다. 12$ cd ~/dev/evan # 원하는 작업 디렉토리로 이동$ git clone https://github.com/evan-moon/test-repo.git 원하는 작업 디렉토리로 이동한 뒤 clone 명령어를 사용하여 레파지토리를 클론하게되면 현재 위치에 레파지토리 이름과 동일한 디렉토리가 생성되고 그 내부에 리모트 서버의 소스가 전부 복사된다. 위 예제의 경우 ~/dev/evan 디렉토리 내부에 test-repo 디렉토리가 생성되고 해당 레파지토리의 소스가 복사될 것이다. 이제 이 복사된 소스를 맘대로 수정하거나 파괴해도 리모트 서버에 업로드만 하지 않는다면, 같은 리모트 서버를 보고 있는 다른 사람이 영향을 받을 일은 절대 없기 때문에 안심하고 맘대로 만지작거려도 된다. pullpull 명령어는 리모트 서버의 최신 소스를 가져와서 로컬 소스에 병합(Merge)해주는 명령어이다. 만약 우리가 처음 소스를 클론한 후에 다른 사람이 리모트 서버를 상태를 갱신했더라도 리모트 서버가 우리에게 그 변경된 사항을 알려주지는 않기 때문에 우리가 직접 서버에 문의를 날려야 하는 것이다. 또한 pull은 단순히 리모트 서버에서 로컬로 소스를 가져온다의 개념보다는 가져와서 합친다의 개념이기 때문에 브랜치끼리도 pull을 통해 소스를 합칠 수 있다. 12$ git pull # 현재 내 로컬 브랜치와 같은 이름을 가진 리모트 서버 브랜치가 타겟$ git pull origin master # origin 리모트 서버의 master 브랜치가 타겟 오픈소스에 관심이 많은 분이라면 Pull Request라는 단어를 들어보았을 것이다. 이 Pull Request는 내가 작업한 브랜치를 가져가서 합쳐줘~라는 의미이다. 필자는 처음에 이게 왜 Merge Request가 아니라 Pull Request인지 이해가 잘 안갔었는데, 나중에 와서 생각해보니 최종적으로 두 브랜치의 소스를 합치는 행위를 하는 주체가 요청을 한 사람이 아니고 요청을 받은 사람이기 때문에 요청을 받은 사람이 브랜치를 가져가서 합친다라는 관점에서 보면 적절한 네이밍인 것 같다는 생각을 했었다. fetchfetch는 리모트 서버의 최신 이력을 내 클라이언트로 가져오되 병합은 하지 않는 명령어이다. 1$ git fetch fetch 명령어를 사용하면 다른 사람들이 리모트 서버에 새로 업데이트한 모든 내역을 받아올 수 있다. 이제 그 내역을 보고 내 로컬에 있는 버전이 리모트 서버에 있는 버전보다 이전 버전이라면 pull 명령어를 사용하여 내 컴퓨터의 소스 코드를 갱신하면 된다. 그럼 이 명령어가 pull의 하위 호환이 아닌가? 라는 생각이 들 수도 있는데, pull과 fetch는 조금 용도가 다르긴 하다. pull 같은 경우는 일단 묻지도 따지지도 않고 바로 리모트 서버의 최신 소스를 가져와서 내 로컬 소스에 합쳐버리기 때문에 조금 위험하긴 하다. 뭐 예를 들면 지금 리모트 서버의 최신 소스가 버그가 있는 상태일 수도 있지 않은가? 그래서 필자같은 경우 보통 로컬 소스와 리모트 소스의 변경 사항을 미리 비교해보고 싶을 때 fetch를 사용한다. 그리고 fetch를 잘 이용하면 이런 얌생이도 가능하다. 123#!/bin/bashgit fetch --all -p; git branch -vv | grep \": gone]\" | awk '{ print $1 }' | xargs -n 1 git branch -d 이 쉘스크립트는 필자가 예전에 만들어 놨던 친구이다. fetch를 통해 리모트 서버의 최신 내용을 받아온 뒤, branch 명령어를 사용하여 리모트 서버에서는 삭제되었지만 로컬에는 남아있는 브랜치를 찾아서 싹 다 지워주는 스크립트이다. 참고로 로컬에는 있지만 리모트에서 삭제된 브랜치는 브랜치 이름 뒤에 : gone이라는 문구가 붙어있기 때문에 구분이 가능하다. 1234$ git branch -vv* master fa0cec5 [origin/master] 마스터 브랜치에욤 test 1f3578f [origin/test: gone] 리모트에선 죽은 브랜치 test2 fa0cec5 로컬에서 만들어지고 리모트에 업데이트는 안된 브랜치 fetch 명령어와 branch 명령어의 특성을 잘 이용하면 이런 꿀 스크립트를 만들 수도 있다. 변경 사항을 리모트 서버에 업데이트하기자, 지금까지는 리모트 서버의 내용을 로컬과 연동하는 명령어를 살펴봤다면 이제는 내 로컬에서 변경한 소스를 리모트 서버로 업로드하는 명령어들을 살펴볼 차례이다. 필자는 이 과정을 설명할 때 보통 택배로 예를 주로 드는 편이기 때문에 이 포스팅에서도 택배를 포장하고 배송하는 과정에 빗대어서 설명을 진행하겠다. add원하는 변경사항만 골라 담는 add 명령어 자, 평화로운 중*나라에서 중고 거래를 했다고 생각해보자. 물론 집에 있는 모든 물건을 보내는 혜자같은 분도 계시겠지만 일반인이라면 그러지 않기 때문에 우리는 상대방한테 물건을 보내기 전에 어떤 물건을 보낼 것인지 부터 정해야한다. 이때 add 명령어가 어떤 물건들을 포장할 것인지 고르는 과정을 담당한다. 1234$ git add . # 현재 디렉토리의 모든 변경사항을 스테이지에 올린다$ git add ./src/components # components 디렉토리의 모든 변경사항을 스테이지에 올린다$ git add ./src/components/Test.vue # 특정 파일의 변경사항만 스테이지에 올린다$ git add -p # 변경된 사항을 하나하나 살펴보면서 스테이지에 올린다 이때 선택된 변경 사항들은 스테이지(Stage)라고 불리는 공간으로 이동하게 된다. 이때 git add 명령어는 해당 경로 안에 있는 변경 사항을 전부 스테이지에 올리게 되는데, 이게 영 불안하다 싶은 사람은 -p 옵션을 줌으로써 변경 사항을 하나하나 확인하면서 스테이지에 올릴 수도 있다. 이렇게 스테이지에 담긴 변경 사항들은 git status 명령어를 사용하여 확인해볼 수 있고, status 명령어에 추가적으로 -v 옵션을 사용하면 어떤 파일의 어떤 부분이 변경되었는지도 함께 볼 수 있다. 12345678910$ git add ./soruce$ git statusOn branch masterYour branch is up to date with 'origin/master'.Changes to be committed: (use \"git reset HEAD ...\" to unstage) modified: source/_drafts/git-tutorial.md commit 변경 사항들을 포장하는 commit 명령어 add를 사용하여 원하는 변경사항을 스테이지에 올렸다면 이제 스테이지에 있는 변경 사항들을 포장할 차례이다. 이때 이 포장하는 행위를 commit이라고 한다. 커밋은 Git에서 상당히 중요한 부분을 차지하는 행위인데, 바로 Git이 하나의 커밋을 하나의 버전으로 정의하기 때문이다. 그렇기 때문에 특정 버전으로 어플리케이션을 변경이라는 기준도 당연히 바로 이 커밋이 된다. 1234567891011121314151617$ git log --graph* commit 20f1ea9 (HEAD -> master, origin/master, origin/HEAD)| Author: Evan Moon || 회원가입 기능 개발 끝!|* commit ca693fd| Author: Evan Moon || 회원가입 비밀번호 입력 폼 추가|* commit f9b6e2d| Author: Evan Moon || 회원가입 이메일 입력 폼 추가| 위의 그래프 상에서 필자의 어플리케이션의 현재 상태는 어떤 커밋일까? 그래프 상에서 HEAD가 20f1ea9 회원가입 기능 개발 끝! 커밋에 위치해 있으므로 현재 필자의 어플리케이션은 회원가입 기능까지 개발이 완료된 상태라는 것을 알 수 있다. 그리고 그래프를 자세히 보면 각각의 커밋들은 20f1ea9과 같은 고유한 해쉬 값을 가지고 있는데, 이 해쉬 값을 사용하여 어떠한 커밋으로든 자유자재로 이동할 수 있다. 예를 들면 회원가입 비밀번호 입력 폼 추가 커밋의 해쉬 값을 사용하여 git checkout ca693fd 명령어로 회원가입 비밀번호 입력 폼이 추가된 시점으로 이동할 수 있다는 것이다. 즉, 시간여행이 가능하다! 이러한 커밋의 기능을 제대로 활용하기 위해서 커밋은 반드시 실행 가능한 단위로 해야한다. 더 쉽게 말하자면 특정 커밋으로 버전을 변경했을 때 어플리케이션이 제대로 실행되지 않고 에러가 발생하면 안된다는 것이다. 그리고 위의 예제에서 볼 수 있듯이 커밋에는 메세지를 담을 수 있다. 이 메세지는 이 커밋으로 인한 변경 사항이 무엇인지 직접적으로 표현해주는 유일한 수단이므로 좋은 커밋 메세지를 작성하기 위한 고민은 필수다. 다행히도 이미 많은 개발자 분들이 좋은 커밋 메세지를 어떻게 작성해야 하는지에 대해 포스팅을 해주셨기 때문에 구글에서 한번 검색해보면 수두룩하게 나온다. 참고로 커밋 메세지는 꼭 영어여야 할 필요는 없다. 조직에 따라서 커밋 메세지를 영어로만 작성하도록 강제하는 룰이 있을 수는 있지만, 사실 커밋 메세지는 결국 커뮤니케이션 수단이므로 언제 누가 보더라도 알아보기 쉽게만 작성하면 장땡이다. 그러니까 영어가 익숙하지 않은데도 불구하고 굳이 영어를 고집할 필요는 없다. 오히려 같이 협업하는 팀원들이 영어에 익숙하지 않다면 그 또한 불필요한 커뮤니케이션 비용이 될 수 있다. 또한 커밋은 아직 리모트 서버에 파일을 전송하는 것이 아니라 사용자의 클라이언트 내에서 수행되는 과정이므로 인터넷에 연결이 되어 있지 않아도 변경 사항을 커밋하는 것은 아무런 지장이 없다.(비행기에서 코딩할 때도 커밋까지는 가능하다!) 사실 필자는 Git 뉴비일 때 이 커밋이라는 단어가 조금 헷갈렸었는데, 많은 개발자 분들이 commit과 push를 같은 의미로 사용하기 때문이었다. 하지만 이 두 명령어는 엄연히 다른 역할을 하기 때문에 되도록이면 구분해서 사용하도록 하자. push 변경 사항들을 리모트 서버로 배송하는 push 명령어 커밋을 통해 포장된 변경 사항들은 push 명령어를 사용하여 리모트 서버로 업로드 된다. 이때는 커밋된 변경 사항들을 실제 리모트 서버에 전송하는 것이기 때문에 반드시 네트워크에 연결이 되어있어야 한다. 그리고 푸쉬할 때 반드시 A 로컬 브랜치는 A 리모트 브랜치에만 푸쉬해야 한다는 룰 따윈 없기 때문에 커밋들을 리모트 서버로 푸쉬할 때는 Git에게 어떤 리모트 서버의 어떤 브랜치로 푸쉬할 것인지도 함께 알려줘야 한다. 1$ git push origin master # origin 리모트 서버의 master 브랜치로 푸쉬해줘! 근데 이게 브랜치 이름이 master정도면 그래도 브랜치 이름을 매번 입력해줄만 하지만 feature/SD-0000-request-api-refactoring 정도 되면 브랜치 이름을 매번 입력한다는 것이 귀찮을 수도 있다. 그래서 Git은 브랜치를 자동으로 추적할 수 있는 기능 또한 제공해준다. 1$ git push --set-upstream origin master --set-upstream 옵션을 사용하고 처음 한번만 브랜치 이름을 입력해주면 그 이후로는 git push 명령어만 입력해도 자동으로 처음 입력했던 브랜치로 변경 사항을 푸쉬할 수 있다. 이렇게 해서 리모트 서버의 레파지토리에서 소스를 내 컴퓨터에 받아온 뒤 파일을 변경하고, 그 변경 사항을 리모트 서버에 다시 업데이트하는 clone -> 파일 수정 -> add -> commit -> push과정을 한번 살펴보았다. 마치며사실 블로그 포스팅 한개로 Git에 대한 것들을 모두 설명한다는 것은 힘들기 때문에 다음 포스팅에서는 branch, checkout, merge, revert와 같이 버전을 실제로 관리하기 위해 사용하는 명령어와 그 개념에 대한 내용을 포스팅하려고 한다. Git은 현업에서 굉장히 많이 사용되고 있지만 정작 대학이나 학원같은 교육 기관에서는 약간 쩌리 취급 받는 것 같기도 하다. 뭐 사실 대학에서는 좀 더 원론적인 공학을 배우는 거지 코딩을 알려주는 곳은 아니기도 하고, 학원 같은 경우는 버전 관리 시스템을 가르칠 시간에 코딩을 알려주는 게 더 취업에 유리하니 그럴 수도 있을 것 같다.(요즘에는 대학에 강사 분들이 오셔서 Git에 대해 강의하는 경우도 있다고 한다.) 어쨌든 Git을 잘쓰면 다른 개발자가 개발한 모듈 중에 하나만 내 브랜치로 가져와야 한다거나, 커밋이 꼬여서 소스가 지워졌다거나하는 등 프로젝트를 진행할 때 발생하는 슬픈 상황들을 잘 헤결할 수도 있기 때문에 팀원들에게 이쁨받는 개발자가 될 수(도) 있다. 이상으로 Git 뉴비를 위한 기초 사용법 - 시작하기 포스팅을 마친다.","link":"/2019/07/25/git-tutorial/"},{"title":"로우 레벨로 살펴보는 Node.js 이벤트 루프","text":"1년 전, 필자는 setImmediate & process.nextTick의 차이점에 대해 설명하면서 Node.js의 이벤트 루프 구조에 대해 살짝 언급한 적이 있었다. 놀랍게도 독자 분들은 원래 설명하려고 했던 부분보다 이벤트 루프 부분에 대해서 더 많이 관심을 주었고, 필자는 그 부분에 대해서 많은 질문을 받았었다. 그래서 이번에는 Node.js의 이벤트 루프를 구성하는 로우 레벨의 동작을 자세하게 설명해보려고 한다. 이 포스팅은 2018년 2월 19일에 Paul Shan이 작성한 Node.js event loop workflow & lifecycle in low level를 번역한 글입니다. 의역이 있을 수 있습니다. 왜 이 포스팅을 작성하게 되었나?만약에 여러분이 구글에서 node.js event loop를 검색하면 나오는 대부분의 아티클들은 자세한 내용을 설명해주지 않는다. (그들은 매우 거시적으로만 이 과정을 묘사하려고 한다.) 위 그림은 구글에서 nodejs event loop를 검색했을 때 나오는 이미지들을 캡쳐한 것이다. 그리고 대다수의 이미지 결과들은 잘못 되었거나 실제 이벤트 루프가 어떻게 작동하는지에 대해서 거시적으로만 설명하고 있다. 이런 방식의 설명들 때문에 개발자들은 종종 이벤트 루프에 대한 잘못된 이해를 하게 된다. 아래 설명할 몇가지 개념은 개발자들이 잘못 알고 있는 몇가지 개념들이다. 대표적인 잘못된 개념들이벤트 루프는 자바스크립트 엔진 내부에 있다대표적인 잘못된 개념들 중 하나는 바로 이벤트 루프가 자바스크립트 엔진(V8, Spider Monkey 등)의 일부라는 것이다. 하지만 이벤트 루프는 단지 자바스크립트 코드를 실행하기위해 자바스크립트 엔진을 이용하기만 할 뿐이다.(역주: 실제로 V8 엔진에는 이벤트 루프를 관리하는 코드가 없다. Node.js나 브라우저가 이벤트 루프를 담당하는 것) 이벤트 루프는 하나의 스택 또는 하나의 큐로만 작동한다먼저, 이벤트 루프에 작업을 담아놓는 스택 같은 것은 존재하지 않는다. 그리고 이벤트 루프가 작동하는 과정은 여러 개의 큐(자료구조에서의 그 큐 맞다.)를 사용하는 복잡한 과정이다. 그러나 대부분의 개발자들은 자바스크립트의 모든 콜백이 단 하나의 큐만 사용하여 수행된다고 알고 있는데, 이것은 완전히 잘못된 생각이다. 이벤트 루프는 여러 개의 스레드에서 실행된다Node.js 이벤트 루프의 잘못된 다이어그램들 때문에 우리는 한 개의 스레드가 자바스크립트의 실행을 담당하고 다른 한 개는 이벤트 루프를 담당하는, 총 두 개의 스레드가 있다고 생각하게 되었다.(필자도 자바스크립트 뉴비 시절에 그렇게 생각했다.) 그러나 실제로는 단일 스레드로 이 모든 것을 처리한다. setTimeout은 일부 비동기 OS API와 관련있다.또 다른 큰 오해는 setTimeout의 딜레이가 끝났을 때 콜백이 외부의 요인으로 인해(OS나 커널 같은) 의해 어떤 작업 큐에 들어가게 된다고 생각하는 것이다. 하지만 이벤트 루프에 이런 외부의 요인 같은 건 없다. 우리는 밑에서 이 메커니즘에 대해서 좀 더 자세히 알아볼 것이다. setImmediate의 콜백은 작업 큐의 가장 첫번째에 위치한다보통 일반적인 이벤트 루프에 대한 설명들은 하나의 큐만 가지고 설명을 진행하기 때문에, 몇몇 개발자들은 setImmediate()가 콜백을 작업 큐의 가장 앞쪽에 배치하는 API라고 생각하게 된다. 하지만 이것은 완전히 틀린 생각이며, 모든 작업 큐들은 FIFO(First In First Out)로만 작동한다.(역주: 큐에 들어있는 작업의 포지션을 절대 변경하지 않는다는 것이다. 무조건 큐에 먼저 들어간 작업이 먼저 실행된다.) 이벤트 루프의 구조일단 이벤트 루프의 구조를 이해하기 위해서는 이벤트 루프의 흐름에 대해서 알고 있어야 한다. 이미 한번 언급했듯이, 거시적인 하나의 큐만 보는 것은 이벤트 루프를 이해하는 데 별로 도움이 되지 않는다. 아래 그림이 이벤트 루프를 제대로 설명한 그림이다. 이 그림에 표기된 각각의 박스는 특정 작업을 수행하기 위한 페이즈들을 의미한다. 각 페이즈는 각자 하나의 큐를 가지고 있으며, 자바스크립트의 실행은 이 페이즈들 중 Idle, prepare 페이즈를 제외한 어느 단계에서나 할 수 있다.(이해를 돕기 위해 큐라고 설명했지만 사실 실제 자료구조는 큐가 아닐 수도 있다.) 그리고 이 그림에서 nextTickQueue와 microTaskQueue를 볼 수 있는데, 이 큐들은 이벤트 루프의 일부가 아니며, 이 큐들에 들어있는 작업 또한 어떤 페이즈에서든 실행될 수 있다. 또한 이 큐들에 들어있는 작업은 가장 높은 실행 우선 순위를 가지고 있다. 이제 우리는 이벤트 루프가 각자 다른 여러 개의 페이즈들과 큐들의 조합으로 이루어져 있다는 것을 알게 되었다. 이제 각각의 페이즈가 어떤 작업을 수행하는 지 알아보자. Timer phaseTimer phase는 이벤트 루프의 시작을 알리는 페이즈이다. 이 페이즈가 가지고 있는 큐에는 setTimeout이나 setInterval 같은 타이머들의 콜백을 저장하게 된다. 이 페이즈에서 바로 타이머들의 콜백이 큐에 들어가는 것은 아니지만 타이머들을 min-heap으로 유지하고 있다가 실행할 때가 된 타이머들의 콜백을 큐에 넣고 실행하는 것이다. 역주: 힙(Heap)은 완전 이진트리의 일종으로 어느 정도의 느슨한 정렬을 사용하여 값들의 집합에서 최대 값이나 최소 값을 찾아내는 작업에 특화되어 있다. min-heap은 상위 레벨의 노드가 하위 레벨의 노드들보다 작거나 같은 구조이므로 타이머가 실행되어야하는 순서대로 저장하기에 알맞는 자료 구조라고 할 수 있다. 그리고 큐에 콜백을 넣는다는 것은, 이 콜백들을 실행하겠다는 것을 의미하므로 타이머가 생성되지마자 큐에 콜백을 넣는 것이 아니라 별도의 힙에 타이머를 저장하고나서 매 Timer phase 때 어떤 타이머가 실행할 때가 되었는지를 검사한 후, 실행되어야 하는 콜백만 큐에 넣는다는 것이다. 이 과정에 대한 자세한 내용은 밑에서 다시 설명하겠다. Pending i/o callback phase이 페이즈에서는 이벤트 루프의 pending_queue에 들어있는 콜백들을 실행한다. 이 큐에 들어와있는 콜백들은 현재 돌고 있는 루프 이전에 한 작업에서 이미 큐에 들어와있던 콜백들이다. 예를 들어 여러분이 TCP 핸들러 콜백 함수에서 파일에 뭔가를 썼다면 TCP 통신이 끝나고 파일 쓰기도 끝나고 나서 파일 쓰기의 콜백이 이 큐에 들어오는 것이다.(파일 쓰기는 보통 비동기로 이루어진다.) 또한 에러 핸들러 콜백도 pending_queue로 들어오게 된다. Idle, Prepare phase이름은 Idle phase이지만 이 페이즈는 매 Tick마다 실행된다. Prepare phase 또한 매 폴링(Polling)때마다 실행된다. 어쨌든 이 두개의 페이즈는 이벤트 루프와 직접적인 관련이 있다기보다는 Node.js의 내부적인 관리를 위한 것이기 때문에 이 포스팅에서는 설명하지 않는다. Poll phase필자가 생각하기에 전체 이벤트 루프 중 가장 중요한 페이즈는 바로 이 Poll phase라고 생각한다. 이 페이즈에서는 새로운 수신 커넥션(새로운 소켓 설정 등)과 데이터(파일 읽기 등)를 허용한다. 우리는 여기서 일어나는 일을 크게 두 가지로 나눠볼 수 있다. 만약 watch_queue(Poll phase가 가지고 있는 큐)가 비어있지 않다면, 큐가 비거나 시스템 최대 실행 한도에 다다를 때까지 동기적으로 모든 콜백을 실행한다. 일단 watch_queue가 비어있다면, Node.js는 곧바로 다음 페이즈로 넘어가는 것이 아니라 약간 대기시간을 가지게 된다. Node.js가 기다리는 시간은 여러 가지 요인에 따라 계산되는데, 이 부분은 밑에서 따로 설명하도록 하겠다. Check phasePoll phase의 다음 페이즈는 바로 setImmediate의 콜백만을 위한 페이즈인 Check phase이다. 이렇게 얘기하면 보통 하시는 질문은, 왜 setImmediate의 콜백만을 위한 큐인가요?이다. 음, 그건 밑에서 필자가 워크 플로우 섹션에서 다시 얘기할 Poll phase에서 수행하는 행동들 때문이기도 하다. 일단 지금은 Check phase가 setImmediate의 콜백 전용 단계라는 사실만 기억하고 있자. Close callbackssocket.on('close', () => {})과 같은 close 이벤트 타입의 핸들러들은 여기서 처리된다. nextTickQueue와 microTaskQueuenextTickQueue는 process.nextTick() API의 콜백들을 가지고 있으며, microTaskQueue는 Resolve된 프로미스의 콜백을 가지고 있다. 이 두개의 큐는 기술적으로 이벤트 루프의 일부가 아니다. 즉, libUV 라이브러리에 포함된 것이 아니라 Node.js에 포함된 기술이라는 것이다. 이 친구들이 가지고 있는 작업들은 현재 실행되고 있는 작업이 끝나자마자 호출되어야한다. 역주: libUV는 Node.js에서 사용하는 비동기 I/O 라이브러리이다. 이 라이브러리는 C로 작성되었고 윈도우나 리눅스 커널을 추상화해서 Wrapping하고 있는 구조이다. 즉, 커널에서 어떤 비동기 작업들을 지원해주는 지 알고 있기 때문에 커널을 사용하여 처리할 수 있는 비동기 작업을 발견하면 바로 커널로 작업을 넘겨버린다. 이후 이 작업들이 종료되어 OS 커널로부터 시스템 콜을 받으면 이벤트 루프에 콜백을 등록하는 것이다. 만약 OS 커널이 지원하지 않는 작업일 경우 별도의 스레드에 작업을 던져서 처리한다. 이 스레드에 관한 내용은 원작자가 밑에서 추가적으로 설명하고있다. 이벤트 루프의 작업 흐름우리가 node my-script.js를 콘솔에서 실행시켰을 때, Node.js는 이벤트 루프를 생성한 다음 이벤트 루프 바깥에서 메인 모듈인 my-script.js를 실행한다. 한번 메인 모듈이 실행되고나면 Node.js는 이벤트 루프가 활성 상태인지, 즉 이벤트 루프 안에서 해야할 작업이 있는지를 확인한다. 만약 이벤트 루프를 돌릴 필요가 없다면 Node.js는 process.on('exit, () => {})를 실행하고 이벤트 루프를 종료하려고 할 것이다. 그러나 만약 이벤트 루프를 돌려야할 상황이라면 Node.js는 이벤트 루프의 첫 번째 페이즈인 Timer phase를 실행한다. Timer phase이벤트 루프가 Timer phase에 들어가게 되면 실행할 타이머 콜백 큐에 뭐가 있는 지 확인부터 시작한다. 그냥 확인이라고 하면 간단해보이지만 사실 이벤트 루프는 적절한 콜백들을 찾기 위해 몇 가지 단계를 수행하게된다.위에서 설명했듯이 타이머 스크립트는 오름차순으로 힙에 저장된다. 그래서 제일 먼저 저장된 타이머들을 하나씩 까서 now - registeredTime === delta 같은 조건을 통해 타이머의 콜백을 실행할 시간이 되었는 지 검사하게 된다.(역주: delta는 setTimeout(() => {}, 10)에서의 10) 만약 조건에 해당된다면 이 타이머의 콜백을 실행하고 다음 타이머를 확인한다. 만약 조건에 해당하지 않는 타이머를 만난다면, 탐색을 바로 종료하고 다음 페이즈로 이동한다. 타이머는 힙 내부에 오름차순으로 정렬되어있기 때문에 그 이후로는 탐색을 해도 의미가 없기 때문이다. 자, 예를 들어 딜레이 값이 100, 200, 300, 400인 4개의 타이머(A, B, C, D)를 어떤 특정 시간 t에 힙에 등록했다고 가정해보자. 이제 이벤트 루프가 t+250에 Timer phase에 진입했다고 생각해보자. 가장 먼저 타이머 A를 찾아낸 후 만료 기간이 t+100이라는 것을 알게 될 것이다. 그러나 지금은 이미 t+250이기 때문에 타이머 A의 콜백은 실행될 것이다. 그리고 타이머 B를 찾아내어 만료 기간이 t+200임을 체크하게되고, 타이머 B의 콜백 역시 실행된다. 이제 타이머 C를 체크하게 되는데 이 타이머의 딜레이는 t+300이기 때문에 바로 페이즈가 종료된다. 이벤트 루프는 타이머 D는 체크하지 않는데, 위에서 설명했듯이 타이머들은 만료 기간 순으로 오름차순 정렬되어있기 때문에 타이머 C의 뒤쪽에는 어차피 타이머 C보다 만료 기간이 긴 타이머들만 있기 때문이다. 참고로 페이즈는 시스템의 실행 한도에도 영향을 받고 있으므로, 실행 되어야하는 타이머가 아직 남아 있다고 하더라도 시스템 실행 한도에 도달한다면 바로 다음 페이즈로 넘어가게된다. Pending i/o phase타임 페이즈가 종료된 후 이벤트 루프는 Pending i/o 페이즈에 진입하고, 가장 먼저 이전 작업들의 콜백이 실행 대기 중인지, 즉 pending_queue에 들어와 있는지를 체크하게 된다. 만약 실행 대기 중이라면 pending_queue가 비거나 시스템의 실행 한도 초과에 도달할 때까지 대기하고 있던 콜백들을 실행한다. 이 과정이 종료되면 이벤트 루프는 Idle Handler Phase로 이동하게 된 후 내부 처리를 위한 Prepare phase를 거쳐 최종적으로 가장 중요한 단계인 Poll Phase에 도달하게 된다. Poll phase이름에서 알 수 있듯이 이 페이즈는 폴링하는 단계이다. 이벤트 루프가 Poll phase에 들어왔을 때 watcher_queue 내부에 파일 읽기의 응답 콜백, HTTP 응답 콜백 같이 수행해야 할 작업들이 있다면 이 작업들을 실행하게 된다. 이 과정 또한 watcher_queue가 비거나 시스템의 실행 한도 초과에 다다를 때까지 계속 된다. 만약 더 이상 콜백들을 실행할 수 없는 상태가 된다면 만약 check_queue, pending_queue, closing_callbacks_queue에 해야할 작업이 있는지를 검사하고, 만약 해야할 작업이 있다면 바로 Poll phase가 종료되고 다음 페이즈로 넘어가게 된다. 하지만 특별히 해야할 작업이 더 이상 없는 경우 Poll phase는 다음 페이즈로 넘어가지 않고 계속 대기하게 된다. 이때도 마냥 무한 대기를 하는 것은 아니고, 타이머 힙에서 첫번째 타이머를 꺼내본 다음에 만약 해당 타이머가 실행 가능한 상태라면 그 타이머의 딜레이 시간만큼만 대기 시간을 결정한다. 역주: 결국 Poll phase에서 더 이상 수행할 작업이 없는 경우 다음 페이즈로 넘어가는 조건은 다음과 같다. Check phase에 실행할 콜백이 있냐? 바로 다음 페이즈로 고고 Check phase에도 실행할 콜백이 없어? 그럼 타이머는 있어? 있다면 그 타이머를 실행 할 수 있는 시간이 될 때까지만 대기하고 바로 Timer phase로 고고 없어? 그럼 일 생길 때까지 대기… 2번 조건에서 타이머가 있다고 바로 Timer phase로 넘어가지 않고 대기하는 이유는, 대기하지않고 바로 Timer phase로 넘어갈 경우 어차피 첫번째 타이머를 실행할 수 있는 시간이 안되었기 때문에 Timer phase에서도 아무것도 수행하지 않고 Timer phase가 끝나버리기 때문이다. 그렇게 되면 굳이 이벤트 루프를 한번 더 돌아야지 해당 타이머를 실행할 수 있게되므로 그냥 Poll phase에서 대기하는 게 더 이득인 것이다. Check phasePoll phase가 지나면 이벤트 루프는 바로 setImmediate() API의 콜백과 관련이 있는 Check phase에 들어서게 된다. 이 페이즈에서는 다른 페이즈와 마찬가지로 큐가 비거나 시스템 실행 한도 초과에 도달할 때까지 계속 해서 setImmediate의 콜백들을 실행한다. Close callbackCheck Phase가 종료된 후에, 이벤트 루프의 다음 목적지는 close나 destory 콜백 타입들을 관리하는 Close callback이다. 이벤트 루프가 Close callback들과 함께 종료되고나면 이벤트 루프는 다음에 돌아야할 루프가 있는지 다시 체크 하게 된다. 만약 아니라면 그대로 이벤트 루프는 종료된다. 하지만 만약 더 수행해야할 작업들이 남아 있다면 이벤트 루프는 다음 순회를 돌기 시작하고 다시 Timer Phase부터 시작하게 된다. 아까 위에서 설명했던 Timer Phase에서의 예시를 다시 가져오자면, 이제 다음 루프의 Timer Phase에서는 타이머 C의 만료 시간이 경과했는지부터 확인을 시작할 것이다. nextTickQueue & microTaskQueue근데 이벤트 루프의 일부가 아닌 이 두 큐에 들어있는 콜백들은 언제 실행되는 걸까? 이 두 큐의 콜백들은 어떤 페이즈에서 다음 페이즈로 넘어가기 전에 자신이 가지고 있는 콜백들을 최대한 빨리 실행해야하는 역할을 맡고 있다.(역주: 페이즈에서 다른 페이즈로 넘어가는 과정을 Tick이라고 부른다.) 다른 페이즈들과는 다르게 이 두 큐는 시스템 실행 한도 초과에 영향을 받지 않기 때문에 Node.js는 이 큐가 완전히 비워질 때까지 콜백들을 실행한다. 그리고 nextTickQueue는 microTaskQueue보다는 높은 우선 순위를 가지고 있다. Thread-pool필자가 자바스크립트 개발자에게 가장 많이 들은 단어는 바로 스레드풀(ThreadPoll)이다. 그리고 이와 관련된 가장 큰 오해는 바로 Node.js가 모든 비동기 명령을 관리하는 별도의 스레드풀을 가지고 있다는 것이다. 그러나 이 스레드풀은 Node.js가 아니라 Node.js가 비동기 작업을 처리하기 위해 사용하는 라이브러리인 libUV에 포함된 기능이라는 것이다.필자가 이벤트 루프의 다이어그램에 스레드풀을 별도로 표시하지 않은 이유는 스레드풀 자체가 이벤트 루프 매커니즘의 일부가 아니기 때문이다. libUV는 OS커널의 비동기 API만을 사용하여 이벤트 드리븐을 유도할 수 있을 만큼 충분히 훌륭하다. 그러나 파일 읽기, DNS Lookup 등 OS 커널이 비동기 API를 지원하지않는 작업들의 경우에는 별도의 스레드풀을 사용하게 되는데, 이때 기본 값으로 4개의 스레드를 사용하도록 설정되어있다. uv_threadpool 환경 변수를 사용하면 최대 128개까지 스레드 개수를 늘릴 수도 있다. Workflow with examples이제 이 동기식 무한 루프가 자바스크립트를 비동기식으로 작동하게 하기 위해 얼마나 큰 역할을 하고 있는 지 이해했기 바란다. 이 구조는 한번에 단 한개의 작업만 실행하고 있지만 그 어떤 것도 블로킹하지 않는다. 어쨌든 백문이불여일견이니, 코드 스니펫으로 이 구조를 다시 이해해보는 시간을 가져보도록 하자. Snippet 1 – 기초 이해123456setTimeout(() => { console.log('setTimeout');}, 0);setImmediate(() => { console.log('setImmediate');}); 위 결과를 예측할 수 있겠는가? 아마도 여러분은 setTimeout이 먼저 출력된다고 생각하겠지만, 사실 장담할 수 없다. 왜냐? 메인 모듈이 실행되고나서 이벤트 루프가 Timer phase에 진입할 때 여러분의 타이머를 찾을 수도 있고 못 찾을 수도 있기 때문이다. 왜냐면 타이머는 시스템의 시간과 사용자가 제공한 시간을 사용하여 등록되기 때문이다. setTimeout이 호출된 순간, 타이머는 메모리에 이 타이머를 저장하게되는데, 그 순간 컴퓨터의 성능이나 Node.js가 아닌 외부 작업 때문에 약간의 딜레이가 발생할 수 있기 때문이다. 또 다른 포인트는 Node.js가 Timer phase에 진입하기 전에 변수 now를 선언하고 그 변수 now를 현재 시간으로 간주한다는 점이다. 그러므로 정확한 계산이라고 하기에는 약간의 노이즈가 껴있다는 것이고, 이게 바로 setTimeout이 반드시 먼저 실행될 것이라고 확신할 수 없는 불확실성의 이유가 된다. 그러나 만약 여러분이 이 코드를 I/O 사이클의 내부로 옮긴다면, 반드시 setTimeout보다 setImmediate가 먼저 실행되는 것을 보장할 수 있게된다. 12345678fs.readFile('my-file-path.txt', () => { setTimeout(() => { console.log('setTimeout'); }, 0); setImmediate(() => { console.log('setImmediate'); });}); 역주: 위 스니펫은 다음과 같은 순서로 실행된다. fs.readFile을 만나면 이벤트 루프는 libUV에게 해당 작업을 던짐. 파일 읽기는 OS 커널에서 비동기 API를 제공하지 않기 때문에 libUV는 별도의 스레드에 해당 작업을 던짐 작업이 완료되면 이벤트 루프는 Pending i/o callback phase의 pending_queue에 작업의 콜백을 등록 이벤트 루프가 Pending i/o callback phase를 지날 때 해당 콜백을 실행 setTimeout이 Timer phase의 큐에 등록됨. 해당 콜백은 다음 Timer phase 때 실행될 것이다. setImmediate의 콜백이 Check phase의 check_queue에 등록됨. Poll phase에서는 딱히 할 일이 없지만, Check phase의 큐에 작업이 있으므로 바로 Check phase로 이동 setImmediate 콘솔 출력. Timer phase에는 타이머가 등록되어 있으므로 다시 이벤트 루프가 시작된다. Timer phase에서 타이머를 검사, 딜레이가 0이므로 setTimeout의 콜백을 바로 실행한다. setTimeout 콘솔 출력 즉, 이런 과정을 거치기 때문에 setImmediate의 콜백이 반드시 setTimeout보다 먼저 실행되는 것을 보장할 수 있는 것이다. Snippet 2 – 타이머들을 더 잘 이해하기123456789101112var i = 0;var start = new Date();function foo () { i++; if (i < 1000) { setImmediate(foo); } else { var end = new Date(); console.log(\"Execution time: \", (end - start)); }}foo(); 위의 예시는 굉장히 간단하다. 함수 foo는 setImmediate()에 의해서 1000번 재귀호출되고 있다. 이 코드를 필자의 맥북 프로에서 Node.js 8.9.1을 사용하여 실행하면 함수가 종료되기까지 6~8ms정도가 걸린다. 이제 위의 스니펫을 setImmediate(foo)에서 setTimeout(foo, 0)으로 변경해보자. 123456789101112var i = 0;var start = new Date();function foo () { i++; if (i < 1000) { setTimeout(foo, 0); } else { var end = new Date(); console.log(\"Execution time: \", (end - start)); }}foo(); 이제 필자의 컴퓨터에서 이 코드가 실행되는데 걸리는 시간은 대략 1400+ ms정도이다. 왜 이렇게 되는걸까? 이 두 경우 모두 폴링에 걸리는 시간은 0이기 때문에 동일하게 작동해야한다. 근데 왜 이렇게 실행 시간이 많이 차이가 나게 된 걸까? 그 이유는 바로 시간을 비교하고 편차를 알아내는 작업이 CPU를 집중적으로 사용하는 작업이기 때문에 시간이 더 오래 걸리기 때문이다. 그리고 타이머 스크립트를 등록하는 것 자체도 시간을 소모한다. 위에서 설명했듯이 Timer phase에서는 타이머를 실행할 시간이 되었는지, 콜백을 실행해야 하는지를 검사하기 위해 몇가지 작업을 거쳐야 하는데 이 과정을 매 이터레이션마다 하고 있으니 느려질 수 밖에 없다. 그러나 setImmediate의 경우 이런 과정이 필요없기 때문에 setTimeout보다 실행 속도가 빠른 것이다. Snippet 3 – nextTick()과 타이머 실행에 대해 이해하기12345678910111213var i = 0;function foo(){ i++; if(i>20){ return; } console.log(\"foo\"); setTimeout(()=>{ console.log(\"setTimeout\"); },0); process.nextTick(foo);} setTimeout(foo, 2); 위 코드의 로그는 어떻게 출력될까? 첫번째 출력은 foo들이 될 것이고 기 후 setTimeout들이 출력된다. 제일 처음 코드를 실행하면 2ms 후에 첫 번째 foo가 출력되고 다음 nextTickQueue에 다시 foo()를 넣게된다. 그리고 nextTickQueue에 들어간 콜백들은 한 페이즈에서 다음 페이즈로 넘어갈 때마다 무조건 콜백들을 동기적으로 실행해야한다. 그렇기 때문에 재귀호출로 nextTickQueue에 들어간 모든 콜백들을 실행하고 나서야 Timer Phase에서 setTimeout 콜백을 처리할 수 있게 되는 것이다. 코드를 약간 수정하고 다시 살펴보자. 1234567891011121314151617var i = 0;function foo(){ i++; if(i>20){ return; } console.log(\"foo\", i); setTimeout(()=>{ console.log(\"setTimeout\", i); },0); process.nextTick(foo);}setTimeout(foo, 2);setTimeout(()=>{ console.log(\"Other setTimeout\");}, 2); 필자는 기존의 setTimeout과 같은 딜레이 시간을 가진 setTimeout을 추가했고, 이 타이머의 콜백은 단지 Other setTimeout을 출력하기만 한다. 뭐, 확실하다고 말할 수는 없지만 한 개의 foo가 먼저 출력된 후 Other setTimeout이 출력될 찬스가 존재하긴 한다. 타이머들이 들어있는 힙 내부에 동일한 딜레이를 가진 타이머들은 어떻게든 그룹화되어있고, nextTickQueue의 체크는 진행 중인 콜백 그룹의 실행이 끝난 후에야 진행되기 때문이다. 역주: 첫 번째 setTimeout의 콜백과 두 번쨰 setTimeout의 콜백이 실행될 시간이 동일하기 때문에 Timer phase에서 두 타이머의 콜백이 모두 큐에 들어갔고, 일단 큐에 들어갔으면 시스템 실행 한도에 걸리지 않는 이상 무조건 해당 페이즈가 끝나기 전에 실행된다. 이런 경우, foo() 내부의 process.nextTick의 콜백은 Timer phase에서 다음 페이즈로 넘어갈 때 실행될 것이기 때문에 foo가 먼저 한 개 출력되고 Other setTimeout이 출력되는 것이다. 몇 가지 일반적인 질문들그럼 자바스크립트는 정확히 어디에서 실행되는건가요?이 포스트를 처음 읽을 때는 자바스크립트가 정확히 어디서 실행되는지 헷갈릴 수 있다. 앞서 말했듯이 이벤트 루프 자체에서 V8 또는 다른 엔진을 사용하여 자바스크립트를 실행하는 것이고 이때, 단 하나의 스레드를 사용하여 자바스크립트가 실행되는 것이다. 실행 자체는 동기적이고, 현재 실행시킨 자바스크립트의 실행이 완료되지 않는다면 이벤트 루프 또한 진행되지 않는다. 왜 우리는 setTimeout(fn, 0)가 있는데도 setImmediate를 사용해야 하나요?일단, setTimeout(fn, 0)은 실질적으로 0이 아니다. 이건 사실 1이다. 여러분이 타이머의 딜레이를 1ms보다 작거나 2147483647ms보다 크게 설정하면 딜레이는 자동으로 1로 세팅된다. 그렇기 때문에 setTimeout의 딜레이를 0으로 설정하려고 한다면 이 딜레이는 자동으로 1이 되는 것이다. 이미 설명했던 대로 setImmediate을 사용하면 타이머를 사용할 때처럼 딜레이의 시간이 지났지 뭐니 하는 추가적인 체크 로직이 필요없어진다. 그래서 setImmediate은 빠른 것이다. 또한 setImmediate의 체크 작업은 Poll phase 직후에 수행되기 때문에 HTTP 요청 콜백과 같은 곳에서 사용된 setImmediate의 콜백 함수는 바로 실행된다고 할 수 있다. 왜 setImmediate는 Immediate(즉시)라고 부르는 건가요?setImmediate와 process.nextTick 모두 이름이 잘못 되었다고 생각한다. 사실 setImmediate은 한번의 Tick 또는 루프가 돌 때마다 관리되고, nextTick은 한 페이즈에서 다음 페이즈로 넘어가는 매 Tick마다 최대한 빨리 호출되도록 작동하고 있다. 그래서 기능적으로는 setImmediate이 nextTick이라는 이름이 더 잘 어울리고 nextTick이 진짜로 즉시(Immediately) 호출되는 친구다. 😛 자바스크립트는 Block 될 수 있는 건가요?이미 위에서 설명했듯이 nextTickQueue는 콜백 실행에 있어서 어떤 제한도 가지고 있지 않다. 그렇기 때문에 만약 process.nextTick()이 재귀 호출된다면, 여러분의 프로그램은 다른 페이즈들의 큐가 어떤 콜백을 가지고 있든간에 절대 그 작업에서 빠져나오지 못할 것이다. 만약 Exit callback phase에서 setTimeout을 호출하면 어떻게 되나요?뭐, 타이머 자체는 시작될 수 있겠지만 setTimeout의 콜백은 절대 호출되지 않을 것이다. 이미 Node.js가 Exit callbacks에 들어와 있다는 것은 이미 이벤트 루프에서 빠져나왔다는 것을 의미한다. 짧은 정리들 이벤트 루프는 작업 스택을 가지고 있지 않다. 이벤트 루프가 별도의 스레드에서 실행되고 자바스크립트 실행은 어떤 큐에서 하나씩 꺼내와서 다른 곳에서 하는 것이 아니라 자바스크립트의 실행 자체가 이벤트 루프 안에서 수행되는 것이다. setImmediate는 콜백을 작업 큐의 앞 쪽에 밀어넣는 것이 아니라 setImmediate 만을 처리하기 위한 전용 페이즈와 큐가 존재한다. setImmediate은 실질적으로 다음 페이즈 혹은 다음 이벤트 루프의 순회에서 실행되고, nextTick이 오히려 실질적으로 더 빠르게 실행된다. nextTickQueue에 담긴 작업이 재귀 호출을 수행하는 경우 Node.js의 작업 프로세스를 블록킹할 수 있다. 주의하도록 하자. 마치며필자는 Node.js 코어 개발 팀이 아니다. 이 포스팅에 관한 필자의 모든 지식은 실험과 다른 포스팅들을 통해 얻은 것이다. 필자가 이 지식들을 얻기 위해 가장 큰 도움을 준 Node.js 공식 문서에 감사한다.두 번째로 도움이 많이 된 포스트는 Saúl Ibarra Corretgé의 talk on libUV 포스팅이다.그리고 세 번째로 VoidCanvas 독자 분들 덕분에 이런 건강한 토론과 실험, 관찰을 할 수 있도록 된 것에 감사한다. 🙂","link":"/2019/08/01/nodejs-event-loop-workflow/"},{"title":"개발자가 공부로 살아남는 방법","text":"이번 포스팅에서는 개발자들에게 뗄레야 뗄 수 없는 키워드인 공부에 대해서 포스팅 해보려고 한다. 물론 다른 직종도 마찬가지겠지만 다른 업계보다 빠르게 변화하는 IT 업계의 특성 상 개발자는 시대의 흐름을 따라가기위해 은퇴 전까지 계속 해서 공부를 하는 수 밖에 없다. 개발자들은 아무래도 기술을 활용하는 최전선에 있는 사람들이기 때문에 이런 변화에 민감하다. 그 변화는 스쳐지나가는 한 때의 유행일수도 있고, 앞으로 20년을 버틸 수 있는 근본적인 지식일 수도 있다. 하지만 아무리 우리가 매일 공부를 한다고 해도 쏟아져 나오는 기술의 양이 워낙 많기 때문에 전부 공부한다는 것은 불가능하다. 그래서 우리는 이 기술이 단순한 유행인지, 오래 써먹을 수 있는 기술인지 혹은 지금 당장 나에게 필요한 기술인지 등을 파악하며 자신에게 맞는 기술을 습득해야한다. 그래서 이번 포스팅에서는 필자가 지금까지 4년 동안 거의 난장판이나 다름없던 웹 프론트엔드 생태계에서 어떤 기준을 가지고 공부할 것을 선택하고, 어떤 방법을 사용하여 공부를 해왔는지 한번 가볍게 적어보려고 한다. 물론 이 내용은 필자 개인의 주관적인 의견이므로 정답도 아닐 뿐더러 여러분과는 맞지않는 비효율적인 방법일 수도 있다. 그냥 이런 사람도 있구나 정도로 가볍게 읽어보고 참고하는 것을 추천한다. 우리가 공부를 게을리하면 안되는 이유필자가 공부하는 방법을 설명하기에 앞서, 먼저 왜 개발자는 공부를 게을리 하면 안되는가?에 대해서 한번 이야기해보려고 한다. 사실 굳이 상세한 이유를 들지 않아도 빠르게 변화하는 업계에서 도태되지 않고 살아남으려면 공부를 해야한다는 간단명료한 대답이 있긴 하지만, 사실 이 대답은 제대로 된 대답은 아니다. 약간 결과론적인 대답이라는 느낌이 든다. 공부 안하면 도태당하니까 공부를 열심히 해야지 이런 느낌이랄까? 이런 대답은 질문자에게 제대로 된 동기 부여도 되지않을뿐더러 왜에 대한 궁금증을 해결해주기 힘든 대답이다. 그래서 필자는 저 조금 더 자세하게 이야기를 해보려고 한다. 기술이 발전하는 속도는 생각보다 빠르다일 시작한지 4년밖에 안된 개발자인 필자조차 일을 하다보면 뭐가 이렇게 빨리 바뀌어?라고 생각한 적이 꽤 있었다. 그렇게 바뀐 것들은 오 좋은 게 새로 나왔네 정도인 가벼운 변화부터 응...? 도대체 이게 뭐지...?라고 생각할만한 급격한 변화까지 아주 다양했다. 꼴랑 4년 차인 필자가 이런 생각을 할 정도면 10년이 넘으신 시니어들은 아마 더할 것이라고 생각한다. 게다가 옛날에 비해서 기술이 발전하는 속도는 점점 빨리지고 있기 때문에 앞으로는 더 빠르게 바뀔 수도 있다고 생각한다. 인류의 기술 발전 속도는 점점 더 빨라지고 있다 위의 예시는 산업혁명이 언제 발생 했는지 나타낸 것인데, 1차 산업혁명이 발생한 1784년에서 시작해서 4차 산업혁명이 발생한 오늘날에 이르기까지 점점 더 간격이 줄어드는 것을 볼 수 있다. 이렇듯 인류의 기술 발전 속도는 선형이 아닌 지수형태로 증가하고 있다. 산업혁명을 예로 들면 너무 거시적이여서 와닿지 않을 수 있다. 하지만 기술자가 평소에 느끼는 기술의 변화 하나하나는 비록 자잘한 것들일 수 있어도 결국 산업 전반에 걸쳐 이런 것들이 쌓이고 쌓여서 서로 시너지 효과를 내며 임계점을 돌파하게 되는 것이기 때문에 기술자들이 기술의 변화를 느끼고 따라갈 수 있는 능력 또한 무시할 만한 것은 아니다. 즉, 사람들이 자주 이야기하는 도태된다라는 것은 이런 의미이다. 예를 들어 농사짓는 법을 마스터한 신라의 장인이 있다고 생각해보자. 그 사람은 1000년이 지난 후의 조선에서도 그 스킬을 가지고 충분히 먹고살 수 있다. 무려 1000년의 간극이 있는데도 말이다. 그에 비해 1970년대에 미국 국방성에서 계산 업무를 하던 인간 컴퓨터들은 2019년인 지금은 아예 사라진 직업이 되었다. 이 변화는 앞으로 점점 더 빨라질 것이며, 그렇기에 사람들이 도태되지 않으려면 공부해야해라는 소리를 하는 것이다. 물리 서버에서 클라우트 컴퓨팅으로 넘어가는 기술의 흐름은 아주 사소한 것처럼 보일 수 있지만, 그렇다고 그냥 가볍게 보고 넘겨버린다면 2년 정도만 지나도 그 이후에 나온 새로운 패러다임을 따라가기는 점점 더 어려워진다. IT 업계의 변화는 파장이 크다사실 당연히 IT를 제외한 다른 계열의 지식들도 세월이 지남에 따라 변화한다. 그러나 IT 업계가 변화한다는 것은 단순히 속도 면에서 빠르게 변한다는 것 외에도 기존의 패러다임이 뒤집어 질 정도로 큰 변화가 자주 일어난다는 것을 의미한다. 예를 들어 물리학 같은 경우, 기존의 이론이 잘못 되었음을 증명하고 새로운 이론을 제시하게되면 전 세계의 물리 교과서가 다시 쓰여질 정도로 그 파장은 어마무시하다. 그러나 그런 경우는 생각보다 자주 일어나는 일이 아니다. 대표적인 예로 아인슈타인이 제시한 시간은 관찰자의 상태에 따라 상대적으로 흐른다라고 제시한 상대성 이론이 있다. 지난 몇천년의 세월동안 늘 절대적인 흐름일 것이라고 생각했던 시간이라는 개념이 한순간에 뒤집힌 사례이다. 또한 법학의 경우, 법이 개정됨에 따라 새로운 법을 공부하고 새로 제정된 법과 비슷한 사례가 적용된 판례를 다시 찾아보는 경우가 있다. 이는 정부에서 크고 작은 법을 개정할 때마다 반복되기 때문에 빈도는 잦을 수 있지만 기존의 법이 가지고 있던 패러다임을 완전 뒤집어 놓지는 않는다. 그 정도로 파격적인 법안을 제정하기란 쉽지 않기 때문이다. 그러나 IT 업계에서 발생하는 변화는 속도 면으로도 빠르게 변화하면서도 기존의 패러다임을 뒤집는 경우가 종종 있다. 몇가지 예를 들어보면, jQuery에서 AngularJS로 넘어갔을 때, MVC 패턴에서 Flux 패턴으로 넘어갔을 때, Docker라는 가상 컨테이너가 처음 나왔을 때도, 서버리스 아키텍처라는 개념이 처음 나왔을 때도 그랬다. 아마 필자가 직접 겪지는 않았지만 클라우드 컴퓨팅 서비스를 제공하는 AWS가 처음 나왔을 때도 그랬을 것 같다. 이렇게 IT의 경우, 기존의 그것들을 구성하고 있는 패러다임을 완전히 버리고 온전히 새로운 것에 집중해야 이해할 수 있는 것들이 많았고, 그때마다 이 생태계는 많은 변화가 있었다. 그리고 그때마다 개발자 뿐만 아니라 개발자가 아닌 사람들의 생활에도 큰 영향을 끼쳤다. 사실 이런 이야기를 하면 이런 것들이 어떻게 사람들의 생활에 영향을 준다는 거지?라고 생각할 수 있는데, 대표적인 예를 몇개 들어보겠다. 이런 변화들 덕분에 지금은 많은 회사들이 실제 물리 서버를 구매한 후 IDC(Internet Data Center)에 입주해서 사용하지 않고 클라우드 컴퓨팅을 사용하여 서버를 운영하고 있다. 이는 서버를 직접 관리해야하는 리소스의 감소와 유연한 트래픽 대처로 이어졌고, 결과적으로 서버를 운영할 때 발생하는 부담을 상당 수 줄여주었다. 또한 요즘에는 JavaScript 하나만 할 줄 알아도 웹 클라이언트, 모바일 어플리케이션, 데스트톱 어플리케이션, 서버까지 전부 만들 수 있으며, 이는 개발자들이 용도에 맞는 어플리케이션을 개발하려고 할 때 여러 개의 언어를 공부하지 않아도 바로 원하는 서비스를 뚝딱 만들어낼 수 있다는 것을 의미한다. 또한 방금 설명한 클라우드 컴퓨팅 서비스를 제공하는 회사에 약간의 돈만 지불하면 백엔드 인프라도 클릭 몇번으로 간단하게 구성 및 관리할 수 있기 때문에 단 한명의 개발자가 거대한 시스템을 운영하는 것도 가능하다. 이런 변화들 덕분에 프로그래밍에 대한 장벽이 옛날에 비해 많이 내려간 상태이고 누구나 아이디어만 있다면 새로운 IT 사업에 도전할 수 있는 시대를 이끌어 냈으며, 그 결과 Google이나 Facebook 같은 IT 대기업들이 생겨나게 되어 우리에게도 직접적인 영향을 주고 있다. 내가 공부하는 방법필자 또한 급격한 기술의 흐름 속에서 뒤쳐지지 않으려 발버둥 치고있는 개발자 중 한 사람이기 때문에, 당연히 평소에 공부를 하고 있다. 그러나 이 포스팅에서 필자는 공부를 해야한다라는 당연한 사실보다는 나는 이렇게 공부하고 있다에 대한 이야기를 하고 싶었다. 필자는 주변에 많은 개발자 분들이나 또는 개발을 공부하고 계신 분들, 혹은 어떤 분야에 있다가 다른 분야로 넘어가시는 분들이 공부에 대한 어려움을 표하는 것을 많이 들어본 적이 있다. 이들의 어려움은 공부해야하는 기술 자체의 난이도보다는 어떤 것부터 공부해야할지 모르겠다라는 것이다. 이건 요즘 같이 새로운 정보가 쏟아져 나오는 시대에는 당연한 이야기인데, 개발 공부를 시작하기 전에 선택해야할 것이 많아도 너무 많다. 언어와 프레임워크를 먼저 공부할 것인지 컴퓨터에 대한 기초부터 공부해야 하는 것인지부터 시작해서, 어떤 언어를 공부해야 하는지, React를 공부해야 하는 지 Vue를 공부해야 하는 지 등 개발 공부를 시작하려고하면 선택해야할 것도 너무나도 많다. 게다가 이런 것들은 정답이랄게 없기 때문에 주위의 개발자들에게 물어보거나 커뮤니티에 물어봐도 사람마다 다른 답변이 돌아올 가능성도 높다. 필자 개인적으로는 우리의 이런 성향에는 대한민국의 사회 분위기가 어느 정도 적용한 것 같다는 생각이다. 솔직히 우리는 어릴 때부터 내가 스스로 정하는 길이 아닌 부모님이 하라는 대로 열심히 학교나 학원가서 공부하고 자란, 그런 세대지 않은가? 심지어 대학 진학 시 과를 고를 때도 자기가 공부하고 싶은 학문이 아니라 성적에 맞춰서 가는 경우도 많다. 수능도 물론 중요하지만, 떨어지거나 인서울 못한다고 해서 인생이 망하는 건 아니더라. 필자도 맨날 놀다가 수능 망쳐서 대학 못갈 뻔 했지만 잘 살고 있다. 대학에 들어간 이후에는 남들이 하는 대로 토익 점수도 만들고 자격증도 따고 인턴십도 하면서 취업 준비를 하고 취업을 한다. 이 과정 속에서 내가 진짜로 원하는 것은 뭘까?라는 생각을 하기란 솔직히 쉽지 않은 현실이다.(남들한테 뒤쳐질까봐 그냥 정신이 없다) 이런 상황에서 갑자기 자, 이제 너는 으른이니까 니가 스스로 공부하고 싶은 것을 찾아보렴이라고 하면 적응 안되는 게 당연한 것일지도 모른다. 자, 일단 필자가 제안하고 싶은 것은 이렇다. 일단 뭘 공부해야 할지에 대한 생각은 잠깐 접어두자. 이건 어차피 뭐부터 공부를 시작하던 간에 해야할 게 너무 많기 때문에 답이 없다. 여기서 중요한 것은 뭘 공부할지를 정하는 것이 아니라 뭘 만들지를 우선 정하는 일이다. 무엇을 만들고 싶은 지 먼저 정해보자필자에게 뭘 공부해야할지 모르겠다라는 이야기를 하는 분들에게 필자가 제일 먼저 던지는 질문이 있다. 그래서, 뭘 만들고 싶은지 생각해봤어요? 이 질문을 하면 놀랍게도 10명 중 9명은 글쎄요라고 대답을 한다. 자신이 뭘 만들고 싶은지 모르는 상태에서 개발에 대한 공부를 시작하려고 하는 것이다. 이런 접근 방식으로 개발을 공부하면 대략 이런 문제가 생긴다. 목표가 없거나 두루뭉술하기 때문에 어디까지 해야 공부가 끝나는 지 알 수가 없다. 배운 걸 바로 써먹지 못하니까 공부가 재미가 없다. 첫번째 문제같은 경우가 심각한 경우인데, 애초에 목표 자체가 없거나 단순히 개발을 잘하고 싶다와 같은 두루뭉술한 이유이기 때문에 아무리 공부해도 본인이 이 달리기의 결승점이 어디인지 알 수가 없다. 이런 경우에는 처음 시작은 열정적이었으나 점점 지쳐서 공부를 멀리 하게될 가능성이 높아진다. 공부는 사실 집중을 얼마나 잘하느냐가 중요하다. 100가지 과목을 10년 걸려서 끝내는 것보다 1가지 과목을 1달 안에 끝내고 다음 걸 하는 게 더 낫다는 이야기이다. 이때 뭐부터 공부해야할지 모르는 상황이라면 이것저것 건드려보다가 아 개어렵다...하고 그만 두게 될 가능성이 높다는 것이다. 두번째 문제의 경우, 그냥 첫번째 문제의 연장이나 마찬가지다. 그냥 재미가 없다. 초중고에서 배우는 영어나 수학이 재미없는 이유도 비슷한데, 이걸 어디다가 쓰는 지도 안 알려주고 무작정 외우라고 하기 때문에 재미가 없는 것이다. 지식이라는 것은 적재적소에 써먹어야 빛을 발하는 법인데 주구장창 외워서 시험 볼때만 사용하려고 공부한 지식은 사실 별 의미가 없다. 그래서 필자같은 경우는 어떤 기술을 공부하고 싶을 때, 그 기술을 사용해서 어떤 것을 만들 수 있을 지 먼저 고민해본다. 잘 고민해보면 분명히 한두개는 나온다. 혹은 만들고 싶은 것을 먼저 생각해보고 그걸 만들 때 해당 기술을 사용하는 방법으로 접근할 수도 있다. 중요한 포인트는 그 기술을 사용하는 것이다. 필자의 경우는 후자를 더 좋아하는 편이다. 일단 만들고 싶은 것을 먼저 생각하고 거기에 필요한 지식을 공부한다. 때로는 논문을 분석해야 할수도 있고, 대학교 때 잠깐 들었던 수업의 교재를 다시 꺼내봐야 하는 경우도 생길 수도 있으며 사용하고자 하는 기술의 공식 문서를 밤새도록 읽어봐야 할 수도 있지만 그래도 이왕 시작한 프로젝트를 완성하고 싶어서 끝까지 공부하게된다. 2017년부터 꾸준히 만들어온 태양계 시뮬레이터 필자는 어릴 때부터 우주를 굉장히 좋아했는데, 코딩을 처음 시작할때부터 태양계 천체의 움직임을 구현하고 싶다는 마음이 있었다. 하지만 이걸 만드려면 컴퓨터 그래픽과 수학, 천체물리학을 어느 정도 이해하고 있어야 했는데, 처음에는 엄두가 안나서 손을 안대고 있었다. 그러다가 개발자로 일을 시작하고 몇년이 지난 어느 날, 이대로 가다간 절대 이 프로젝트를 시작할 수 없겠다라는 마음이 들었고, 그 이후 그냥 무작정 케플러 궤도 방정식과 선형대수학을 공부하기 시작해서 결국 오랫동안 그려왔던 프로젝트를 어느 정도 완성할 수 있었다. 물론 굉장히 어렵고 힘들었다. 필자는 수학을 그렇게 좋아하는 편이 아니기 때문에 온통 수학 떡칠인 저 어플리케이션을 만들 수 있을지도 의문이었다. 그래서 필자는 한 1년 정도 공부 하면서 만들면 기본적인 틀을 완성할 수 있을 것이라고 생각했는데 1년은 무슨… 3개월만에 만들었다. 물론 필자가 3개월 만에 케플러 궤도 방정식과 같은 어려운 이론을 전부 이해했다는 뜻이 아니다. 그냥 태양계 시뮬레이터를 만들 수 있을 정도의 수준으로 이해한 것이다. 그리고 애초의 필자의 목적은 천체 물리학을 공부하는 것보다는 태양계 시뮬레이터를 만드는 것이었으므로 완벽하게 이해할 필요도 없었다. 필자는 그렇게 머리가 좋은 편도 아니다. 그냥 매일 잠을 줄여가면서 꾸준히 공부한 것이다. 수식이 잘 이해가 안되면 코드로 포팅해서 한 라인씩 돌려보면서 이해했다. 그리고 이런 노가다성 공부는 진짜로 이루고 싶은 목표가 있다면 사실 누구나 다 할 수 있다.(진짜 머리가 좋은 사람은 태양계 시뮬레이터 같은 거 만들 시간에 비트코인을 샀을 것이다) 필자도 여러분과 마찬가지로 면접에 조져지고 시험에 조져지고 한다 결국 공부란 이런 것이다. 뭔가에 사용하기 위해 필요한 지식을 습득하는 것이라는 것이다. 아무런 목표가 없는 공부는 우리가 단지 수능을 잘보기 위해 공부했던 고3 시절과 다름이 없다. 물론 그 당시 공부의 목표는 수능을 잘 보는 것이었겠지만 우리는 이제 그런 수박 겉핥기 같은 목표가 아니라 좀 더 본질적인 목표를 가져야 한다. 그렇게 자기 자신에게 목표를 부여함으로써 강한 동기를 이끌어내고, 그 동기로 끈기있는 공부를 할 수 있는 원동력을 만들어 가는 것이 중요하다. 무작정 공부를 시작하기 전에 내가 왜 이걸 공부해야하는 지부터 한번 만들어보자. 자기 주관대로 공부하자방금 필자는 공부의 목표가 얼마나 중요한 것인지에 대해서 이야기했다. 필자가 방금 설명한 만들고 싶은 것을 정하자라는 목표는 나에게 당장 필요하지 않은 지식일지라도, 그 필요를 만들어내는 하나의 방법이다. 이 지식이 나에게 필요한 상태로 정의됨에 따라서 동기를 부여하는 방법이다. 하지만 이 방법은 공부를 시작하고나서 중간에 포기하지 않도록 만들어 주는 힘이 더 강하다. 그래서 우리는 어떤 지식이 나에게 필요한 지식인가?도 함께 생각해봐야한다. 단, 이 필요한 지식이라는 기준이 사람마다 다르다. 필자같은 경우 필요한 지식의 우선 순위는 철저하게 내가 뭘 만들 때 필요한 지식이다. 어떤 기술이 유행하든 세계 점유율이 90%가 넘든 간에 별로 흥미가 안가면 공부를 안하는 편이다. 그러다가 어떤 회사에 들어갔는데 그 지식이 필요하다싶으면 그때가서 공부하기도 한다. 필자가 지금까지 만든 프로젝트들만 봐도 알겠지만, 필자는 웹 프론트엔드 개발자이기 때문에 태양계 시뮬레이터나 오디오 이펙터같은 걸 만들어봤자 거기서 얻은 지식을 써먹을 확률은 상당히 낮다. 그래서 면접에 들어갔을 때 면접관이 이건 왜 만드신거에요?라고 물어보면 그냥 쿨하게 자기만족이요라고 대답한다. 이렇게 사람마다 필요한 지식의 정의는 달라질 수 있기 때문에 필자는 어떤 것이 필요한 지식이고 필요없는 지식인지를 알려줄 수 없다. 단, 필자가 이야기하고 싶은 것은 공부를 할 때도 자기 주관이 있어야 한다는 것이다. 이 자기 주관이 무엇인지에 대해 설명하기 위해 구글에서 만든 크로스 플랫폼 프레임워크인 Flutter를 공부하고 있는 분들을 예로 들어볼 수 있겠다. 이걸 공부하는 분들은 Flutter가 갑자기 잘나가서 공부하는 게 아닐 것이다. 애초에 Flutter는 아직 유명하지도 않을 뿐더러 이 프레임워크에서 사용하는 언어는 Dart이기 때문에 사실 상 Flutter가 아니면 써먹을 데도 별로 없다. codementor가 조사한 2018년도에 배우길 추천하지 않는 언어 Dart가 당당히 3개 부문 1위를 차지했었다. 물론 2019년에는 많이 좋아졌다. 그럼 왜 하는 것일까? 뭐 사람마다 여러가지 이유가 있겠지만 아마도 재밌을 것 같아서, 신기하니까, 테스트 해보고 싶어서 등의 이유가 많지 않을까? 왜냐면 아직 그렇게 유명하지도 않고, 성능이나 버그 등이 제대로 검증되지 않은 프레임워크이기 때문에 직장에 적용하기도 쉽지 않기 때문이다. 그래서 필자는 아마 이 분들의 개인적인 흥미가 크게 작용했을 것이라고 생각한다. 자기 주관이 있는 사람들은 유행에 크게 흔들리지 않는다. 자신에게 필요한 것을 계속 찾아서 공부하고 자기가 하고 싶은 공부를 한다. 이런 사람들은 본인이 네트워크에 관한 지식이 부족하다고 느끼면 어떤 부분이 부족한 건지 찾아내서 그걸 공부하지, 유행 따라서 쿠버네티스(Kubernetes)부터 공부하지는 않을 것이다. 하지만 내가 지금 어떤 것이 부족한 상황인가?라는 질문의 해답을 찾기는 꽤 힘든 과정이다. 끊임없이 자기 자신에게 질문하고, 다른 사람과 비교도 해보고 자신이 앞으로 어떤 길을 가고 싶은 지도 생각해봐야 한다. 혹시 지금까지 이런 것에 대한 고찰을 깊게 해본 적이 없다면 한번 생각해보자. 마치며만약, 수능 쪽집게 강의처럼 공부 잘하는 방법을 기대하고 들어온 독자에게는 미안하지만, 필자는 공부를 잘 하는 방법을 알려줄 수 없다. 공부에는 왕도가 없다. 그냥 꾸준히 하다보면 느는 것이기 때문이다. 대신 필자가 이 포스팅에서 이야기하고 싶었던 것은 공부를 조금이라도 효율적으로 하는 방법이었다. 위에서 이야기한 두 가지를 한 문장으로 정리해보자면 대략 이런 느낌이다. 내가 이 공부를 왜 해야하는 지 알고 하자 남들이 다 React한다고 나도 반드시 React를 해야할 필요는 없다. 1번 같은 경우는 위에서 거듭 강조했던 동기부여에 대한 말이다. 공부가 아니라 그 어떤 것을 하던 간에 동기부여가 제대로 되지 않은 일은 재미도 없고 기계적으로 하게 될 수 밖에 없다. 그냥 열심히만 하는데는 한계가 있기 마련이다. 2번은 사람마다 의견이 조금 갈릴 수 있는데, 일단 필자 생각은 이렇다. 남들이 다 React를 사용할 때 나도 React를 공부한다면 취업은 조금 더 쉬워질 수 있을 것이다. 그러나 대부분의 경우, 특히 프레임워크같은 경우는 유행을 어느 정도 타기 마련이다. 그 시대에서 원하는 패러다임과 여러가지 한계 상황을 반영한 것이기 때문이다. 2~3년 뒤에 React보다 더 좋고 획기적인 프레임워크가 나오지 않을 것이라고 어느 누가 말할 수 있을까? 사실 여러 커뮤니티에서 자주 보이는 React가 좋아요? Vue가 좋아요?와 같은 질문도 어떤 것을 선택해야 최소 비용을 투자하여 최대 이윤을 얻을 수 있는 지를 고민하는 과정에서 나오는 질문인데, 그냥 아무거나 이름이나 로고가 맘에 드는 거 골라서 공부하자. 어차피 둘 중에 뭘 선택하든 3년 뒤에는 둘 다 버리고 새로운 거 공부해야할 수도 있다. 중요한 것은 지금 나에게 필요한 것이 무엇인가?이지, 아무 이유없이 단지 사람들이 많이 사용하고 있다거나 사람들이 이 프레임워크가 좋다고 말하는 정도로 그 기술의 공부를 시작하지는 말자. 물론 사람들이 많이 사용하고 있으니까 공부를 하려고 한다는 것도 하나의 이유가 될 수 있다. 필자가 말하는 것은 자기가 결정한 것이 아니라 친구따라 강남가는 식의 결정을 말하는 것이다. 사람들이 그 프레임워크가 좋다고 말해서 공부해보고 싶다면 적어도 진짜 그 프레임워크가 어떤 점이 좋다고 하는 것인지, 진짜 좋은 것인지 정도를 스스로 판단하고 공부를 시작할 수 있는 주관이 필요하다. 면접볼 때 본인이 어떤 기술을 공부했다고 하면 그걸 왜 공부했냐, 어떤 점이 좋았고 나빴냐고 물어보는 게 괜히 물어보는 것이 아니다. 글의 서두에서도 한번 언급했지만 이 포스팅은 절대 정답이 아니다. 필자는 필자만의 공부 방법을 찾은 것이고 여러분의 방법은 필자와 같을 수도, 또 다른 방법 일수도 있다. 가장 중요한 것은 여러분 스스로 어떤 것에 흥미를 느끼는 지, 어떨때 집중이 잘되는 사람인지 계속 자기 자신에게 질문하고 답을 찾아가는 것이다. 어쨌든 이 포스팅은 필자의 주관적인 생각이지만, 그래도 개발을 공부할 때 뭐부터 시작해야할지 고민하고 있는 분들에게 도움이 되었길 바란다. 이상으로 개발자가 공부로 살아남는 방법 포스팅을 마친다.","link":"/2019/08/26/how-does-developer-study/"},{"title":"프라하에서 디지털 노마드로 살아남기","text":"필자는 지난 9월 1일에 체코 프라하에 도착해서 휴가를 보내고 있는 중이다. 근데 마냥 휴가라고 하기에는 뭐한게, 전 직장과 프리랜서 계약을 했기 때문에 여기서도 결국 코딩을 하고 있기 때문이다. 나름 디지털 노마드 흉내를 내고 있는 셈이다. 체코는 유럽치고 물가가 저렴한 편이고 인터넷도 빠른 편이기 때문에 전 세계의 많은 개발자들이 디지털 노마딩을 하러 온다고 한다. 독자분들 중에서 혹시라도 나중에 필자처럼 프라하에서 한 달 정도 살아보고 싶으신 분들이 있을 것 같아서 이번 포스팅에서는 필자가 잠깐 동안 지내면서 느낀 프라하의 모습들과 몇가지 팁을 적어보려고 한다. 필자는 여행 블로거도 아니고 프라하에 관광을 목적으로 왔다기 보다 한번 살아보고 싶어서 온 것이기 때문에 다른 여행 블로그처럼 관광지에 대한 이야기를 하기보다는 그냥 생활에 관련된 이야기를 주로 하려고 한다. 프라하에 대한 간단한 설명다들 아시겠지만 프라하는 중유럽에 위치한 체코 공화국의 수도이다. 체코는 2차 세계대전 때 독일군이 침공했을 당시 나치와 싸우지 않고 일찍 항복했기 때문에 다른 국가의 도시와 다르게 문화유산들이 파괴되지 않고 많이 남아있는 편이다. 그래서 프라하는 현대적인 모습보다는 예전 중세 유럽의 모습이 아직 많이 남아있는 도시 중 하나이다. 프라하의 올드 스퀘어가 있는 구 시가지는 유네스코 세계유산으로 지정되어 있을 만큼 역사가 깊다. 카를교 쪽은 진짜 옛날 건물들 밖에 없다 화폐 단위는 유로가 아닌 코루나(Kč)를 사용하며 환율은 대략 1코루나에 50~53원 정도로 왔다갔다 한다. 필자는 현지에서 200코루나 == 10,000원 정도로 생각하고 생활하고 있다. 그리고 유럽치고는 물가가 싼 편이기 때문에 그냥저냥 생활비는 한국이랑 비슷하게 나가는 수준이다. 단, 유럽이다 보니 한국과 시차가 7시간이라 필자처럼 한국에서 일을 받아서 가지고 온 사람은 커뮤니케이션을 위해 아침에 조금 일찍 일어나야 할 수도 있다.(참고로 프라하는 UTC +02:00이다) 필자도 처음에는 시차 때문에 저절로 눈이 일찍 떠지긴 했는데, 한 3일 정도 지난 후부터는 다시 한국에서의 생활 패턴으로 돌아가고 있어서 점점 일찍 일어나기 힘들어지고 있다. 그리고 언어는 슬라브어 계통인 체코어를 사용한다. 체코어는 영어와 비슷한 부분도 있지만 다른 부분이 더 많기 때문에 따로 공부를 안하면 알아듣기나 읽기는 힘들다. 필자는 체코어를 제대로 공부할 것도 아니고 공부할 시간도 없었기 때문에 그냥 아무 생각없이 왔다. 그래도 이왕 체코에 왔으면 체코어를 써보고 싶기도 해서 필자는 간단한 몇 가지 문장만 외운 후 열심히 조합해서 사용 중이다. 한가지 꿀팁을 주자면 제일 먼저 감사합니다(Děkuji)나 안녕하세요(Dobrý den)와 같은 기본적인 말부터 외우고 나면, 그 다음으로 반드시 못 알아듣겠어요(Nerozumím), 영어하실 수 있어요?(Mluvíte anglicky?)라는 말을 외우자. 이러면 현지어를 사용함으로써 현지인에게 좋은 첫인상도 주고 영어로 도움도 받을 수 있다. 어느 나라를 가던 똑같겠지만, 서툴더라도 자국어를 사용하려고 하는 외국인을 나쁘게 보는 사람은 없다.(마트에서 뎨꾸이했다가 지나가던 할아버지한테 칭찬받은 1인) 그럼 이제 필자가 며칠 동안 지내며 겪었던 내용들을 몇가지 이야기 해보도록 하겠다. 맥북 프로 레티나 기내 반입에 대해서이건 필자도 출발하기 하루 전에 알게 된 것이라 조금 당황했던 항목인데, 딱히 프라하에만 관련된 내용은 아니지만 혹시나 모르시는 분들이 계실까봐 이야기 하도록 하겠다. 2015년에서 2017년 사이에 제조된 맥북 프로 레티나 중 일부 제품이 배터리 불량으로 인해 폭발 사고가 몇 번 발생했다. 그래서 2019년 8월부터 EU 회원국의 항공사에서 운행하는 비행기를 탈 때 위탁 수하물로는 맥프레를 운송할 수 없고 무조건 기내 수하물로 가지고 타야한다. 그리고 기내에서 절대 전원도 키면 안된다. EU가 아닌 다른 국가의 경우는 아예 비행기에 맥프레를 반입하지 못하게 금지시킨 항공사도 있으니 잘 알아보고 타도록 하자. 필자같은 경우는 EU로 왔기 때문에 가지고 올 수는 있었지만 다시 귀국할 때 규정이 어떻게 변경될 지 모르기 때문에 안전한 2009년형 맥북 프로를 데려왔다. 필자가 체코항공에 직접 문의해본 결과, 기내에서 전원만 안키면 괜찮다는 답신을 받기는 했으나 나중에는 상황이 어떻게 바뀔지 모른다. 탑승객의 생명과 직결되는 항공사 안전 규정의 특성 상, 맥프레 배터리 폭발 사고가 한두번만 더 발생해도 유예 기간없이 해당 규정이 변경될 수도 있다. 게다가 한국에서 반입 금지당하면 택배로 집에 보낼수라도 있지만 체코에서 귀국할 때 반입 금지당하면 처리가 곤란해지기 때문에 필자는 그냥 조금 답답하더라도 2009년형 맥북을 가져오는 것을 선택했다. 자세한 내용은 관련 기사를 참고하자. 물가가 싸다?필자가 프라하에 오기 전에 사전조사를 하면서 알게된 사실은, 체코가 생각보다 물가가 싸다는 것이다. 물론 이 싸다는 것이 유럽치고 싸다는 것이지 한국에 비해서 엄청 싸거나 그렇다는 의미는 아니다. 그래도 한국보다 전체적으로 물가가 싼 느낌이 있긴 하다. 하지만 해외 물가라는 것이 어떤 특정 아이템은 한국보다 쌀 수도 있지만 다른 아이템은 또 비쌀 수도 있는 것이라서 막상 며칠 살아보니 한국과 그렇게 큰 차이는 못 느꼈다. 확실히 식재료 같은 것은 한국에 비해 싸지만 멀티탭 같은 전기 제품은 한국보다 훨씬 비쌌다.(멀티탭 하나에 만원이 넘는다…) 필자 같은 경우는 요리를 잘하는 편이 아니기 때문에 주로 밖에서 사먹거나 마트에서 즉석식품을 사와서 먹는 편인데, 이 경우에는 한국과 거의 비슷한 금액이 지출된다. 뭐 맥도날드에서 햄버거 사먹으면 5,000원 정도, 길거리에서 샌드위치 하나 사먹으면 3,000원 정도 이런 느낌이다. 그리고 필자가 출발하기 전에 친구가 유럽은 물이 엄청 비싸다라고 해서 겁먹었는데, 테스코에서 사면 1.5L짜리 생수병 1통이 600원정도 밖에 안하더라. 참고로 생수는 파란색 포장지에 Neperlivá Voda라고 적혀있다. Ne는 아니라는 뜻이고 perlivá는 스파클링, Voda는 물이다. 즉, Neperlivá Voda는 탄산수 아닌 물이라는 뜻이다. 탄산수를 별로 안 좋아하시는 분들은 참고하도록 하자. 한국에 비해서 확실히 싸다고 느낀 건 바로 교통비인데, 한 달 교통비로 35,000원 정도만 지불하면 대중 교통을 무제한으로 이용할 수 있다. 한국에서 필자는 한 달에 교통비로만 거의 70,000원 가까이 지출하기 때문에 확실히 교통비는 한국에 비해서 싸다고 할 수 있다. 생각보다 영어가 잘 안통한다사실 필자는 프라하가 유명한 관광지이기 때문에 사람들이 영어를 다 잘할 것이라고 생각했다. 하지만 관광객들이 주로 다니는 프라하 성이나 카를교, 올드 스퀘어 쪽을 제외한 지역은 생각보다 영어가 잘 안통한다. 물론 일본이나 중국같은 아시아 국가처럼 아예 못 알아듣는 정도는 아니지만 너무 빨리 이야기하거나 너무 복잡한 어휘를 사용하면 잘 못 알아듣는다. 그리고 아무래도 나이가 어린 사람들 보다는 나이가 조금 있으신 분들 일수록 더 영어를 못하신다. 그리고 이건 우리나라도 비슷하지만 테스코와 같은 대형마트에 가면 제품명이라던가 코너 간판이 전부 체코어로만 적혀있기 때문에 조금 헤멜 수 있다. 필자는 도착한 첫날에 Mléko라고 적혀있길래 우유인줄 알고 샀는데 알고보니 요거트 밀크였다.(시리얼에 붓고 나서 알았다) 필자처럼 오랜 기간 동안 머문다면 체코에 오기 전 기본적인 단어나 파닉스 정도는 공부를 해서 오면 도움이 될 것 같다. 필자는 영어만 믿고 왔다가 꽤나 고생 중이다. 대중 교통타고 다니기프라하의 대중 교통은 버스, 트램, 지하철이 있다. 필자는 개인적으로 버스보다는 트램이나 지하철이 더 편한 것 같다. 여기서는 교통권을 사용하거나 한국의 T머니와 같은 라테츠카 카드를 사용해서 대중교통을 이용할 수 있는데, 라테츠카 카드는 시청까지 가서 발급 받아야 하고 여권 사진도 제출해야 해서 귀찮다. 그래서 필자는 그냥 한 달 교통권을 구매해서 사용 중 이다. 교통권이 시간제로 운영된다프라하의 교통권은 우리나라처럼 거리비례제가 아니라 시간제이다. 티켓을 구매한 이후에 펀칭 기계라고 부르는 요상한 기계에 교통권을 넣고 사용 시작 시간을 찍는다. 즉, 90분 이용권을 구매했다면 사용 시작 시간이 찍힌 이후로부터 90분 동안은 트램을 타던 지하철을 타던 뭘 타던 환승도 자유다. 하지만 시간이 지나버렸는데 내 몸뚱아리가 아직 대중교통 안에 있고 게다가 검표원한테 걸렸다면 얄짤없이 벌금행이다. 참고로 무임승차 벌금은 무조건 현금으로 내야하는데, 현장에서 바로 내면 800Kč이고 당장 현금이 없어서 ATM에서 뽑아서 내야한다면 1,500Kč이다. 블로그 후기를 봐도 이 티켓 펀칭을 안해서 벌금냈다는 사람들이 꽤 많았다. 근데 필자도 막상 트램이나 지하철을 타보고 나니까 그런 실수를 할만 하다고 생각하긴 했다. 그냥 저 기둥 사이로 들어가면 된다 처음 프라하에 왔을 때 당황했던 것 중에 하나가 대중교통을 이용할 때 표를 검사하는 기계적인 시스템 자체가 없다는 것이였다. 우리나라는 카드를 찍거나 교통권을 넣는 개찰구가 칸막이로 막혀있고 인증이 되면 들여보내주는 방식인데, 여기는 그런 게 없이 그냥 뻥 뚫려있다. 트램도 마찬가지다. 위 사진에 보이는 저 노란색 장치가 바로 아까 말한 티켓 펀칭 기계이다. 지하철에 들어갈 때 반드시 저 기계에 교통권을 넣고 반드시 티켓 사용 시작 시간을 찍어야 한다. 트램이나 버스같은 경우에도 그냥 트램이나 버스가 오면 탄 다음에 차량 내부에 있는 펀칭 기계에 사용 시작 시간을 찍으면 된다. 근데 솔직히 티켓 펀칭을 안하고 탄다고 해서 우리나라처럼 무슨 경고음이 울리고 하는 것도 아니기 때문에 정신놓고 타다가는 까먹기 딱 좋은 것 같다. 교통권의 종류프라하의 교통권은 총 4개 종류가 있고 가격과 사용 시간은 다음과 같다. 사용가능시간 가격 30분 24Kč 90분 32Kč 1일 110Kč 3일 310Kč 필자같은 경우 집에서 한 달 교통권을 팔고 있는 프라하 중앙역까지 가기 위해 교통권을 구매해야했는데, 트램타고 25분 정도의 거리였지만 그냥 90분 짜리를 끊었다. 뭐 중간에 어떤 변수가 생길지 모르기 때문에 나름 방어한다고 그렇게 한 것인데, 그냥 딱 맞춰서 사도 상관없었을 것 같다. 하단에 화살표처럼 되어있는 부분을 펀칭 기계에 넣으면 시작 시간이 찍힌다 교통권은 어디서 구매하나요?자, 그럼 교통권은 어디서 살 수 있을까? 뭐 뻔한 이야기이지만 당연히 대중교통을 탈 수 있는 곳 근처에서 살 수 있다. 교통권은 지하철역에서 팔거나 정류장 근처의 슈퍼에서 살 수 있다. 지하철역 같은 경우는 역에 들어가면 왠지 티켓 판매기가 있어야할 것 같은 위치에 티켓 판매기같이 생긴 게 떡하니 들어가 있기 때문에 그냥 마음이 시키는 대로 가서 돈 넣고 표를 뽑으면 된다. 현대인이라면 누구든지 할 수 있는 난이도니까 걱정하지말자. 대놓고 Tickets라고 적혀있으니 못 찾을 걱정은 안해도 된다 정류장 근처의 슈퍼에서 교통권을 사야하는 경우에는 조금 난이도가 있다고 생각이 드는게, 이 동네는 간판에 영어가 잘 안적혀있다. 대부분 체코어로 적혀있는 데다가 처음 오는 타지에서 다 비슷비슷하게 생긴 유럽식 건물들을 하나하나 흝어보면서 슈퍼를 찾는다는 건 생각보다 힘든 일이다. 그래서 필자는 될 수 있으면 그냥 근처 지하철역에서 사는 걸 추천한다. 한 달 교통권 구매하기대부분의 경우에는 프라하에 2~3일 정도 머물다 다른 도시나 다른 나라로 떠나기 때문에 위에서 설명한 교통권만으로도 충분하겠지만 필자처럼 길게 머무는 사람은 매일 저 티켓을 사는 게 여간 귀찮은 일이 아니다. 보통 필자처럼 오래 머무는 사람들은 한 달 교통권을 구매하거나 라테츠카 카드를 등록해서 사용하는데, 라테츠카는 시청까지 가서 발급받아야하고 여권 사진도 제출해야해서 좀 귀찮다. 그래서 필자는 그냥 프라하 중앙역에 가서 한 달 교통권을 구매했다. 블로그를 뒤져가며 알아본 정보로는 어느 정도 규모가 있는 역에서는 다 판다고 하는데 어느 정도가 진짜 얼마나 큰 역을 의미하는 지 애매하기 때문에 그냥 프라하 구 시가지 구경도 할겸 프라하 중앙역으로 갔다. 이렇게 생긴 인포메이션 센터에서 한 달 교통권을 판매한다 한 달 교통권의 가격은 670Kč이고 다른 교통권과 다르게 따로 펀칭은 필요없다. 검표원이 검사할 때 그냥 보여주기만 하면 된다. 한 달 교통권을 구매할 때 판매하시는 분이 교통권을 언제부터 이용할 것인지 물어보는데 그냥 I'm gonna use it right now라고 하면 바로 날짜랑 시간을 찍어준다. 한 달 교통권은 이렇게 생겼다 그리고 대중 교통 외에도 한국의 킥고잉같은 Lime이라는 서비스가 있어서 전동 킥보드를 타고 다닐 수도 있다. 사용법은 한국과 동일하게 앱을 설치하고 결제 수단을 등록한 후 킥보드에 있는 QR 코드를 스캔하는 방식이다. 이 Lime이라는 회사는 체코 회사는 아니고 미국 회사인데 아시아 지역을 제외한 다른 지역에서는 많이 사용하는 서비스라고 한다. 현지 통신사 유심필자는 보통 단기로 여행을 가면 그냥 국내 통신사 로밍을 하는 편인데 이번에는 한 달이나 있어야 하는 만큼 로밍을 하면 요금 폭탄을 맞을게 뻔했기 때문에 유심을 구매해서 사용해야 했다. 유심 구매하기유심 구매는 공항에서 해도 되고 시내에서 휴대폰 매장을 직접 찾아가도 된다. 우리나라에 SKT, KT, LGT라는 거대 통신사들이 있듯이 체코에는 T-Mobile, Vodafone, O2라는 3개의 거대 통신사가 있다. 필자는 이 중 필자에게 익숙한 Vodafone에서 유심을 구매했다. 이 통신사는 다국적 통신사라 중국이나 홍콩에서 사용 했던 적이 있어서 선택한 것이다. 사실 Vodafone이 통신 커버리지가 안좋다고 소문이 자자하긴 한데, 필자 경험상 상하이나 홍콩같은 대도시에서는 딱히 문제가 없었기 때문에 프라하도 마찬가지일 것이라고 생각했고, 실제로도 그냥저냥 잘 된다. 300미터 밖에서도 눈에 띄는 존재감을 뽐내는 Vodafone 매장 Vodafone 매장은 우리나라의 KT 마냥 뻘건색으로 도배가 되어있기 때문에 어디서든 쉽게 눈에 띈다. 매장을 발견했다면 그냥 매장에 들어가서 놀고 있는 직원에게 I'd like to buy prepaid sim card라고 하면 다 알아서 해준다.(거의 한국만 유심이라고 부르고 다른 나라는 대부분 심카드라고 한다.) 이때 직원이 필자에게 3가지 정도를 물어봤는데, 며칠 동안 사용할 것인지와 데이터 사용만을 원하는지, 몇 GB의 데이터를 원하는지였다. 필자는 사실 한국에서 500MB짜리 초저렴 요금제를 사용하고 있기 때문에 한 달에 얼마 정도의 데이터가 적당한지 잘 몰랐다. 그래서 고민하느라 잠깐 멍때리고 있으니 바로 10GB를 추천하길래, 됐고 그냥 4GB만 달라고 했다. 데이터 4GB짜리 심카드의 가격은 500Kč으로 한화로 대략 26,000원정도이다. 한국에서 이런 식으로 유심을 구매해본 경험이 없기 때문에 적당한 가격인지는 잘 모르겠지만 가격 자체만 보면 그렇게 부담스럽지 않았다. 나중에 블로그를 조금 찾아보니 아시안처럼 딱 봐도 외국인처럼 보이는 경우 속사포처럼 영어를 쏟아내며 바가지를 씌우는 경우도 있다고 하니 필자처럼 아무 생각없이 매장에 찾아가는 것보다는 미리 어느 정도의 데이터가 필요한지 생각해놓고 가는 것이 좋을 것 같다. 유심 등록하기통신사에서 유심을 구매할 때 통신사에서 개통까지 한번에 알아서 해주기 때문에 활성화만 시키면 바로 사용할 수 있는 상태가 된다. 개통에 별다른 문서 작성은 필요없고 그냥 유심받아서 바로 사용하면 된다. 뒷면에 휴대폰 번호와 핀 번호가 적혀있다 유심 케이스의 뒷면에는 발급받은 유심에 대한 정보를 적어주는데, Telefonní číslo는 발급받은 휴대폰 번호를 의미하고 Poznámka는 핀 번호를 의미한다. 이때 발급받은 유심에는 락이 걸려있기 때문에 휴대폰을 껐다 킬때마다 이 핀 번호를 입력해줘야한다. 보다폰은 그냥 1234를 사용하지만 다른 통신사는 유심 구매시 동봉되어있는 카드를 긁으면 핀 번호가 노출되도록 되어있다. 참고로 이 동네는 1하고 7을 이런 식으로 쓰는 사람이 많으니 헷갈리지 말자 이렇게 유심을 구매해서 휴대폰에 끼우고 나면 개통은 되었지만 활성화가 되지않은 상태가 된다. 통신사는 잡혀서 상태표시바에 Vodafone CZ LTE라는 글자는 보이지만 실제 인터넷 연결은 막혀있는 상태이다. 유심 케이스에 유심을 활성화시킬 수 있는 방법이 적혀있기 때문에 잘 보고 따라하면 된다…라고 하기엔 좀 애매한 것이, 이 방법이 체코어로 적혀있다. 만약 와이파이를 사용할 수 있는 환경이라면 그냥 번역기에 돌리면 되지만 와이파이를 쓰지 못하는 환경인 경우에는 그냥 통신사 직원한테 도와달라고 하자. 필자는 당연히 영어로 되어 있을 줄 알고 패기있게 그냥 나왔다가 길 한복판에서 지나가는 사람 붙잡고 도와달라고 했다. 어쨌든 Vodafone의 유심 활성화 방법은 다음과 같다. 유심이 개통되면 Vodafone에서 형식적인 개통 축하 문자와 함께 비밀번호를 보내준다. *77에 전화를 건다. 체코어가 나와도 당황하지말자. *을 누르면 영어로 진행할 수 있다. 1번에서 확인한 비밀번호를 입력한다. 요금과 관세에 해당하는 설명을 해주는데 그냥 끊어도 된다. 문제는 이게 바로 활성화되는 게 아니고 시간이 조금 걸린다. 통신사 직원 말로는 10분이면 될 거라고 그랬는데, 필자는 3시간 조금 넘게 걸렸다. 그리고 유심이 개통되면 보내주는 문자에 현재 남은 데이터나 사용 중인 서비스를 확인할 수 있는 링크도 함께 보내주니까 문자 지우지 말자. 만약 이렇게 해도 안된다면 APN 세팅을 한번 확인해보자. 필자 같은 경우 저번에 일본에 갔을 때 한번 현지 통신사로 APN 세팅을 했었는데 이후 한국에 돌아와서 다시 세팅하는 과정에서 APN이 lte.sktelecom.com으로 설정되어 있었다. 이렇게 타 통신사의 APN이 상수로 잡혀있다면 당연히 인터넷이 되지 않으니 해당 값을 지워주자. 카드 결제 시 현지 통화로 결제하자필자는 지금 거의 카카오 체크카드 하나만 믿고 프라하를 돌아다니고 있다. 해외에서 카드를 사용하여 결제를 자주 해보신 분들은 알겠지만 결제할 때 현지 통화 결제와 원화 결제를 선택할 수 있다. 이때 원화 결제를 하게되면 당일 환율로 결제 대금이 환산되어 결제되어 우리에게 원화로 얼마가 결제되었는지 알려준다. 그래서 내가 지금 얼마를 썼는지 한 눈에 알아볼 수 있다는 장점이 있긴 하지만, 문제는 이거 해외 결제 수수료 외에도 DCC 수수료라는 게 추가로 붙기 때문에 수수료가 2중으로 나간다. 게다가 현지 통화를 원화로 변경할 때 적용되는 환율은 고객보다는 가맹점과 은행에 유리한 환율이 적용되기 때문에 될 수 있으면 안하는 것이 좋다. 카카오 체크카드 같은 경우에는 카카오뱅크 앱 내에서 간단하게 해외 원화결제를 차단할 수 있는 설정이 있기 때문에 해당 옵션을 켜두면 원화 결제로 인한 2중 수수료 부과를 막을 수 있다. 카드 > 내 카드 > 카드관리에서 간단하게 설정할 수 있다 근데 진짜 문제는 대부분의 매장이 카드 결제를 할 때 직원이 카드를 받아서 결제해주는 것이 아니라 고객이 직접 카드 결제기에 카드를 넣어서 결제하는 방식이라는 것이다. 우리나라도 스타벅스 같은 곳에 가면 손님이 직접 카드를 결제기에 꽂아서 결제하는 경우가 있는데, 여기는 대부분이 그런 방식을 택하고 있다. 직원이 결제해주면 그냥 I'll pay in local currency라고 하면 왠만큼 알아듣고 알아서 해주지만, 내가 직접 카드 결제기에 카드를 넣고 결제하는 경우에는 아무래도 이 기계를 사용해본 경험도 없고 체코어로 나오기 때문에 당황할 수 밖에 없다. 하지만 당황하지말자. 카드를 기계에 꽂아넣고 금액을 확인하는 화면이 뜨는데 자세히 보면 Select currency?라고 물어보고 그 밑에는 CZK와 KRW이 떠있다. 이때 빨간색의 취소 버튼을 누르면 코루나로 결제가 되고 초록색의 확인 버튼을 누르면 원화로 결제가 된다. 필자는 처음에 화면을 자세히 안보고 그냥 최종 금액 확인만 하는 건줄 알고 초록색 버튼을 눌렀었다. 어떤 통화로 결제할 것인지 선택할 수 있다 그리고 대부분 위 사진과 동일한 기계를 사용하지만 맥도날드 같이 다른 결제기를 사용하는 경우에는 가끔 체크카드 비밀번호를 입력해야 결제가 되는 경우도 있다. 이때 4자리 비밀번호를 눌러보고 안되면 뒤에 00을 추가로 붙혀서 6자리로 만들면 결제가 된다.(방금 맥도날드에서 이것 때문에 삽질하고 왔다) 코워킹 스페이스 알아보기필자는 프라하에 오면서 전 직장과 프리랜서 계약을 했고 이 계약은 9월 2일부터 시작이었기 때문에 프라하에 있는 동안 일을 하긴 해야한다. 다행히 집에 와이파이가 빵빵하게 잘 터지기 때문에 집에서 일을 해도 상관없긴 하지만 이왕 해외에 온 김에 다른 나라 개발자들이랑 얘기도 해볼 겸 코워킹 스페이스에 가서 작업을 하고 있다. 프라하는 은근히 다른 나라에서 디지털 노마딩을 하러 오는 사람들이 많기 때문에 나름 코워킹 스페이스가 꽤 있는 편이다. 그리고 대부분의 코워킹 스페이스는 하루 동안만 이용할 수 있는 Day pass를 제공하기 때문에 여러 군데를 다녀보면서 괜찮은 곳을 물색해보는 것을 추천한다. 필자는 Coworker.com에서 프라하에 있는 코워킹 스페이스를 몇 개 골라놓고 시간될 때 한번씩 가보고 있다. 일반적으로 하루 이용료는 200Kč에서 300Kč 정도 된다. 필자 생각에 카페에 비해서 코워킹 스페이스가 좋은 점은 다른 나라의 개발자들이랑 네트워킹을 할 수 있는 기회가 생긴다는 것과 물과 커피, 화장실을 공짜로 사용할 수 있다는 것이다.(유럽은 화장실 돈 내고 쓴다. 게다가 동전만 받는다.) Coworker에서 Prague를 검색하면 많은 코워킹 스페이스들이 나온다. 그리고 카페에서는 화장실이나 흡연으로 인해 자리를 비울때 테이블에 짐을 놓고 가면 분실할 위험이 높지만 코워킹 스페이스는 안전한 편이다. 필자는 처음에 코워킹 스페이스도 믿을 수 없어서 짐 다 싸서 화장실에 다녀왔는데 거기 직원이 그냥 놓고 가도 된다고 해서 좀 뻘쭘했다. 혹시 프라하 5구역의 안뎰(Andêl)역 근처에서 머물 예정이라면 Impact Hub D10을 추천한다. 일단 카운터 직원이 영어를 굉장히 잘하고 친절했으며 내부 공간 인테리어도 좋았다. 대신 Day pass 가격이 390Kč로, 다른 코워킹 스페이스에 비해서 조금 더 나가는 편이다. Impact Hub D10의 중앙 홀 근데 의외로 개발자는 생각보다 별로 없었다. 필자 포함 5~6명 정도? 그때만 그랬던 것일 수도 있지만 한국의 코워킹 스페이스에 비하면 생각보다 개발자가 많이 없어서 놀랬다. 그래서 그런지 필자가 코딩하고 있으니까 다른 개발자 분들이 먼저 관심도 가져주고 말도 걸어주고 했다. 어떤 영국 개발자 분이 필자한테 어디서 왔냐, 뭐 만드는 개발자냐, 지금은 뭐 만들고 있는거냐라고 물어보길래 지금 작업 중인 부분은 기존의 레거시 어플리케이션을 새로 만든 어플리케이션으로 마이그레이션하는 작업이라고 했더니 레거시 구조에 대해서도 관심을 가지고 물어보고 하더라. 근데 이 아저씨… 정작 레거시에 대한 설명을 듣더니 심각한 얼굴로 Good luck, mate를 외치고 커피타러 가버렸다.(뭔가 팁이라도 줄 거라고 기대했다) 필자는 지금 프라하 5구역에서 머물고 있기 때문에 D10 스페이스에만 갔었지만 프라하 2구역에도 K10 스페이스가 또 있다. 근데 여기는 Day pass 가격이 500Kč이다. 사실 여기는 안가봐서 왜 이렇게 비싼지는 잘 모르겠는데 사진을 보면 뭔가 인테리어가 더 고급스러운 것 같기도 하다. 어쨌든 Impact Hub는 글로벌 기업이기 때문에 뭔가 검증된 시설을 원한다면 좋은 선택이 될 수 있을 것 같다. 마치며사실 필자는 프라하에 도착한지 겨우 5일 정도 밖에 지나지 않았기 때문에 프라하의 구석구석을 다 아는 것은 아니다. 게다가 관광이 목적이 아니기 때문에 관광지를 많이 둘러보지도 않았고 그냥 산책 겸 왔다갔다 정도만 하고 있기 때문에 관광지에 대해서도 잘 모르는 상태다. 사실 한국에서처럼 카페가서 코딩하고 있으면 여기가 프라하라는 것도 잊어버릴 때가 있다. 그러나 한 달이라는 긴 시간을 머무는 만큼 일반적인 여행보다는 조금 더 여유를 가지고 이 도시에서만 느낄 수 있는 일상을 느낄 수 있다는 점이 좋은 것 같다. 그냥 집 밖을 나가서 동네만 돌아다녀도 한국과는 전혀 다른, 이국적인 풍경이 펼쳐지기 때문에 신기하기도 하고, 코워킹 스페이스에서 다른 나라의 개발자들과 얘기해볼 수 있다는 것도 재밌는 것 같다.(사실 개발자라는 종족이 어느 나라든 다 비슷비슷한 느낌이기는 하다) 확실히 이 동네는 유럽 느낌이 팍팍 난다 어차피 다음 달에 한국에 다시 돌아가면 면접도 보러다녀야하고 프리랜서 일도 마무리 해야해서 정신없을테니 여기 있는 동안 만큼은 최대한 여유를 가지고 지내려고 한다. 9월이라 날씨도 좋아서 공원에 가서 책 읽기도 좋다. 사실 출발하기 전에는 이런 저런 걱정이 많았지만 막상 와보니 여기도 결국 사람사는 곳이라 문제가 생겨도 어떻게든 해결할 수는 있는 것 같다. 그리고 체코 사람들 무뚝뚝 하다고 해서 걱정했는데, 그냥 표정만 무표정이고 행동은 다들 친절하다.(츤데레) 한 달 동안 문화도, 언어도 다른 타국에서 생활한다는 것이 쉽지만은 않은 일이겠지만 해외에 나가본 경험이라고는 여행이나 출장 밖에 없는 필자에게 이번 경험은 굉장히 소중한 추억이 될 것 같다. 이상으로 프라하에서 디지털 노마드로 살아남기 포스팅을 마친다.","link":"/2019/09/06/life-in-prague-tip/"},{"title":"동기(Synchronous)는 정확히 무엇을 의미하는걸까?","text":"이번 포스팅에서는 I/O와 네트워크 등 전반적으로 다양한 모델에서 사용하는 개념인 동기(Synchronous)가 정확히 무엇을 의미하는 것인지, 그리고 동기 방식과 비동기 방식의 차이에 대해서 한번 이야기 해보려고 한다. 그리고 이 두 가지 개념과 많이 혼동되는 개념인 블록킹(Blocking)과 논블록킹(Non-Blocking)에 대해서도 간단하게 짚고 넘어갈 예정이다. 본격적인 포스팅에 들어가기에 앞서 한가지 확실하게 이야기하고 싶은 것은 동기와 비동기는 프로세스의 수행 순서 보장에 대한 매커니즘이고 블록킹과 논블록킹은 프로세스의 유휴 상태에 대한 개념으로 완전한 별개의 개념이라는 것이다. 아무래도 동기와 블록킹, 비동기와 논블록킹의 작동 매커니즘이 더 직관적이기 때문에 많은 사람들이 이 개념들을 같은 것 혹은 비슷한 것으로 오해하고 있는데, 방금 이야기 했듯이 이 두가지 개념은 서로 전혀 다른 곳에 초점을 맞춘 개념들이므로 서로 직접적인 관련은 거의 없다고 봐도 된다. 단지 조합하여 사용되는 것 뿐이다. 출처 - Boost application performance using asynchronous I/O, IBM 이미 많은 능력자 분들이 이 주제에 대해서 잘 정리해놓은 포스팅들이 있지만, 대부분 이 개념들을 묶어서 함께 다루고 있기 때문에 이 개념들을 처음 접하거나 컴퓨터 공학에 대해 잘 모르는 사람은 이 개념들이 서로 뭔가 연관이 있는 것으로 오해하기 쉽다. 하지만 많은 포스팅에서 이 주제들을 묶어서 다루는 이유는 단지 이 개념들을 구현한 구현체에서 이 두 가지 개념이 함께 사용되고 있기 때문에 이 개념을 분리해서 따로 설명하는 것이 더 어렵기 때문이다. 그래서 필자도 어쩔 수 없이 이 개념들을 함께 설명하기는 하지만, 직관적인 개념인 블록킹 & 논블록킹보다는 좀 더 추상적인 개념인 동기 & 비동기에 초점을 맞춰서 진행할 것이다. 동기는 정확히 뭘 뜻하는 걸까?동기에 관련된 포스팅들을 읽어보면 사람마다 동기라는 단어를 해석한 결과가 가지각색이다. 어떤 사람은 동시에 발생하는 것, 어떤 사람은 특정한 클럭을 정해 통신하는 것, 어떤 사람은 상태를 동일하게 만드는 것 등 동기라는 단어의 쓰임새가 다양한 만큼 다양한 해석들이 존재한다. 도대체 뭐가 맞는 건지 알기가 힘들다. 필자 또한 처음 동기라는 개념을 배울 때 동기라는 단어와 매커니즘이 잘 와닿지 않아서 혼란스러웠었다. 뭐 블록킹 같은 단어는 농구같이 일상에서 접할 수 있는 스포츠에서도 사용하고 있는데다가 뭔가를 막는다라는 뜻이 바로 와닿기 때문에 조금 이해하기 쉬운 편이지만 동기는 아니다. 일상에서 주로 접하는 동기의 쓰임새 중 대표적인 예는 동기화(Synchronization)정도가 있을 것 같다. 아이폰에 음악을 넣을 때 아이튠즈에서 사용하는 동기화 기능 같은 것 말이다. 이때 동기화라는 행위는 서로 다른 상태를 같은 것으로 만드는 것을 의미한다. 동기화 한번 잘못해서 아이폰에 있는 음악이 다 날아가는 경험은 다들 한번쯤 있지 않을까 하지만 컴퓨터 공학에서는 대부분 동기를 동시에 발생하는 것이라고만 설명하고 있기 때문에 느낌이 조금 다르다. 물론 데이터베이스 동기화와 같이 동일한 의미로 사용되는 경우도 있지만, 많은 포스팅이 I/O나 네트워크에 대한 내용을 다루고 있기 때문에 전자의 의미로 해석되는 경우가 많다. 동기와 비동기라는 단어 중 비동기는 동기가 아니다라는 의미이기 때문에 우리는 동기가 정확히 무엇을 말하는 것인지에 초점을 맞춰서 생각해봐야한다. 그래서 필자는 동기라는 단어가 정확히 무엇을 의미하는지부터 한번 이야기해보려고 한다. 왜 동기는 이렇게 다양한 해석을 가지는 것일까? 단어의 뉘앙스를 파악해야한다고등학교 때 언어 영역이나 외국어 영역을 공부할 때 지문 안에 답이 있다라는 말을 들어본 적이 있을 것이다. 공학이나 과학 분야에서 사용되는 용어는 주로 영어에서 파생되었고, 이를 한국어로 번역하는 과정에서 오히려 뜻을 알기가 어려운 단어로 번역되는 경우가 있기 때문에 원본인 영어 단어의 뜻을 제대로 파악하는 것이 중요하다. 갑자기 분위기가 외국어 영역이 된 것 같지만, 필자는 개인적으로 이런 과정 또한 중요하다고 생각한다. 이 과정을 통해 왜 동기라는 단어가 상황에 따라 다른 의미를 가질 수 있는지 알 수 있기 때문이다. 암기하지말고 이해하자 먼저 동기(同期)라는 단어의 한자를 보면 같을 동(同), 기약할 기(期)를 사용하고 있으며, 일반적으로 우리가 입사 동기, 군대 동기 등을 이야기 할 때 쓰는 동기와 같은 단어다. 이 단어의 한자만 보면 같은 기간 또는 같은 주기라는 뜻이다. 역시 우리가 일반적으로 사용하는 동기화의 의미랑은 조금 다른 것 같다. 우리가 아이폰과 아이튠즈를 동기화하는 것이 이 두 대상의 기간이나 주기를 같게 맞추는 것은 아니지 않은가? 게다가 일반적으로 한국어에서 ~화라고 함은 앞에 붙은 단어의 의미를 그대로 가져가는 경우가 많아서 더 헷갈린다. 하지만 사실 우리가 동기의 번역으로 많이 사용하는 Synchronous는 살짝 다른 뉘앙스를 가지고 있다. synchronous [adjective]: happening, existing, or arising at precisely the same time Websterwww.merriam-webster.com/dictionary/synchronous 자, 일단 Synchronous는 동기와 다르게 형용사다. 그래서 사실 한국어로 정확히 번역하면 동기적인과 같은 뜻이 되어야 하지만, 한국어로 형용사를 단독으로 사용하는 경우는 별로 없기 때문에 그냥 편의상 명사로 번역하는 것 같다. 그러나 이런 과정에서 한국어 단어와 영어 단어의 뉘앙스가 달라지는 경우가 발생한다. 그리고 의미를 보면 정확히 같은 시간에 발생, 존재하는 것이라고 한다. 그리고 이 단어는 형용사이기 때문에 무엇이 정확하게 같은 시간에 발생하는지는 적혀있지 않다. 그럼 이제 동기화를 의미하는 명사인 Synchronization의 사전적 의미를 한번 살펴보자. synchronization: the state of being synchronous Websterwww.merriam-webster.com/dictionary/synchronization Synchronization은 Synchronous한 상태라고 한다. 즉, 동기와 동기화는 근본적으로 같은 뜻이라는 말이다. 같은 단어를 공유하는 이 두 단어가 한국어로 변형되며 다른 뜻이 되는 것은 영어를 한국어로 번역하는 과정에서 영어 특유의 뉘앙스를 제대로 표현하기가 어렵기 때문에 발생하는 문제이다. 게다가 이 단어들의 원형인 Synchro는 단어 자체가 뜻을 의미하는 변태적인 단어이기 때문에 한국어로 번역하기도 쉽지 않다.(한국어로 치면 “애매하다”같은 느낌이다. 이런건 반대로 영어로 번역하기 쉽지 않다.) Synchronize, Synchronization, Synchronous 등 Synchro를 공유하는 이 단어들이 공통적으로 가지는 뉘앙스는 바로 동시에 똑같이 진행되는 느낌이다. 그것이 상태이든 동작이든 사건이든 동시에 똑같이 진행되는 느낌을 말하는 것이다. 대표적인 싱크로의 예 즉, 아이폰과 아이튠즈의 상태를 동일하게 만드는 것은 작업이 끝남과 동시에 아이폰과 아이튠즈가 같은 상태가 되므로 Synchronous한 상태가 된 것이고, 일반적으로 컴퓨터 공학에서 이야기하는 동기의 해석인 동시에 발생하는 사건 또한 Synchronous한 사건이라고 할 수 있는 것이다. 심지어 이 단어들의 어원인 Syn-은 단순히 Together라는 의미를 내포하는 단어이기 때문에 이런 상황에서도 사용할 수 있다. He and I are out of sync in everything그와 나는 모든 면에서 맞지 않는다 이렇게 한국어로 직역하기 어려운 단어는 뉘앙스를 통해서 뜻을 이해하는 편이 더 좋다. 교수님들이나 과학자들이 한국어로 말하는 중간에 영어 단어를 섞어가면서 사용하는 것은 이런 이유도 있다고 생각한다. 사실 한국어의 동기라는 의미에만 초점을 맞추면 Synchro에서 변형된 단어들 간의 공통점을 연상하기가 쉽지 않다. 다시 정리하자면, Synchro-를 사용하는 단어들은 모두 동시에 똑같이 진행되는 느낌의 뉘앙스를 가지는 단어이다. 결국 우리가 상태의 통일을 의미하는 동기화든 컴퓨터 공학에서 말하는 동시에 발생한 사건이든 모두 같은 뉘앙스를 가지고 있다는 것이다. 컴퓨터 공학에서의 동기많은 포스팅에서 동기의 의미를 설명할 때 현재 작업의 요청과 응답이 동시에 발생하는 것으로 설명하고 있지만, 필자는 이 동시라는 단어가 가지는 의미와 다르게 요청과 응답 사이에는 일정한 시간이 존재할 수 밖에 없기 때문에 뭔가 모순이 느껴진다고 생각했다. 사실 동시라는 단어는 동시다발적에서의 용법과 같이 반드시 찰나의 순간만을 의미하는 것이 아니기 때문에 이렇게 설명할 수도 있긴 하지만, 일반적인 동시라는 단어의 용법을 생각해보면 직관적이지 않다고 생각한다. 그래서 필자는 동시에 발생하는 것은 현재 작업의 요청과 응답이 아니라 현재 작업의 응답과 다음 작업의 요청이라고 설명하는 게 더 맞지 않나 싶다.(애초에 Synchronous는 형용사라 주어가 없다) 동기 방식은 현재 작업의 응답과 다음 작업의 요청의 타이밍을 맞추는 방식이다 즉, 현재 작업의 응답이 발생함과 동시에 다음 작업을 요청한다는 것은 작업이 어떠한 순서를 가지고 진행된다는 것을 의미한다. 그리고 이 응답이라는 것도 사실 귀에 붙히면 귀걸이고 코에 붙히면 코걸이로, 네트워크 모델에서는 서버의 응답일 수도 있고 I/O 모델에서는 프로세스 제어권의 반납일 수도 있다. 그럼 먼저 우리가 가장 흔하게 접할 수 있는 동기 방식의 예를 한번 보자. 동기 방식 + 블록킹 방식우리가 가장 흔하게 접하는 동기 방식의 예는 바로 동기 & 블록킹 방식이다. 동기 방식이기 때문에 작업의 흐름도 순차적으로 진행되는 것이 보장되고, 블록킹 방식이기 때문에 어떠한 작업이 진행 중일 때는 다른 작업을 동시에 진행할 수가 없다. 12345678910111213function employee () { for (let i = 1; i < 101; i++) { console.log(`직원: 인형 눈알 붙히기 ${i}번 수행`); }}function boss () { console.log('사장: 출근'); employee(); console.log('사장: 퇴근');}boss(); 123456사장: 출근직원: 인형 눈알 붙히기 1번 수행직원: 인형 눈알 붙히기 2번 수행...직원: 인형 눈알 붙히기 100번 수행사장: 퇴근 이 코드를 보면 우리는 자연스럽게 이 작업들이 순서를 가지고 진행될 것이라는 것을 알 수 있다. 내부적으로는 하나의 콜 스택에 작업을 넣고 Last In First Out으로 진행되기 때문이라는 것을 알고 있지만, 여기서는 그런 내부 로직보다는 그냥 작업이 순서대로 진행된다는 것이라는 것에만 집중하자. 모든 인형의 눈알을 다 붙히기 전까지 퇴근은 없다 상위 프로세스인 boss 함수는 출근 작업을 수행한 뒤 하위 프로세스인 employee 함수에게 인형 눈알 붙히기 작업을 요청하고 있고, 이 인형 눈알 붙히기 작업이 완료되고나서야 boss 함수는 퇴근 작업을 수행한다. 쉽게 말해서 작업을 시킨 놈인 상위 프로세스는 작업을 하는 놈인 하위 프로세스가 종료될 때까지 절대 퇴근할 수 없다는 것이다. 이 예제와 같이 동기 방식과 블록킹 방식을 함께 사용하는 매커니즘은 일반적으로 사람들이 동기 방식이라고 하면 가장 먼저 떠올리는 방식이고 직관적으로 이해하기도 쉬운 편이다. 그렇다면 이 예제와 같이 동기적인 작업의 흐름을 유지하면서 employee 함수가 인형의 눈알을 붙히는 동안 boss 함수가 다른 일을 할 수도 있을까? 동기 방식 + 논블록킹 방식물론 할 수 있다. 뭐가 어찌됐건 동기라는 것은 작업들이 순차적인 흐름을 가지고 있다는 것을 의미하기 때문에 이 전제만 지켜진다면 나머지는 어떻게 지지고 볶든 간에 동기 방식이라는 것은 변하지 않기 때문이다. 그래서 동기 === 블록킹이라고 말할 수 없는 것이다. JavaScript의 제너레이터를 사용하면 작업의 순서를 지키면서도 상위 프로세스가 다른 작업을 하도록 만들 수 있다. 1234567891011121314151617181920212223function* employee () { for (let i = 1; i < 101; i++) { console.log(`직원: 인형 눈알 붙히기 ${i}번 수행`); yield; } return;}function boss () { console.log('사장: 출근'); const generator = employee(); let result = {}; while (!result.done) { result = generator.next(); console.log(`사장: 유튜브 시청...`); } console.log('사장: 퇴근');}boss(); 123456789사장: 출근직원: 인형 깔알 붙히기 1번 수행사장: 유튜브 시청...직원: 인형 눈알 붙히기 2번 수행사장: 유튜브 시청......직원: 인형 눈알 붙히기 100번 수행사장: 유튜브 시청...사장: 퇴근 이 예제를 보면 상위 프로세스인 boss 함수는 출근한 후 하위 프로세스인 employee를 호출하여 인형 눈알 붙히기 작업을 시키고 주기적으로 이 작업이 끝났는지를 검사하고 있다. 그리고 아직 작업이 끝나지 않았다면 자신 또한 열심히 유튜브 시청을 수행하는 것을 볼 수 있다. 이 코드는 분명히 동기적인 흐름을 가지고 진행하고 있지만 boss 함수 또한 중간중간 자신의 작업을 수행하고 있으므로 블록킹이 아니라 논블록킹 방식을 사용하고 있는 것이다. 니가 일하는 동안 난 짬짬히 유튜브를 보겠다 이 예제에서도 동기 & 블록킹 방식과 마찬가지로 boss 함수는 employee 함수의 작업이 끝나기 전까지는 절대 퇴근할 수 없다. 작업의 순서가 지켜지고 있는 것이다. 즉, 동기 방식이라는 것은 작업의 순차적인 흐름만 지켜진다면 블록\u001b킹이든 논블록킹이든 아무 상관이 없다고 할 수 있다. 컴퓨터 공학에서의 비동기필자는 동기 방식을 현재 작업의 응답과 다음 작업의 요청의 타이밍이 일치하는 것이라고 이야기 했다. 비동기 방식은 말 그대로 동기 방식이 아니라는 의미이기 때문에 반대로 생각하면 된다. 즉, 현재 작업의 응답과 다음 작업의 요청의 타이밍이 일치하지 않아도 되는 것이다. 작업을 지시하고나면 그 작업이 언제 끝나는 지는 신경쓰지않는다. 동기 방식은 상위 프로세스가 하위 프로세스에게 작업을 지시할 때 작업의 종료 시점을 알고 있어야한다. 하위 프로세스의 작업이 완료되어 결과물을 뱉어내든 혹은 작업이 아직 진행 중이든 작업의 종료 시점은 항상 작업을 시킨 놈인 상위 프로세스가 신경쓰고있다. 하지만 비동기 방식은 다르다. 상위 프로세스는 작업을 일단 지시했으면 그 다음부터는 작업이 진행 중이든 종료가 되었든 신경쓰지않는다. 동기 방식을 설명했을 때와 마찬가지로 이때 상위 프로세스가 자신의 작업을 할 수 있냐 없냐는 별개의 문제이다. 또한 상위 프로세스가 하위 프로세스의 작업 종료 여부를 신경쓰지 않기 때문에 작업의 종료가 순차적으로 이루어지는 것을 보장하지 않는다. 그럼 먼저, 우리에게 익숙한 방식인 비동기 & 논블로킹 방식부터 한번 살펴보자. 비동기 방식 + 논블로킹 방식비동기 방식과 논블로킹 방식을 조합한 방법은 우리에게 굉장히 익숙한 방식이다. 비동기 방식이기 때문에 상위 프로세스는 하위 프로세스의 작업 완료 여부를 따로 신경쓰지 않는다. 이후 하위 프로세스의 작업이 종료되면 스스로 상위 프로세스에게 보고를 하든 아니면 다른 프로세스에게 일을 맡기든 할 것이다. 그리고 논블로킹 방식이기 때문에 상위 프로세스는 하위 프로세스에게 일을 맡기고 자신의 작업을 계속 수행할 수도 있다. 12345678910111213141516171819function employee (maxDollCount = 1, callback) { let dollCount = 0; const interval = setInterval(() => { if (dollCount > maxDollCount) { callback(); clearInterval(interval); } dollCount++; console.log(`직원: 인형 눈알 붙히기 ${dollCount}번 수행`); }, 10);}function boss () { console.log('사장: 출근'); employee(100, () => console.log('직원: 눈알 결산 보고')); console.log('사장: 퇴근');}boss(); 1234567사장: 출근사장: 퇴근직원: 인형 눈알 붙히기 1번 수행직원: 인형 눈알 붙히기 2번 수행...직원: 인형 눈알 붙히기 100번 수행직원: 눈알 결산 보고 이 예제를 보면 boss 함수는 employee 함수에게 인형 눈알 100개를 붙히라고 지시한 후 자신은 바로 퇴근해버렸다. 상위 프로세스인 boss 함수는 employee 함수의 작업이 언제 끝나는지는 관심이 없으며 작업의 완료 신호는 콜백으로 넘겨진 눈알 결산 보고 작업이 대신 받아서 처리하고 있다. 사장님은 작업만 지시하고 바로 퇴근하신다 비동기 & 논블로킹 방식은 여러 개의 작업을 동시에 처리할 수 있는 부분에서 효율적이라고 할 수 있지만, 너무 복잡하게 얽힌 비동기 처리 때문에 개발자가 어플리케이션의 흐름을 읽기 어려워지는 등의 문제가 있을 수 있다. JavaScript에서 Promise나 async/await와 같은 문법을 사용하는 이유도 이런 비동기 처리의 흐름을 좀 더 명확하게 인지하고자 하는 노력인 것이다. 또한 NodeJS의 이벤트 루프와 같이 비동기 방식도 내부 구현을 뜯어보면 동기적인 패턴이 포함되어있기 때문에 남발하게되면 어딘가에 병목이 생길 수도 있다. 비동기 방식 + 블로킹 방식그럼 이제 마지막으로, 평소에 접하기 힘든 개념인 비동기 & 블로킹을 살펴보자. 이 방식은 일반적인 어플리케이션 레이어에서는 자주 사용되지 않고 Linux와 Unix 운영체제의 I/O 다중화 모델 정도의 저레벨에서 사용되고 있다. 그래서 지금까지 예제로 사용하던 사장님과 직원은 이제 그만 퇴근시켜주고 설명을 진행할 것이다. 일단 이 개념은 얼핏 들으면 비효율적이기만 할 수도 있다. 비동기 방식의 장점은 하위 프로세스의 작업이 끝나는 것을 기다리지 않음으로써 여러 개의 작업을 동시에 처리할 수 있다는 것인데, 프로세스가 블록킹되어버려서 유휴 상태에 빠진다면 아무 것도 처리할 수 없기 때문이다. 하지만 이 개념이 나오게 된 이유는 다음과 같다. 동기 & 블록킹 I/O의 경우 직관적이나, 여러 개의 I/O를 동시에 처리할 수 없다. 논블록킹 I/O는 프로세스들의 작업을 컨트롤하는 것이 까다롭다.(대부분 이런 저레벨 프로그램은 C로 짠다. JS나 Python 같은 걸 생각하면 안된다.) 그렇다고 동기 & 블록킹 I/O와 멀티 프로세싱이나 쓰레딩을 결합해서 쓰자니 자원 문제도 있고 프로세스/쓰레드 간 통신이나 동기화가 빡셈 그래서 나온 개념이 바로 그럼 그냥 프로세스를 블록킹해놓고 비동기로 여러 개의 I/O를 다중화해서 받아버리는 놈을 만들면 어때?인 것이다. 즉, 직관적인 코드의 흐름을 유지하면서도 작업을 동시에 처리하겠다는 것이다. 참고로 이 내용은 IBM에서 2006년에 작성한 Boost application performance using asynchronous I/O이라는 포스팅에도 소개되어 있다. 비동기 + 블록 방식의 워크 플로우 위 그림을 보면 중간에 select()라는 함수가 있는데, 이 친구가 바로 프로세스를 블록킹함과 동시에 여러 개의 I/O를 받아서 처리하는 역할을 한다. 이 함수는 C언어의 API로 제공되고 있으며, 그냥 include 와 같이 헤더를 가져와서 쓰면 된다. 12int select (int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); 이때 nfds는 감시할 파일의 개수를, fd_set 구조체들은 각각 읽을 데이터, 쓰여진 데이터, 예외처리가 발생한 것을 감시할 파일 목록이다. 이때 fd...는 File Descriptor의 약자이며, 파일을 감시하고 있다가 해당 I/O가 발생하면 자신이 가지고 있는 비트 배열 구조체의 해당 값을 1로 변경한다. 12[0, 0, 1, 0, 0, 0, 0, 0]// 파일 목록 중 3번째 파일에 변경사항이 있다면 FD의 비트 배열 구조체가 변경된다 그리고 timeval 구조체인 timeout 인자는 감시할 시간을 의미한다. 즉, 이 timeout 인자에 넘겨준 시간 동안 상위 프로세스를 블록킹하면서 자신이 넘겨받은 파일 목록을 계속 감시하고 있는 것이다. 그리고 감시하고 있는 동안 파일에 읽기, 쓰기, 예외가 발생하면 select 함수가 종료될 때 자신이 감시하던 있는 파일들 중 해당 변경사항이 발생한, 즉 처리해야할 파일의 개수를 반환한다. 대충 이 정도가 비동기 & 블록 방식의 대표적인 예인 select 함수가 작동하는 방식이다. 정리하자면 일정 시간동안 프로세스를 멈춰놓고 자신이 감시하고 있는 파일들에서 I/O가 발생하는지를 감시하는 것이다. 그리고 일정 시간이 지나면 함수가 종료되며 그동안 감시했던 파일들의 I/O 결과를 반환하고 프로세스의 블록킹이 풀린다. 이 방식은 블록킹 방식으로 진행되기 때문에 개발자에게도 직관적으로 다가오고, 비동기 방식이기 때문에 여러 개의 I/O를 동시에 감시하며 처리할 수 있다. 하지만 성능이 그렇게 좋은 편은 아니므로 IBM에서는 높은 성능이 필요한 어플리케이션에서는 되도록 쓰지 말라고 한다. 마치며필자는 대학교 때 네트워크 과목을 수강하며 동기에 대한 개념을 처음 배웠었는데, 사실 그때는 굉장히 단편적인 내용만을 알고 있었다. 단순히 두 디바이스간의 클럭과 일정 크기의 프레임을 맞추어 통신하는 것이 동기식 통신이라는 것 정도? 하지만 개발자로 일을 하면서 공부를 더 하다보니까 동기식 I/O, 동기식 프로그래밍 등 동기에 대한 다른 개념들을 계속 해서 접하게 되었는데, 서문에서 이야기했듯이 누구는 동기라는 단어가 동시에 발생하는 것이라고 하고 누구는 특정한 클럭을 맞춰서 통신하는 것, 또 누구는 이전 작업이 끝날 때까지 기다리는 것이라고 하는 등 해석이 다 달라서 더 혼란스러웠다. 아니 왜 말하는 게 다 달라…? 그래서 이번 포스팅을 작성하면서 Synchronous라는 단어가 정확히 어떤 상태를 의미하는 것인지부터 다시 공부했었는데, 확실히 단어의 뉘앙스를 이해하고나니 왜 이렇게 다른 해석들이 나오게 되었는지 약간은 이해가 가는 것 같다. 단어 자체가 뜻을 의미하는 단어다보니 이건 그냥 해석하기 나름인것 같기도 하다. 어쨌든 다른 건 다 차치하고서라도 Synchronous라는 단어가 동시에 똑같이 진행되는 느낌이라는 뉘앙스를 알게 된 것이 이번 포스팅의 최대 수확이 아닐까라는 생각을 해본다. 이상으로 동기(Synchronous)는 정확히 무엇을 의미하는걸까? 포스팅을 마친다.","link":"/2019/09/19/sync-async-blocking-non-blocking/"},{"title":"내가 겪었던 번아웃, 그리고 극복했던 경험","text":"필자는 작년인 2018년, 번아웃(Burn-out)이라 불리는 탈진 증상을 한 차례 격하게 겪은 적이 있다. 번아웃은 2019년 5월 WHO(세계보건기구)에서도 ICD-11에 정식으로 등록할 만큼 관심을 가지고 있는 증상 중 하나이다. WHO는 번아웃이 의학적인 질병에는 포함되지 않지만 직업 관련 증상 중 하나라고 이야기하고 있다. 흔히 번아웃은 나 자신이 다 타버리고 더 태울 것이 없어 재만 남은 상태에 비유되고는 한다. 필자 또한 처음 번아웃을 경험했을때 이런 비슷한 느낌을 받았었다. 사실 이런 경험은 필자 뿐만 아니라 많은 사람들이 이미 겪고 있는 경험이라 일상 속에서도 이제 번아웃이라는 단어를 자주 들을 수 있는 시대인 것 같다. 무엇보다 필자가 번아웃을 경험했을때 힘들었던 것은, 이 증상이 어느 날 갑자기 예고없이 찾아왔기 때문이다. 어제까지만 해도 개발하는 것을 즐기던 내가 하루 아침에 변해버리는 경험은 꽤나 당혹스러웠다. 물론 진짜로 하루 아침에 번아웃이 생길리는 없으니, 필자도 모르게 조금씩 쌓여왔던 요소들이 어떤 작은 사건 하나로 터져버린 것일테다. 평소에 이 정도는 직장인으로써 당연한거지, 회사에 개발 리소스가 부족하니 어쩔 수 없어, 주말에라도 공부를 안하면 도태될거야 라고 생각하면서 매일 야근하고 자신을 채찍질했던 것들이 조금씩 쌓여가면서 더 이상 견딜 수 없게 되었을 때 한번에 큰 폭탄이 되어 돌아왔던 것이다. 그래서 이번 포스팅에서는 필자가 번아웃을 경험하며 느꼈던 점과 어떻게 대처했었는지에 대한 회고를 한번 해보려고 한다. 갑자기 찾아온 번아웃필자는 번아웃이 어느 날 갑자기 찾아왔다고 이야기 했다. 물론 조금씩 쌓여왔던 스트레스와 부담감들이 쌓여온 결과였겠지만, 필자는 그런 스트레스들을 당연한 것으로 생각하고 받아들이고 있었기 때문에 당황스러운 것은 마찬가지였다. 처음 필자가 처음 개발에 재미를 붙히게 된 이유는, 그냥 뭔가를 만든다는 것이 너무 재미있었고 내 아이디어를 코딩을 통해 실현할 수 있다는 것이 너무 신기했기 때문이었다. 그래서 친구들과 루비콘이라는 팀도 만들어서 이것저것 만들어보고는 했었는데, 이 당시 팀원들과 평일, 주말 가리지 않고 매일 모여서 함께 아이디어를 논의하고 새로운 것을 만들어보는 재미에 흠뻑 빠져있었다. 그 후 대학을 졸업하고 개발자로 회사에서 일을 할 때도 몇년 정도 이런 생활 패턴을 유지했었는데, 회사 생활을 오래 하면서 조금씩 생활 패턴이 흐트러지기 시작했다. 밤 11시까지 야근을 하는 경우가 부지기수였지만 그래도 그냥 자기에는 오늘 하루가 아깝다는 생각에 매일 새벽 2~3시까지 코딩을 하고 주말에는 평일에 못한 만큼 더 많이 공부를 해둬야 한다는 생각에 아침부터 저녁까지 코딩만 했다. 그러던 어느 날, 사무실에서 평소처럼 코딩을 하고 있었는데 갑자기 문득 이런 생각이 들었다. 재미없네… 결국 코딩이라는 것에 권태기가 온 셈이다. 처음에는 맨날 Vue만 하니까 권태기가 온 것이라고 생각하고 React Native를 사용하고 있는 모바일 챕터로 넘어가서 일을 하기도 했었지만, 결국 새로운 프레임워크와 개발 환경이 주는 즐거움은 처음 일주일 정도만 지속됐고 그 이후 해당 기술들이 익숙해지면 권태감이 느껴지는 것은 똑같았다. 분명 내가 좋아서 시작한 일이었는데 갑자기 재미없어지는 경험은 필자도 처음 겪는 일이었기 때문에, 처음에는 어떻게 대처해야할 지 감도 못 잡았었다. 분명 필자는 내일도 일을 똑같이 해야하고, 필자의 생산성이 떨어지면 다른 팀원들에게 피해가 갈 수도 있는 상황이기에 난감하기도 했다. 게다가 필자가 이런 생각을 하고 있다는 것을 다른 팀원들이 알게 되면 괜히 팀원들도 분위기에 흔들릴까봐 팀에는 말도 하지 못하고 혼자만 꽁꽁 싸매고 있었다. 가끔 주변 지인들에게 이런 상황에 대해서 이야기하기도 했었는데, 대부분 돌아오는 대답은 직장인이 다 그런거지 뭐 또는 좀 쉬어라 였던 걸로 기억한다. 필자는 회사에 다니고 있었기 때문에 마냥 쉴 수는 없었다. 그리고 주말에 아무리 푹 쉰다고 해도 평일이 되면 다시 권태감이 찾아왔다. 그래서 필자는 그냥 직장인이 다 그런거라고 생각하면서 어느 정도 포기하고 지내고 있었다. 그러나 필자가 잘못 생각했던 것은 여기서 번아웃 증상이 더 심화될 것이라고 생각하지 못한 것이었다. 제대로 대처를 하지 않으니 권태감은 점점 심해졌고, 결국 필자는 개발팀 리더에게 회사를 그만두고 싶다라고 말하기에 이르렀다. 이때 필자의 감정은 매일 같은 업무에 지친 권태감과 이런 감정 상태로 일을 하면서 팀원들에게 피해를 주고 있다는 죄책감, 코딩이 싫어진 나 자신에 대한 실망감 등 꽤나 복합적인 감정이었다. 필자는 원래 성격도 급한 편인데다가 늘 자기 자신을 채찍질하며 쫓기듯이 살아왔던 사람이었기 때문에 이 번아웃에 대한 결론도 생각보다 빨리 안 낫네? 그럼 팀원들한테 더 피해주느니 그냥 퇴사하는게 좋겠다라고 성급한 결론을 내려버린 것이다. 하지만 당시 팀 리더는 필자에게 번아웃은 개인의 문제가 아니라 팀의 문제라고 말해주었고, 힘든 게 있으면 팀원들한테 언제든지 말하라는 말도 해주며 필자에게 용기를 북돋아 주었다. 사실 이 한마디가 필자에게는 굉장히 컸던 것 같다. 당시 팀 리더가 필자에게 해줬던 이 한마디가 필자에게 약간이지만 여유를 가져다 줬었다. 이때부터 개발보다는 나 자신에 대한 생각을 하게 되면서 지금까지 잊고 지냈던 것들에 대해 다시 깨닿게 되는 계기가 되었다. 개발자가 되기 전에는 없었던 경험번아웃을 극복하고자 본격적으로 나 자신에 대한 생각을 하게된 다음 가장 의아했던 것은 개발자가 되기 전에는 이런 경험을 해본 적이 한번도 없다는 것이다. 개발자가 되기 전에는 학생이었으니까 당연하다고 생각할 수도 있겠지만, 필자는 2004년부터 2011년까지 프로팀에서 활동하는 비보이였기 때문에 어떻게 보면 학생임과 동시에 직업도 가지고 있었다고 할 수 있다. 거의 10년 전 추억이 되버린 배틀 그리고 알고 있을지는 모르겠지만 대한민국 비보이들의 하루 연습량은 직장인들이 사무실에서 근무하는 시간과는 비교할 데가 못된다. 필자의 경우 기본적으로 평일에는 학교를 마치면 바로 연습실로 가서 밤 10시까지 매일 6시간 정도 연습을 했고, 주말이나 방학에는 오전 9시부터 밤 10시까지 매일 13시간 정도 연습을 했었다. 물론 공연이나 배틀이 잡히면 연습량을 늘려야 하기 때문에 밤새 새벽 연습을 하고 학교를 가는 경우도 있었다. 솔직히 육체적으로나 정신적으로나 힘든 건 그때가 더 힘들었는데도 불구하고 이상하게도 필자는 비보잉을 할 때 번아웃을 겪어본 경험이 없었지만 개발을 할 때는 번아웃을 겪고 있는 것이다. 직장 상사보다 더 직설적인 말로 갈구는 형들도 있었고, 심지어 연습을 제대로 안한다고 맞기도 했으며, 에어트랙이라는 기술을 연습할 때는 두 바퀴를 못해서 1년 동안 매일 5시간 넘게 이 기술만 연습했던 적도 있었다. 애증의 에어트랙, 결국 두 바퀴를 못 만들고 군대를 갔다 그래서 필자가 번아웃을 경험하면서 고민했던 것은 왜 그때는 안그랬고, 지금은 번아웃이 온 것인가?였다. 필자는 비보이로 활동할 당시에 매일 강도 높은 연습을 했고, 돈도 많이 못 벌었기 때문에 육체적으로나 정신적으로나 더 힘든 시기라고 생각했는데 그럼에도 불구하고 번아웃을 경험하지 않았다면 분명히 뭔가 이유가 있어서라고 생각했기 때문이다. 내가 스스로 만들어낸 채찍필자가 생각했을 때, 비보이였던 당시와 개발자인 현재 필자의 마음에 차이가 있다면 내가 좋아서 하는 것 자체로 만족하지 못하고 있다는 것이였다. 분명 비보잉을 할 때는 춤 자체를 즐기고 있었다. 배틀에서 우승하지 못해도 상관없었으며, 누군가에게 인정받지 않아도 상관없었다. 이 마인드는 처음 춤을 시작했을 때도, 7년 동안 춤을 추고 난 다음에도 바뀌지 않았었다. 그리고 비록 군대갔다와서 생각이 바뀌긴 했지만 한창 프로 팀으로 활동할 당시에는 돈도 딱히 중요하지 않았던 것 같다. 하지만 개발자인 필자는 처음 개발을 시작할 때와는 다르게 여러가지를 신경쓰고 있었다. 남들에게 인정받는 실력을 가지고 싶다던가, 연봉이라던가, 좋은 회사에 들어가고 싶다던가 하는 것들 말이다. 이런 욕구들이 건강하게 작용하면 발전의 원동력이 되겠지만 필자는 조금 과했던 것 같다. 오랜 기간 생각 끝에 필자가 내린 결론은 내가 나를 너무 채찍질하고 있구나였다. 외부에서는 필자에게 어떤 압력도 주지 않았다. 남들에게 인정받고 싶다는 것, 돈을 많이 버는 것, 좋은 회사에 들어가고 싶다는 것들은 모두 필자가 만들어낸 채찍이었던 것이다. 회사 업무는 회사 업무일 뿐필자가 개발 뿐만 아니라 다른 것을 공부할 때도 가장 좋아했던 것은 바로 모르는 것을 이해하게 되었을 때 느끼는 카타르시스였다. 이런 카타르시스를 느끼면서 뭔가 내가 더 나은 사람이 되어가고 있다는 성취감이랄까, 내가 더 쓸만한 사람이 되었다는 안도감이랄까, 그런 것들을 느끼기 위해 계속 새로운 지식을 찾아서 헤매였던 것 같다. 물론 신입 때는 회사에서 하는 모든 것들이 새로웠기 때문에 일만 해도 이런 카타르시스를 느낄 수 있었지만 점점 연차가 쌓일 수록 이런 느낌은 점점 무뎌졌고, 결국은 퇴근 후 따로 공부를 하면서 이 욕구를 충족시켜 나갔다. 그러나 퇴근 후에 매일 새벽 2~3시까지 공부를 하는 것이 점점 힘에 부치기 시작했다. 29살인 지금, 슬슬 이 짤이 공감되기 시작했다 오랜 기간 쌓여온 피로 누적인지, 운동 부족인지, 아니면 그냥 나이를 먹어서인지는 모르겠지만 어쨌든 언제부턴가 집에 가면 그냥 기절하는 생활이 반복되었다. 그래서 필자는 회사 외적인 시간을 내는 것이 힘드니 최대한 회사에서 일을 하면서 새로운 것들을 배워나가야한다는 일종의 강박 관념을 가지게 되었다. 그래서 회사에서 일을 하는 것과 내 실력이 느는 것을 같은 선상에 놓고 생각했던 것이다. 회사에서 일을 하면 무조건 내 실력도 함께 늘어야 한다는 그 생각 때문에, 연차가 쌓여가며 더 이상 회사에서 하는 일상적인 코딩으로는 실력이 늘지 않는다는 사실을 느끼게 되자 권태감이 찾아왔다고 생각한다. 하지만 이 생각이 잘못 되었다는 것을 깨닿는 데는 그리 오랜 시간이 걸리지 않았다. 애초에 회사는 내 전문 지식을 사용하기위해 나와 계약한 클라이언트 아닌가? 회사에서 뭔가 하나를 하더라도 반드시 배워가야할 이유는 없는 것이다. 그래서 필자는 이때부터 마음 속으로 한가지 기준을 세웠다. 회사는 내가 갈고 닦은 지식을 써먹는 곳이지 새로운 지식을 탐구하는 학교가 아니다. 돈 받는 만큼 제대로 하자. 이 변화로 인해 필자는 돈 받는 만큼만 하자라는 마인드를 가지게 되었는데, 어찌 보면 계산적이고 차가운 마인드일수도 있지만 어찌 보면 자신이 받는 연봉에 대한 책임감을 가지게 되는 마인드이기도 하다. 당연히 야근도 할 수 있고 능동적으로 새로운 아이디어를 제시하기도 하지만 그건 어디까지나 필자가 돈을 받음으로써 회사에 제공해야 하는 전문성의 일종이고, 자발적인 의사인 것이다. 난 직장인이니까라는 마인드때문에 하기 싫은데 하는 것이 아니란 얘기다. 이렇게 생각을 바꾸고 나니 회사 업무를 할 때 이 업무는 나한테 별 도움이 안되는데라는 마음이 내가 가진 기술로 최선의 결과를 만들어내보자는 마음으로 바뀌게 되었다. 이러한 생각의 변화가 필자의 부담도 덜어냄과 동시에 한층 더 개발자로써 프로페셔널한 마인드를 가지게 되었다고 생각한다. 실력에 대한 강박관념을 버리다그리고 필자가 또 한가지 다짐했던 것은, 매일 코딩하지 않기 였다. 남들은 1일 1커밋하자고 하는 이 판국에 무슨 소리인가 싶겠지만, 필자는 1일 1커밋이 아니라 1일 10커밋을 4년 넘게 해오다가 이 사단이 난 것이므로 어느 정도 코딩과 거리를 두는 것이 더 좋다고 생각했다. 이렇게 매일 코딩을 하는 습관은 대학생때부터 시작된 것인데, 처음에는 코딩이 너무 재밌고 좋아서 자연스럽게 매일 한 것이지만 어느 순간부터 반 정도는 습관처럼, 반 정도는 공부에 대한 강박관념으로 이어왔던 것 같다. 주변의 개발자들을 보면 워낙 뛰어나신 분들이 많기에, 그 모습에 더 압박을 느꼈던 것 같기도 하다. 그래서 새로운 기술이나 패러다임에 대한 공부를 게을리 하게되면 결국 이 시장에서 도태되고 말 것이라고 생각했기 때문에 계속 기술 포스팅도 읽어보고 새로운 기술로 프로젝트를 만들고 했었다. 사실 아무리 필자가 매일 공부를 하더라도 기술마다 러닝커브가 존재하기 때문에 모든 기술을 따라가는 것은 당연히 불가능하다. A라는 기술이 새로 나와서 그 기술을 공부하다가 어느 정도 이해가 될 때 쯤에는 새로운 패러다임을 가진 B라는 기술이 나오는 것이 IT업계의 현실이니 말이다. 결국 필자는 불가능한 목표에 압박을 느끼고 목을 메고 있던 것이다. 그러나 곰곰히 생각해보니 필자는 다른 장점이 많은 사람이었다. 예전부터 워낙 이것저것 해왔다 보니까 WebGL이나 천체물리학, 사운드 엔지니어링 등 일반적인 웹 개발자들은 잘 모르고 관심도 없는 분야에 대한 어느 정도 지식이 있었기 때문이다.(일종의 잡캐라고 할 수 있다) 열심히 홍보한 결과, 간혹 스타를 찍어주시는 고마운 분들도 있다 물론 일반적인 어플리케이션을 개발할 때는 딱히 쓸모없는 지식들이기는 하지만 간혹 이런 기술이 필요한 분야에서 사람을 구할 때 한번씩 기웃거려 볼 수라도 있기는 하다.(그리고 워낙 비주류 전문 지식이라 돈도 많이 준다) 필자는 이런 부분들이 다른 개발자들과 차별화되는 필자만의 무기가 아닐까라는 생각을 했었다. 결국은 처음 개발을 시작할 때 가졌던 마음이 정답이었던 것이다. 실력 향상 같은 것은 신경쓰지말고 그냥 내가 공부하고 싶은 것을 공부하고 만들고 싶은 것을 만들자는 마음 말이다. 지금도 실력이라는 것은 그냥 내가 만들고 싶은 것을 만들다보면 자연스럽게 따라오는 것이라고 생각한다. 애초에 누가 실력이 좋고 나쁘고를 따지는 기준 자체도 굉장히 주관적이고 애매하기 때문에 그냥 그런 거 신경쓰지 않고 하고 싶은대로 하면서 살기로 했다. 연봉? 그렇게 중요하지 않다필자는 처음 개발자로 취업을 할 때 30살이 되면 연봉을 n원 정도는 받아야지하는 식의 일종의 기준이 있었다. 사실 30살이라는 나이는 그렇게 많은 나이는 아니지만 10대 이후 처음으로 나이의 앞 자리가 바뀌는 만큼 뭔가 이루고 싶었던 것 같다.(지금 생각해보면 그게 뭔 의미가 있나 싶긴하다) 그리고 필자는 내가 사회로부터 인정받은 결과가 숫자로 나타나는 것이 연봉이라고 생각했기 때문에 더 연봉에 집착했던 것도 있다. 하지만 문제는 필자가 노련한 협상가가 아니라는 것이다. 연봉이라는 것은 기본적으로 협상을 통해 이루어지는 경우가 많은데, 이 협상이라는 것이 단순히 말을 잘해서 되는 것이 아니라 조직 내에서 지속적인 자기 어필과 홍보, 팀원들의 평가, 개발 실력 등 많은 요소들이 맞물렸을 때 비로소 연봉 협상 테이블에서 유리한 위치를 점할 수 있게 된다. 게다가 연봉이라는 숫자가 완성되기까지는 변수가 너무 많다. 첫 직장이 어떤 곳이였는지, 그 곳에서 나의 실력을 어느 정도로 평가해주었는지, 게다가 어느 정도는 운빨도 따라줘야한다. 나를 제외한 회사 내의 모든 프론트엔드 개발자가 그만 둬서 내가 이 회사의 유일한 프론트엔드 개발자가 되는 상황처럼 연봉 협상에 유리한 상황 같은 것 말이다. 하지만 사회 생활 경험이 많지도 않은 필자가 이런 것들을 알 리가 없었고, 당연히 필자가 세운 무리한 목표와는 점점 멀어질 수 밖에 없었다. 지금 생각해보면 당연한 것인데 당시에는 목표에서 멀어진다는 불안감에 조바심을 냈었다. 사실 필자의 연봉은 그런 조바심을 가질 만큼 적지도 않고, 많지도 않다. 뭐 돈에 대한 기준은 사람마다 다르겠지만 그냥 29살 청년이 혼자 먹고 살 수 있을 만큼의 돈을 벌고 있다고 생각한다. 하지만 사람 마음이라는 것이 참 간사한 것이, 내가 먹고 살기에는 충분한 연봉을 받고 있다는 사실을 알고 있더라도 나보다 더 많은 연봉을 받는 사람을 보면 부러움이 샘솟기 마련이다. 돈이 가진 마력이란 참 무섭다. 그런 이유로 필자는 연봉이 진짜 그렇게 중요한 것인가에 대해서도 다시 생각해보게 되었는데, 필자가 이때 내린 결론은 얼마를 받든 어차피 부질없다는 것이었다. 필자가 친구들에게 이런 얘기를 하면 무슨 스님이냐고 하는 친구들도 간혹 있는데, 이건 필자가 무슨 깨달음을 얻어서 내린 결론이 아니고 단지 계산기를 조금 두드려 보았을 뿐이다. 필자의 첫 취업 이후 수입과 매달 지출, 그리고 지금까지 필자의 평균 연봉 인상률까지 적용해서 10년 뒤를 생각해보면 음, 딱히 답이 없다. 40대 쯤 되었을 때 3억 모으면 잘 모은 느낌이랄까. 어떤 분은 3억이 어디냐고 하시겠지만 필자는 2억 모으나 3억 모으나 어차피 인생의 큰 틀은 바뀌지 않는다고 생각했다. 솔직히 필자 같은 일반인은 로또라도 당첨되거나 주식이나 코인이 대박나지 않는 이상, 그냥 능력 껏 먹고살 만큼만 벌고 나머지는 결국 대출로 영혼까지 끌어모아서 집 사고 차 사고 하면서 남들처럼 그렇게 살 수 밖에 없다는 것이다. 조금은 냉정한 생각일 수도 있지만 그냥 필자가 계산기 두들겨 봤을 때 나오는 월급쟁이의 현실은 이 정도인 것 같다. 필자 같은 일반인은 어차피 저걸 잡아탈 용기도 운도 없다 여기까지 생각이 닿고 다니까 고작 연봉 몇 백만원 인상에 일희일비 하는 것이 부질없다고 생각이 들게 되었다. 그래서 지금 필자는 연봉이 오르던 말던 딱히 신경쓰지 않는 편이다. 물론 당연히 돈은 많을 수록 좋으니까 오르면 좋기는 하지만 딱히 안 올라도 조바심 내지 않고 언젠가 오르겠지 뭐 하는 느낌이다. 게다가 연봉 올라서 기쁜 기분이래봤자 어차피 몇 달 지나면 금방 또 그 액수에 익숙해져서 무뎌지기 마련이다. 근데 참 웃긴 것이 이렇게 마음 먹는다고 돈이 안 벌리는 것도 아니더라. 연차가 쌓이며 연봉도 오르기는 올랐고, 책도 하나 출판했고, 블로그에서도 수익이 조금씩 나고 있으니 오히려 예전보다 점점 더 잘 벌고 있다고 할 수 있다. 그래서 딱히 돈에 집착하든 하지않든 그냥 자기 할 일만 잘하고 있으면 돈은 자연스럽게 따라오는 것이 아닌가하는 생각이 든다. 이처럼 기존에 필자 스스로 채찍질하고 있던 것들에 대해 다시 한번 생각해보게 되면서 개발에 대한 중압감을 많이 덜게 되었던 것 같다. 코딩 외에 다른 취미를 가져보자그리고 필자가 또 한가지 중요하게 생각한 것은 내가 어떤 사람인지를 다시 한번 생각해보는 것이였다. 개발자로 열심히 달려온 지난 4년 동안 필자는 취미나 문화 생활 같은 것은 거의 하지 않을 정도로 코딩에 빠져있었다. 즉, 일상이나 취미 등 모든 것이 코딩과 귀결되는 생활을 하고 있었다는 것이다. 필자가 번아웃이 온 이유에는 물론 중압감과 스트레스도 한 몫 단단히 했겠지만, 내가 원래 어떤 사람인지를 놓쳐서라고도 생각했다. 이건 사실 필자가 딱히 개발자라서가 아니라 그냥 하루하루 업무에 치여사는 직장인이라 그런 것 같기도 하다. 어쨌든 번아웃을 경험하면서 필자는 위에서 언급한 매일 코딩하지 않기를 실천하면서 남는 시간에 코딩 말고 내가 좋아하는 것을 해보자라는 생각을 가지게 되었다. 사실 이렇게 블로그에 글을 쓰는 것도 어떻게 보면 취미의 일환이라고 볼 수 있겠지만 대표적인 건 아무래도 음악 정도인 것 같다. 사실 필자는 비보잉 외에도 어릴 때부터 꾸준히 음악을 공부했었다. 슬프게도 천재들의 재능을 보고 쫄아서 포기하긴 했지만 나름 작곡 입시도 준비했었고, 연예 기획사에서 사운드 엔지니어로 일하기도 했을 정도로 음악을 좋아하는 편이다. 나름 행복했던 프로페셔널 베짱이 시절 그러나 개발자로 일을 하기 시작하면서 평일이고 주말이고 밤낮없이 매일 코딩만 하다보니, 자연스럽게 음악과도 멀어지게 되었다. 음악을 듣는 것은 매일 일상 속에서 하고 있었지만, 예전처럼 곡을 분석하거나, 악기를 연습하거나, 화성학을 공부하거나 하는 일은 우선 순위에서 밀려서 점점 하지 않게 되었다. 그래서 필자가 번아웃을 극복하고자 마음먹고 집에 와서 가장 먼저 한 것이 바로 집에 있는 피아노를 청소하는 것이였다. 그 이후로도 필자는 꾸준히 피아노나 기타도 치고 보컬 레슨도 받고 화성학도 공부하면서 취미로 음악을 하고 있다.(그 결과 회사에서 별명이 베짱이가 되었다) 음악은 나이 먹어도 즐길 수 있는 평생 취미이기 때문에 여러분도 살면서 악기 하나 정도는 배워보는 것을 추천한다. 마치며필자가 번아웃을 경험하기 전에 위에서 언급했던 것들을 실천하지 못했던 이유는 도태되고 싶지 않다는 불안감, 그리고 실력으로 인정받는 개발자가 되고 싶다는 압박감 때문이었다. 매일 코딩하고 공부하기도 하루는 짧으니까 취미 생활이나 여유는 사치라고 생각했었다. 하지만 결국 이런 채찍들은 결국 필자에게로 다시 돌아와 번아웃이라는 결과를 안겨주었다. 필자는 스스로를 채찍질함으로써 짧은 시간안에 빠른 성장을 이룰 수 있었지만 지금 생각해보면 그게 결국 무슨 의미가 있었나 싶다. 지금의 필자는 트렌드에 별로 관심도 없고 그냥 필요하거나 궁금하면 그때 공부하자는 마인드이기 때문에 새로운 기술을 습득하는 속도가 남들보다 확실히 느려졌기 때문이다. 요즘 겁나 핫한 쿠버네티스(Kubernetes)도 사람들 쓰기 시작한 지 한참 뒤에 친구가 말해줘서 알았다. 하지만 적어도 예전보다는 나름 행복하게 개발을 하고 있다고 말할 수는 있을 것 같다. 애초에 필자가 개발을 시작한 이유는 단지 재미있어서였기 때문에, 지금의 필자가 개발자로 살아가면서 가장 중요하게 여기는 가치도 재미이기 때문이다. 필자가 중요하게 생각하는 개발의 가치는 뛰어난 멘토가 만들어주는 것도 아니고 좋은 회사가 만들어주는 것도 아니기 때문에 더 이상 주위 환경에 신경쓰지 않는다. 그리고 사실 지금 필자는 한국과 외주 계약을 체결하고 체코 프라하에서 지내면서 나름 디지털 노마드 체험을 하고 있는 중인데, 한 달이라는 공백기와 백수에게는 꽤 부담되는 금전적 비용을 감수하고 이런 큰 결심을 내리게 된 것도 이때의 경험이 도움이 되었던 것 같다. 카페랑 코워킹 스페이스가 함께 운영되는 꿀 카페 프라하에서 디지털 노마딩 하실 분들께 추천한다. 솔직히 프라하로 오기 전에 이런 것들에 대한 걱정을 안했다면 거짓말이겠지만, 어차피 백수된 마당에 눈치볼 것도 없으니 그냥 질러버렸다. 그리고 3주 정도 여기서 살아보니 음, 걱정했던 것과 다르게 딱히 문제는 없는 것 같다. 사실 필자가 회사를 그만 두고 나서 몇몇 분들이 면접 제의를 해주셨는데, 퇴사하고 2주 뒤에 바로 프라하로 출발해야해서 아쉽게 다음을 기약했었다. 그래서 필자는 솔직히 한달 동안 유럽에 있는 동안 이런 제안들이 흐지부지 해질 것이라고 생각하고 별로 기대를 안하고 있었다. 하지만 감사하게도 그 중 몇몇 분들과 꾸준히 연락을 주고 받고 있고, 한국에서 외주를 받아왔기 때문에 적당히 수입도 유지되고 있다. 그리고 결정적으로 생각보다 체코에서 체감하는 물가가 그리 비싸지도 않아서 지출이 별로 없다. 뭐 여행다니는 것도 아니고 기껏 해야 동네 산책이나 하는 정도라서 그런 걸 수도. 과거의 필자였다면 한달 동안 유럽행이라는 결정을 내릴 수 있었을까? 아마 그렇게 하지 못했을 것 같다. 한 달이라는 공백기동안 뒤쳐진다고 생각했을테니까. 번아웃을 겪으며 힘들기도 했지만 그 과정 속에서 불필요한 것들을 내려놓으면서 오히려 예전보다 더 건강하게 개발자로써의 삶을 살고 있는 것 같다. 코딩은 예전처럼 다시 즐거운 일이 되었으며, 나의 장점을 다시 생각해보면서 예전보다 자신감도 생겼고, 지금처럼 외국에서 한달 동안 살아보는 재미난 경험도 할 수 있으니 말이다. 이상으로 내가 겪었던 번아웃, 그리고 극복했던 경험 포스팅을 마친다.","link":"/2019/09/23/how-to-overcome-burnout/"},{"title":"블로그 개설을 망설이고 있는 사람들에게","text":"과거에는 블로그를 운영하는 개발자들이 오히려 손에 꼽을 정도였지만, 최근 많은 개발자들이 블로그를 운영하며 다양한 주제에 대한 자신의 생각이나 특정 기술에 대한 분석을 포스팅으로 기재하고 공유하고 있다. 하지만 필자는 개인적으로 블로그를 운영하는 것이 생각보다 진입 장벽이 높다고 생각하는데, 그건 블로그 세팅과 같은 기술적인 이유 때문이 아니다. 블로그 세팅은 velog나 티스토리같은 블로그 전문 서비스를 사용할 수도 있고 깃허브 호스팅과 jekyll이나 hexo같은 정적 사이트 생성기를 사용하여 세팅할 수도 있는데, 사실 뭘 사용하든간에 그런 건 개발자들에게 크게 어려운 문제가 아니다. 회사나 학교에서 맨날 복잡한 문제로 삽질하면서 밥 벌어 먹고 사는 사람들한테 정적 사이트 생성기를 세팅할 때 발생하는 문제를 해결하는 것 정도는 꽤 쉬운 편이라고 생각한다. 진짜 문제는 블로그를 세팅한 다음부터 발생한다. 이런저런 삽질 끝에 블로그 세팅을 끝내고 나면 이제 글만 쓰면 되는데, 음… 막상 하얗게 비어있는 에디터를 보고 있으면 머리 속도 함께 햐얘지는 기분이 들기 시작하기 때문이다. 뭘 어떻게 쓰기 시작해야 하지…? 글쓰기도 프로그래밍과 마찬가지로 꾸준한 연습과 연구가 필요한 영역이다. 필자 같은 경우는 평균적으로 8000~10000 단어 정도의 포스팅을 작성하는데, 이렇게 하나의 주제를 가지고 긴 호흡으로 글을 작성하게 되면 조금만 딴 생각을 해도 금새 주제가 흐트러지기 쉽상이기 때문에 꽤 많은 집중을 요한다. 사실 필자 같은 경우만 해도 평소에 글쓰기를 그렇게 많이 해볼 기회가 많지 않았다. 물론 최근에 커피 한 잔 마시며 끝내는 VueJS라는 책을 집필하면서 글을 토할 정도로 쓰기는 했지만, 이건 조금 특이한 케이스이기 때문에 제외하고 생각해본다면 아마 대학교 때 과제로 레포트를 썼던 것이 자신의 생각을 길게 적어본 마지막 경험인 것 같다. 필자가 주변 사람들한테 블로그를 써보라고 권했을 때 많은 분들이 뭘 써야할 지 모르겠다라고 이야기하고는 했는데, 사실 일상 속에서 자기 생각을 글로 표현해볼 기회 자체가 많지 않기 때문에 글을 쓴다는 것에 많은 분들이 어려움을 느끼는 부분이 있는 것 같다. 그래서 이번 포스팅에서는 블로그 개설을 망설이고 있는 분들이 고민하고 있는 몇 가지 주제에 대해서 이야기를 한번 해보려고 한다. 어떤 주제로 글을 써야할까?일단 첫번째로 이야기하고 싶은 것은 바로 포스팅의 주제이다. 많은 분들이 첫 포스팅을 작성할 때 어떤 주제를 선정해야하는지 고민하시는데, 사실 주제는 뭐가 됐든 크게 의미없다. 주제에 대해서 이야기한다고 하면서 주제는 큰 의미가 없다고 하니 이게 무슨 소리인가 싶겠지만, 진짜로 별 의미 없다. 그냥 아무거나 써도 된다는 것이다. 필자 주변에 있는 분들의 이야기를 들어보면 첫 포스팅의 주제를 정할 때 고민하는 것이 대부분 비슷했다. 내가 알고 있는 지식은 남들도 다 알고 있을 것이라고 하시는 분도 있었고, 내가 알고 있는 지식이 너무 쉽고 간단한 것이라서 섣불리 공유하기가 조금 꺼려진다고 하시는 분도 있었다. 그런데 이유가 뭐가 됐든 사실 별로 상관이 없는 것이, 일단 블로그 포스팅은 책과 같이 출판하고 나면 수정하기 힘든 컨텐츠가 아니다. 마음에 안들면 나중에 지워버릴 수도 있고, 내용을 수정하는 것도 자유롭다. 그렇기 때문에 나중에 다시 포스팅을 읽어 보았을 때 주제가 이상하다고 느껴지거나 문체가 오글거려서 도저히 못 봐주겠으면 그냥 미련없이 rm 명령어를 때려버리면 그만이다. 그래서 필자는 어떤 주제를 고르는 것이 좋다 같은 이야기는 하지 않을 것이다. 대신 주제 자체가 왜 그리 중요하지 않은지, 그리고 필자가 실수했던 경험을 토대로 어떤 주제를 피해야하는지에 대한 이야기를 조금 해보려고 한다. 내가 알고 있는 지식은 생각보다 값지다일단 필자가 주제는 별로 중요하지 않다라고 하는 이유 중 하나는 여러분이 블로그 포스팅을 통해 무엇을 적든 간에 분명 누군가에게는 도움이 될 것이기 때문이다. 의외로 많은 분들이 내가 알고 있는 것은 남들도 알고 있을 것이다라는 마음 때문에 자신이 알고 있는 것을 공개적으로 공유하기를 꺼려한다. 물론 이건 겸손한 마음에서 우러나오는 생각이지만 그래도 여러분이 알고 있는 지식을 공유하는 행위 자체가 생각보다 값지다는 사실을 알아야 한다. 여러분이 선정한 주제가 이미 해당 분야에서 널리 알려진 지식일지라도 그 지식에 대해서 아직 모르는 사람도 분명 존재한다. 필자는 개인적으로 그 사람이 단 1명일지라도 내 포스팅을 읽고 그 사람에게 도움이 되었다면 잘 쓴거라고 생각한다. 이렇게 자신의 지식이나 생각을 아무 대가 없이 타인과 공유하는 것에 대해 익숙한 문화가 개발자들의 장점이 아닐까? 우리가 좋아하는 오픈소스도 결국은 지식과 기술을 공유하는 것이다 사실 지금은 필자가 이렇게 말하고는 있지만, 필자도 본격적으로 블로그에 포스팅을 작성하기 시작했을 때 제일 고민했던 게 이런 문제였다. 예를 들어 필자가 작성했던 JavaScript의 let과 const, 그리고 TDZ 포스팅 같은 경우도 쓰기 전에 고민을 많이 했었다. 필자가 해당 포스팅을 작성할 당시인 2019년 6월은 이미 JavaScript ES6 버전이 배포된지 지나도 한참 지난 후였기 때문에 이제 와서 이 주제에 대해서 작성하기에는 너무 늦은 게 아닌가라는 생각을 했었다. 그러나 막상 포스팅을 작성하고나니 생각보다 많은 분들에게서 도움이 되었다는 메세지를 받을 수 있었다. 필자는 이 지식이 별로 중요한 지식이 아니라고 생각했지만 누군가에게는 도움이 되었다는 것이다. 비록 필자가 해당 포스팅을 작성한 시점은 JavaScript ES6가 배포된 지 3년이 지난 후였지만 그때도 JS를 처음 접하는 사람들은 있을 수밖에 없고, 그 분들에게는 필자의 포스팅이 큰 도움이 된 것이다. 사람마다 알고 있는 지식과 모르고 있는 지식은 전부 다를 수 밖에 없다. 내가 A나 B를 알고 있다고 해서 다른 사람들도 모두 A나 B를 알고 있는 것이 아니라는 소리다. 개발자들은 모두 각자가 걸어온 길에 따라 각기 다른 기술에 대한 전문성을 보유한 사람들이기 때문에 이런 지식의 공유가 더 가치있는 것이다. 그러니까 이미 내가 알고 있는 지식을 저평가 하지말고 일단 그 지식에 대한 주제로 포스팅을 한번 작성해보자. 포스팅을 작성하기 위해서는 기존에 알고 있는 지식이라고 할 지라도 다시 정리하는 과정이 필요하기 때문에 스스로도 지식의 깊이를 다질 수 있는 좋은 기회가 된다. 처음부터 너무 어려운 주제는 피하자이건 필자가 처음 블로그를 작성할 때 실수했던 것이다. 사실 필자가 본격적으로 블로그에 포스팅을 기재하기 시작한 것은 2019년 6월, 즉 약 3개월 전이지만 처음 포스팅을 작성하기 시작한 것은 2017년 쯤이다. 그 당시에는 포스팅을 상당히 뜸하게 올렸는데, 그 이유는 바로 주제 선정에 대해 눈이 너무 높아서 였다. 당시 필자가 생각하기에는 이미 많은 개발자들이 블로그를 운영하고 있기 때문에 너무 일반적인 주제로 포스팅을 작성하면 별로 눈에 띄지도 않을 것이고 PR에도 별로 도움이 안될 것이라고 생각했었다. 그래서 필자는 포스팅 주제를 선택할 때 남들이 많이 선택하지 않은 주제나 최신 기술, 난이도가 어느 정도 있다고 생각하는 것들 위주로 선택했는데, 지금 생각해보면 이거 그냥 허세다. 포스팅만 어려운 주제로 쓴다고 해서 내가 진짜로 가치있는 사람이 되는 것이 아니기 때문이다. 뭔가 있어보이는 포스팅을 쓰고 싶었다 문제는 남들이 어렵다고 생각할 정도인 주제는 당연히 필자한테도 어렵다는 것이다. 알다시피 기술 포스팅을 작성하기 위해서는 그 주제에 대해서 단순한 이해 정도가 아니라 남들에게 쉽게 설명할 수 있을 정도의 이해도가 필요하기 때문에 일반적으로 투자하는 공부 시간보다 더 많은 시간을 투자해서 공부하게된다. 어떻게 보면 포스팅을 작성하기 위해 어려운 주제에 대한 공부를 강제로 하게되니 좋다고 생각할 수도 있지만, 이렇게 되면 포스팅을 하나 작성하는데 투자해야하는 시간이 거의 몇 주 단위가 될 수도 있다. 필자는 스스로도 많은 공부가 필요한 주제들로만 포스팅을 작성하려고 했기 때문에 하나의 포스팅을 작성하기 위해서 많은 연구가 필요했다. 그 결과 포스팅을 하나 작성하는데 걸리는 시간이 점점 늘어나기 시작하면서 포스팅 작성에 대한 의욕 또한 점차 떨어지게 되어, 결국 꾸준한 포스팅 업로드를 하지 못하게 되었다. 블로그를 운영하면서 중요한 것은 주제의 난이도가 아니라 양질의 포스팅을 꾸준히 생산하는 습관을 들이는 것이다. 물론 컴포트존을 벗어나고자 하는 목적으로 가끔씩 본인의 수준보다 약간 더 어려운 수준의 주제를 선정하고 공부를 하는 것은 좋지만, 필자처럼 모든 포스팅을 그런 식으로 작성하려고 하면 꾸준히 포스팅을 작성하는 것이 점점 힘에 부칠 수 밖에 없다. 여러분이 이미 알고 있는 분야의 지식은 상대적으로 모르는 분야의 지식에 비해 쉬운 것이라고 느껴질 지 모르지만, 분명 그 지식도 다른 사람들에게는 도움이 될 수 있는 가치있는 지식이다. 그 지식이 비록 단순한 변수 선언에 대한 내용일지라도 그 변수 선언에 대한 지식이 없는 누군가에게는 큰 도움이 될 것이다. 그러니 필자처럼 이건 너무 단순한 내용인데?라는 생각으로 어려운 주제의 포스팅을 고집하다가 제 풀에 지쳐 나가떨어지는 것보다는, 우선 내가 이미 알고있는 분야에 대해서 정리하는 포스팅을 작성해보는 것이 좋다고 생각한다. 기술 서적이 아닌 책도 많이 읽어보자블로그 포스팅을 작성할 때 물론 주제도 중요하지만 일단 포스팅이라는 행위 자체가 기본적으로 글을 쓰는 것이라는 사실을 잊어서는 안된다. 아무리 주제가 좋더라도 그 주제를 몰입력있게 풀어나갈 수 있는 능력이 없다면 사람들은 읽지 않는다. TIL(Today I Learned)처럼 본인의 기록용으로 짧은 포스팅을 남기는 경우에는 딱히 글쓰기 스킬의 중요성이 부각되지는 않지만 자신의 생각을 표현하거나 특정 기술에 대해서 깊히 파헤치는 등 어느 정도 컨텐츠성을 가지고 있는 포스팅의 경우는 저자의 글쓰기 스킬이 많이 중요해진다. 글쓰기 스킬의 향상이라는 주제에서 제일 많이 언급되는 내용 중 하나는 아마 독서의 중요성일 것이다. 사실 개발자들은 평소에 책을 많이 읽는 편이기는 하다. 그러나 대부분 기술과 관련된 서적 위주의 독서를 하기 때문에 문어체 특유의 어휘력을 기르기에는 별로 도움이 안되는 것이 사실이다. 애초에 이런 책들은 특정 기술의 사용법 같은 명확한 사실 전달에 초점을 맞추기 때문에 문체의 유려함은 중요한 포인트가 아니다. 필자도 VueJS 관련 서적을 집필할 때 딱히 문체는 신경쓰지 않았던 것 같다. 기술 서적을 집필해 보신 분들은 공감하시겠지만, 이런 책을 집필할 때는 문체보다 첨부된 코드의 오류를 검수하거나 확실한 정보를 전달하고 있는지 검증하는데만 해도 정신이 없다. 반면 에세이나 소설 같은 장르는 자신의 생각이나 상상 속의 이야기를 긴 호흡으로 풀어나가야 하는 장르이다보니 다양한 접속사의 활용이나, 간결하고 논리적인 문장의 구조, 명확한 주제의 제시 등 독자가 이야기의 흐름을 놓치지 않도록 하는 여러가지 방법들이 많이 사용될 수 밖에 없다. 이러한 방법들을 체득하기위해 따로 공부를 할 수도 있겠지만, 우리는 전공자도 아니고 프로 작가가 될 것도 아니므로 독서를 통해 자연스럽게 체득하는 방법을 추천하는 것이다. 요리도 많이 먹어본 사람이 잘하는 것처럼 글쓰기도 많이 읽어본 사람이 잘하기 마련이다. 필자가 그런 방법들을 이 포스팅에서 모두 소개하기에는 지식도 짧고, 포스팅 분량도 길어질 것이므로 그 중에서 가장 기초라고 생각하는 문어체에 대한 이야기를 조금 해보려한다. 문어체에 익숙해져야 한다우리가 평소 말할 때 사용하는 구어체와 글을 쓸 때 사용하는 문어체는 각기 다른 특성을 가지고 있다. 구어체의 경우에는 화자의 생각을 실시간으로 표현하는 방식이기 때문에 어순이 변경되거나 필요한 표현이 생략되는 등 문법에 크게 구애 받지 않는다. 야, 저번에 했던 그 게임 또 할까? 그 뭐였더라? 오버워치…가 아니고 배틀그라운드였다! 참고로 구어체의 이런 특성은 한국어에만 국한되는 것은 아니고, 영어 같은 다른 언어들 또한 동일하다. 구어체는 애초에 완벽한 문장을 만드는 것이 목적이 아니라 커뮤니케이션 자체에 초점을 맞춘 용법이기 때문에 한국어든 영어든 간에 문법은 딱히 중요하지 않다. 우리가 평소에 말할 때 딱히 문장의 구조를 생각하고 말하지 않는 것처럼 말이다. 또한 위의 예시를 보면 알 수 있듯이 실시간으로 자신의 생각을 표현하는 구어체에서는 앞에서 말한 내용을 수정하는 경우 또한 자주 발생한다. 그렇기 때문에 글을 구어체로 작성하게 되면 문장이 명료하게 완성되지 않고 문체가 산만해질 수 있으며 독자가 글을 읽어나가는 흐름이 끊기게 된다. 물론 글의 장르에 따라 이런 구어체를 글쓰기에 활용하는 경우도 있지만, 그건 대화를 하는 상황을 묘사하는 등 현실감을 불어넣기 위한 일종의 스킬이라 블로그 포스팅같은 컨텐츠에서는 많이 쓰이지 않는 방법이다. 이런 이유로 블로그 포스팅과 같이 정보성을 띄고 있는 글은 하나의 문장을 깔끔하게 완성하는 문어체로 글을 작성하는 것이 좋다. 긴 글의 흐름을 끊기지 않게 이어나가기 위해서는 깔끔하고 논리적인 문장의 구사와 다양한 접속사의 활용과 같은 스킬들을 사용해야 하는데, 구어체에는 이런 요소들이 문어체에 비해 상대적으로 약하거나 없기 때문이다. 하지만 문제는 문어체가 우리가 일상 속에서 잘 사용하지 않는 용법이기 때문에 익숙하지 않다는 것이다. 문어체에 익숙해지는 여러가지 방법이 있겠지만 그 중 필자가 추천했던 것은 기술 서적이 아닌 책, 그 중에서도 저자의 생각을 표현하는 주제를 가진 책을 많이 읽어보는 것이었다. 예를 들면 에세이 같은 것들 말이다. 단, 필자가 말하는 에세이는 짤막한 감성 글귀들이 송송 박혀있는 그런 책을 말하는 게 아니다. 필자가 말하는 에세이는 서론에서는 주제를 제시하고 본론에서는 그 주제에 대한 실증적인 방법을 제시하며 결론에서는 그에 따른 자신의 의견을 제시하는, 명확하게 구조화 되어있는 에세이를 말하는 것이다.(외국 학교나 기업에 들어갈 때 요구되는 그 에세이다) 빌 게이츠나 버락 오바마가 추천하는 책으로도 유명한 팩트풀니스 장르 분류는 인문학으로 되어있지만 이런 주제도 크게 보면 에세이라고 할 수 있다 개발자들은 주로 튜토리얼, 기술 분석, Dev Log와 같은 주제의 포스팅을 많이 작성한다. 이런 기술적인 포스팅에 왠 저자의 생각이냐고 반문할 수도 있겠지만, 결국은 포스팅의 주제가 남들에게 공유하고 싶은 무언가라면 그 포스팅에는 기본적으로 그 주제에 대한 저자의 의견, 정리 또는 생각이 어느 정도 묻어있을 수 밖에 없다. 또한 개발자들이 작성하는 포스팅들은 논리적이고 실증적인 방법으로 주제를 풀어나가는 경우가 많기 때문에 저런 에세이를 추천하는 것이다. 이렇게 자신이 작성할 블로그 포스팅과 유사한 스타일의 글을 많이 읽다보면 점점 문어체에서 사용하는 문법과 단어들, 주제를 풀어나가는 구성 등에 대해서 익숙해지기 때문에 자신의 글쓰기에도 많은 도움이 된다. 정 뭐부터 읽어봐야할지 모르겠다면 필자가 위에 올려놓은 팩트풀니스부터 한번 읽어보자. 사람마다 취향이 다르니 재미있을 것이라고는 할 수 없겠지만, 자신의 생각을 논리로 풀어나간다는 것이 어떤 전개로 이루어지는지는 알 수 있을 것이다. 피드백을 두려워 하지 말자아무래도 블로그라는 것이 불특정 다수에게 노출되는 컨텐츠다 보니 다른 사람들이 주는 피드백에 대한 두려움이 있을 수 있다. 간단하게 말하자면 욕먹는 것에 대한 두려움이랄까. 잘못된 내용을 전파했다가 피드백을 받는 경우도 있고, 자신의 생각을 기재했다가 나와 생각이 다른 사람들이 그 생각에 대한 피드백을 주는 경우도 있다. 일단 피드백을 받는 것을 두려워 해서는 안된다는 이야기를 먼저 하고 싶다. 피드백은 단순히 내 생각에 반대하는 의견, 나를 까는 의견이 아니라 나를 제 3자의 눈으로 바라본 냉정한 평가이기 때문이다. 필자도 물론 포스팅을 배포하고 공유하다보면 피드백을 받게 되는데, 뭐 가끔씩 공격적으로 이야기하시는 분들도 있긴 하지만 결국 뭐가 됐든 그 분들은 필자의 포스팅을 읽어보고 거기에 대한 자신의 생각을 표현했을 뿐이니 딱히 기분 나쁘거나 한 것은 없다. 그리고 그런 피드백은 대부분 필자의 포스팅에 뭔가 오류가 있었다거나, 필자의 생각과 다른 부분을 말씀해주시는 것이니 음, 그렇군하고 그냥 받아들이면 그만이다. 특히 잘못된 정보에 대한 피드백을 주시는 경우는 오히려 가만히 앉아서 오류를 파악하고 고칠 수 있으니 개이득아닌가. 최근 동기/비동기 포스팅에 대해 OKKY의 하마님께서 주신 피드백 지금까지 필자가 받은 피드백 중 가장 상세한 피드백을 주셔서 굉장히 감사했다 하지만 사람 마음이라는 것이 뭔가 지적받는 것에 대해서 한없이 무뎌질 수는 없는 법이다. 당연히 두렵기도 하고 부끄럽기도 하고 때로는 기분 나쁘기도 할 수 있다. 솔직히 말하자면 필자도 포스팅을 공유할 때마다 마음 한켠으로는 비난 받고 싶지 않다라는 마음이 든다. 개인적으로 정답이 명확한 기술 관련 포스팅을 작성할 때보다 지금 이 주제와 같이 필자의 생각을 표현하는 포스팅을 공유할 때가 더 그렇다. 기술 포스팅같은 경우는 주로 필자가 잘못된 정보를 기재했을 경우에 피드백이 들어오기 때문에 잘못된 정보를 기재했음을 인정하고 고치면 되지만, 이렇게 자신의 생각을 표현하는 주제처럼 주관성을 가지고 있는 포스팅의 경우에는 필자와 다른 생각을 가지신 분들이 가끔 공격적인 피드백을 주시는 경우도 있기 때문이다. 하지만 필자가 읽어 보았을 때 좀 심한데...?라는 생각이 들 정도로 거친 피드백을 주신 분은 지금까지 단 한 분밖에 없었고, 이 정도로 공격적인 피드백은 그냥 무시하면 그만이다. 개인적으로 그 피드백에 담긴 생각과는 별개로 타인에게 전달하는 말의 가치를 잘 모르는 사람과는 별로 생각을 나누고 싶지 않다.(비판과 비난은 다르다는 것을 명심하자) 아마도 이렇게 밑도 끝도 없이 비난하는 케이스가 사람들이 두려워 하는 욕먹기 싫다의 원인일 것 같은데, 생각보다 세상에 그렇게 이상한 사람들이 많지는 않다. 대부분은 여러분의 포스팅을 읽고 뭔가 아쉬운 마음에, 더 발전했으면 하는 좋은 마음으로 피드백을 주시는 분들이다. 오히려 그런 피드백들을 통해 다양한 사람들의 다양한 생각을 들어볼 수도 있을 뿐더러 피드백을 토대로 자신이 성장할 수 있는 좋은 기회이기도 하니까 너무 피드백을 두려워하지 않았으면 한다. 마치며필자가 글쓰기에 대한 포스팅을 작성한 이유는 많은 개발자들이 자신의 생각을 공유하는 생태계가 더욱 활성화되었으면 하는 마음에서 비롯되었다. 자신의 생각이나 지식을 공유하는 것이 얼마나 가치있는 행위인지는 매일 구글이나 스택오버플로우를 사용하면서 느끼고 있을 것이다. 그리고 이런 공유 행위는 오픈소스 활동이나 블로그 포스팅, 발표 등 다양한 방법으로 이루어지고 있지만, 그 중에서도 글쓰기는 자신의 생각을 남들에게 보여줄 수 있는 가장 기초적인 수단 중 하나이다. 글쓰기를 잘하기 위해서는 기본적으로 올바른 문장 구성을 위한 어휘력과 논리력이 뒷받침되어야 하므로 글쓰기를 통해 커뮤니케이션에 대한 기본적인 소양을 기를 수 있다고 볼 수도 있다. 이렇게 불특정 다수에게 자신의 생각을 공유하는 행위가 두려울 수도 있지만, 필자가 위에서 이야기했듯이 피드백을 두려워하지 않았으면 한다. 그런 피드백을 통해 나 자신도 조금씩 성장할 수 있는 것이고, 다른 사람의 생각을 다양하게 들어볼 수도 있는 좋은 기회이기 때문에 딱히 두려워할 이유가 없다. 거듭 이야기하지만 욕 먹는다라고 표현할 수 있을 정도로 모욕적인 맹비난을 쏟아붓는 사람은 생각보다 많지 않고, 만약에라도 그런 비난을 받는다면 그냥 해당 메세지를 삭제하고 없었던 일로 생각하면 된다. 원래 이상한 사람과 술 취한 사람과는 아예 안 엮이는 게 스트레스 덜 받는 방법이다. 피드백을 주시는 대부분의 사람들은 포스팅을 읽고 뭔가 아쉬운 마음에서, 좋은 마음으로 주시는 것이기 때문에 자신의 생각을 공유하는 것을 주저하지 않았으면 한다. 그리고 블로그에 당장 뭐부터 써야할 지 모르겠다면, 일단 내가 알고있는 것이 무엇인지부터 한번 차근차근 정리해보자. 장담하건데 본인이 생각하는 것보다 많은 주제들이 나올 것이다. 그리고 위에서는 이야기하지 않았지만 진짜 아무리 생각해도 도저히 쓸 주제가 없다면 재밌게 읽었던 외국 포스팅을 번역해보는 것도 좋은 방법이다.(생각보다 이런 포스팅은 인기도 많다) 이상으로 블로그 개설을 망설이고 있는 사람들에게 포스팅을 마친다.","link":"/2019/09/28/how-do-i-write-postings/"},{"title":"[JS 프로토타입] 자바스크립트의 프로토타입 훑어보기","text":"이번 포스팅에서는 자바스크립트(JavaScript)하면 빠질 수 없는 프로토타입(Prototype)에 대해서 한번 이야기해보려고 한다. 프로토타입은 자바스크립트를 ES5 시절부터 사용해오던 분들에게는 매우 익숙하지만 ES6부터 시작하신 분들은 대부분 클래스를 사용하기 때문에 익숙한 개념은 아닐 것이라고 생각한다. 필자가 처음 프론트엔드 개발을 시작했을때는 자바스크립트의 ES5 버전에서 막 ES6로 넘어가고 있던 시절이었는데, 기존에는 자바(Java)를 주로 사용하고 있던 필자가 프론트엔드 개발로 넘어오면서 제일 애먹었던 부분이 바로 이 프로토타입이었다.(물론 애먹는 건 현재진행형이다) 물론 지금은 자바스크립트의 위상이 많이 올라가면서 프로토타입 패턴에 대한 관심도 많아지기 시작했지만, 그래도 당시나 지금이나 여전히 주류는 C 계열 언어나 Java에서 사용하는 클래스를 기반으로한 객체 생성 방식이다. 그래서 자바스크립트를 처음 접하는 개발자에게 프로토타입 기반 프로그래밍은 상대적으로 낯선 방식일 수 밖에 없고, 이로 인해 기존 개발자들이 자바스크립트로 진입하는데 어려움이 있었다. 그런 이유로 ES6에서는 class 예약어가 등장한 것이다. 사실 필자도 아직 클래스 기반의 객체 생성 방식이 익숙하기 때문에 프로토타입에 대한 공부가 더 필요하다. 그래서 이번 포스팅에서는 프로토타입 패턴이 무엇인지, 자바스크립트 내에서 프로토타입이 어떤 방식으로 사용되고 있는지에 집중해서 한번 이야기해보려고 한다. ES6부터 클래스를 지원하는데도 프로토타입을 굳이 알아야 하나요?자바스크립트는 ES6부터 class 키워드를 사용하여 클래스를 지원하고 있다. 정확히 말하면 프로토타입으로 클래스를 흉내내서 구현한 것이라고 말하는 것이 맞다. 그런 이유로 많은 개발자들이 자바스크립트의 클래스를 단순한 문법 설탕(Syntactic Sugar)라고 이야기하지만, 사실 개인적으로 자바스크립트의 클래스는 ES5 시절 프로토타입을 사용하여 객체를 생성했던 방법보다 더 엄격한 제약을 가지고 있기 때문에 단순한 문법 설탕이라기보다는 상위 요소(Superset)라고 하는게 맞지 않나 싶다. 그러면 그냥 클래스를 쓰면 되는데 왜 프로토타입을 알아야 하는 것일까? 그 이유는 ES6에서 class 키워드를 통해 클래스를 지원하고 있기는 하지만, 이건 자바스크립트가 클래스 기반 언어가 되었다는 의미는 아니기 때문이다. 결국 자바스크립트 안에서의 클래스는 클래스의 탈을 쓴 프로토타입이다. 그리고 예전에 작성된 레거시 프론트엔드 코드의 경우에는 ES5로 작성된 것도 많기 때문에 아직까지 프론트엔드 개발자들은 ES5를 만져야하는 경우가 왕왕 있는 것이 현실이다. 물론 ES5를 ES6 이상의 버전으로 마이그레이션하려고 해도 기존의 프로토타입 기반의 객체 생성이나 상속이 구현된 코드를 이해할 수 없다면 마이그레이션 또한 불가능하다. 프로토타입은 디자인 패턴이다프로토타입이라고 하면 일반적으로 자바스크립트를 떠올리지만, 사실 프로토타입은 자바스크립트에서만 사용되는 것은 아니고, 그냥 일종의 디자인 패턴 중 하나이다. 자바스크립트 뿐만 아니라 ActionScript, Lua, Perl 등 프로토타입 기반 프로그래밍을 지원하는 다른 언어도 많다. 그래서 자바스크립트의 프로토타입을 자세히 알아보기 전에 디자인 패턴으로써의 프로토타입을 먼저 알아볼까 한다. 프로토타입 패턴은 객체를 효율적으로 생성하는 방법을 다루는 패턴 중 하나인데, 주로 객체를 생성하는 비용이 클 때 이를 회피하기 위해 사용된다. 객체를 생성할 때의 비용이 크다는 말은, 말 그대로 객체를 생성할 때마다 뭔가 일을 많이 해야한다는 뜻이다. 예를 들어 RPG 게임의 캐릭터를 하나 구현해본다고 생각해보자. 이 캐릭터는 여러가지 장비를 장착할 수 있는 기능을 가지고 있는데, 처음 캐릭터가 생성될 때 딸랑 맨 몸으로 시작하면 유저들이 싫어할 것 같으니 기본적인 장비 몇 가지를 장착한 상태로 생성될 수 있도록 만들어주려고 한다. Player.java1234567891011121314class Weapon {}class Armor {}class BasicSward extends Weapon {}class BasicArmor extends Armor {}class Player { public Weapon weapon; public Armor armor; public Player() { this.weapon = new BasicSward(); // 초심자의 목도 this.armor = new BasicArmor(); // 초보자용 갑주 }} 간단하게 만들어보면 대충 이런 느낌이다. Player 객체는 자신이 생성될 때 BasicSward 객체와 BasicArmor 객체까지 함께 생성해야한다. 이런 경우 그냥 Player 객체만 생성하는 상황보다는 객체의 생성 비용이 높다고 할 수 있다. 게다가 캐릭터 생성 시 처음 부여하는 아이템의 종류가 많아질수록 Player의 객체의 생성 비용 또한 계속 높아질 것이다. 음… 근데 곰곰히 생각해보니 캐릭터가 처음 생성되며 가지고 있는 아이템이 항상 같다는 전제 조건이 있다면 생성 비용이 높은 Player객체를 딱 한번만 생성하고 그 다음부터는 생성된 객체를 복사해서 사용해도 될 것 같다는 생각이 든다. 12345678910// 이건 너무 객체 생성 비용이 높으니까...Player evan = new Player();Player john = new Player();Player wilson = new Player();// 이런 방법으로 접근해보는 것은 어떨까?Player player = new Player();Player evan = player.clone();Player john = player.clone();Player wilson = player.clone(); 이런 관점으로 접근하는 것이 바로 프로토타입 패턴이라고 할 수 있다. 프로토타입, 즉 원본 객체가 존재하고 그 객체를 복제해서 새로운 객체를 생성하는 방법인 것이다. 실제로 자바에서 프로토타입 패턴을 사용할때, 복제 대상이 되는 클래스는 보통 Cloneable 인터페이스를 사용하여 구현한다. Cloneable 인터페이스에는 clone 메소드가 정의되어 있기 때문에, 이 인터페이스를 사용하는 클래스는 반드시 clone 메소드를 오버라이딩해서 구현해야한다. 1234567class Player implements Cloneable { //... @Override public Player clone () throws CloneNotSupportedException { return (Player)super.clone(); }} clone 메소드를 구현하고나면 이제 Player 객체는 복사 가능한 객체가 된다. 즉, 다른 객체들의 원본 객체가 될 수 있는 기능을 가지게 되었다는 것이다. 이제부터는 Player 객체를 추가로 생성하고 싶을 때는 기존에 생성되어 있던 객체를 그대로 복사하면 되기 때문에 높은 객체 생성 비용이 드는 것을 피할 수 있다. 12Player evan = new Player();Player evanClone = evan.clone(); 또한 Player 객체는 복사되어 새로운 메모리 공간을 할당받지만, 깊은 복사를 하지 않는 이상 Player객체가 가지고 있는 BasicSward 객체와 BasicArmor 객체는 새롭게 생성되지 않고 기존에 이 객체들이 할당된 메모리 공간을 참조하기만 한다. 즉, 잘만 쓴다면 메모리 공간을 아낄 수도 있다는 것이다. 자바스크립트에서 원시 자료형은 Call by value, 그 외 자료형은 Call by reference를 사용하는 것과 동일한 원리이다. 여기까지 듣고 나서 예상하신 분들도 있겠지만, 그 말인 즉슨 잠깐 정신줄 놓고 코딩하다보면 이런 슬픈 상황도 발생할 수 있다는 뜻이다. 1234567891011Player evan = new Player();try { Player evanClone = evan.clone(); evanClone.weapon.attackPoint = 40; System.out.println(\"에반 무기 공격력 -> \" + evan.weapon.attackPoint); System.out.println(\"에반 복사본 무기 공격력 -> \" + evanClone.weapon.attackPoint);}catch (Exception e) { System.err.println(e);} 12에반 무기 공격력 -> 40에반 복사본 무기 공격력 -> 40 디버깅 지옥이 펼쳐진다… 정리해보자면 프로토타입 패턴이란, 객체를 생성할 때 원본이 되는 객체를 복사해서 생성하는 패턴이라고 할 수 있다. 물론 자바스크립트의 프로토타입은 단순히 몇 개의 객체가 복제 관계를 가지는 것이 아니라, 자바스크립트 내의 모든 객체 전체가 복제 관계로 얽혀있기 때문에 이것보다는 약간 더 복잡하긴 하지만, 근본적인 원리 자체는 프로토타입 패턴을 따라간다. 그럼 이제 자바스크립트가 객체를 생성할 때 프로토타입 패턴을 어떤 식으로 사용하고 있는 지 한번 알아보도록 하자. 자바스크립트의 프로토타입앞서 설명했듯이 프로토타입 패턴은 객체를 생성할 때 사용하는 패턴이다. 필자가 위에서 예시로 사용한 언어인 자바는 클래스 기반 프로그래밍을 지원하기 때문에, 특수한 패턴을 사용해야지만 프로토타입이라는 개념을 사용할 수 있다. 그러나 애초에 프로토타입 기반 프로그래밍을 지원하는 자바스크립트의 경우에는 애초에 모든 객체를 생성할 때 프로토타입을 사용하기 때문에, 객체를 생성하기만 해도 위에서 필자가 설명한 프로토타입 패턴이 적용된다. 그렇기 때문에 우선 자바스크립트에서 말하는 객체(Object)가 무엇인지, 그리고 그 객체가 생성된다는 것이 무엇을 의미하는 것인지 알아볼 필요가 있다. 자바스크립트가 객체를 생성하는 방법컴퓨터 공학에서의 객체(Object)는 현실의 사물을 프로그램에 반영한 것이다. 즉, 여러 개의 프로퍼티(특징)와 메소드(행위)를 가지고 현실의 사물을 흉내내는 존재인 것이다. 클래스 기반 언어에서는 클래스를 생성하고 그 클래스를 사용하여 객체를 생성해야하지만, 자바스크립트는 간단한 문법만으로 객체를 생성할 수 있다. 1234567const evan = { name: 'Evan', age: 29, say: function () { console.log(`Hi, I am ${this.name}!`); }}; 이런 방식을 우리는 리터럴(Literal)로 객체를 선언한다고 한다. 리터럴은 소스 코드의 고정된 값을 대표하는 일종의 단축어 같은 개념이기 때문에, 우리는 간단한 문법만으로 객체를 생성했다고 느끼지만 내부적으로는 객체를 생성하는 일련의 매커니즘이 작동하고 있다. 예를 들어, 다른 언어에서는 이런 리터럴 문법을 사용하여 객체를 생성할 때 내부적으로 클래스를 사용하게된다. 파이썬 같은 경우, 딕셔너리를 리터럴로 선언하고 타입을 찍어보면 dict 클래스가 출력되는 것을 볼 수 있다. 12345my_dict = { 'name': 'Evan', 'age': 29}type(my_dict) 1 우리는 dict({ 'name': 'Evan', 'age': 29 })와 같이 클래스를 명시적으로 사용하지않고 리터럴로 딕셔너리를 생성했지만 내부적으로는 제대로 dict 클래스를 사용해서 객체를 생성했다는 것이다. 자바 또한 리터럴 문법을 지원하는 배열(Array)을 선언한 후 출력해보면 결국 클래스를 기반으로 배열 객체를 생성한다는 것을 알 수 있다. 12String[] array = {\"Evan\", \"29\"};System.out.println(array); 1[Ljava.lang.String;@7852e922 이 말인 즉슨, 다른 언어와 마찬가지로 자바스크립트의 객체도 갑자기 혼자서 뿅 하고 생성되는 것이 아니라 분명히 뭔가를 사용해서 만들어내고 있다는 말이다. 하지만 자바스크립트에는 클래스라는 개념 자체가 없는데 뭘 사용해서 객체를 만들어내고 있는 것일까? 답은 바로 함수(Function)이다. 자바스크립트에서 객체가 생성되는 원리를 조금 더 파헤쳐보기 위해서 위에서 리터럴로 선언했던 evan 객체를 이번에는 다른 방법으로 선언해보도록 하겠다. 1234const evan = new Object({ name: 'Evan', age: 29,}); 왠지 클래스 기반 언어에서 클래스를 사용하여 객체를 생성하는 것과 유사한 문법이 나타났다. 이런 방식을 생성자(Constructor)를 사용하여 객체를 생성한다고 한다. 클래스 기반 언어라면 Object는 클래스겠지만, 자바스크립트에서는 클래스가 아닌 함수이다. 즉, 자바스크립트에서의 생성자는 함수가 가지고 있다는 것이다. 저게 진짜 함수인지 알고 싶으니, 브라우저 콘솔 창을 열고 Object를 한번 출력해보도록 하겠다. 12console.log(Object);console.log(typeof Object); 12ƒ Object() { [native code] }\"function\" 음, 콘솔로 찍어보니 Object는 확실하게 빼박캔트 함수가 맞다. 필자가 처음 자바스크립트를 사용하기 시작했을 때 받아들이기 어려웠던 부분이 바로 이 부분이었다. 클래스 기반 프로그래밍에 익숙했던 필자에게 new 키워드와 생성자는 클래스만 가질 수 있는 것이었는데 갑자기 뜬금없이 함수가 나와버리니 받아들이기 힘들었던 것 같다.(머리로는 알겠는데 마음이…) 어쨌든 이제 자바스크립트가 객체를 생성할 때 함수를 사용해서 생성한다는 것을 알게되었다. 지금까지 알아낸 내용을 정리해보자면 다음과 같다. 프로토타입 패턴이란 객체를 생성할 때 원본 객체를 복제하여 생성하는 방법이다. 자바스크립트는 객체를 생성할 때 프로토타입 패턴을 사용한다. 자바스크립트는 객체를 생성할 때 함수를 사용한다. 그렇다는 것은 자바스크립트가 함수를 사용하여 객체를 생성할 때 뭔가를 참조하고 복제해서 객체를 생성한다는 말이다. 이제부터 그 뭔가를 알아 볼 시간이다. 도대체 뭘 복제해서 객체를 만드는 걸까?사실 디자인 패턴으로써의 프로토타입 패턴은 생각보다 그렇게 어렵지 않다. 그저 객체를 생성할 때 원본 객체를 복제해서 생성한다는 개념이기 때문이다. 마찬가지로 자바스크립트 또한 뭔가를 복제해서 새로운 객체를 생성하고 있다. 그럼 이제 자바스크립트가 도대체 뭘 복제해서 객체를 생성하고 있는 것인지 알아보기 위해 간단한 함수를 하나 선언해보도록 하겠다. 123456function User () {}const evan = new User();console.log(evan);console.log(typeof evan); 12User { __proto__: Object }object 위에서 이야기했듯이 자바스크립트는 함수를 사용하여 객체를 생성하기 때문에, 이렇게 클래스를 사용하는 것과 유사한 느낌으로 객체를 생성할 수 있다. 그렇다면 evan 객체는 무엇으로부터 복제된 것일까? 간단하게 생각하면 User 함수라고 생각해볼수 있겠지만, 사실은 User 함수를 복제한 것이 아니라 User 함수의 프로토타입 객체를 복제한 것이다. 이렇게 갑자기 프로토타입이 나온다고…? 뜬금없어서 바로 이해가 안될 수도 있겠지만, 단순하게 생각해보면 쉽다. 만약 객체를 생성하면서 함수를 복제했다면 생성된 객체는 object 타입이 아니라 function 타입이어야 하지 않겠는가? 하지만 evan 객체는 object 타입을 가지고 있다. 즉, 이 함수 자체가 아니라 다른 객체 타입의 무언가를 복제했다는 것이고, 그 원본 객체가 User 함수의 프로토타입 객체인 것이다. 필자는 User 함수의 프로토타입을 명시적으로 선언하지 않았지만, 자바스크립트는 함수가 생성될 때 자동으로 그 함수의 프로토타입 객체(Prototype Object)도 함께 생성하고 해당 함수의 prototype 프로퍼티에 연결해둔다. 1234function User () {}console.log(User.prototype);console.log(typeof User.prototype); 12{ constructor: f User(), __proto__: Object }object 분명히 필자는 함수만 선언했는데, User.prototype 프로퍼티에 뭔가 이것저것 가지고 있는 객체 녀석이 1+1으로 붙어나왔다. 함수를 생성하면 무조건 그 함수의 프로토타입 객체도 함께 생성된다는 것이 키포인트다. 그리고 이 프로토타입 객체는 함수를 사용해서 새로운 객체를 생성할 때 원본 객체 역할을 해줄 객체를 의미한다. 즉, new User()라는 문법을 사용하여 새로운 객체를 만들게 되면 User 함수 자체가 아니라 User 함수가 생성될 때 함께 생성된 User 함수의 프로토타입 객체를 복제해서 새로운 객체를 만든다는 것이다. evan 객체야, 내가 아니라 내 프로토타입 객체를 복제하렴 이때 User 함수가 생성되며 함께 생성된 User 함수의 프로토타입 객체를 프로토타입 프로퍼티(Prototype Property)라고 한다. 그럼 이 프로토타입 객체가 가지고 있는 프로퍼티인 constructor와 __proto__는 뭘 의미하는 걸까? constructor함수가 생성되며 함께 생성된 프로토타입 객체는 모두 constructor라는 프로퍼티를 가지고 있다. 그리고 이 프로퍼티에는 이 프로토타입 객체가 생성될 때 선언했던 함수가 들어있다. 1console.log(User.prototype); 1234{ constructor: f User(), __proto__: Object} 함수를 선언하면 함수와 함께 해당 함수의 프로토타입 객체도 함께 생성되며 이 둘을 연결하게 된다. 이때 함수는 프로토타입 객체의 constructor 프로퍼티로 연결되고, 프로토타입 객체는 함수의 prototype 프로퍼티로 연결되는 것이다. 함수와 프로토타입 객체는 서로 연결되어있다 1console.log(User.prototype.constructor === User); 1true 이 생성자 프로퍼티는 이 함수를 통해 생성된 객체 입장에서 보았을 때 나를 만들 때 어떤 함수가 호출되었냐?를 의미한다. 만약 이 연결이 없다면 새로 생성된 객체는 자신을 만들 때 어떤 생성자 함수가 호출되었는지 알 수가 없다. 새롭게 생성된 객체는 자신을 생성할 때 어떤 원본 객체를 복사했는지에 대한 링크는 가지고 있지만 어떤 생성자가 호출되었는지에 대한 링크는 가지고 있지 않기 때문이다. 하지만 원본 객체의 constuctor 프로퍼티에 생성자 함수가 연결되어있기 때문에 새롭게 만들어진 객체는 자신의 원본 객체에 접근해서 이 프로퍼티를 참조함으로써 자신이 만들어질때 어떤 생성자 함수가 호출되었는지를 알 수 있다. 12const evan = new User();console.log(evan.__proto__.constructor === User); 1true 이때 생성된 객체가 자신의 원본 객체에 접근할 수 있는 프로퍼티가 바로 __proto__ 프로퍼티이다. __proto__방금 생성자를 설명하면서 함수를 통해 새롭게 생성된 객체는 원본 객체와의 연결을 가지고 있다고 했다. 이때 이 연결을 프로토타입 링크(Prototype Link)라고 한다. Object.prototype을 제외한 자바스크립트 내의 모든 객체는 원본 객체를 기반으로 복사되어 생성되었기 때문에, 자신의 원본 객체로 연결되어있는 프로토타입 링크 또한 모든 객체가 가지고 있다. 이때 이 링크가 담기는 프로퍼티가 __proto__ 프로퍼티이다. Object.prototype.__proto__가 존재하지 않는 이유는 밑에서 후술하도록 하겠다. 우선은 객체들이 자신의 원본 객체로 통하는 프로토타입 링크를 가지고 있다는 사실에만 집중하자. 이 포스팅에서는 이해를 돕기위해 __proto__ 프로퍼티를 그대로 사용하고 있다.그러나 해당 프로퍼티는 ECMAScript 2015에서는 표준이었지만 현재는 표준이 아니므로 Object.getPrototypeOf()를 사용하는 것을 추천한다. 즉 User 함수를 사용하여 생성한 객체는 User.prototype 객체를 복사하여 생성된 객체이기 때문에, 이 객체들은 원본인 User.prototype 객체를 자신의 __proto__ 프로퍼티에 연결해두는 것이다. 123function User () {}const evan = new User();console.log(evan.__proto__ === User.prototype); 1true 그렇다면 이 프로토타입 링크를 사용해서 계속 해서 원본 객체를 추적하다보면, 결국은 자바스크립트 내의 모든 객체들이 최종적으로 어떤 원본 객체를 복사해서 생성된 것인지 알 수 있지 않을까? 프로토타입 체인자바스크립트 내의 사용되는 모든 객체들은 전부 이런 프로토타입 기반 방식으로 정의되고 생성된다. 즉, String, Boolean, Array와 같이 우리가 일반적으로 사용하고 있는 빌트인 객체들도 모두 같은 방식을 사용해서 만들었다는 것이다. 그렇다면 이 객체들은 어떤 프로토타입 객체를 복사해서 만들어진 것일까? String, Boolean, Array든 뭐가 됐든 자바스크립트 내에 존재하는 모든 것들은 바로 Object 함수의 프로토타입인 Object.prototype을 시작으로 해서 복제된다. 위에서 __proto__를 설명하면서 Object.prototype 객체는 프로토타입 링크, 즉 원본 객체로 통하는 링크가 없다고 이야기했었는데, 그 이유는 바로 Object.prototype이 모든 객체들의 조상님이기 때문이다. 이게 정말인지 확인해보고 싶다면, 아무 객체나 골라잡아서 그 객체의 __proto__ 프로퍼티를 통해 쭉쭉 올라가보면 된다. 일단 만만한 String을 사용해서 조상을 추적해보려고 하는데, String 객체를 생성하는 함수부터 출발할 것인지, String 함수를 통해 생성된 객체부터 출발할 것인지에 따라 조상까지 올라가는 길이 달라진다. String 함수와 String 객체는 당연히 원본이 되는 객체도 다르기 때문이다. 필자는 그 중 String 객체를 생성할 수 있는 String 생성자 함수를 선택했다. 12345const first = String.__proto__;const second = first.__proto__;console.log('첫번째 조상 -> ', first.constructor.name);console.log('두번째 조상 -> ', second.constructor.name); 12첫 번째 조상 -> Function두 번째 조상 -> Object 자바스크립트의 모든 함수는 자신의 원본으로 Function.prototype 객체를 원본으로 가진다. 그리고 Function.prototype은 결국 객체이기 때문에, 당연히 원본으로 Object.prototype 객체를 원본으로 가진다. 그럼 여기서 한번 더 올라가면 어떻게 될까? 12const third = second.__proto__;console.log(third.constructor.name); 1Uncaught TypeError: Cannot read property 'constructor' of null at :1:28 앗, TypeError가 발생했다. 에러메세지를 보아하니 Object.prototype 객체의 원본 객체인 Object.prototype.__proto__는 null인 모양이다. 즉, Object의 위로는 더 이상 조상이 없는 것이다. 지금 살펴본 이 관계를 간단한 다이어그램으로 나타내어보면 다음과 같다. 뭔가 복잡해보이지만 별 것 없다. String 함수의 원본 객체는 Function.prototype이다. 그리고 const a = 'evan'과 같이 선언된 String 객체는 자신을 생성한 String 함수의 String.prototype을 원본으로 가질 것이고, String.prototype은 객체이기 때문에 당연히 Object.prototype을 원본으로 가지는 것이다. 이렇게 프로토타입으로 이루어진 객체들의 관계를 프로토타입 체인(Prototype Chain)이라고 한다. 마치며사실 필자가 프로토타입에 대한 포스팅을 쓰려고 했던 이유는 얼마 전 면접에서 자바스크립트의 프로토타입을 사용하여 Private Static 메소드를 구현하라는 문제를 받았다가 결국 못 풀었기 때문이다. 자바스크립트의 클로저와 프로토타입을 활용하여 풀어야하는 문제였는데, 필자는 기본기가 부족한 나머지 풀어내지 못했다. 그래서 원래는 프로토타입을 사용한 다양한 상속 기법들과 클로저를 사용한 멤버의 은닉 등도 함께 소개해보려고 했지만, 늘 그렇듯 분량 조절 실패로 인해 다른 포스팅에서 별도로 다뤄야할 것 같다. 사실 애초에 분량 조절 따위를 생각하고 쓰지 않는다 필자처럼 기존의 클래스 기반 객체 생성방식에 익숙한 개발자들에게 자바스크립트의 프로토타입은 꽤나 복잡하게 느껴진다. 디자인 패턴으로써의 프로토타입은 단순히 객체를 복제해서 새로운 객체를 생성한다는 정도의 개념에 그치지만 자바스크립트의 프로토타입 체인은 그것보다 훨씬 더 복잡하게 연결되어있기 때문이다. 하지만 프로토타입 체인이나 프로토타입을 사용한 각종 상속 기법은 어렵게 느껴질 수 있어도, 프로토타입의 뼈대 자체는 그렇게 어렵지 않다고 생각한다. 객체는 함수를 사용해서 만들어지고, 객체는 함수의 프로토타입 객체를 복제하여 생성된다. 모든 객체는 자신이 어떤 원본 객체를 복제하여 생성된 것인지에 대한 정보를 가지고 있다. 물론 원본 객체에 대한 정보를 런타임에 동적으로 변경할 수 있는 등 변태같은 짓들이 가능하기도 하고, 이를 사용한 다양한 기법들도 있기는 하지만 기본은 결국 저 두 가지라고 할 수 있을 것 같다. 다음 포스팅에서는 본격적으로 프로토타입을 사용한 상속 기법과 객체의 프로퍼티를 탐색하는 방법인 프로토타입 룩업 등에 대해서 설명하도록 하겠다. 이상으로 자바스크립트의 프로토타입 훑어보기 포스팅을 마친다.","link":"/2019/10/23/js-prototype/"},{"title":"수학과 함께 복잡한 문제를 단순하게 만들자!","text":"최근 많은 IT 기업들이 개발자를 채용할 때 코딩 테스트를 시행하고 있다. 회사마다 어떤 스타일의 문제를 출제하는지 차이는 있지만, 대부분 간단한 알고리즘 풀이 또는 Codility나 프로그래머스와 같은 사이트처럼 실무에서 겪을 만한 상황을 살짝 섞어놓는 느낌의 문제를 선호하는 것 같다. 이런 문제들의 특성 상 CS 기초와 문제 분석 능력, 직감 등을 다양하게 사용하여 해결해야 하기 때문에 단기간 연습한다고 실력이 확 느는 것은 아닌 것 같다. 이런 문제들은 우리에게 단순히 너 이 알고리즘 알아?라고 물어보는 것이 아니라 어떤 방법을 사용해서 풀어볼래?라고 물어본다. 사실 자료구조나 알고리즘 자체는 보면 공부하고 몇 번 구현해보면 어느 정도 숙달될 수 있지만, 이렇게 문제를 분석하고 단순화해서 적합한 방법을 선택할 수 있는 능력은 단순히 공부로 만들어 낼 수 있는 종류의 것은 아닌 것 같다. 필자는 최근 취업 준비를 하면서 이런 문제를 종종 풀어보고 있는데, 확실히 CS 기초도 부족하긴 하지만, 문제를 분석하고 좋은 방법을 선택할 수 있는 능력이 많이 부족함을 느꼈다. 그래서 자료구조나 알고리즘을 처음부터 다시 공부하면서, 동시에 문제 해결 능력을 기르기 위한 방법이 어떤 것이 있을지 고민해보기 시작했다. 수학적인 사고 방식으로 문제를 단순화하자필자는 최근 면접에 거하게 털리고 나서 CS 기초나 자바스크립트 기초를 처음부터 다시 공부하기 시작했는데, 막상 이렇게 공부한 지식을 가지고 코딩 테스트 문제를 한번 풀어보려고 했더니, 생각처럼 잘 되지 않았다. 대부분 알고 있겠지만, 많은 코딩 테스트 문제은행 서비스에서는 문제를 해결하고 나면 다른 사람들은 이 문제를 어떤 방식으로 해결했는지도 함께 보여준다. 필자같은 경우는 사실 이게 궁금해서 문제를 푸는 것도 있는 것 같다. 그러던 와중에 대부분의 사람들이 완전탐색으로 풀었던 문제를 어떤 굇수 분이 단순한 산수 연산 몇 번으로 풀어내는 것을 본 적이 있었다. 역시 세상은 넓고 굇수는 많다 당연히 필자도 해당 문제를 완전탐색으로 풀었고 그 방법 밖에 없을 것이라고 생각했지만, 그 굇수분은 문제의 패턴을 찾아내어 문제를 단순화 시킨 것이다. 물론 조금 난해한 감이 있어서 실무에서 사용하기에는 조금 이견이 갈릴 수 있는 코드이긴 했지만, 대부분의 사람들이 완전탐색으로 풀었던 복잡한 문제를 단순한 식 몇개로 풀어냈다는 사실이 충격이었다. 이때 필자가 느낀 점은, 수학적인 사고에 대한 필요성이었다. 물론 알고리즘 역시 이런 수학적인 사고를 바탕으로 효율적인 해결 방식을 일반화한 것이긴 하지만, 필자가 원했던 것은 좀 더 근본적인 문제 해결 능력이었다. 물론 수학적인 사고라고 해서 문제를 읽고 막 복잡한 식을 세우는 것이 아니다. 자연어로 이루어진 문제를 분석하고, 해결 가능한 수준으로 나누고, 패턴을 찾아내는 과정 또한 수학적인 사고에서 비롯된다. 애초에 수학 자체가 복잡한 문제를 단순화하고 패턴을 찾아내어 일반화하는 학문이다. 그리고 프로그래머는 수학을 잘해야할까? 포스팅에서 한번 이야기한 적 있지만, 필자가 이야기하고싶은 수학은 어려운 이론이나 공식을 말하는 것이 아니다. 개인적인 생각이기는 하지만, 필자는 수의 성질을 이해하는 것이 제일 중요하다고 생각했다. 예를 들면 홀수에 1을 더하면 짝수가 된다던가, 1부터 100까지의 합을 구할 때 101 * 100 / 2를 하면 된다던가와 같은 것들이 그렇다. 그런 이유로 최근 프로그래머, 수학으로 생각하라라는 책을 읽게 되었는데, 이 책의 초입부부터 재미있는 문제 해결 방법이 몇개 나와서 그 문제들과 해결 방법에 대해서 공유를 해볼까 한다. 오늘로부터 100억일 후는 무슨 요일일까?$n$일 후의 요일을 구하는 문제는 수학적인 사고를 필요로 하는 대표적인 문제 중 하나이다. 게다가 굳이 코딩 테스트까지 가지 않고 일상 속에서 비즈니스 로직만 만지고 있더라도 꽤나 자주 접할 수 있는 문제이다. 그래서 워밍업으로 상대적으로 익숙한 요일 구하기 문제를 먼저 살펴보려고 한다. 필자가 이 글을 작성하고 있는 2019년 10월 29일은 화요일이다. 그럼 오늘로부터 100억일 후는 과연 무슨 요일일까? 음, 단순하게 생각해보면… 오늘은 화요일이니까 1일 후는 수요일, 2일 후는 목요일과 같은 순차적인 방법으로 접근할 수도 있겠다. 123456789101112131415console.time('calc');const week = ['일', '월', '화', '수', '목', '금', '토'];let today = 2;let shift = 0;for (let i = 0; i week.length - 1) { today -= week.length;}console.log(week[today]);console.timeEnd('calc'); 12토calc: 60948.138ms 아무리 요즘 컴퓨터가 연산 능력도 좋고 무보수로 일해주는 SCV라고 하지만 100억회를 반복하는 루프를 계산하게 하는 것은 너무나도 가혹한 처사이다. 이 알고리즘은 시간 복잡도가 $O(n)$이기에, 루프만 돌았을 뿐인데도 수행 시간이 1분이 넘는다. 이렇게 무식하게 풀어낼 수는 없으니, 다른 방법을 찾아야한다. 다행히 우리는 요일이 7일 마다 반복된다는 것을 알고 있다. 오늘이 화요일이라면 7일 후도 당연히 화요일이고, 14일 후도 화요일이다. 즉, 요일이 반복된다는 주기성이 존재한다는 것이다. 오늘부터 7의 배수만큼 지난 날은 무조건 화요일이라는 패턴을 찾았다면 그 다음부터는 간단해진다. 어떤 수를 1씩 증가시켜가면서 계속 7로 나누면 0~6이 순차적으로 나타나는 주기성이 있으므로, 배수를 구할 때와 마찬가지로 100억을 7로 나누고 그 나머지를 확인하면 되기 때문이다. 1234567891011console.time('calc');const week = ['일', '월', '화', '수', '목', '금', '토'];let today = 2;let shift = Math.pow(10, 10) % week.length;today += shift;if (today > week.length - 1) { today -= week.length;}console.log(week[today]);console.timeEnd('calc'); 12토calc: 0.156ms 수행 시간이 60000ms에서 0.156ms로 줄었다. 이렇게 문제에서 주기성을 찾아내고, 나머지의 주기성과 연관지을 수만 있다면, 완전탐색을 하지 않고도 나머지를 사용하여 문제를 가볍게 풀 수 있다. $10^{100000000}$일 후의 요일도 구해보자자, 그럼 여기서 한번 더 나아가보자. 이런 방법으로 우리가 $10^{100000000}$일, 즉 10의 1억승일 이후의 요일도 구할 수 있을까? 10의 1억승을 뭐라고 부르는지는 모르겠지만, $10^{68}$이 무량대수라고 부르는 엄청 큰 숫자이니 쉽게 가늠이 안되는 수인 것은 분명하다. 당연히 $10^{100000000}$은 자바스크립트의 Number.MAX_SAFE_INTEGER 값을 아득히 넘어서는 숫자이기 때문에 위와 같은 방식으로는 계산이 불가능 하다. 여기서부터는 컴퓨터한테 계산을 맡기는 것보다는 문제를 단순화하고 주기성을 찾아내는 일이 더 중요해진다. 오늘은 10월 29일 화요일이니 오늘부터 $n$일 후의 요일을 쭉쭉 살펴보도록 하자. 방금 예제로 만들었던 로직을 활용하여 $10^{30}$일 이후까지 살펴보니, 대략 다음과 같은 패턴이 있다는 것을 알 수 있었다. 모든 결과를 적으면 너무 표가 길어지니, $10^{12}$일 이후의 결과만 기재하도록 하겠다. 일자 요일 인덱스 $10^0$일 후 수 3 $10^1$일 후 금 5 $10^2$일 후 목 4 $10^3$일 후 월 1 $10^4$일 후 토 6 $10^5$일 후 일 0 $10^6$일 후 수 3 $10^7$일 후 금 5 $10^8$일 후 목 4 $10^9$일 후 월 1 $10^{10}$일 후 토 6 $10^{11}$일 후 일 0 $10^{12}$일 후 수 3 필자는 이 과정에서 두 가지 정보를 얻을 수 있었다. 요일이 수, 금, 목, 월, 토, 일의 순서로 계속 반복되고 있다는 것과 오늘 요일인 화요일이 등장하지 않는다는 것이다. 즉, 10의 지수가 6 증가할 때마다 같은 요일이 돌아온다. 바꿔말하면 0의 개수가 6개씩 늘어날 때마다 같은 요일이 돌아온다는 말과 같다. 그렇다면 결국 10의 지수를 6으로 나눈 나머지 값을 사용하여 방금 전과 동일한 방법으로 요일을 구할 수 있다는 말이다. 1234const week = ['수', '금', '목', '월', '토', '일'];const exp = Math.pow(10, 8);console.log(week[exp % week.length]); 1토 비록 $10^{100000000}$이라는 어마무시한 수를 컴퓨터가 담아낼 수 없기 때문에 직접 계산할 수는 없지만, 지수의 증가로 인한 요일의 주기를 파악함으로써 상상도 안가는 먼 미래의 요일을 구할 수 있게 되었다.(사실 이걸 구하는 게 뭔 의미가 있겠냐만…) 만약 위에서 요일을 구했던 정직한 방법으로 이 문제를 풀려고 했다면 불가능했겠지만, 문제를 분석하고 주기성을 찾아냄으로써 어찌어찌 풀 수는 있었다. 욕실 바닥에 타일 깔기사실 방금 풀어보았던 요일 맞추기 문제처럼 눈에 띄게 일정한 주기를 가지고 반복되는 숫자를 찾아내는 문제는 익숙해지는데 그렇게 오랜 시간이 걸리지는 않는다. 그러나 우리가 일상에서 겪는 대부분의 문제는 저렇게 패턴을 대놓고 보여주지 않는 경우가 많다. 이때 필요한 것이 문제를 분석하고 패턴을 찾아내는 일이다. 사실 주기성이라는 수의 성질을 이용할 수 있다는 것의 진짜 의의는 바로 패턴을 만들고 찾아낼 수 있다는 것에 있다. 이번에는 그 패턴을 이용하여 유효성을 검사하는 문제이다. 에반은 타일 시공 업체에 취직해서 첫 욕실 바닥 시공을 하게 되었다.그러나 에반은 실수로 가로 1cm, 세로 2cm의 직사각형 타일들만 챙겨나오게 되었다… 다행히 모든 욕실 바닥은 표준화가 되어있어서 가로 1cm, 세로 1cm의 정사각형 칸으로 이루어져있지만, 욕실 바닥 모양과 칸의 수는 모두 제각각이다. 에반은 자신의 직사각형 타일로 욕실 바닥을 빠짐없이 메꿔야하지만, 욕실 바닥의 모양에 따라 작업이 불가능한 곳도 있다.게다가 에반은 힘이 없어서 타일을 반으로 쪼개서 사용할 수도 없다. 에반은 어떻게 작업의 가능 여부를 알 수 있을까? 123456789const floor = [ [0, 0, 0, 0, 0, 1, 0, 0], [0, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 0, 1], [1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 0, 1, 1, 1, 0],]; 이 문제의 경우, 타일로 욕실 바닥을 채울 수 있는 경우의 수를 하나씩 검사해볼 수도 있겠지만, 그렇게 풀어내기에는 워낙 경우의 수가 많기도 하고 로직도 복잡해질 것이 뻔하다. 그렇다면 욕실 바닥에 있는 칸의 개수를 세어보면 어떨까? 만약 칸의 개수가 홀수라면 에반이 가진 타일로는 절대 바닥을 채울 수가 없을 것이다. 하지만 이 문제에 나와있는 바닥의 총 칸 수는 슬프게도 34칸이다. 게다가 홀수, 짝수 여부만으로는 해당 타일로 전부 바닥을 채울 수 있을지는 장담할 수 없다. 조금 더 확실한 검증 방법이 없을까? 이 문제는 주기성과 전혀 관련이 없을 것 같지만, 사실 굉장히 간단한 패턴이 숨어있다. 바로 에반이 가지고 있는 타일이 두개의 칸으로 이루어져 있다는 것이다. 조금 더 생각을 쉽게 하기 위해 타일과 바닥에 색을 칠해보도록 하자. 이렇게 색을 칠하고나니 에반이 가지고 있는 타일은 검은색 1칸과 흰색 1칸으로 이루어진 두 칸짜리 타일이 되었다. 즉, 만약 에반이 가지고 있는 타일로 욕실의 바닥을 빈틈없이 메꿀 수 있다면, 욕실 바닥의 검은색 칸의 수와 흰색 칸의 수가 같아야 한다는 것이다. 그러나 우리에게 주어진 욕실 바닥의 검은색 칸의 수는 16칸, 흰색 칸의 수는 18칸이다. 즉, 이 욕실 바닥은 에반이 가진 타일로는 채울 수 없는 바닥이라는 뜻이 된다. 이 문제는 단순히 두 칸으로 이루어진 에반의 타일에 검은색과 흰색이라는 주기성을 부여하여 풀어나가는 문제이다. 에반의 타일이 가지고 있는 색의 주기와 욕실 바닥의 주기가 동일하지 않다면 그 욕실 바닥은 채울 수 없는 바닥이 되는 것이다. 그럼 검은색 칸을 -1, 흰색 칸을 1으로 정의하고, 욕실 바닥의 해당 칸을 만날 때마다 -1과 1을 번갈아가며 더한 후 마지막에 값이 0이 되면 검은색 칸과 흰색 칸의 수가 동일하다고 생각할 수 있겠다. 12345678910111213141516171819202122232425const floor = [ [0, 0, 0, 0, 0, 1, 0, 0], [0, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 0, 1], [1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 0, 1, 1, 1, 0],];const tile = [-1, 1];let count = 0;let tileIndex = 0;floor.forEach((row, index) => { tileIndex = Number(index % 2 === 0); row.forEach(col => { if (col === 1) { count += tile[tileIndex]; } tileIndex = tileIndex === 0 ? 1 : 0; });});console.log(`검은 타일과 흰 타일의 개수 차이는 ${Math.abs(count)} 입니다.`); 1검은 타일과 흰 타일의 개수 차이는 2 입니다. 각 row를 순회할 때 tileIndex를 다시 교정해주는 이유는, 이 행렬의 컬럼의 개수가 짝수이기 때문이다. 타일의 주기 또한 짝수이기에 다음 줄에서는 이전 줄의 가장 마지막에 있던 타일의 색이 다시 한번 나오게 된다.(컬럼을 홀수로 만들면 이 과정이 필요없는데, 문제 잘못 만들었다…) 문제만 보면 전혀 주기성과 관련이 없어보이는 문제였지만, 이렇게 문제 내에서 반복되는 패턴을 찾아내고 주기성을 부여함으로써 조금 더 간단한 방법으로 문제를 해결할 수 있다. 한 붓 그리기, 쾨니히스베르크의 다리 증명하기쾨니히스베르크의 다리는 현대 위상 수학의 시작을 이끌었던 굉장히 유명한 문제로, 프로이센의 쾨니히스베르크(현재 러시아 칼리닌그라드)라는 도시에 있는 다리를 사용한 문제이다. 쾨니히스베르크의 한 가운데에는 프레골라 강이 흐르고 있고, 여기에는 가운데의 섬들과 연결되어있는 7개의 다리가 있다. 임의의 지점에서 출발하여 이 다리들을 한 번씩만 건너서 모든 다리를 건널 수 있을까? 즉, 한 붓 그리기 문제인 것이다. 이 문제를 그대로 보면 생각하기가 어려우니, 조금 더 그림을 단순하게 그리고 각 지역에 식별자를 부여한 후, 문제의 조건들을 정리해보도록 하자. 임의의 지점에서 출발할 수 있다. 모든 다리를 건너야 한다. 한번 건넌 다리는 다시 건널 수 없다. 각 구역은 몇 번을 들리든 상관없다. 출발한 구역으로 다시 돌아와도 되고 안 돌아와도 상관없다. 사실 몇 번 펜으로 쭉쭉 그어보면 대충 불가능하다는 감이 온다. 하지만 절대로 건널 수 없다라는 결론을 내리기 위해서는 왜 불가능하다는 것인지 증명하는 과정이 필요하다. 혹시 방법이 있는데 단순히 못 찾을 것일수도 있으니 말이다. 우선 이 문제를 조금 더 쉽게 생각해보기 위해 복잡한 지도 모양의 그림이 아닌, 단순화된 그래프로 다시 그려보도록 하겠다. 이때 그래프 내에서 A, B, C, D 구역의 역할을 하는 점을 정점(Vertex)이라고 하고, a~g 다리의 역할을 하는 선을 간선(edge)라고 하며, 각 정점에 붙어있는 간선의 개수를 차수(Degree)라고 한다. 쾨니히스베르크의 문제에서 다리를 건넌다는 것은 어떤 한 정점에서 다른 정점으로 넘어가는 것을 의미하며, 한번 건넌 다리는 다시 건널 수 없다는 것은 다른 정점으로 넘어갈 때 사용한 간선을 삭제해야한다는 것을 의미한다. 다리를 건너 이동할 수 있는 케이스를 한번 쭉 살펴보면 대략 처음 출발할 때, 마지막 도착할 때, 통과할 때의 3가지 케이스로 분류해볼 수 있는데, 이 3가지 케이스에서 간선이 삭제되는 개수에는 패턴이 숨어있다. 처음 출발할 때 어느 정점에서 출발하던 다른 정점으로 이동하는 경우는 출구 역할을 하는 간선만 삭제될 것이다. 즉, 출발 정점의 차수가 1 줄어든다. 마지막 도착할 때 출발할 때와는 반대로, 도착하는 정점은 입구의 역할을 했던 간선만 삭제하면 되므로, 해당 정점의 차수는 1 줄어든다. 통과할 때 통과할 때는 입구의 역할을 하는 간선과 출구의 역할을 하는 간선을 삭제해야하므로, 해당 정점의 차수가 2씩 줄어든다. 즉, 어떤 그래프에서 한 붓 그리기가 성공했다는 것은 정점을 순회하다가 더 이상 건널 수 있는 간선이 없어졌을 때, 반드시 모든 정점의 차수가 0이어야 한다는 것이다. 만약 차수가 0이 아닌 정점이 존재한다면 그 정점에는 아직 건너지 않았던 간선이 연결되어 있다는 말이 되므로 한 붓 그리기는 실패한 것이 된다. 이때 우리는 각 정점의 차수가 1이나 2 씩 줄어들고 있다는 점에서 이 문제를 풀 수 있는 힌트를 얻을 수 있다. 차수의 홀짝 여부에 집중하자차수가 1씩 줄어드는 경우는 한번 수행될 때마다 차수의 홀짝 여부가 변경되고, 2씩 줄어드는 경우는 차수의 홀짝 여부가 절대 변하지 않는다. 정점 차수의 홀짝 여부를 이야기하고 있는 이유는 바로 0이 짝수이기 때문이다. 즉, 정점에서 출발, 도착, 통과 시 변하는 차수의 홀짝 패턴을 파악하면 모든 경우의 수를 그려보지 않더라도 간단하게 이 그래프가 한 붓 그리기가 가능한 그래프인지 아닌지 알 수 있다. 출발지와 도착지가 같은 경우우선 위에서 살펴본 바와 같이 중간에 통과하는 정점의 차수는 무조건 2씩 줄어들기 때문에 몇 번을 통과하든 차수의 홀짝 여부가 절대 변하지 않는다. 즉, 어떤 방식으로 건너든 통과 정점의 차수가 0이 되려면, 해당 정점의 차수는 처음부터 짝수여야한다는 것이다. 만약 통과 정점의 차수가 홀수라면 반드시 마지막에는 차수가 1이 되고, 이 간선을 타고 해당 정점에 도착하게되면 더 이상 남아있는 간선이 없기 때문에 다른 정점으로 건너갈 수 없게 된다. 차수가 홀수인 통과 정점에 들어서면 맘대로 나갈 수 없다 또한 출발지와 목적지가 같은 경우에는 맨 처음 출발할 때 출발 정점의 차수를 1 줄이고 도착할 때 다시 1을 줄여야하기 때문에 해당 정점의 차수가 총 2만큼 줄어들게 된다. 이 경우에도 출발 정점의 차수는 홀짝 여부가 변경될 수 없기 때문에 반드시 처음부터 짝수인 차수를 가지고 있어야 한다는 말이 된다. 즉, 출발지와 도착지가 같은 경우 한 붓 그리기가 성공하려면 모든 정점의 차수가 짝수여야 한다는 결론이 나온다. 출발지와 도착지가 다른 경우출발지와 도착지가 다른 경우에도 통과 정점의 차수는 처음부터 짝수여야 한다는 점은 달라지지 않지만, 이번에는 출발지와 도착지가 다르기 때문에 출발 정점과 도착 정점의 차수는 반드시 홀수여야 한다는 점이 다르다. 중간에 통과하는 정점은 반드시 차수가 2 씩 줄어들기 때문에 홀짝이 변하지 않지만, 출발과 도착 시에는 차수가 1만 줄어들기 때문에 홀짝 여부가 변하기 때문이다. 즉, 출발지와 도착지가 다른 경우는 출발지와 도착지는 홀수 차수, 그 외 정점은 짝수여야한다. 쾨니히스베르크의 다리는 왜 한 붓 그리기가 불가능할까?이 두 가지 조건을 정리해보자면 그래프의 정점을 순회하며 한 붓 그리기가 가능한 경우는 모든 정점이 짝수 차수를 가지고 있거나 홀수 차수가 2개인 경우라고 정리해볼 수 있다. 다시 쾨니히스베르크의 다리를 도식화한 그래프를 살펴보고 이 조건에 맞아떨어지는지 확인해보자. 이 그래프의 정점들의 차수를 정리해보면 A=3, B=5, C=3, D=3으로 모든 정점의 차수가 홀수이므로, 위에서 찾아낸 어떤 조건과도 맞지 않는다. 즉, 쾨니히스베르크의 다리는 한 붓 그리기가 불가능한 구조라는 것이 증명된 것이다. 그래프 이론에서 이렇게 한 붓 그리기, 즉, 그래프의 모든 경로를 단 한 번씩만 통과하는 경로를 오일러 경로(Eulerian Trail)라고 부르는데, 그 이유는 갓 레온하르트 오일러 형님이 이미 1735년에 이 문제를 증명하고 자기 논문에 써먹었기 때문이다. 해당 논문은 Solutio Problematis ad Geometriam Situs Pertinentis에서 확인할 수 있지만… 제목에서도 느껴지듯이 이게 영어가 아니다. 이 시대의 가방끈 기신 분들이 작성한 논문이 다들 그러하듯 라틴어로 작성되어있기 때문에 읽어보는 건 사실 힘들다. 그래도 혹시 필자의 상상을 뛰어넘어 라틴어가 가능하신 굇수분들이 있을 수 있으니 일단 첨부하겠다. 어쨌든 오일러 형님의 문제 풀이에서 주목해야하는 아이디어는 각 정점의 차수를 조사할 때 차수 자체가 아닌 수의 홀짝에 주목했다는 점이다. 정점에서 출발할 때, 도착할 때, 통과할 때 정점이 가진 차수의 상태가 홀짝으로 변화하는 그 패턴을 파악하지 못했다면 이 문제를 해결하기는 힘들었을 것이다. 마치며이번 포스팅에서 살펴본 3개의 문제는 어려운 수학 공식을 사용하는 문제가 아니다. 7일마다 반복되는 패턴에서 착안하여 $10^{100000000}$일 후의 요일도 뭔가 패턴이 있을 것이라는 추론, 두 칸짜리 타일을 보고 검은색과 흰색이 반복되는 패턴을 떠올릴 수 있는 것, 그래프 순회의 모든 경우의 수를 따져보지 않고 각 정점의 차수가 홀짝으로 변화하는 패턴을 생각해낼 수 있는 것 등은 복잡한 수학 공식을 모르더라도 수의 성질만 알고 있다면 누구든지 접근할 수 있는 문제 해결 방식이다. 이렇게 수의 근본적인 성질을 파악하고 이용하면 복잡한 문제를 단순하게 풀 수 있다. 슬픈 점은 이게 단순히 공부로 얻어질 수 있는 능력이 아니라는 것이다. 이런 능력을 키우기 위해서는, 그냥 이렇게 생각하는 연습을 많이 해야하는 것 같다. 코딩 테스트를 많이 풀어보는 것도 물론 좋지만, 일상 속에서 겪는 다양한 문제들 속에서 이렇게 패턴을 찾아내고 분석해보는 것도 나름 도움이 되지 않을까? 예를 들면 친구가 11월 10일에 약속을 잡자고 했는데, 그때가 무슨 요일인지 핸드폰으로 확인하는 것이 아니라 한번 직접 계산해본다던가 하는 식으로 말이다. 이상으로 수학과 함께 복잡한 문제를 단순하게 만들자! 포스팅을 마친다.","link":"/2019/10/30/make-simple-with-math/"},{"title":"TCP의 헤더에는 어떤 정보들이 담겨있는걸까?","text":"저번에 HTTP/3는 왜 UDP를 선택한 것일까? 포스팅을 진행하며 TCP에 대해 간단한 언급을 했었지만, 해당 포스팅에서는 기존의 HTTP에서 사용하던 TCP에 어떤 문제가 있었는지에 집중해서 이야기했었지만 이번에는 TCP 자체에 조금 더 집중해서 이야기해보려고 한다. 원래는 이 포스팅에서 TCP의 개괄적인 내용을 모두 다루려고 했으나 생각보다 양이 너무 많아서 몇 개의 포스팅으로 나누어 작성하려고 한다.(파도파도 끝이 없는 이 놈의 할배 프로토콜…) 그런 이유로 이번 포스팅에서는 TCP의 헤더 안에 들어 있는 필드들이 어떤 의미를 가지고 있는지에만 집중해서 이야기 해보도록 하겠다. TCP, Transmission Control ProtocolTCP(Transmission Control Protocol)는 OSI 7계층 중 전송 계층에서 사용되고 있는 프로토콜로, 장비들 간의 통신 과정에서 정보를 안정적으로, 순서대로, 에러없이 교환할 수 있도록 하는 것에 목적을 둔 프로토콜이다. 컴퓨터 공학에서는 컴퓨터에게 가까운 부분일 수록 낮다거나 뒤에 있다는 표현을, 사람에게 가까운 높다거나 앞에 있다라는 표현을 자주 사용하는데, OSI 7계층에서도 마찬가지로 낮은 계층일수록 기계에 가까운 부분이고 높은 부분일수록 사람에게 가까운 부분이라고 생각하면 편하다. 이때 우리에게 친숙한 HTTP, SMTP, FTP와 같은 프로토콜 친구들이 가장 높은 계층인 응용 계층에 위치한다. 그에 비해 더 낮은 계층에 존재하는 TCP, UDP, IP 같은 프로토콜들은 상대적으로 접할 일이 많이 없기는 하다. 이런 프로토콜들은 대부분 OS에서 알아서 처리해주기 때문에 상위 계층에서 프로그래밍을 하는 개발자가 굳이 여기서 일어나는 일까지 하나하나 신경쓸 필요가 없기 때문이다. 애초에 이게 레이어 모델이 존재하는 이유 중 하나이다. 네트워크라는 것이 수많은 기술의 집약체인 만큼 한 명의 개발자가 모든 것을 다 알기는 힘들다. 그래서 각 계층 간 철저한 역할 분담을 통해 어떤 작업을 할 때 신경써야하는 범위를 좁혀주는 것이다. 덕분에 우리는 HTTP를 사용할 때 DNS는 어디를 사용할지, 패킷은 어떻게 처리할지 등 여러 가지 작업을 한번에 신경쓸 필요가 없다. 하지만 아무리 레이어가 나누어져 있다고 한들 하위 레이어에서 일어나는 일을 전혀 모르고 있다면, 어플리케이션 레이어에서는 아무 문제 없지만 하위 레이어에서 문제가 발생했을 때 전혀 손도 못 대는 케이스도 발생할 수 있다. 이런 이유로 자신이 사용하고 있는 프로토콜의 대략적인 작동 원리와 개요 정도는 알고 있으면 좋다고 생각하기 때문에, 이번 포스팅을 작성하며 그 동안 대략적인 몇 가지 특징으로만 알고 있던 TCP를 조금 뜯어보려고 한다. TCP는 왜 만들어진걸까?개인적으로 어떤 기술을 공부할 때, 무작정 외우는 것이 아니라 이게 왜 필요한 것인지를 알고 그 이유에 대해 공감하며 공부하는 편이 효과적이라고 생각한다. TCP는 워낙 옛날에 나온 기술이니 당시 상황을 100% 공감하기는 쉽지 않겠지만, 그래도 이 프로토콜이 개발된 이유를 살펴보면 당시 엔지니어들의 고충을 알아볼 수 있다. 패킷 교환 방식을 사용해보자!TCP는 방금 이야기 했듯이 1970년 냉전 당시 미 국방성이 개발하던 알파넷 프로젝트의 일부로 개발되었는데, 그 당시 알파넷을 연구할 때 관심을 가진 주제 중에 하나가 바로 핵전쟁이 나도 살아남는 네트워크였다.(핵전쟁의 상대방은 당연히 마더 러씨아…) 왜냐하면 1970년대의 네트워크는 회선 교환 방식을 사용하고 있었기 때문에 중계국이 폭격을 맞아서 박살나거나 중간에 연결된 선이 하나가 잘려나가면 그대로 통신이 끊어져 버렸기 때문이다. 직접 보지는 않았지만 이런 느낌이지 않았을까…? 저 당시 중계국이 하는 일은 그냥 이거다. A가 중계국에 “B랑 연결해주세요!”라고 하면, 위의 사진과 같이 케이블이 마구 꽂혀있는 패치 테이블에서 A 라벨이 붙은 구멍과 B 라벨이 붙은 구멍을 찾아서 케이블로 연결해준다. 말 그대로 회선을 교환하는 방식인 것이다. 저러다가 A가 C랑 통신하고 싶으면 B 구멍에서 케이블을 빼서 C 구멍에 꽂으면 된다. 이렇게 회선 교환 방식의 경우에는 통신을 하고 싶은 상대방과 물리적으로 회선을 하나 딱 잡아놓고 계속 통신을 하는 것이기 때문에 회선의 효율이 낮을 수 밖에 없다. 우리가 전화를 걸 때 상대방이 통화 중이면 상대방이 통화 중이니... 어쩌고 나오는 것과 같은 원리이다. 물론 회선을 독점하기 때문에 대량의 데이터를 빠른 속도로 주르륵 보낼 수 있는 등의 장점도 있긴 하지만, 이때 미국에게 중요한 것은 핵이 터져도 끊기지 않는 연결이었기 때문에 하나의 회선에 전적으로 의존하는 연결이라는 건 큰 단점으로 다가왔을 것이다. 그래서 나온 아이디어가 바로 패킷 교환 방식이다. 데이터를 하나의 회선을 사용하여 보내다가 해당 회선이나 중계국이 개박살나면 전송되던 데이터와도 영원히 이별하게 되니, 데이터를 잘게 쪼갠 후 여러 개의 회선을 통해 보내자는 것이다. 일종의 분산투자랄까. 이렇게 되면 노드 하나가 박살나도 모든 데이터가 유실되진 않을 것이다 최악의 경우 중간에 있는 회선이나 중계국이 박살나서 데이터가 약간 유실될 수는 있겠지만 전체 네트워크를 한 번에 타격하지 않는 이상 모든 데이터가 유실될 가능성은 적다. 또한 하나의 회선을 잡아놓고 계속 통신하는 것이 아니라 패킷에 목적지를 마킹해놓고 그냥 보내기만 하면 되니, 회선의 사용 효율 또한 높아질 수 있다. 이런 이유로 미 국방성은 이 아이디어를 채택하여 알파넷에 적용했고, 초기 테스트도 대성공하여 패킷 교환 방식의 실용성을 증명했다. 이후 몇 개의 대학과 군에서만 사용되던 알파넷이 대중들에게 공개되고 전 세계적으로 연결되며 인터넷으로 발전하게 되었고, 덩달아 알파넷의 통신 프로토콜이었던 TCP도 함께 떡상하게 된 것이다. 패킷 교환 방식의 문제점하지만 패킷 교환 방식도 당연히 만능이 아니기에, 몇 가지 문제가 있었다. 우리가 TCP를 공부할 때 함께 따라오는 ARQ나 SYN, ACK 등의 개념들이 바로 이런 문제들을 해결하기 위해 과거의 엔지니어들이 머리를 싸맨 결과인 것이다. Q: 전송 중간에 패킷이 쥐도새도 모르게 사라지거나 훼손되면 어떡해요?A: 그럼 그 패킷만 다시 보내라고 해!(ARQ) Q: 송신 측이 패킷을 쪼갠 순서를 알아야 수신 측이 재조립할 수 있겠는데요?A: 그럼 순서번호를 패킷이랑 같이 보내!(시퀀스 번호) Q: 수신 측이 처리할 수 있는 속도보다 송신 측이 패킷을 빠르게 보내버리면 어떡하죠?A: 그럼 수신 측이 처리할 수 있는 양을 송신 측에 알려주고 그 만큼만 보내라고 해! (슬라이딩 윈도우) TCP가지고 있는 많은 기능과 개념들은 마냥 글로만 봤을 땐 복잡해보이고 뭔가 외울 것도 많아보이지만, 당시 상황을 생각해보면 반드시 필요한 것들이었음을 알 수 있다. 그리고 이런 기능들은 상대방이 보낸 세그먼트의 헤더에 들어있는 정보를 파악하여 작동하기 때문에, 이 기능들을 하나씩 알아보기 전에 TCP의 헤더에는 어떤 정보들이 들어있고, 이 정보들이 의미하는 것이 무엇인지 살펴보려고 한다. TCP의 헤더를 까보자HTTP, TCP, IP와 같은 프로토콜들은 각자 자신이 맡은 역할이 있고, 보내고자 하는 데이터에 자신의 헤더를 붙혀서 데이터의 정보를 표현한다. TCP는 전송의 신뢰성과 흐름 제어, 혼잡 제어 등의 역할을 맡고 있는 프로토콜이기 때문에, TCP 헤더에도 이러한 기능을 사용하기 위한 여러가지 값들이 담겨있다. 즉, 이 헤더를 보면 개괄적인 TCP의 기능들을 한 차례 쓱 훑어볼 수 있다는 말이고, 그런 이유로 필자는 TCP 포스팅의 첫 번째 스텝으로 헤더 까보기를 골랐다. TCP는 여러 개의 필드로 나누어진 20 bytes, 즉 160 bits의 헤더를 사용하며, 각 필드의 비트를 0 또는 1로 변경하여 전송하고자 하는 세그먼트의 정보를 나타낸다. 하지만 이 20 bytes라는 것은 아무 옵션도 없는 기본적인 헤더일 때의 용량이고, TCP의 여러가지 옵션들을 사용하면 헤더 맨 뒤에 옵션 필드들이 추가로 붙기 때문에 최대 40 bytes가 더해진 60 bytes까지도 사용할 수도 있다. 그럼 이 그림에 표기된 순서대로 각 필드가 어떤 정보를 담고 있는지 한번 살펴보도록 하자. Source port, Destination port 이 필드들은 세그먼트의 출발지와 목적지를 나타내는 필드로, 각각 16 bits 를 할당받는다. 이때 출발지와 목적지의 주소를 판별하기 위해서는 IP 주소와 포트 번호가 필요하다. IP 주소는 당연히 한 계층 밑인 네트워크 계층에 있는 IP의 헤더에 담기기 때문에, TCP 헤더에는 IP 주소를 나타내는 필드가 없고 포트를 나타내는 필드만 존재한다. Sequence Number 시퀀스 번호는 전송하는 데이터의 순서를 의미하며, 32 bits를 할당받는다. 최대 4,294,967,296 까지의 수를 담을 수 있기 때문에 시퀀스 번호가 그리 쉽게 중복되지는 않는다. 이 시퀀스 번호 덕분에, 수신자는 쪼개진 세그먼트의 순서를 파악하여 올바른 순서로 데이터를 재조립할 수 있게 된다. 송신자가 최초로 데이터를 전송할 때는 이 번호를 랜덤한 수로 초기화 하며, 이후 자신이 보낼 데이터의 1 bytes당 시퀀스 번호를 1씩 증가시키며 데이터의 순서를 표현하다 4,294,967,296를 넘어갈 경우 다시 0부터 시작한다. Acknowledgment Number 승인 번호는 데이터를 받은 수신자가 예상하는 다음 시퀀스 번호를 의미하며, 32 bits를 할당받는다. 연결 설정과 연결 해제 때 발생하는 핸드쉐이크 과정에서는 상대방이 보낸 시퀀스 번호 + 1로 자신의 승인 번호를 만들어내지만, 실제로 데이터를 주고 받을 때는 상대방이 보낸 시퀀스 번호 + 자신이 받은 데이터의 bytes로 승인 번호를 만들어낸다. 예를 들어 1 MB짜리 데이터를 전송한다고 생각해보자. 이렇게 큰 데이터를 한번에 전송할 수는 없으므로, 송신자는 이 데이터를 여러 개의 세그먼트로 쪼개서 조금씩 전송해야한다. 이때 송신자가 한번에 전송할 수 있는 데이터 양은 네트워크나 수신자의 상태에 따라 가변적이긴 하지만, 그냥 100 bytes라고 가정해보자. 송신자는 첫 전송으로 100 bytes 만큼만 데이터를 전송하며 시퀀스 번호를 0으로 초기화한다. 시퀀스 번호는 1 bytes당 1씩 증가하기 때문에 첫 번째 바이트 뭉치는 0, 두 번째 바이트 뭉치는 1, 세 번째 바이트 뭉치는 2와 같은 순서로 매겨질 것이다. 즉, 이번 전송을 통해 수신자는 0~99까지 총 100개의 바이트 뭉치를 받았고, 그 다음 전송 때 받아야할 시퀀스 번호는 2가 아닌 100이 되는 것이다. 100 bytes 만큼 하나의 세그먼트로 묶어서 전송한다 tcpdump를 사용하여 패킷을 캡쳐해보면 실제로 송신 측이 보낸 데이터의 길이만큼 수신 측의 승인 번호가 증가하는 모습을 확인해 볼 수 있다. 12localhost.http-alt > localhost.49680: Flags [P.], seq 160:240, ack 161, win 6374, length 80localhost.49680 > localhost.http-alt: Flags [.], ack 240, win 6374 송신 측이 보낸 세그먼트를 보면 시퀀스 번호가 seq 160:240로 찍혀있고, 수신 측은 자신의 승인 번호로 콜론 뒤 쪽의 값을 사용하고 있다. 이때 시퀀스 번호의 형식은 n 이상:m 미만의 범위를 나타낸다. 콜론 뒤쪽의 번호는 송신 측의 시퀀스 범위에 포함되지 않으므로 수신 측이 저 번호를 그대로 가져다 쓰는 것이다. 즉, 승인 번호는 다음에 보내줘야하는 데이터의 시작점을 의미한다는 것을 알 수 있다. Data Offset 데이터 오프셋 필드에는 전체 세그먼트 중에서 헤더가 아닌 데이터가 시작되는 위치가 어디부터인지를 표시한다. 이 오프셋을 표기할 때는 32비트 워드 단위를 사용하며, 32 비트 체계에서의 1 Word = 4 bytes를 의미한다. 즉, 이 필드의 값에 4를 곱하면 세그먼트에서 헤더를 제외한 실제 데이터의 시작 위치를 알 수 있는 것이다. 이 필드에 할당된 4 bits로 표현할 수 있는 값의 범위는 0000 ~ 1111, 즉 0 ~ 15 Word이므로 기본적으로 0 ~ 60 bytes의 오프셋까지 표현할 수 있다. 하지만 옵션 필드를 제외한 나머지 필드는 필수로 존재해야 하기 때문에 최소 값은 20 bytes, 즉 5 Word로 고정되어 있다. 이 필드가 필요한 이유는, 밑에서 설명할 옵션(Option) 필드의 길이가 고정되어 있지 않기 때문이다. Reserved (3 bits) 미래를 위해 예약된 필드로, 모두 0으로 채워져야 한다. 상단의 헤더 그림에도 그냥 0 0 0으로 찍혀있는 것을 확인해볼 수 있다. Flags (NS ~ FIN) 9개의 비트 플래그이다. 이 플래그들은 현재 세그먼트의 속성을 나타낸다. 기존에는 6개의 플래그만을 사용했지만, 혼잡 제어 기능의 향상을 위해 Reserved 필드를 사용하여 NS, CWR, ECE 플래그가 추가되었다. 먼저 기존에 존재하던 플래그들의 의미는 다음과 같다. 필드 의미 URG Urgent Pointer(긴급 포인터) 필드에 값이 채워져있음을 알리는 플래그. 이 포인터가 가리키는 긴급한 데이터는 높게 처리되어 먼저 처리된다. 요즘에는 많이 사용되지 않는다. ACK Acknowledgment(승인 번호) 필드에 값이 채워져있음을 알리는 플래그. 이 플래그가 0이라면 승인 번호 필드 자체가 무시된다. PSH Push 플래그. 수신 측에게 이 데이터를 최대한 빠르게 응용프로그램에게 전달해달라는 플래그이다. 이 플래그가 0이라면 수신 측은 자신의 버퍼가 다 채워질 때까지 기다린다. 즉, 이 플래그가 1이라면 이 세그먼트 이후에 더 이상 연결된 세그먼트가 없음을 의미하기도 한다. RST Reset 플래그. 이미 연결이 확립되어 ESTABLISHED 상태인 상대방에게 연결을 강제로 리셋해달라는 요청의 의미이다. SYN Synchronize 플래그. 상대방과 연결을 생성할 때, 시퀀스 번호의 동기화를 맞추기 위한 세그먼트임을 의미한다. FIN Finish 플래그. 상대방과 연결을 종료하고 싶다는 요청인 세그먼트임을 의미한다. 기존의 Reserved 필드를 사용하여 새롭게 추가된 NS, CWR, ECE 플래그는 네트워크의 명시적 혼잡통보(Explicit Congestion Notification, ECN)을 위한 플래그이다. ECN을 사용하지 않던 기존의 네트워크 혼잡 상황 인지 방법은 타임아웃을 이용한 방법이었다. 그러나 처리 속도에 민감한 어플리케이션에서는 이런 대기 시간 조차 아깝기 때문에, 송신자와 수신자에게 네트워크의 혼잡 상황을 명시적으로 알리기 위한 특별한 매커니즘이 필요하게 되었는데, 이것이 바로 ECN이다. 이때 CWR, ECE, ECT, CE 플래그를 사용하여 상대방에게 혼잡 상태를 알려줄 수 있는데, 이 중 CWR, ECE는 TCP 헤더에 존재하고 ECT, CE는 IP 헤더에 존재한다. 필드 의미 NS ECN에서 사용하는 CWR, ECE 필드가 실수나 악의적으로 은폐되는 경우를 방어하기 위해 RFC 3540에서 추가된 필드 ECE ECN Echo 플래그. 해당 필드가 1이면서, SYN 플래그가 1일 때는 ECN을 사용한다고 상대방에게 알리는 의미. SYN 플래그가 0이라면 네트워크가 혼잡하니 세그먼트 윈도우의 크기를 줄여달라는 요청의 의미이다. CWR 이미 ECE 플래그를 받아서, 전송하는 세그먼트 윈도우의 크기를 줄였다는 의미이다. ECN은 이 포스팅의 주제와는 또 다른 이야기이므로 궁금하신 분들은 MR.ZERO님의 Explict Congestion Notification? 블로그를 참고하길 바란다. Window Size 윈도우 사이즈 필드에는 한번에 전송할 수 있는 데이터의 양을 의미하는 값을 담는다. $2^{16} = 65535$ 만큼의 값을 표현할 수 있고 단위는 바이트이므로, 윈도우의 최대 크기는 64KB라는 말이 된다. 하지만 이 최대 크기는 옛날 옛적에 생긴 기준이라 요즘같이 대용량 고속 통신 환경에는 맞지 않는 경우도 있다. 그래서 비트를 왼쪽으로 시프트하는 방식으로 윈도우 사이즈의 최대 크기를 키울 수 있는 방식도 사용하고 있으며, 몇 번 시프트할 지는 옵션 필드의 WSCALE 필드를 사용하여 표기한다. Checksum 체크섬은 데이터를 송신하는 중에 발생할 수 있는 오류를 검출하기 위한 값이다. TCP의 체크섬은 전송할 데이터를 16 Bits씩 나눠서 차례대로 더해가는 방법으로 생성한다. 방식은 단순하지만 16 bits의 덧셈을 그대로 보자니 숫자가 너무 길어질 것이 뻔하므로 간단하게 반토막인 8 bits로만 한번 해보도록 하겠다. 1234 11010101+ 10110100----------- 110001001 앗, 8 bits인 두 수를 더 했더니 자리 수가 하나 올라가서 9 bits가 되었다. 이렇게 자리 수가 넘쳐버리면 체크섬 필드에 담을 수 없다. 이렇게 두 개의 수를 더했을 때 자리 수가 하나 올라간 부분을 캐리(Carry)라고 하는데, 계산 결과에서 이 부분만 떼어내서 다시 계산 결과에 더해주면 된다. 1234 10001001+ 1 (방금 해에서 넘친 부분)----------- 10001010 이런 방식을 Warp Around라고 한다. 이제 마지막 계산 결과에 1의 보수를 취해주면 체크섬이 된다. 1의 보수라고 하면 뭐지 싶겠지만 그냥 비트를 반전하면 된다. 121000101001110101 (1의 보수를 취한 모습) 이제 01110101이 이 데이터의 체크섬이 되는 것이다. 이 예제에서는 8 bits를 가지고 진행했기 때문에 8 bits짜리 체크섬이 나왔지만, 실제로는 16 bits 단위로 데이터를 잘라서 이 과정을 진행하기 때문에 16 bits인 체크섬 필드에 딱 들어맞는 이쁜 값이 나온다. 수신 측은 데이터를 받으면 위의 과정을 동일하게 거치되 1의 보수를 취하지 않은 값인 10001010까지만 만든 다음, 이 값과 송신 측이 보낸 체크섬을 더해서 모든 비트가 1이라면 이 데이터가 정상이라고 판단할 수 있다. 1234 10001010+ 01110101----------- 11111111 만약 이 값에 0이 하나라도 있으면 송신 측이 보낸 데이터에 뭔가 변조가 있었음을 알 수 있다. Urgent Pointer 말 그대로 긴급 포인터이다. URG 플래그가 1이라면 수신 측은 이 포인터가 가르키고 있는 데이터를 우선 처리한다. Options 옵션 필드는 TCP의 기능을 확장할 때 사용하는 필드들이며, 이 필드는 크기가 고정된 것이 아니라 가변적이다. 그래서 수신 측이 어디까지가 헤더고 어디서부터 데이터인지 알기 위해 위에서 설명한 데이터 오프셋 필드를 사용하는 것이다. 데이터 오프셋 필드는 20 ~ 60 bytes의 값을 표현할 수 있다고 했는데, 아무런 옵션도 사용하지 않은 헤더의 길이, 즉 Source Port 필드부터 Urgent Pointer 필드까지의 길이가 20 bytes이고, 옵션을 모두 사용했을 때 옵션 필드의 최대 길이가 40 bytes이기 때문이다. 만약 데이터 오프셋 필드의 값이 5, 즉 20 bytes보다 크지만 TCP의 옵션을 하나도 사용하고 있지 않다면, 초과한 bytes 만큼 이 필드를 0으로 채워줘야 수신 측이 헤더의 크기를 올바르게 측정할 수 있다. 대표적인 옵션으로는 윈도우 사이즈의 최대 값 표현을 확장할 수 있는 WSCALE, Selective Repeat 방식을 사용하기 위한 SACK 등이 있으며, 이외에도 거의 30개 정도의 옵션을 사용할 수 있기 때문에 이 친구들을 하나하나 설명하는 것은 조금 힘들 것 같다. 마치며이렇게 간략한 TCP의 개요와 헤더 구조에 대해서 알아보았다. 사실 이 내용들은 TCP라는 놈의 껍데기 한 겹 정도에 불과한 내용이지만, 이게 거의 50년 묵은 프로토콜이다보니 포스팅 하나로 정리하기에는 내용이 굉장히 방대하다. 서두에서 이야기 했듯이 TCP나 IP 같은 프로토콜은 소켓 프로그래밍이라도 하지 않는 이상 직접적으로 마주할 기회가 흔치 않은 것이 사실이다. 하지만 직접 마주하지 않더라도 필자는 매일 HTTP를 사용하는 웹 개발자이기 때문에, 자신이 매일 사용하는 프로토콜이 어떤 식으로 굴러가는 지 정도는 알고 있는 것이 좋다고 생각한다. TCP가 커널에 어떻게 구현되어있는지 직접 확인해보고싶은 분은 깃허브에 올라가있는 리눅스 소스인 linux/net/ipv4 안에 있는 구현체들을 통해 확인해볼 수 있다.(리눅스 소스 자체가 너무 커서 클론 받는 데 한 세월이라는 게 함정) 혹시 자신이 직접 TCP 통신 과정을 확인해보고 싶은 분은 간단한 TCP 예제 프로그램과 tcpdump, netstat 등의 유틸리티를 통해 확인해볼 수 있다. tcpdump를 클라이언와 서버가 주고 받는 패킷의 내용을 확인해보고, netstat을 사용하여 클라이언트와 서버의 TCP 상태를 확인해볼 수도 있다. 다음 포스팅에서는 TCP의 핸드쉐이크나 흐름 제어, 혼잡 제어 기법에 대해서 한번 다뤄보도록 하겠다. 이상으로 TCP의 헤더에는 어떤 정보들이 담겨있는걸까? 포스팅을 마친다.","link":"/2019/11/10/header-of-tcp/"},{"title":"불안한 마음 정면으로 마주보기","text":"지난 달, 다니고 싶었던 회사의 면접에서 기초 실력 부족으로 시원하게 박살났다. 다행히 면접이 끝난 직후 필자가 대답하지 못했던, 풀어내지 못했던 질문들과 문제를 깃허브에 정리해두었기 때문에 어떤 것을 공부해야하는지 바로 알 수 있었고, 한 달동안 컴퓨터 사이언스의 기초에 대한 내용을 집요하게 파헤쳤다. 대학생 시절 이후로 꺼내보지 않았던, 먼지 쌓인 전공 서적을 다시 펼쳐놓고 정독하기 시작했고, 자바스크립트의 기초 지식을 닥치는대로 구글링하면서 스크랩했다. 사실 작성한 포스팅들의 주제만 봐도 지난 한 달 동안 필자가 공부한 것들이 어떤 주제였는지 대충 보인다. 필자가 프라하에서 지냈던 한 달 동안 5편의 에세이 포스팅과 1편의 기술 포스팅을 작성한 것과 비교해보면 최근 한 달간의 포스팅들은 전부 기술 포스팅이다. 그것도 기초적인 내용들인 힙, JS 프로토타입, 수학으로 알고리즘 문제 풀기, 최근의 TCP 집중 분석까지… 그렇게 한 달동안 미친듯이 컴퓨터 사이언스의 기초를 공부하다가, 오늘 오전에 다른 회사의 면접 기술 과제를 만들기 위해 IDE를 켰는데 문득 이런 생각이 스쳤다. 내가 마지막으로 뭘 만들어 본게 언제였지? 지난 한 달동안 필자는 어플리케이션을 개발한 적이 없었던 것이다. 만드는 것이 너무 즐거워서 개발자가 되었건만, 정작 한 달동안이나 아무것도 만들지 않고 있었다. 필자의 지난 한 달은 기초에 대한 공부의 연속이었고, 지금까지 놓치고 있었던 것들을 다시 한번 살펴보는 복습의 시간이었다. 하지만 딱히 공부를 하면서 즐겁지는 않았던 것 같다. 아니, 사실 즐겁게 공부하려는 마음도 없었다고 말하는 게 맞을지도 모른다. 여기까지 생각이 닿고나니 갑자기 현타가 와서 잠시 공부를 접고 스스로를 돌아보는 시간을 가지려고 한다. 나는 불안했다지난 한 달 동안 필자는 현실에 쫓기고 있었다. 프라하에서 다짐했던 여유를 가지며 살자라는 마음은 서울에 도착하고 몇 주가 지나자 온데 간데 없어져버렸다. 사실 이런 불편한 감정의 근본적인 원인은 바로 불안감이다. 이런 불안감은 필자의 일상을 흑백으로 바꿔버렸다. 친구들을 만나면 순간순간은 즐거웠지만, 틈만 나면 빨리 집에 가서 바로 책과 노트북을 피고 공부해야한다는 생각이 떠올랐다. 그냥 딱 봐도 건강한 마음의 상태는 아니다. 물론 겉으로는 아닌 척하고 있지만 속에서부터 곪아가고 있는 상태인 것이다. 그래서 필자는 공부를 잠깐 멈추고, 도대체 무엇 때문에 이렇게 불안해하며 쫓기고 있는 것인지 생각해보기 시작했다. 물론 간만에 면접에서 털려보면서 필자의 약점을 날 것으로 마주하게 되었다는 이유가 크겠지만, 단순히 이것 때문에 이 정도까지 불안한 마음이 생긴다는 것은 조금 이상했다. 솔직히 면접 털려본 것이 한 두번도 아니니 말이다. 그렇게 하루 정도 깊게 고민을 해보고나니 몇 가지 불안 요인이 정리가 되었고, 이후 필자는 이 요인들을 리스트업하고 하나씩 살펴보기 시작했다. 현실과 이상의 괴리어릴 때 부모님이나 어른들이 했던 이야기 중에서 가장 이해되지 않았던 말은 하고 싶은 것만 하면서 살 수는 없다였다. (물론 여기에는 반드시 “공부해라”라는 말이 따라온다) 시간이 지나 군대를 다녀오고 대학교에 복학하면서 당시 어른들이 했던 이 말의 의미를 조금씩 이해하게 되었다. 세상 만사 내 맘대로 되는 일만 있는 것은 아니니 어느 정도는 마음을 내려놓으라는 가르침이었으리라. 죽어도 공부하기 싫은 날에도 장학금을 위해 밤새워가며 공부를 해야했고, 군대에 가기 싫었지만 국방의 의무 때문에 23개월동안 구르다 왔던 경험을 하고 나니 음, 그 말이 바로 이런 느낌이군이라는 생각을 했던 것 같다. 그래도 하기 싫은 일을 돈 때문에 억지로 하면서 평생 살고 싶지는 않았기에, 내 인생의 행복을 위해서는 하고 싶은 일을 가장 우선 순위로 높게 잡고 있었고, 다행히도 개발자라는 직업을 가지게 되면서 사회의 일원으로써의 자리와 직장인으로써의 안정적인 수입, 그리고 내가 즐기며 할 수 있는 일까지 거머쥔 덕업일치의 생활을 할 수 있었다. 그러나 개발자로 일을 하게되면 행복하기만 할 것이라는 필자의 생각과는 달리 개발자로의 삶은 치열했다. 이제부터는 시험 등수나 장학금을 위한 경쟁이 아닌 진짜 생존을 건 경쟁이 시작된 것이다. 아직 성 차별, 학력 차별 등 여러가지 문제가 산재하기는 하지만 기본적으로 2019년의 대한민국은 모든 사람에게 평등한 기회를 주려고 노력하는 사회이고, 그런 기회는 보통 개인의 능력이나 실력으로 거머쥘 수 있다. 그 중에서도 특히 IT업계의 대표적인 전문직인 개발자는 진짜 실력 하나로 비벼서 먹고 살아가는 실력몰빵주의라고 할 수 있다. 도태되지 않기 위해서는 끊임없이 공부를 하면서 빠르게 변화하는 기술 트렌드나 패러다임을 따라가야하는 것이다. 그래서 개발자들 사이에서는 치킨 집을 차리기 전까지는 계속 공부해야한다는 자조섞인 농담을 나누기도 한다. 그래도 필자는 프로그래밍 공부를 한다는 것 자체가 너무 재밌고 좋았다. 애초에 내가 하고 싶은 일을 선택한 것이기도 하고 공부하면 할수록 내가 만든 어플리케이션이 더 좋은 구조, 좋은 성능을 가질 수 있으니까. 하지만 앞서 이야기 했듯이, 최근 한 달 동안 컴퓨터 사이언스의 기초를 공부하면서 필자는 마냥 즐겁지만은 않았다. 사실 필자가 즐겁지 않은 공부를 하며 가장 의아했던 부분은 이렇게 기초를 공부하는 과정은 이미 대학생 때 한번 겪었던 경험인데 그때 당시와 지금 느끼는 감정이 너무나도 다르다는 것이었다. 당시에는 똑같은 공부를 해도 하나하나 신기하고 너무 재미있었기 때문이다. 그럼 대학생 때는 그렇게 재미있었던 내용들이 왜 지금와서는 재미가 없어진 것일까? 그 이유는 바로 공부의 동기 때문이다. 개발자가 공부로 살아남는 방법 포스팅에서도 한 번 이야기한 적이 있지만, 사실 공부라는 것은 주제의 난이도보다도 건강한 동기 부여가 더 중요하다. 하지만 필자는 현재 자의적으로 컴퓨터 사이언스 기초에 대한 공부를 하는 것이 아니라 구직 중이라는 외부 상황에 떠밀려서 공부를 하고 있는 상황이다. 더 쉽게 이야기해보자면, 필자가 원하는 주제를 필요에 따라 공부하는 것이 아니라 당장 하고 싶지 않은데 외부 상황 때문에 억지로 하고 있는 공부인 것이다. 어차피 나중에 공부할건데 엄마가 지금 공부하라고 하면 왠지 더 하기 싫어지는 마음이랄까 사실 공부할 주제를 선택할 때 이걸 공부하면 내가 어떤 점이 더 나아질까?라는 생각으로 두근거려야 하는데, 최근 한 달간의 공부는 이걸 면접에서 물어볼까?라는 마음으로 주제를 선택했으니 재미있을리가 만무하다. 그래도 어릴 적 어른들이 이야기했던 하고 싶은 것만 하면서 살 수는 없다는 말처럼 필자가 면접에 합격하기 위해서 부족한 부분을 채우는 과정이 필요하다는 것 또한 분명한 사실이다. 필자는 일을 재미있게 하고 싶어서 개발자라는 직업을 선택했는데, 현실적인 상황 때문에 재미없게 공부를 하고 있는 이 상황에 대해서 괴리감과 불편함을 느끼기 시작한 것이다. 스스로에 대한 실망감사실 지금까지 필자는 별 생각없이 회사를 선택해왔었다. 재밌게만 일할 수 있으면 됐지, 어디서 일하느냐가 그렇게 중요하지 않다고 생각했기 때문이다. 그런 식으로 직장을 고르다보니까 스스로 좀 더 많은 권한을 부여받고 많은 일을 할 수 있는 작은 규모의 스타트업을 주로 다니게 되었다. 그러나 최근 주변에 있는 많은 개발자분들이 작은 곳도 경험해보고 큰 곳도 경험해봐라라는 조언을 많이 해주셨다. 조금 더 다양한 경험을 해보고 성장하라는 의미인 것이다. 그런 이유로 이번에는 지금까지 다녔던 직장보다는 조금 더 규모가 있는 곳을 목표로 잡고 있다. 그러나 큰 회사들의 경우, 채용 과정과 원하는 인재상까지 필자가 지금까지 다녔던 규모의 회사들과는 조금 다르다. 규모가 작은 스타트업의 경우 말 그대로 일당백의 당장 일할 수 있는 개발자가 필요하다. 만들어야하는 프로덕트는 산더미같은데 리소스는 늘 부족하다보니 한 사람 한 사람이 많은 역할을 할 수 밖에 없는 것이다. 게다가 리소스가 부족하다보니 누군가를 체계적으로 교육하거나 스스로 학습할 수 있는 여유도 큰 회사에 비해 상대적으로 부족한 경우가 많다. 이에 반해 큰 회사들은 당장 입사해서 일할 수 있는 개발자를 급하게 찾는 입장은 아니다. 물론 어떤 회사던지 개발자라는 자원은 많으면 많을수록 좋기 때문에 계속 채용을 하기는 하지만, 규모가 작은 스타트업에 비교해봤을 때 당장 일할 사람이 부족해서 허덕이고 있는 정도의 상황은 아닐 가능성이 높다. 게다가 시스템의 규모 또한 해당 개발팀의 규모에 비례해서 늘어날 수 있기 때문에, 이미 많은 개발자를 보유하고 있는 회사의 시스템은 크기도 방대하고 구조도 복잡한 경우가 많다. 이런 이유들로 인해 규모가 큰 곳일수록 특정 프레임워크에 익숙한지 물어보기보다는 니가 이걸 진짜 제대로 알고 쓰는거니?에 집중하는 경우가 많기 때문에 컴퓨터 사이언스나 사용하는 언어에 대한 기초를 깊게 물어보는 것이다. 필자도 당연히 이런 사실들을 알고는 있었지만, 한국에 돌아오고 처음 면접을 볼 때까지만 해도 솔직히 별 걱정이 없었다. 그냥 붙으면 다니고 아니면 다른 데 가지 뭐 정도의 마음이었던 것 같다. 실제로 첫 면접에 떨어진 직후에는 불안한 마음이고 뭐고 멘탈에 별 타격이 없었다. 그래도 면접에 떨어졌으니 필자가 현재 부족한 것이 무엇인지 알 수 있었고, 당연히 이 부분을 보완하고 싶다는 마음이 생기게 되었다. 하지만 이상하게도 공부를 하면 할수록 뭔가 마음이 싱숭생숭해지기 시작했다. 얼마 전 SNS에서 여친이랑 헤어진 남자의 심리 변화라는 짤을 본 적이 있다. 여친이랑 헤어진 직후는 후련하고 자유로운 기분이지만 시간이 지나면서 그리운 감정이 커지게되고 후회한다는 그런 내용이었다. 뭔가 그런 느낌이랄까. 공부를 하면 할수록 면접장에서 제대로 대답하지 못했던 상황들이 계속 떠올랐고, 뒤늦은 부끄러움과 후회가 밀려온 것이다. 그와 동시에 스스로에 대한 실망감도 들기 시작했다. 솔직히 말하자면 나름 컴퓨터 사이언스에 대해서 알고 있다고 생각했다. 학교에서도 이미 한 번씩 배운 내용들이고, 나름 이것저것 분석해보면서 쌓은 지식들도 있었기 때문이다. 그러나 현실은 달랐다. 막상 면접장에 들어서고나서 GC에 대해서 설명해주세요와 같은 질문을 들으면 머리가 하얘졌다. 이런 질문을 들을 때 마다 필자의 머릿 속에는 수많은 개념들과 그림들이 동시에 떠올랐지만 정작 이걸 말로 정리해서 말하지 못했다. 즉, 제대로 알고 있지 않은 것이다. 이런 경험을 몇 번 하고나니 지금까지 내가 알고있다고 생각했던 지식들이 사실은 제대로 알고 있는 것이 아니었다는 것을 깨닿게 되었고, 지금까지 헛공부한 게 아닌가라는 생각, 다른 개발자들에 비해서 내가 뒤쳐질 수도 있다는 불안감이 스물스물 피어나게 되었다. 주목당하는 게 부담스럽다아이러니하게도 지금 글을 작성하고 있는 바로 이 블로그도 필자의 불안감의 요인 중 하나이다. 정확히 말하면 불안정한 필자의 심리 상태에 불을 붙혀서 불안감을 증폭시킨 요인이라고 할 수 있겠다. 지금까지 필자는 포스팅을 적고 다른 사람들과 내 지식과 생각을 공유하기 위한 목적으로 약간의 홍보를 통해 포스팅을 공유해왔다. 그러다보니 필자의 글을 재밌게 읽어주신 분들이 포스팅 잘 읽었다, 좋은 정보 공유 감사하다 정도의 말씀을 해주시는 경우가 생기게 되었는데, 이때까지는 내 지식이 다른 사람들에게 도움이 될 수 있다는 사실에 그냥 기분이 좋았다. 하지만 최근 오프라인에서 필자를 알아보는 사람이 나타나는 경험을 몇 번 겪으면서 약간의 부담감이 스물스물 피어나게 되었다. (이럴거라고는 진짜 상상도 못 했다) 이 감정이 최대로 증폭되는 경우는 바로 면접관이 평소에 블로그 잘 보고 있어요라고 하는 경우인데, 이때의 당혹감과 부담감이란 말로 표현할 수가 없다. 긴장을 안하고 있다가도 한 순간에 긴장하게 만들어버리는 마법의 단어인 것 같다. 그냥 사람들이 알아보는 것이 부담스럽다는 뜻은 아니다. 사람들이 필자를 알아보기 시작하면서 조금씩 이 사람들이 날 어떻게 생각할까?를 신경쓰기 시작했다는 것이 문제다. 사실 필자가 블로그 포스팅을 작성하는 이유는 필자의 지식을 타인에게 공유하고 싶다는 목적도 있지만 기본적으로는 공부한 내용을 정리하려는 목적이 크다. 자신의 지식을 글로 정리하는 과정에서 공부의 효율성을 높히고 기억에 오래 남는 효과를 기대하는 것이다. 하지만 필자가 엄청 뛰어난 기억력을 가지고 있지 않은 이상, 몇 번 공부한 내용이라고 한들 실제로 그 지식을 사용하지않고 오랜 기간이 지난다면 기억에서 점점 잊혀지기 마련이다. 결국 뭐가 되었든 공부의 기본은 반복 학습이고, 한 번 포스팅을 작성했다고 해서 그 지식이 온전히 필자의 것이 되는 게 아니라는 말이다. 그런 이유 때문에 필자는 계속 스스로 과거에 작성했던 포스팅을 읽어보면서 되새김질하기도 한다. 그러나 글을 읽는 독자 입장에서는 포스팅을 작성한 사람이 이 지식을 완벽하게 알고 있다는 생각이 들 수도 있다. 필자만 해도 다른 개발자 분들의 블로그 포스팅을 읽어보면서 막연하게 블로그 포스팅을 잘 쓰는 사람과 실력이 좋은 사람을 동일시했었다. 이 내용에 대해서 빠삭하게 잘 아니까 이렇게 글도 잘 쓰겠지라는 생각 때문이었다. 하지만 글을 잘 쓰고 정리를 잘 한다는 것이 필자의 프로그래밍 실력이 좋다는 것을 의미하진 않는다. 독자들이 생각하고 기대하는 것보다 필자의 실력이 만족스럽지 않을 수도 있다는 것이다. 필자가 느끼는 부담감은 이런 상황에서 비롯된 것이었다. 다른 사람이 에반님 그때 정리해주신 그 내용 말이에요~라고 운을 떼는 순간, 이거 대답 못하면 어쩌지?라는 생각이 들 게 된다. 특히 그 사람이 면접관이라면 더 그렇다. 또한 간혹 고수, 존경과 같은 과분한 말을 해주시는 경우도 있는데, 애초에 필자는 고수도 아니고 누군가에게 존경받을 만한 사람은 더더욱 아닌, 그저 일개 4년차 꼬꼬마 개발자이기 때문에 감사한 마음과 동시에 약간의 부담감이 생기기도 햇다. 그래서 이런 상황들을 겪을 때마다 이런 생각이 들기 시작했다. 내 실력은 3 정도 밖에 안 되는데, 사람들이 내 실력을 6이나 7 정도로 생각하면 어떡하지? 내가 쓴 포스팅에 대한 질문에 제대로 답변하지 못하면 어떡하지? 난 존경받을만한 사람도 아니고 고수도 아닌데… 내가 이런 말을 들을 자격이 되는건가…? 그런 이유로 언젠가부터 점점 사람들이 생각하는 나라는 껍데기를 만들어야한다는 강박관념이 스물스물 피어나기 시작했고, 그 와중에 면접에서 거하게 털리면서 이 불안감 또한 함께 증폭된 것이다. 결국은 마인드의 문제다사실 이런 불안한 마음들은 사람이라면 자연스럽게 생길 수 있는 마음이다. 그러나 이런 감정들은 막연하게 불안하다 정도의 느낌이라서, 명확하게 무엇 때문에 내가 불안하다라는 생각까지는 평소에 잘 생각하지 않는다. 하지만 자신이 불안한 이유를 확실히 파악하지 못하면 그 문제를 해결할 수 없다. 그래서 필자는 불안함의 원인에 대해서 리스트업을 하면서 이 문제들과 정면으로 마주한 것이다. 필자는 스스로 불안감의 요인에 대한 원인을 정리하고 리스트업하면서, 자연스럽게 이 문제에 대해 스스로 질문하고 답변하는 과정을 통해 나름의 해결 방법들을 정의할 수 있었다. 동기 부여를 확실히 하자앞서 이야기했듯이 필자가 컴퓨터 사이언스 기초 공부를 하며 재미없다고 느끼게 된 이유는 하기 싫어도 해야한다라는 마음 때문이다. 물론 현실적으로 보면 틀린 말은 아니다. 면접에 합격하기 위해서는 필자의 약점을 보완해야하니 말이다. 그러나 이번 한 번만 공부하고 말 것이 아니라면 이런 식의 동기 부여는 좋지 않다. 외압에 의한 동기 부여는 잠깐 동안 스스로를 몰아 세우며 빠르게 성장할 수는 있어도, 지속적인 성장에는 오히려 걸림돌이 된다. 사실 근본적으로 생각해보면 대학생 때나 지금이나 공부하고 있는 내용은 크게 다르지 않다. 하지만 그때는 기초 공부가 재미있었고 지금은 아니었던 이유가 바로 여기에 있다. 당시에는 내가 이걸 공부함으로써 성장할 수 있다는 사실이 동기가 되었지만, 지금은 면접에 붙어야한다는 사실이 동기가 되었기 때문에 같은 것을 공부해도 상당히 다른 느낌을 받을 수 밖에 없다는 것이다. 그래서 필자는 다시 초심으로 돌아가 성장에 초점을 맞춰 보려고 한다. 비록 빡센 기초 공부의 시작은 면접 때문이었을지 몰라도, 결과적으로는 필자가 성장할 수 있는 좋은 기회이기도 하기 때문이다. 면접은 그냥 면접일 뿐사실 기껏해야 한 두시간의 짧은 면접 시간 동안 한 사람의 역량을 오롯히 파악하는 것 자체가 굉장히 어려운 일이다. 그래서 대부분의 회사는 1차 기술, 2차 임원 등으로 면접 전형을 나눠서 평가하고 있지만, 이렇게 하더라도 컴퓨터 사이언스라는 학문의 범위가 너무 넓기 때문에 진짜 그 사람의 모든 강점과 약점을 파악한다는 것은 거의 불가능하다. 하지만 회사든 구직자든 면접 한번 보자고 서로 며칠 씩이나 붙어있을 수는 없는 노릇이니 최대한 짧은 시간 안에 객관적이고 효율적으로 실력을 평가할 수 있는 방법이나 질문을 통해 구직자를 평가하려고 노력하는 것이다. 당연히 이러한 사실은 회사도 알고 있기 때문에 면접관 스스로 혹은 회사의 가이드대로 어느 정도 객관성을 띄고 있는 질문을 준비하기는 하지만, 이 질문이 구직자의 강점이나 약점을 얼마나 캐낼 수 있는지는 며느리도 모르는 일이다. 그 말인 즉슨, 면접의 평가 결과는 어느 정도 단편성을 지니고 있을 수 밖에 없다는 것이다. 하지만 면접 과정에서 구직자가 면접관이 물어본 질문에 대답을 못했을 경우, 그 질문이 구직자의 약점을 관통했다는 것 자체는 물론 사실이다. 그래서 필자도 질문에 대답하지 못했던 내용을 정리하고 공부했던 것이다. 그러나 약점이 존재한다는 것이 실력없고 형편없다는 의미는 아니다. 약점은 보완하면 되는 것이고, 면접은 무슨 수능처럼 1년에 한 번만 볼 수 있는 것도 아니니까 다음 기회에 다시 도전하면 된다. 필자의 보컬 선생님 말을 빌리자면, 오히려 그런 기회를 통해 자신의 약점을 발견할 수 있음에 감사하라고 한다. 이런 기회조차 없는 상황이라면 오히려 더 성장하기 힘들 것이라는 이야기도 들었다. 이게 그저 정신승리라고 생각하는 사람도 있겠지만, 스스로 건강한 마음을 가질 수 있어야만 꾸준한 도전이 가능하기 때문에 이런 식의 멘탈 케어는 개인의 성장이나 발전에도 도움이 된다고 생각한다. 중요한 것은 면접에 떨어졌다는 것이 아니라, 면접을 통해 자신의 약점을 발견하고 올바른 동기 부여를 통해 꾸준한 공부를 할 수 있는 원동력을 확보하는 것이기 때문이다. 블로그는 그냥 꾸준히 쓰자필자는 애초에 자기 자신에게 후한 평가를 주는 성격은 아니다. 그냥 이건 필자 본연의 성격이기 때문에 개발자가 아니라 그냥 한 명의 인간으로써도 그렇다. 그래서 필자의 글을 읽은 분들이 칭찬과 격려의 의미로 해주시는 말들도 잘 받아들이지 못했던 것 같다. (칭찬을 잘 안 받아들이는 타입이다) 사실 앞서 이야기했던 두 가지 케이스는 나름 혼자 생각해보면서 결론을 내렸지만, 이 문제만은 아직 명확한 결론을 내지 못한 상태이다. 그냥 다들 큰 의미없이 좋은 뜻으로 하는 말이니까 듣고 흘리라는 조언도 듣긴 했지만, 성격이 성격인지라 그러기도 쉽지 않기 때문이다. 그렇다고 블로그 작성을 그만 둘거냐면 그것도 아니다. 블로그 포스팅 작성은 공부와 공유의 목적도 있지만 그 전에 필자의 취미 생활이기도 하기 때문이다. 그래서 처음에는 포스팅은 꾸준히 쓰되 홍보를 하지말까라는 생각도 했었지만, 뭔가 명확한 결론이 나기 전까지는 그냥 하던대로 하는 것이 좋겠다는 결론을 내렸다. 사실 이건 필자 스스로 느끼는 부담감이고, 이것 외에는 블로그 포스팅 작성이 필자에게 득이 되었으면 되었지 실이 될 만한 것은 없기 때문이다. 그래서 이런 저런 생각보다는 글쓰기라는 행위 자체에만 집중하려고 한다. 그래서 지금도 아무 생각없이 일단 글을 쓰고 있다. 그리고 이런 이유 때문에 생기는 불안한 마음은 필자가 꾸준히 공부해서 점점 성장하고 자신감이 많이 붙게 된다면 자연스럽게 흐려질 것이라고 생각한다. 이런 과정 또한 필자가 성장하는 과정이고 컴포트 존을 벗어날 수 있는 좋은 기회일테니, 정면으로 부딫혀서 극복해봐야 하는 것 아닐까싶다. 일단 지금은 그냥 필자에게 관심을 가지고 지켜봐주시는 분들이 있다는 것에 감사하며 꾸준히 글이나 끄적이는 게 최선인 것 같다. 마치며이번 포스팅은 어떻게 보면 실로 오랜만에 적어본, 남들을 위한 글이 아닌 나를 위한 글이었다. 글을 적으면서 생각을 정리하다보니 불안했던 마음도 어느 정도 사그러드는듯 하다. 비단 필자 뿐만 아니라, 필자와 비슷한 상황에 있는 다른 사람들도 비슷한 감정을 느낄 것이라고 생각한다. 일 안하고 하고 싶은 거 하면서 사는 백수 생활이라는 것이 물론 편하긴 하지만 그에 따른 부담감과 불안함도 당연히 따라오는 것이니 말이다. 게다가 이런 상태에서 발생한 불안감은 사소한 일상의 좌절만으로도 쉽게 증폭되기 때문에, 더욱 더 이런 멘탈 관리가 중요한 것 같다. 그래도 이렇게 글로 생각을 정리하는 과정을 거치면서 필자가 왜 불안한 마음이 드는 지 확실히 알 수 있었고, 이에 따른 나름의 해결책 또한 정의해볼 수 있었다. 이런 게 바로 글이 가진 매력이 아닐까. 물론 완전히 떨쳐내지는 못할 것이다. 사람인 이상 불확실한 미래를 걱정하며 불안한 감정이 생기는 것은 자연스러운 것이니까. 하지만 막연하게 불안함에 떨고 쫓기면서 하기 싫은 공부를 억지로 하거나 부담감을 가지고 포스팅을 작성하는 상황보다는 내가 이 공부를 함으로써 어떤 방향으로 성장할 수 있을 지 확실히 인지하고, 내가 작성한 글들이 타인에게 좋은 영향을 줄 수 있을거란 믿음을 가지는 것이 훨씬 건강한 마인드라는 것은 자명하다. 면접 떨어진 건 아쉽긴 하지만, 부족한 실력은 공부해서 채우면 된다. 어차피 백수니까 공부할 시간도 남아돈다. 사실 이 시간 또한 언젠가 다시 직장인이 되면 그리워질 시간일테니, 최대한 마음 편하게 즐겨보려고 한다. 이렇게 불안한 마음을 다스릴 수 있는 글쓰기라는 취미 생활을 가지고 있다는 것도 어찌보면 행복일지도.","link":"/2019/11/16/the-way-to-control-anxiety/"},{"title":"사이 좋게 네트워크를 나눠 쓰는 방법, TCP의 혼잡 제어","text":"혼잡 제어란, 말 그대로 네트워크의 혼잡 상태를 파악하고 그 상태를 해결하기 위해 데이터 전송을 제어하는 것을 이야기한다. 네트워크는 워낙 광대한 블랙박스이기 때문에 정확히 어디서 어떤 이유로 전송이 느려지는지는 파악하기 힘들지만, 단순히 느려지고있다 정도는 각 종단에서도 충분히 파악할 수 있다. 그냥 데이터를 보냈는데 상대방으로부터 응답이 늦게 오거나 안오면 뭔가 문제가 있다는 것이니 말이다. 이때 위에서 이야기한 흐름 제어나 오류 제어 기법들만을 사용하다보면 자연스럽게 재전송이라는 작업이 계속 반복될 수 밖에 없다. 이게 한 두 녀석이 그러면 별 문제가 안될지도 모르지만, 네트워크는 워낙 다양한 친구들이 함께 이용하는 공간이다보니까 한번 네트워크가 뻑나기 시작하면 여기저기서 나도 재전송할꺼야!가 반복되면서 문제가 점점 악화될 것이다. 이를 네트워크의 혼잡 붕괴라고 부른다. 그래서 이런 식으로 네트워크의 혼잡 상태가 감지되면, 이런 최악의 상황을 최대한 회피하기 위해 송신 측의 윈도우 크기를 조절하여 데이터의 전송량을 강제적으로 줄이게 되는데, 이것이 바로 혼잡 제어인 것이다. 혼잡 윈도우(Congestion Window, CWND)필자는 패킷의 흐름과 오류를 제어하는 TCP 포스팅에서 TCP의 흐름 제어를 설명할 때 송신 측의 윈도우 크기는 수신 측이 보내준 윈도우 크기와 네트워크 상황을 함께 고려해서 정해진다는 이야기를 했었다. 송신 측은 자신의 최종 윈도우 크기를 정할 때 수신 측이 보내준 윈도우 크기인 수신자 윈도우(RWND), 그리고 자신이 네트워크의 상황을 고려해서 정한 윈도우 크기인 혼잡 윈도우(CWND) 중에서 더 작은 값을 사용한다. 즉, 아래 후술할 혼잡 제어 기법들이 늘였다 줄였다 하는 윈도우 크기는 송신 윈도우가 아니라 송신 측이 가지고 있는 혼잡 윈도우 크기인 것이다. 참고로 RWND나 CWND가 그냥 윈도우라는 의미를 가지고 있기 때문에, 슬라이딩 윈도우에서 사용하는 윈도우와 같은 개념이라고 생각할 수 있는데, 얘네는 각각 수신자 윈도우 크기와 혼잡 윈도우 크기를 의미하는 숫자다. 그럼 통신 중에는 어떻게든 네트워크의 혼잡도를 파악해서 혼잡 윈도우 크기를 유연하게 변경한다고 해도, 통신을 시작하기 전에는 이 혼잡 윈도우 크기를 어떻게 초기화하는 것일까? 혼잡 윈도우 크기 초기화하기통신을 하는 중간에는 ACK가 유실된다거나 타임아웃이 난다거나 하는 등의 정보를 사용하여 네트워크의 혼잡 상황을 유추할 수 있지만, 통신을 시작하기 전에는 그런 정보가 하나도 없기 때문에 혼잡 윈도우의 크기를 정하기가 조금 애매하다. 여기서 등장하는 것이 바로 MSS(Maximum Segement Size)이다. MSS는 한 세그먼트에 최대로 보낼 수 있는 데이터의 양을 나타내는 값인데, 대략 다음과 같은 계산을 통해 구할 수 있다. MSS = MTU - (IP헤더길이 + IP옵션길이) - (TCP헤더길이 + TCP옵션길이) 여기서 등장하는 MTU(Maximum Transmission Unit)라는 친구는 한번 통신 때 보낼 수 있는 최대 단위를 의미한다. 즉, MSS는 한번 전송할 때 보낼 수 있는 최대 단위가 정해져있는 상황에서 IP 헤더, TCP 헤더 등 데이터가 아닌 부분을 전부 발라내고 진짜 데이터를 담을 수 있는 공간이 얼마나 남았는지를 나타내는 것이다. 시스템 환경 설정에서 잘 찾아보면 기본 MTU를 변경할 수 있는 설정이 있다 OSX 같은 경우는 MTU 기본 값으로 이더넷 표준인 1500 bytes가 설정되어있다. 이때 TCP와 IP의 헤더크기가 각각 20 bytes라고 하면 MSS는 1500 - 40 = 1460 bytes가 되는 것이다. 송신 측은 처음 통신을 시작할 때 이렇게 계산한 MSS를 사용하여 혼잡 윈도우의 크기를 1 MSS로 설정한다. 이후 통신을 하면서 네트워크의 혼잡 상황을 고려하며 혼잡 윈도우 크기를 증가시키거나 감소시키는 것이다. 혼잡 회피 방법할배 프로토콜인 TCP는 지난 50년 동안 지속적으로 개선된 다양한 혼잡 제어 정책들을 가지고 있다. 각 혼잡 제어 정책은 어떤 시점을 혼잡한 상태라고 파악할 것인지, 혼잡 윈도우 크기를 줄이거나 키우는 방법을 개선하여 점점 발전해왔지만, 가장 기본적인 혼잡 제어 방법은AIMD와 Slow Start라는 혼잡 회피 방법을 상황에 맞게 조합하는 것이다. 그래서 이 포스팅에서도 기본적인 혼잡 회피 방법인 AIMD와 Slow Start를 먼저 설명하고나서 대표적인 혼잡 제어 정책인 Tahoe와 Reno를 이야기할 것이다. AIMDAIMD(Additive Increase / Multicative Decrease) 방식은 우리 말로 직역하면 합 증가 / 곱 감소 방식이라는 뜻이다. 즉, 네트워크에 아직 별 문제가 없어서 전송 속도를 더 빠르게 하고 싶다면 혼잡 윈도우 크기를 1씩 증가시키지만, 중간에 데이터가 유실되거나 응답이 오지 않는 등의 혼잡 상태가 감지되면 혼잡 윈도우 크기를 반으로 줄인다. 늘어날 때는 ws + 1, 줄어들 때는 ws * 0.5이므로 말 그대로 합 증가 / 곱 감소인 것이다. 이렇게 늘어날 때는 선형적으로 조금씩 늘어나고 줄어들 때는 반으로 확 줄어드는 AIMD 방식의 특성 상, 이 방식을 사용하는 연결의 혼잡 윈도우 크기를 그래프로 그려보면 다음과 같은 톱니 모양이 나타난다. 이 방식은 굉장히 심플하지만 생각보다 공평한 방식이다. 예를 들어 여러 친구들이 이미 네트워크를 점유하고 있는 상태에서 한 친구가 뒤늦게 이 네트워크에 합류했다고 생각해보자. 당연히 나중에 진입한 쪽의 혼잡 윈도우 크기가 작기 때문에 처음에는 불리하다. 그러나 네트워크가 혼잡해지면 혼잡 윈도우 크기가 작은 놈보다 혼잡 윈도우 크기가 큰 놈이 무리하게 데이터를 왕창 보내려다가 유실될 확률도 더 크다. 이런 상황이라면 네트워크에 일찍 참여해서 이미 혼잡 윈도우 크기가 큰 놈은 자신의 윈도우 크기를 줄여서 혼잡 상황을 해결하려고 할 것이고, 이때 남은 대역폭을 활용하여 나중에 들어온 놈들이 자신의 혼잡 윈도우 크기를 키울 수 있는 것이다. 그런 이유로 시간이 가면 갈수록 네트워크에 참여한 순서와 관계 없이 모든 호스트들의 윈도우 크기가 평행 상태로 수렴하게 되는 것이다. 그러나 AIMD의 문제점은 네트워크 대역이 펑펑 남아도는 상황에도 윈도우 크기를 너무 조금씩 늘리면서 접근한다는 것이다. 그런 이유로 AIMD 방식은 네트워크의 모든 대역을 활용하여 제대로 된 속도로 통신하기까지 시간이 조금 걸린다. 윈도우 크기가 수렴한다고?사실 AIMD는 워낙 단순한 방식이다 보니 간단한 예제를 통해 네트워크의 상황을 재현해볼 수도 있다. 이렇게 직접 구현해보면 말로만 들었을 때는 애매한 호스트들의 윈도우 크기가 평행으로 수렴한다는 것이 어떤 의미인지 직접 눈으로 확인해볼 수도 있다. 코드를 직접 포스팅에 첨부하려고 했는데, 최대한 실제 네트워크와 비슷한 환경을 재현하려고 하다보니 코드가 생각보다 길어져서 결과만 첨부하려고 한다. 우선 필자의 실험 방식은 다음과 같다. 네트워크의 혼잡도 최고치는 50이며, 이 혼잡도는 네트워크에 참여한 호스트들이 가진 혼잡 윈도우 크기의 총합으로 결정된다. 네트워크에 호스트를 300ms에 하나씩 생성해서 추가한다. 각 호스트는 100 ~ 200ms 마다 네트워크의 현재 혼잡도를 계산하고 자신의 윈도우 크기를 조절한다. 각 호스트는 윈도우 크기 조절을 300회까지 수행하고 네트워크를 빠져나간다. 참고로 이 테스트에는 약간의 허점이 있다. 필자는 호스트들의 연산에 동시성을 부여하기 위해 setInterval을 사용했는데, setInterval의 콜백도 이벤트 루프를 거친 후 결국 콜 스택을 쌓아가며 수행되기 때문에 실제 네트워크처럼 호스트들의 연산에 완전한 병렬성이 보장되지는 않는다. 그래서 테스트의 목적인 네트워크에 늦게 참여한 호스트들도 충분한 혼잡 윈도우 크기를 가질 수 있는지 확인하는 용도로는 딱히 별 문제가 없기 때문에 그대로 진행했다. 이에 대한 자세한 개념은 로우 레벨로 살펴보는 Node.js 이벤트 루프에 자세히 설명되어있으니 확인해보도록 하자. 어쨌든 이렇게 테스트를 해보고 나온 데이터들을 시계열 그래프로 시각화해보았다. 먼저 각 호스트들의 혼잡 윈도우 크기 변화를 살펴보자. 그래프를 살펴보니 먼저 네트워크에 들어와있던 호스트들과 뒤늦게 네트워크에 합류한 호스트들의 혼잡 윈도우 크기에 그렇게 큰 차이가 없는 모습을 확인할 수 있었다. 마지막에 혼자 치솟고 있는 친구는 다른 호스트들이 다 빠져나간 후에 네트워크가 텅텅 비어서 그런 것이다. 이런 식으로 시각화를 해보니 각 호스트들의 혼잡 윈도우 크기가 큰 차이 없이 어느 부분에 수렴하고 있다는 것을 확인할 수 있었다. 그렇다면 네트워크에 들어와있는 호스트들의 전체 윈도우 크기는 시계열 그래프로 시각화해보면 어떨까? 확실히 네트워크 내 전체 호스트의 혼잡 윈도우 크기를 보니 필자가 설정해놓은 네트워크 혼잡도인 50을 넘어가지 않는 선에서 자기들끼리 엎치락뒤치락 윈도우 크기를 조절하고 있는 모습을 확인할 수 있었다. 마찬가지로 이 그래프를 확인해보아도 호스트들이 네트워크에 입퇴장하는 초반부와 후반부를 제외하면 중간 즈음에서는 각 호스트들의 혼잡 윈도우 크기가 크게 차이나지 않는다. 이렇게 직접 상황을 시뮬레이션해보고 데이터를 확인하니 말로만 들었을 때는 잘 와닿지 않았던 윈도우 크기가 평행 상태로 수렴한다는 이야기를 조금 더 잘 이해할 수 있었다. 만약 이 코드를 직접 실행시켜서 모든 과정을 확인하고 싶은 분들은 필자가 올려놓은 깃허브 레파지토리에서 클론받아서 실행시켜보면 된다. Slow Start위에서 이야기했듯이 AIMD 방식은 윈도우 크기를 선형적으로 증가시키기 때문에, 제대로 된 속도가 나오기까지 시간이 조금 걸리는 단점이 있다. 사실 요즘에는 네트워크의 대역폭이 워낙 넓고 통신 인프라도 좋다보니 예전에 비해서 네트워크의 혼잡 상황 발생하는 빈도가 많이 줄어들었기 때문에, 혼잡이 발생하지도 않았는데 제대로 속도를 내는데까지 오래걸리는 AIMD 방식의 단점이 점점 부각되었다. 반면, Slow Start는 기본적인 원리는 AIMD와 비슷하지만 윈도우 크기를 증가시킬 때는 지수적으로 증가시키다가 혼잡이 감지되면 윈도우 크기를 1로 줄여버리는 방식이다. 이 방식은 보낸 데이터의 ACK가 도착할 때마다 윈도우 크기를 증가시키기 때문에 처음에는 윈도우 크기가 조금 느리게 증가할지 몰라도, 시간이 가면 갈수록 윈도우 크기가 점점 빠르게 증가한다는 장점이 있다. 기본적으로 AIMD와 어떤 방식으로 윈도우 크기를 증가시키냐, 감소시키냐의 차이만 존재하기 때문에, 위에서 작성한 예시에서 윈도우 사이즈를 변경하는 부분만 변경하면 Slow Start의 차트를 그려볼 수 있다. 데이터를 확인해보면 필자가 정해놓은 네트워크 혼잡도의 최고치인 50보다 호스트들의 혼잡 윈도우 크기가 커지는 경우가 발생하는데, 이건 자바스크립트가 setInterval의 콜백을 처리하는 과정에서 발생한 문제이므로 실제 네트워크에서는 이러지 않는다. (애초에 혼잡도를 이렇게 간단히 계산하지도 않는다) AIMD와 Slow Start 그래프의 가장 큰 차이점은 네트워크 혼잡도의 최고치에 도달하는 시간이다. AIMD는 윈도우 크기를 선형적으로 키워나가기 때문에 다른 호스트들이 새로 들어올 때 많은 부분을 점유할 수 없었지만, Slow Start 방식을 사용하면 처음에는 윈도우 크기를 느리게 키우다가 아직 여유가 있다고 판단되면 지수적으로 팍팍 증가시킬 수 있으므로 결과적으로 AIMD보다 윈도우 크기를 더 빠르게 키울 수 있는 것이다. 최근의 TCP에서 사용하고 있는 Tahoe나 Reno와 같은 정책들은 AIMD와 Slow Start를 적절히 섞어서 사용하되, 네트워크 혼잡 상황이 발생했을 때 어떻게 대처하는 지에 따라서 나뉘어지게 된다. 혼잡 제어 정책TCP에는 Tahoe, Reno, New Reno, Cubic, 최근 나온 Elastic-TCP까지 일일히 나열하기에도 숨 가쁠 정도로 다양한 혼잡 제어 정책이 존재한다. 이러한 혼잡 제어 정책들은 공통적으로 혼잡이 발생하면 윈도우 크기를 줄이거나, 혹은 증가시키지 않으며 혼잡을 회피한다라는 전제를 깔고 있고, 최근에 나온 방법은 예전 방법들에 비해서 더 똑똑하게 혼잡 상황을 감지하고 네트워크 대역을 최대한 빠르고 안전하게 활용할 수 있는 방향으로 개발된 것이다. 하지만 이 친구들을 이 포스팅에서 전부 설명하기는 힘드니, 이 중에서 가장 대표적이고 유명한 정책인 Tahoe와 Reno를 위주로 혼잡 제어의 기본 방식에 대해서 설명하려고 한다. Tahoe와 Reno는 기본적으로 처음에는 Slow Start 방식을 사용하다가 네트워크가 혼잡하다고 느껴졌을 때는 AIMD 방식으로 전환하는 방법을 사용하는 정책들이다. 붉은 색이 Tahoe, 녹색이 Reno의 윈도우 크기를 나타낸다 위 그래프의 Y축은 혼잡 윈도우, X축은 시간으로 하여 Tahoe와 Reno의 작동 방식을 설명하고 있다. 본격적으로 Tahoe와 Reno를 알아보기에 앞서, 일단 이 그래프가 뭘 설명하고 있는지 빠르게 이해하기 위해 간단한 용어를 몇 가지만 짚고 넘어가도록 하겠다. 그래프가 꺾이는 곳에 적혀있는 3 ACK Duplicated, Timeout과 그래프의 상승 폭이 변하고 있는 Threshold에 대해서 알아보도록 하자. (그림의 철자가 이상한 이유는 영어가 아니라 이탈리아어라서 그런 것이니 신경쓰지말자) 3 ACK Duplicated, Timeout일단 두 방식 모두 3 ACK Duplicated와 Timeout이라는 두 가지 시나리오가 발생하면 윈도우 크기를 줄이는 것을 확인해볼 수 있다. 즉, 이 두 상황이은 혼잡 제어 정책들이 혼잡 발생을 감지하는 기본적인 상황인 것이다. 타임아웃은 말 그대로 여러가지 요인으로 인해 송신 측이 보낸 데이터 자체가 유실되었거나, 수신 측이 응답으로 보낸 ACK가 유실되는 경우를 뜻하는 것이다. 3 ACK Duplicated, 즉 송신 측이 3번 이상 중복된 승인 번호를 받은 상황 또한 정상적으로 데이터가 전송된 상황은 아닌데, 수신 측은 자신이 정상적으로 처리한 데이터에 대해서만 ACK를 보내기 때문에 동일한 승인 번호를 세 번이나 받은 상황은 어떤 이유로 인해 수신 측이 특정 시퀀스 번호 이후의 데이터를 제대로 처리하지 못한 상황이라고 볼 수 있다. 단, 패킷 전송 방식을 사용하는 TCP의 특성 상 수신 측이 받는 패킷의 순서가 늘 순서대로 받으리라는 보장은 없으므로, 한 두개의 중복 승인 번호가 발생했다고 해서 바로 네트워크가 혼잡하다고 판단하지는 않는다. TCP는 현재까지 누적된 정상 데이터 중 가장 마지막 데이터에 대한 승인 번호를 보내주는 누적 승인 방식(Cumulative Acknowledgement)를 사용하기 때문에, 송신 측이 같은 승인 번호를 계속 중복해서 받는다면 해당 데이터까지는 정상적으로 전송되었으나 그 이후부터는 뭔가 문제가 발생했다고 알 수 있는 것이다. 이때 3번의 중복 승인 번호로 인해 송신 측이 바로 해당 승인 번호에 해당하는 데이터를 전송하고나면 수신 측은 Go back N이나 Selective Repeat과 같은 오류 제어 방식에 따라서 다음에 어떤 패킷부터 보내줘야하는지 알려줄 것이다. 이런 상황일 경우, 송신 측은 자신이 설정한 타임아웃 시간이 지나지 않았어도 바로 해당 패킷을 재전송할 수 있는데, 이 기법을 빠른 재전송(Fast Transmit)이라고 한다. 만약 빠른 재전송 기법을 사용하지 않는다면, 송신 측은 자신이 설정한 타임아웃 시간이 지난 이후에야 대처할 수 있기 때문에 에러난 데이터를 재전송하기까지 시간이 낭비되게 될 것이다. Slow Start 임계점(ssthresh) Tahoe와 Reno를 비교하는 그래프를 보면 Threshold(임계점)이라는 단어가 꽤 많이 등장하고 있는 것을 볼 수 있는데, 이 임계점은 Slow Start Threshold(ssthresh)를 뜻하는 것으로, 여기까지만 Slow Start를 사용하겠다라는 의미를 가진다. 이 값을 사용하는 이유는 Slow Start를 사용하며 윈도우 크기를 지수적으로 증가시키다보면 어느 순간부터는 윈도우 크기가 기하급수적으로 늘어나서 제어가 힘들어지기도 하고, 네트워크의 혼잡이 예상되는 상황에서는 빠르게 값을 증가시키는 것보다 돌다리 두들겨 건너듯이 조금씩 증가시키는 편이 훨씬 안전하기 때문이다. 쉽게 생각해서 현재 윈도우 크기가 10이고 현재 네트워크에 남은 공간이 15라고 할 때, Slow Start 방식으로 윈도우 크기를 증가시키면 20을 넘겨버리지만 합 증가를 하게 되면 앞으로 한 5번은 윈도우 크기를 더 증가시킬 수 있다. 그래서 특정한 임계점(Threshold)를 정해놓고, 임계점을 넘어가게되면 AIMD 방식을 사용하여 선형적으로 윈도우 크기를 증가시킨다. 그래서 이 임계점을 칭하는 단어가 ssthresh(Slow Start Threshold)인 것이다. 송신 측은 본격적인 통신이 시작하기 전에 ssthresh 값을 자신의 혼잡 윈도우의 절반 크기인 0.5 MSS으로 초기화하고, 이후 어떤 혼잡 제어 방법을 사용하냐에 따라 다르게 대처하게된다. TCP TahoeTCP Tahoe는 Slow Start를 사용한 혼잡 제어 정책의 초기 버전으로, 위에서 설명한 빠른 재전송(Fast Transmit) 기법이 처음으로 도입된 방법이다. Tahoe 이후의 혼잡 제어 정책은 Tahoe와 마찬가지로 빠른 재전송 기법을 기본으로 사용하되, 효율을 위해 몇 가지 양념을 더 치게 된다. 참고로 TCP Tahoe는 미국 네바다주에 있는 타호 호수의 이름을 따서 지어졌으므로 그냥 타호라고 읽으면 된다. (타호에가 아니다!) 왜 여기서 이름을 따왔는지는 모르겠지만 호수가 이쁘긴 하다 Tahoe는 처음에는 Slow Start를 사용하여 자신의 윈도우 크기를 지수적으로 빠르게 증가시키다가 ssthresh를 만난 이후부터는 AIMD에서 사용하는 합 증가 방식을 사용하여 선형적으로 윈도우 크기를 증가시킨다. 그러다가 위에서 언급한 ACK Duplicated나 Timeout이 발생하면 네트워크에 혼잡이 발생했다고 판단하고, ssthresh와 자신의 윈도우 크기를 수정하게 된다. 조금 더 편한 이해를 위해 Tahoe의 혼잡 윈도우 크기 변화 그래프를 살펴보도록 하자. 위 그래프에서 청록색 선은 송신 측의 혼잡 윈도우 크기를, 굵은 검정선은 ssthresh 값을 보여주고 있다. 이 시나리오에서 송신 측의 혼잡 윈도우 크기는 8로 초기화되었고, 그에 따라 ssthresh는 8 * 0.5 = 4로 설정된 것을 볼 수 있다. 송신 측은 임계점을 만나기 전까지 Slow Start 방식을 사용하여 자신의 윈도우 크기를 지수적으로 증가시키다가 ssthresh를 넘어선 이후부터는 선형적으로 증가시키고 있다. 이때 3 ACK Duplicated나 Timeout과 같은 혼잡 상황이 발생하면 어떻게 될까? 그래프를 보면 가장 처음 혼잡 상황이 발생한 상태의 혼잡 윈도우 크기는 6이다. 이때 송신 측은 ssthresh를 6의 절반인 3으로 변경하고, 자신의 혼잡 윈도우 크기를 다시 1로 변경하는 모습을 확인할 수 있다. 이후 다시 Slow Start로 시작하여 임계점에 도달하면 합 증가로 변경하는 방법을 반복하게 된다. 즉, 이전에 한 대 맞았던 지점을 기억하고 그 지점이 가까워지면 조금씩 몸을 사리는 원리인 것이다. 이런 접근법은 나름 합리적인 방법이다. 그러나 실버 불렛은 없는 법. Tahoe의 단점은 초반의 Slow Start 구간에서 윈도우 크기를 키울때 너무 오래 걸린다는 것이다. 물론 전체적으로 보았을 때 합 증가 방식보다는 지수 증가 방식이 빠르겠지만, 그래도 혼잡 상황이 발생했을 때 다시 윈도우 크기를 1부터 키워나가야 한다는 점은 어찌보면 낭비일 수도 있다. 그래서 나온 방법이 빠른 회복(Fast Recovery) 방식을 활용한 TCP Reno이다. TCP RenoTCP Reno는 TCP Tahoe 이후에 나온 정책으로, Tahoe와 마찬가지로 Slow Start로 시작하여 임계점을 넘어서면 합 증가로 변경하는 방법이다. 그러나 Tahoe는 명확한 차이가 있는데, 바로 3 ACK Duplicated와 Timeout을 구분한다는 것이다. Reno는 3개의 중복 ACK가 발생했을 때, 윈도우 크기를 1로 줄이는 것이 아니라 AIMD처럼 반으로만 줄이고 sshthresh를 줄어든 윈도우 값으로 정하게 된다. 위 그래프는 앞서와 마찬가지로 청록색으로 송신 측의 혼잡 윈도우 크기를, 굵은 검정선으로 ssthresh를 표현하고 있다. Reno는 Tahoe와 다르게 3개의 중복 ACK가 발생했을 때는 혼잡 윈도우 크기를 1로 줄이는 것이 아니라 반으로 줄인 후 합 증가를 하게 되는데, 이 방식은 혼잡 윈도우 크기를 1로 줄이고 처음부터 다시 시작하는 Tahoe에 비해서 빠르게 원래 윈도우 크기에 도달할 수 있기 때문에 빠른 회복(Fast Recovery)라고 불린다. 이때 ssthresh는 줄어든 윈도우의 크기와 동일하게 설정하게 된다. 위 그래프에서도 혼잡 상황이 발생하자 혼잡 윈도우 크기를 6에서 3으로 줄이고 ssthresh 또한 3으로 설정하는 모습을 볼 수 잇다. 그러나 만약 타임아웃에 의해 데이터가 손실되면 Tahoe와 마찬가지로 윈도우 크기를 바로 1로 줄여버리고 Slow Start를 진행하고 이때는 ssthresh를 변경하지 않는다. 즉, ACK가 중복된 상황과 타임아웃이 발생한 상황을 구분하면서 대처를 다르게 하고 있는 것이다. ACK 중복은 타임아웃에 비해 그리 큰 혼잡이 아니라고 가정하고 혼잡 윈도우 크기를 1로 줄이지도 않는다는 점에서 어느 정도 혼잡 상황에 경중을 따지고 있다는 것도 알 수 있다. 마치며이번에는 TCP의 혼잡 제어 정책에 대해서 알아보았다. 사실 필자가 포스팅에서 소개한 Taheo와 Reno 같은 혼잡 제어 정책은 최근에는 많이 사용되지 않는 구식이다. 예전에 개발된 Tahoe나 Reno 같은 경우는 말 그대로 예전의 네트워크 환경을 고려하며 설계된 친구들이라 최근의 대역폭 빵빵한 네트워크에는 다소 맞지 않는 것이 현실이다. 높은 대역폭에서의 Tahoe와 CUBIC의 효율 차이를 보면 어마무시하다 사실 Tahoe나 Reno가 개발된 시절의 네트워크 대역폭에 비교하면 최근의 네트워크 대역폭은 적어도 1,000배는 더 여유가 있지 않을까? 그 말인 즉슨, 송신 측이 자신의 혼잡 윈도우 크기를 마음 놓고 팍팍 늘렸을 때 문제가 발생할 확률이 예전보다 많이 낮아졌다는 뜻이다. 그래서 최근의 혼잡 제어 정책들은 얼마나 더 빠르게 혼잡 윈도우 크기를 키우고, 어떻게 혼잡 감지를 더 똑똑하게 할 것이냐에 대해 초점이 맞춰져있다. 3차 함수의 특성을 사용하는 TCP CUBIC은 혼잡을 회피할 때는 거의 윈도우 크기를 증가시키지 않다가, 혼잡이 해결되면 다시 폭발적으로 증가시킬 수 있다 그러나 필자가 굳이 예전 방식인 Tahoe와 Reno를 소개한 이유는 TCP 사용 초반에 개발된 혼잡 제어 방식인 만큼 원리가 간단하기도 하고, 이후 개발된 방식들도 큰 틀은 Tahoe와 Reno와 같은 매커니즘에서 크게 벗어나지 않기 때문이다. 이 포스팅은 다양한 혼잡 제어 정책을 알아보는 것이 아니라, 혼잡 제어라는 매커니즘 자체가 어떤 식으로 흘러가는지 아는 것이 목표이므로 굳이 다른 혼잡 제어 정책은 소개하지 않았다. 만약 최근에 사용하는 혼잡 제어 정책이 궁금하신 분들은 아래 첨부한 링크에서 조금 더 살펴보도록 하자. CUBIC RED Elastic TCP 이상으로 사이 좋게 네트워크를 나눠 쓰는 방법, TCP의 혼잡 제어 포스팅을 마친다.","link":"/2019/11/26/tcp-congestion-control/"},{"title":"TCP가 연결을 생성하고 종료하는 방법, 핸드쉐이크","text":"저번에 작성했던 TCP의 헤더에는 어떤 정보들이 담겨있는걸까? 포스팅에 이어 이번에는 TCP의 핸드쉐이크 과정과 그 속에서 변화하는 TCP 상태에 대해서 한번 알아보려고 한다. TCP는 신뢰성있는 연결을 추구하기 때문에 연결을 생성하고 종료하는 순간에도 나름의 신뢰성 확보를 위해 핸드쉐이크(Handshake)라고 하는 특별한 과정을 거치게 된다. TCP를 사용하여 통신을 하는 각 종단은 핸드쉐이크 과정을 통해 어떤 TCP 옵션들을 사용할 지, 패킷의 순서 번호 동기화와 같이 통신에 필요한 몇 가지 정보를 주고 받는다. 하지만 말로만 설명하면 재미가 없으니, C를 사용하여 직접 간단한 클라이언트와 서버를 작성해보고 이 친구들이 핸드쉐이크 과정에서 주고 받는 패킷을 몰래 엿본 결과물도 조금씩 첨부하려고 한다. 연결 지향의 의미에 대해서핸드쉐이크를 이야기하기에 앞서, TCP가 생성하고 종료하는 연결에 대한 이야기를 먼저 하려고 한다. 아마 TCP에 대해서 공부해보신 분들은 TCP의 대표적인 특징 중 하나인 연결 지향(Connection Oriented)이라는 키워드에 대해서 들어보았을 것이다. 연결 지향은 말 그대로 연결되어 있는 상태를 지향한다는 의미이다. 사실 연결과 비연결은 네트워크를 공부하다보면 여러 번 마주치게 되는 단어인데, 필자는 개인적으로 이 단어들의 의미가 조금 헷갈렸었다. 상식적으로 두 기기가 통신을 하려면 케이블이든 뭐든 연결이 되어있어야 할텐데, 굳이 왜 연결 지향과 비연결 지향을 나누어 놓은 것인지 이해가 되지 않았기 때문이다. 이게 헷갈리는 이유는 물리적인 연결과 논리적인 연결의 차이 때문이다. 우리가 일반적으로 기기와 다른 기기를 연결했다고 할 때 떠올리는 생각은 컴퓨터와 모니터를 연결하거나, USB와 컴퓨터를 연결하는 등의 상황이다. 즉, 기기 간의 물리적인 연결이다. 케이블을 사용하여 두 개의 장치를 물리적으로 연결한다 반면, 연결 지향이라는 단어에서 사용하고 있는 연결의 의미는 논리적인 연결(Logical Connection)을 의미한다. 이때 당연히 여러 개의 기기가 서로 통신을 하기위해서는 물리적인 연결 또한 동반되어야한다. 조금 더 쉽게 이야기해보자면, 두 기기가 서로 연결되어 있는 상태를 유지하는 것이다. 전화를 예로 들자면, 전화가 전화선에 연결되어있는 것이 물리적인 연결이고 실제로 다른 전화와 통화를 하고 있는 상황이 논리적인 연결, 즉 연결되어 있는 상태인 것이다. 그렇다면 왜 TCP는 이런 연결 상태를 유지하는 걸까? 그 이유는 간단하다. 바로 연속적인 데이터 전송의 신뢰성을 위해서이다. 기본적으로 TCP는 패킷 전송 방식을 사용하기 때문에 보내려고 하는 데이터를 여러 개의 패킷으로 쪼개서 보낸다. 이때 네트워크를 통해 모든 데이터를 한번에 팍! 보내는 것이 아니라 일정 단위로 묶어서 스트림처럼 상대방에게 흘려보내게 된다. 그럼 한번 데이터를 받는 수신자 입장에서 생각해보자. 패킷 전송 방식의 장점 중 하나는 회선을 점유하지 않고 적은 양의 회선으로도 동시에 통신을 할 수 있다는 점이다. 그렇다는 것은 각 종단이 동시다발적으로 여러 기기들과 패킷을 주고 받고 있다는 의미인데, 이때 누가 보낸 몇 번째 패킷이라는 정보가 없다면 수신 측은 굉장히 혼란스러울 것이다. 연결 상태가 없는 패킷을 구분한다는 것은 한 양동이에 담긴 물을 구분하고 싶다는 말과 같다 위 그림에서 파이프는 물리적인 연결, 각 파이프 끝의 구멍은 포트, 양동이는 패킷을 처리할 프로세스라고 생각해보자. 이때 연결 상태에 대한 구분을 하지 않고 패킷을 구분하고 싶다는 것은 마치 한 양동이에 담긴 물 중에서 어떤 한 파이프 구멍에서 나온 물을 구분해내고 싶다는 말과 비슷하다. 그렇기 때문에 TCP는 A와 B의 연결 상태, A와 C의 연결 상태 등 각 기기간의 연결 상태를 따로 구분하고 있는 것이다. 이때 TCP는 상대방과 연결 상태를 만들거나 해제하기 위해 특별한 과정을 거치는데, 이 과정을 핸드쉐이크(Handshake)라고 한다. 3 Way Handshake먼저, 연결을 만드는 과정부터 살펴보도록 하자. 연결을 생성할 때 거치는 핸드쉐이크 과정을 3 Way Handshake라고 하는데, 3 Way라는 말 그대로 총 3번의 통신 과정을 거친다. 이 과정을 거치면서 통신을 하는 양 종단은 내가 누구랑 통신하고 있는지, 내가 받아야할 데이터의 시퀀스 번호가 몇 번인지와 같은 정보를 주고 받으면서 연결 상태를 생성하게 된다. 이때 요청자(Initiator)는 연결 생성 요청을 먼저 보낸 쪽, 수신자(Receiver)는 연결 생성 요청을 받은 쪽을 의미한다. 이렇게 표현하는 이유는 일반적으로 우리가 생각하는 클라이언트와 서버, 둘 중에 어느 쪽이든 자유롭게 먼저 연결 생성 요청을 보낼 수 있기 때문이다. 그럼 한 번 각각의 상태가 어떤 것을 의미하는지, 두 기기가 서로 주고 받고 있는 SYN과 ACK는 무엇을 의미하는지 살펴보도록 하자. CLOSED 아직 연결 요청을 시작하지 않았기 때문에 아무런 연결도 없는 상태이다. LISTEN 수신자가 요청자의 연결 요청을 기다리고 있는 상태이다. 이후 요청자가 연결 요청을 보내기 전까지 수신자는 계속 이 상태로 대기하게 된다. 즉, 적극적으로 상대방에게 대시하지 않는다는 것인데, 그래서 이 상태를 수동 개방(Passive Open)이라 하고, 수신자를 Passive Opener라고도 한다. 소켓 프로그래밍을 할 때, 소켓 바인딩을 한 후 listen 함수를 호출하게 되면 수신자가 LISTEN 상태로 들어가게 된다. 1234567if ((listen(sockfd, 5)) != 0) { printf(\"Listen failed...\\n\"); exit(0);}else { printf(\"Server listening..\\n\");} 이후 수신자는 요청자의 연결 요청이 확인되면 accept 함수를 호출하여 다음 단계로 넘어가게 된다. SYN_SENT 요청자가 수신자에게 연결 요청을 하면서 랜덤한 숫자인 시퀀스 번호를 생성해서 SYN 패킷에 담아 보낸 상태이다. 이제 요청자와 수신자는 이 시퀀스 번호를 사용하여 계속 새로운 값을 만들고 서로 확인하며 연결 상태와 패킷의 순서를 확인하게 된다. 1localhost.initiator > localhost.receiver: Flags [S], seq 3414207244, win 65535 TCP 세그먼트를 캡쳐할 수 있는 tcpdump 유틸리티로 이 과정을 확인해보면 요청자가 패킷의 플래그를 SYN 패킷을 의미하는 S로 설정하고 시퀀스 번호로 3414207244라는 값을 생성해서 수신자에게 보내고 있음을 알 수 있다. 이 경우는 요청자가 수신자에게 연결을 생성하자고 적극적으로 대시하는 상황이므로 이 상태를 능동 개방(Active Open)이라고 하고, 요청자를 Active Opener라고도 한다. SYN_RECV SYN_RECV는 요청자가 보낸 SYN 패킷을 수신자가 제대로 받은 상태를 의미한다. 이후 수신자는 제대로 된 시퀀스 번호를 받았다는 확인의 의미인 승인 번호(Acknowledgement) 값을 만들어서 다시 요청자에게 돌려줘야한다. 이때 승인 번호는 처음 요청자가 보낸 시퀀스 번호 + 1이 된다. 이 승인 번호 만드는 과정은 어렵게 생각할 필요가 없는게, 저번 포스팅에서 이야기했듯이 TCP를 사용하여 실제로 데이터를 주고 받을 때에는 상대방이 보낸 시퀀스 번호 + 상대방이 보낸 데이터의 byte를 합쳐서 승인 번호를 만들어낸다. 즉, 내가 여기까지 받았으니, 다음에는 여기부터 보내달라는 일종의 마킹인 것이다. 그러나 이런 핸드쉐이크 과정에서는 아직 데이터를 주고 받지 않기 때문에 시퀀스 번호에 더할게 없다. 그렇다고해서 시퀀스 번호를 같은 번호로 주고 받자니 패킷의 순서를 구분할 수 없지 않은가? 그래서 그냥 1을 더하는 것이다. 방금 전과 마찬가지로 tcpdump 유틸리티를 사용하여 이 과정을 확인해볼 수 있다. 1localhost.receiver > localhost.initiator: Flags [S.], seq 435597555, ack 3414207245, win 65535 수신자가 요청자에게 보내는 패킷을 캡처해보았더니 패킷의 플래그가 S.로 설정되어있다. 이때 .가 의미하는 것은 헤더의 ACK 플래그 필드가 1이라는 것이므로 이 패킷에는 유효한 승인 번호가 담겨있음을 알 수 있다. 수신자는 이번 통신을 통해 요청자에게 3414207245 이라는 승인 번호를 전달하고 있는데, 이 값은 방금 전 요청자가 보냈던 시퀀스 번호인 3414207244에 1을 더한 값이다. 또한 랜덤한 수로 자신의 시퀀스 번호인 435597555를 다시 생성하여 함께 요청자에게 보내주고 있는 것을 확인할 수 있다. ESTABLISHED(요청자) 요청자는 자신이 맨 처음에 보냈던 시퀀스 번호와 수신자가 응답으로 보내준 승인 번호, 즉 내 시퀀스 번호 + 1를 사용하여 연결이 제대로 성립되었는지 확인할 수 있다. 자신이 보냈던 시퀀스 번호와 이번에 받은 승인 번호의 차가 1이라면 제대로 연결이 되었다고 판단하는 것이다. 이후 요청자는 연결이 성립되었다고 판단하고 ESTABLISHED 상태로 들어가면서, 이번에는 수신자가 새롭게 만들어서 보내줬던 시퀀스 번호에 1을 더한 값을 다시 승인 번호로 사용하여 다시 수신자에게 보내준다. 즉, 마지막으로 수신자가 보내줬던 시퀀스 번호인 435597555에 1을 더한 값인 435597556이 요청자의 승인 번호가 될 것이다…만 tcpdump의 동작은 필자의 예상과 달랐다. 1localhost.initiator > localhost.receiver: Flags [.], ack 1, win 6379 왜 1이 거기서 나와…? 원래대로라면 435597556이 되어야할 요청자의 마지막 승인 번호가 뜬금없이 1이 되었다. (처음엔 진심 당황했다) 사실 이건 TCP의 자체 동작은 아니고 tcpdump가 제공하는 기능이다. tcpdump가 패킷들의 시퀀스 번호를 알아보기 쉽게끔 상대적인 위치로 알려주기 때문이다. 이후 이 두 종단이 주고 받는 데이터를 tcpdump로 캡처해보면 이게 무슨 말인지 조금 더 쉽게 알 수 있다. 1localhost.initiator > localhost.receiver: Flags [P.], seq 1:81, ack 1, win 6379, length 80: HTTP 원래대로라면 요청자가 마지막으로 보내는 승인 번호는 435597556이 될 것이기 때문에 첫 번째로 전송하는 데이터의 시퀀스 번호의 범위 또한 435597556:435597637로 출력되어야한다. 그러나 인간이 이렇게 큰 숫자를 계속 보면서 분석하기는 쉽지 않기 때문에 승인 번호를 1로 보여주고, 이후 주고받는 첫 번째 시퀀스 번호를 1부터 시작해서 알아보기 쉽게 만들어주는 것이다. 확실히 435597556:435597637보다는 1:81이 알아보기 쉽다. 하지만 이건 인간이 알아보기 쉽게 tcpdump가 친절함을 베푼 것일뿐 실제로 값이 1로 변경된 것은 아니기 때문에 tcpdump의 -S 옵션을 사용하여 이 기능을 비활성화하면 원래 승인 번호와 시퀀스 번호를 그대로 출력할 수도 있다. 12$ sudo tcpdump -Slocalhost.initiator > localhost.receiver: Flags [.], ack 435597556, win 6379 ESTABLISHED(수신자) 요청자와 마찬가지로 수신자 또한 자신이 보냈던 시퀀스 번호와 이번에 받은 승인 번호의 차가 1이라면 제대로 연결이 되었다고 판단하고 ESTABLISHED 상태로 들어가게된다. 여기까지 오면 요청자와 수신자는 안전하고 신뢰성있는 연결이 생성되었다고 판단하고 본격적인 통신을 시작할 수 있다. 4 Way Handshake연결을 생성할 때와 마찬가지로, 연결을 종료할 때도 특정한 과정을 거쳐서 연결을 종료해야한다. 그냥 연결을 끊어버리면 안되냐고 할 수도 있지만, 한 쪽에서 일방적으로 연결을 끊어버리면 다른 한 쪽은 연결이 끊어졌는지 지속되고 있는지 알 방법이 없다. 또한 연결을 종료하기 전에 아직 다 처리하지 못한 데이터가 있을 수도 있기 때문에 양 쪽이 다 정상적으로 연결을 종료할 준비가 되었는 지를 확인하는 과정이 필요한 것이다. 이때 요청자와 수신자가 총 4번의 통신 과정을 거치기 때문에, 이 과정을 4 Way Handshake라고 부른다. 이번에도 요청자(Initiator)와 수신자(Receiver)라는 용어를 사용하고 있는데, 3 Way Handshake와 마찬가지로 클라이언트와 서버, 둘 중에 어느 쪽이든 연결 종료 요청을 시작할 수 있기 때문에 이런 용어를 사용하는 것이다. 먼저 연결 생성 요청을 했던 쪽이 먼저 연결 종료 요청을 보낼 수도 있고, 반대로 처음에는 연결 생성 요청을 당했던 쪽이 이번에는 먼저 연결 종료 요청을 보낼 수도 있다. 사실 개발자들은 3 Way Handshake보다 연결을 종료하는 과정인 4 Way Handshake에 더 예민하게 반응할 수 밖에 없는데, 연결을 생성하는 과정에서 문제가 발생하여 연결이 생성되지 않는다면 다시 시도하면 그만이지만, 이미 생성된 연결을 종료하는 과정인 4 Way Handshake에서 문제가 발생하면 그대로 연결이 남아있기 때문이다. 게다가 4 Way Handshake는 3 Way Handshake처럼 순차적으로 주고받는 방식이 아니라 상대방이 응답을 줄 때까지 대기하는 과정이 포함되어있기 때문에 중간에 뭐 하나 엇나가면 서로 계속 대기만 하고 있는 데드락(Deadlock) 상황이 연출될 수도 있다. 물론 조건에 따라 일정 시간이 지나면 타임아웃이 되며 연결을 강제로 종료하거나 다음 단계로 넘어갈 수도 있지만 그래도 그 시간 동안 프로세스가 메모리와 포트를 점유하고 있으므로 트래픽이 많은 서버라면 이로 인해 병목이 발생할 가능성은 늘 있다. FIN_WAIT_1 먼저 연결을 종료하고자 하는 요청자가 FIN 패킷을 상대방에게 보내면서 FIN_WAIT1 상태로 들어서게 된다. 이때 FIN 패킷에도 시퀀스 번호가 포함되어있긴한데, 이번에는 랜덤한 값으로 생성해서 보내는 것이 아니다. 3 Way Handshake는 시퀀스 번호가 없는 상황에서 새로 만들어야하는 상황이라 랜덤한 값으로 초기화했지만, 이번에는 시퀀스 번호를 새롭게 생성할 필요가 없으므로 그냥 자신이 이번에 보내야할 순서에 맞는 시퀀스 번호를 사용하면 되는 것이다. 요청자 —SEQ: 1—> 수신자 요청자 수신자 즉, FIN 플래그만 1로 변경해서 보낸다고 생각하는 게 편하다. 이 플래그의 의미를 쉽게 얘기해보자면 “나 더 이상 할 말 없음” 정도이다. 이때 요청자가 먼저 적극적으로 연결 종료 요청을 보내는 것이기 때문에 요청자를 Active Closer, 이 상태를 능동 폐쇄(Active Close)라고 한다. 1localhost.initiator > localhost.receiver: Flags [F.], seq 701384376, ack 4101704148, win 6378 하지만 요청자가 수신자에게 보낸 연결 종료 요청 패킷을 캡처해보니 F 플래그가 아니라 FIN+ACK를 의미하는 F. 플래그가 설정되어있다. tcpdump를 사용하여 패킷을 캡처한 다른 블로그를 봐도 대부분 필자와 같은 상황을 겪고 있음을 알 수 있었다. 분명 이론적으로는 FIN 패킷을 보내야하는데 왜 승인 번호를 함께 묶어서 FIN+ACK로 보내고 있는 것일까? Half-Close 기법요청자가 FIN+ACK 패킷을 보내는 이유는 바로 Half-Close라는 기법을 사용하고 있기 때문이다. Half-Close 기법은 말 그대로 연결을 종료하려고 할 때 완전히 종료하는 것이 아니라 반만 종료하는 것이다. Half-Close를 사용하면 요청자가 처음 보내는 FIN 패킷에 승인 번호를 함께 담아서 보내게 되는데, 이때 이 승인 번호의 의미는 “일단 연결은 종료할 건데 귀는 열어둔다. 이 승인 번호까지 처리했으니까 마저 보낼 거 있으면 보내”라는 의미가 된다. 즉, 반만 닫겠다는 말의 의미는 연결을 종료할 때 전송 스트림과 수신 스트림 중 하나만 우선 닫겠다는 것을 의미하는 것이다. 이후 수신자는 미처 못 보낸 데이터가 있다면 열심히 보낼 것이고, 이에 요청자는 아직 살아있는 수신 스트림을 사용하여 데이터를 처리한 후 ACK 패킷을 응답으로 보낼 수 있다. 이후 수신자가 모든 데이터를 처리하고나면 다시 요청자에게 FIN 패킷을 보냄으로써 모든 데이터가 처리되었다는 신호를 보내준다. 그럼 요청자는 그때 나머지 반을 닫으면서 조금 더 안전하게 연결을 종료할 수 있는 것이다. 소켓 프로그래밍을 할 때 연결 종료 함수로 close()와 shutdown()을 사용할 수 있는데, 이때 shutdown() 함수를 사용하면 Half-Close를 사용할 수 있다. 1shutdown(sockfd, SHUT_WR); 만약 요청자가 close() 함수를 사용하면 호출 즉시 OS에게 소켓의 리소스를 반환하며 모든 스트림이 파기되므로 FIN 패킷을 받은 수신자가 미처 못 보낸 데이터를 뒤늦게 전송하더라도 더 이상 처리할 수 없는 상황이 된다. 위의 예제에서는 SHUT_WR 값을 두 번째 인자로 사용함으로써 전송 스트림만 우선 닫겠다고 선언한 것이다. 이와 관련된 더 자세한 정보는 구글에 Half-Close나 우아한 종료 등의 키워드로 검색하면 많은 자료가 나오니 한번 살펴보도록 하자. CLOSE_WAIT 요청자으로부터 FIN 패킷을 받은 수신자는 요청자가 보낸 시퀀스 번호 + 1로 승인 번호를 만들어서 다시 요청자에게 응답해주면서 CLOSE_WAIT 상태로 들어간다. 1localhost.receiver > localhost.initiator: Flags [.], ack 701384377, win 6378 아까 요청자가 FIN 패킷의 시퀀스 번호로 701384376을 보냈으니 이번에 수신자가 응답해줄 승인 번호는 701384377이 되는 것이다. 이후 수신자는 자신이 전송할 데이터가 남아있다면 이어서 계속 전송한 후, 모든 전송이 끝났다면 명시적으로 close()나 shutdown()과 같은 함수를 호출하여 다음 단계로 넘어갈 것이다. 즉, 요청자는 언제 수신자의 데이터 처리가 끝날지 모르는 상태이기 때문에 수신자가 작업을 마치고 다시 연결 종료 승인을 의미하는 FIN 패킷을 보내줄 때까지 대기해야한다는 말이 된다. 만약 이 단계에서 수신자의 데이터 처리가 끝나도 연결 종료 함수가 명시적으로 호출되지 않으면 다음 상태로 넘어갈 수 없기 때문에 데드락이 발생할 가능성이 있다. 구글의 자동 완성 검색어가 개발자들의 심정을 대변해주고 있다 이때 수신자는 상대방으로부터 연결 종료 요청을 받은 후에야 수동적으로 연결을 종료할 준비를 하기 때문에 수신자를 Passive Closer, 이 상태를 수동 폐쇄(Passive Close)라고 한다. FIN_WAIT_2 요청자는 수신자로부터 승인 번호를 받고 자신이 보냈던 시퀀스 번호와 승인 번호의 차가 1이 맞는지 확인한다. 하지만 아직 수신자의 데이터 전송이 전부 끝나지 않았을 수도 있기에 FIN_WAIT2 상태로 들어가서 수신자가 연결 종료를 허락하는 FIN 패킷을 보내줄 때까지 기다린다. 방금 CLOSE_WAIT 섹션에서 설명했듯이 여기서부터는 수신자가 다시 FIN 패킷을 보내줄 때까지 요청자는 계속 대기하는 시간이다. 하지만 CLOSE_WAIT와 다르게 무한정 대기만 하는 것은 아니고 커널 파라미터로 타임아웃이 정해져있는 경우, 일정 시간이 경과하면 자동으로 다음 단계로 넘어갈 수 있다. LAST_ACK 수신자는 자신이 처리할 데이터가 더 이상 없다면 연결을 종료하는 함수를 명시적으로 호출하고, 아까 요청자가 보냈던 연결 종료 요청에 합의한다는 의미로 요청자에게 다시 FIN 패킷을 보낸다. 이때 수신자가 보내는 FIN 패킷에 담기는 시퀀스 넘버는 자신이 이번에 전송해야 하는 데이터의 시퀀스 번호를 그대로 사용하며, 승인 번호는 마지막으로 자신이 응답했던 승인 번호를 그대로 사용한다. 이후 수신자는 LAST_ACK 상태로 들어가며 요청자가 다시 승인 번호를 보내줄 때까지 대기한다. TIME_WAIT 수신자가 보낸 FIN 패킷을 받은 요청자는 다시 수신자가 보낸 시퀀스 번호 + 1로 승인 번호를 생성하여 수신자에게 ACK 패킷으로 응답한다. 이후 요청자는 TIME_WAIT 상태로 들어가며, 실질적인 연결 종료 과정에 들어가게 된다. 이때 TIME_WAIT의 역할은 의도하지 않은 에러로 인해 연결이 데드락에 빠지는 것을 방지하는 것이다. TIME_WAIT에서 대기하는 시간은 2 MSL(Maximum Segement Lifetime)으로 정의되어 있으며, 정확한 MSL의 시간 값은 커널 파라미터로 정의되어있다. 12$ sysctl net.inet.tcp | grep mslnet.inet.tcp.msl: 15000 필자의 컴퓨터인 OSX의 MSL은 15초로 설정되어있다. 즉, 필자의 컴퓨터는 TIME_WAIT 상태에서 30초 정도 대기한다는 것이다. 참고로 이 값은 변경할 수 없기 때문에 TIME_WAIT에서 소비되는 시간은 변경할 수 없다. 보통 TCP 타임아웃 파라미터로 많이 언급되는 net.ipv4.tcp_fin_timeout은 FIN_WAIT2의 타임아웃을 조절할 수 있는 값이라 TIME_WAIT 상태에는 해당 사항이 없다. 하지만 CLOSE_WAIT와 마찬가지로 여기서도 데드락이 발생할 수 있다. 그런 이유로 많은 네트워크 엔지니어들이 여기서 소비되는 시간을 줄이거나 운 나쁘게 발생한 데드락을 없애기 위해 tcp_tw_reuse 커널 파라미터를 변경하는 등 여러가지 방법을 사용하고 있다. (데드락 피하자고 만든 상태인데 데드락이 발생하는 현실) 하지만 역시 그냥 가만 냅두는 게 제일 좋다고들 한다. CLOSED(수신자) 요청자가 보낸 ACK 패킷을 받은 수신자는 CLOSED 상태로 들어가며 연결을 완전히 종료한다. CLOSED(요청자) TIME_WAIT 상태에서 2 MSL만큼 시간이 지나면 요청자도 CLOSED 상태로 변경된다. 위에서 설명했듯이 이 시간은 커널 파라미터에 고정되어 있고, 필자가 사용하고 있는 OSX의 경우 30초 정도이다. 마치며이렇게 두 번째 TCP 주제인 핸드쉐이크에 대한 포스팅을 마쳤다. TCP에 대해서 학교에서 배우긴 했지만 이렇게 각 상태에 대해서 자세히 공부하진 않았기 때문에 나름 새로운 경험이었다. 이 포스팅을 작성하면서 TCP가 단순히 연결을 생성하고 종료하는데만 해도 신뢰성을 확보하기 위해 얼마나 많은 작업을 하는지 알 수 있었다. (더불어 구글이 왜 HTTP/3에 UDP를 사용했는지 알 것 같았다…) 처음에는 필자 블로그 로컬 서버와 브라우저의 핸드쉐이크를 캡처해보려고 했는데, 이 친구들은 단순한 몇 개의 메세지를 주고 받는 수준이 아니라 대량의 데이터를 주고 받는 사이다보니 필자가 원하는 부분을 추적하기가 쉽지 않았다. 그래서 오랜만에 간단한 소켓 프로그래밍을 하게 되었는데, 음… 하도 오랜만에 C를 사용하다보니 손에 안 익어서 꽤나 고생하긴 했지만 나름 재미있었다. C는 역시 가끔 해야 재밌는 것 같다. 혹시 필자가 예제로 사용한 어플리케이션을 직접 실행해보고 싶으신 분은 필자의 깃허브 레파지토리에서 클론 받을 수 있다. 간단한 메세지만 서로 주고 받는 어플리케이션이니 tcpdump를 사용해서 패킷을 들여다보기도 한결 편할 것이다. 이상으로 TCP가 연결을 생성하고 종료하는 방법, 핸드쉐이크 포스팅을 마친다.","link":"/2019/11/17/tcp-handshake/"},{"title":"변하지 않는 상태를 유지하는 방법, 불변성(Immutable)","text":"이번 포스팅에서는 순수 함수에 이어 함수형 프로그래밍에서 중요하게 여기는 개념인 불변성(Immutable)에 대한 이야기를 해보려고 한다. 사실 순수 함수를 설명하다보면 불변성에 대한 이야기가 꼭 한번은 나오게 되는데, 대부분 “상태를 변경하지 않는 것”이라는 짧은 정의로 설명하거나, 혹은 불변성을 해치는 행위들을 예시로 들고 이런 행위들을 금지 행위로 규정하며 설명을 진행하게된다. 그러나 개인적으로 이런 설명 방식은 상태와 메모리에 대한 개념이 확실하게 정립되지 않은 사람에게 별로 와닿지 않는 방식일 수도 있다고 생각한다. 그래서 이번 포스팅에서는 정확히 불변이라는 것이 무엇을 의미하는지에 대한 이야기를 해보려고 한다. 순수 함수와 불변성은 무슨 관계인가요?저번에 작성했던 수학에서 기원한 프로그래밍 패러다임, 순수 함수에서 한 번 이야기 했듯이, 순수 함수는 수학의 함수를 프로그래밍의 세계로 가져온 모델이다. 프로그래밍의 세계에는 무언가를 저장하고 변경하고 불러올 수 있는 상태라는 개념이 존재하지만, 수학의 세계에는 그런 개념이 없기 때문에 모든 함수는 함수 외부의 무언가에 절대 영향을 받지 않고 독립적으로 존재한다. 그렇기 때문에 상태라는 개념 자체가 존재하지 않는 수학의 함수를 프로그래밍으로 구현한 모델인 순수 함수 또한 함수 외부의 상태에 영향을 받지 않아야한다는 규칙을 가질 수 밖에 없는 것이다. 또한 수학의 세계에는 상태라는 개념이 없기에 당연히 상태를 변경한다는 개념도 없을 수 밖에 없고, 우리는 이를 불변성(Immutable)이라고 부른다. 하지만 프로그래밍의 세계에서 상태를 변경하지 않는다는 것은 꽤나 신경을 많이 써줘야 하는 일이다. 그래서 우리는 “변수에 값을 재할당하지 않는다”와 같은 몇 가지 규칙들을 정해놓고 프로그래밍을 하면서 불변성을 유지한다. 하지만 프로그램에서 변이(Mutation)가 발생하는 근본적인 원인을 파악하고 불변성을 스스로 지켜나간다면, 이러한 규칙들이 커버할 수 없는 변태같은 상황을 마주치더라도 대응할 수 있기 때문에 우리는 불변(Immutation)이 정확히 무엇을 의미하는 지 알아야 할 필요가 있다. 불변성이란?보통 불변성의 의미는 상태를 변경하지 않는 것이라는 간단한 정의로 설명된다. 그러나 대부분 불변성에 대한 설명을 할 때, “함수 외부의 변수에 접근, 재할당해서는 안된다”, “함수의 인자를 변경하면 안 된다”와 같이 상태를 변경하는 행위를 금지하는 예시 정도만 설명하고, 상태를 변경한다는 것이 정확히 무엇을 의미하는지는 자세히 설명하지 않는다. 그래서 이런 설명 방식은 상태를 변경한다는 것이 정확히 어떤 의미인지 모르는 사람에게는 잘 와닿지 않을 수 있다고 생각한다. 그렇다면 불변성이 이야기하고 있는 상태의 변경이라는 것이 정확히 어떤 행위를 의미하는 것일까? 단순히 프로그램의 변수를 변경하거나 재할당 하지 않는 것을 이야기하는 것일까? 사실 불변성이 이야기하는 상태의 변경이라는 것은 단순한 변수의 재할당을 이야기하는 것이 아니다. 정확히 말하면 메모리에 저장된 값을 변경하는 모든 행위를 의미하며, 여기에 변수의 재할당과 같은 행위도 포함되는 것이다. 즉, 상태의 변경이라는 행위를 제대로 이해하기 위해서는 컴퓨터가 값을 어떤 방식으로 메모리에 저장하고 접근하는지에 대한 간단한 지식이 필요하다. 우리는 변수를 통해 메모리에 접근한다대부분의 프로그래밍 언어에서는 메모리의 특정 공간에 저장된 값에 조금 더 쉽게 접근할 수 있도록 도와주는 변수라는 기능을 제공하고 있다. 변수라는 개념은 프로그래밍을 배울 때 가장 처음 배우는 것이기 때문에, 개발자라면 누구나 다 알고 있는 개념일 것이다. 한번 간단한 변수를 선언해보도록 하자. 1234let a;a = 1;console.log(a); 11 필자는 a라는 변수를 “선언”하고, 그 다음 라인에서 a 변수에 1이라는 값을 “할당”했다. 일반적으로는 let a = 1;와 같이 선언과 동시에 할당을 진행하지만, 엄밀히 말해서 선언과 할당은 다른 행위이기에 조금 더 편한 이해를 위해 코드를 나눠서 작성했다. let a;라는 명령을 사용하여 변수를 선언하면 자바스크립트는 메모리에 a라는 변수를 통해 접근할 수 있는 메모리 공간을 마련한다. 필자는 변수를 선언만 하고 값을 할당하지 않았으니 이때 a 변수에 접근하려 한다면, “아무것도 정의되지 않았다”라는 의미의 undefined를 뱉어낼 것이다. 그 후 필자는 a = 1이라는 명령을 사용하여 마련된 메모리 공간에 1이라는 값을 저장했고, 그 이후부터 필자가 a라는 변수를 통해 해당 메모리 공간에 접근하면 저장되어 있던 1이라는 값을 얻어낼 수 있는 것이다. 즉, 변수라는 것은 메모리에 저장되어 있는 어떠한 값에 접근하는 일종의 단축어같은 개념이며, 만약 변수가 없다면 우리는 일일히 0x0018fa와 같은 메모리 주소를 사용하여 메모리에 값을 저장할 공간을 마련하고 값을 저장하거나 접근해야한다는 것이다. 만약 필자가 a = 2처럼 해당 변수의 값을 다시 할당한다면, 0x0018fa라는 주소를 가진 메모리 공간에 저장되어 있는 값을 변경하는 것이며, 상태를 변경하는 행위라고 말할 수 있는 것이다. 자바스크립트는 재할당 할 수 있는 변수를 선언하는 let 키워드와 재할당 할 수 없는 const 키워드를 구분하여 제공함으로써 개발자가 실수로 메모리 공간에 저장되어있는 값을 변경하는 행위를 방어할 수 있는 기능을 제공한다. 그렇다면 우리가 변수를 재할당하지만 않는다면 불변이라는 개념을 지킬 수 있는 것일까? 슬프게도 그렇지 않다. 프로그램이 변수가 가리키고 있는 메모리 공간에 있는 값을 불러오고 사용하는 방법은 그렇게 단순하지 않기 때문이다. 바로 여기서 그 유명한 값에 의한 호출(Call by value)과 참조에 의한 호출(Call by reference)이 등장한다. 값에 의한 호출과 참조에 의한 호출값에 의한 호출과 참조에 의한 호출은 특정 컨텍스트에서 다른 컨텍스트에게 변수를 넘길 때 어떤 방식으로 값을 넘겨줄 것인지에 대한 방법들이다. 이렇게 컨텍스트 간 변수를 넘기는 상황은 함수 외부의 스코프에서 함수에게 인자를 넘겨주는 상황으로 많이 표현되며, 또 실제로도 그런 상황이 대부분이다. 이에 대해서 조금 더 쉽게 알아보기 위해 간단한 함수를 선언해보도록 하겠다. 123function foo (s) { return str.substring(0, 2);}; foo 함수는 문자열을 인자로 받아서 가장 앞의 두 글자만 잘라내어 반환하는 순수 함수이다. 즉, foo 함수는 자신의 인자로 받은 값을 재료로 하여 자신의 반환 값을 만들어내는 셈이다. 그럼 foo 함수를 한번 사용해보도록 하자. 12const str = 'Hello, World!';foo(str); 1He foo 함수는 자신의 인자로 받은 문자열을 잘라서 반환하기 때문에, 마치 인자로 받은 str 변수를 직접 수정하는 것처럼 보인다. 하지만 foo 함수의 인자로 사용했던 str 변수를 콘솔에 출력해보면 처음 필자가 할당했던 값인 Hello, World!가 그대로 저장되어 있는 것을 확인할 수 있다. 12console.log(foo(str));console.log(str); 12HeHello, World! 이게 어떻게 된 것일까? 정답은 str 변수의 자료형인 string형의 호출 방식이 값에 의한 호출 방식을 사용하기 때문이다. 자바스크립트에서 string, number, boolean과 같이 원시 자료형을 사용하는 변수들은 모두 값에 의한 호출 방식을 사용한다. 값에 의한 호출 방식은 함수의 인자로 어떤 변수를 넘길 때 해당 변수가 가지고 있는 값을 그대로 복사하여 함수에게 넘겨주는 방식을 의미하기 때문에, 기존에 str 변수가 가리키고 있는 메모리 공간에 있는 값을 함수에 인자로 넘기는 것이 아니라 그 값을 복사하여 새로운 메모리 공간에 저장하고나서 넘겨준다는 뜻이다. 결국 foo(str)라는 코드로 함수를 호출하며 인자로 넘긴 str이라는 변수가 가지고 있는 값과, foo 함수 내부에서 s라는 변수를 통해 접근하는 값은 전혀 다른 메모리 공간에 저장되어 있는 새로운 값이다. 그렇기 때문에 foo 함수가 아무리 자신의 인자로 받은 변수를 지지고 볶아도 원본 변수는 절대로 영향을 받지 않는다. 심지어 foo 함수 내부에서 s 변수를 재할당하더라도 원본 변수에 담겨져 있는 값은 변하지 않는다. 123456789const str = 'Hello, World!';function foo (s) { s = '재할당합니다'; return s.substring(0, 2);}foo(str);console.log(str); 1Hello, World! foo 함수는 인자로 넘어온 변수에 값을 재할당했음에도 함수 외부에 있는 str 변수의 값은 변하지 않았다. 즉, 불변성을 유지한다는 것은 단순히 “함수의 인자를 변경하지 않는다”라던가 “변수를 재할당하지 않는다”는 개념이 아닌 것이다. 포인트는 메모리에 이미 담겨있는 값을 변경하지 않는 것이다. 반면 값에 의한 호출 방식을 사용하지 않는 Array, Object와 같은 객체들은 조금 상황이 다르다. 이번에는 인자로 배열을 받은 후 그 배열에 hi라는 문자열 원소를 추가하는 간단한 함수를 선언하고, 어떤 결과가 나오는지 살펴보도록 하겠다. 1234function bar (a) { a.push('hi'); return arr;}; 원시 자료형이었던 str 변수는 값에 의한 호출 방식을 사용하지만, 객체인 Array는 참조에 의한 호출 방식을 사용한다. 그럼 함수를 사용해보고 원본 변수가 어떻게 되는지 확인해보자. 1234const array = [];bar(array);console.log(array); 1['hi'] 함수 내부에서 인자를 지지고 볶아도 원본 변수에는 전혀 영향이 없었던 foo 함수와 다르게, 이번에는 bar 함수의 인자로 넘겼던 array 변수의 값이 변경된 것을 확인해볼 수 있다. 함수의 인자로 변수를 넘길 때 값을 복사하여 새로운 공간에 저장한 후 넘겨주는 값에 의한 호출 방식과 다르게, 참조에 의한 호출 방식은 “변수가 가리키고 있는 메모리 공간의 주소”를 넘기는 방식이다. 즉, array 변수가 가리키고 있는 메모리 공간에 저장된 배열과 bar 함수가 인자로 받은 배열은 정확히 같은 메모리 공간에 저장되어 있는, “같은 배열” 이라는 것이다. 즉, bar 함수의 인자로 받은 배열은 참조에 의한 호출 방식을 사용하는 객체이기 때문에, 함수 내에서 이 배열을 지지고 볶아 버린다면 원본 배열 자체가 지지고 볶아지는 것이다. 이 경우에는 메모리 공간에 저장되어있던 배열을 직접 변경해버리는 것이므로, 상태가 변경되었다고 말할 수 있고, 불변성이 깨져버린 것이다. 똑같이 함수의 내부에서 인자를 수정하는 행위지만 인자가 값에 의한 호출 방식을 사용하는 자료형인지 참조에 의한 호출 방식을 사용하는 자료형인지에 따라 결과는 큰 차이가 나기 때문에, 불변성을 지키고 싶다면 항상 이 점을 염두에 두고 코드를 작성해야한다. 불변성을 지키면 어떤 점이 좋은가요?프로그래밍을 하면서 상태의 불변성을 지키려면 자연스럽게 이것저것 신경써줘야하는 것들이 늘어날 수 밖에 없다. 그럼에도 불구하고 불변이라는 개념은 현재 많은 개발자들에게 환영받고 있는 개념이라는 것이 사실이다. 도대체 상태가 변경되지 않게 함으로써 얻을 수 있는 것이 무엇이길래, 다들 이렇게 불변불변하는 것일까? 무분별한 상태의 변경을 막는다상태는 프로그램의 현재 상황을 보여주는 좋은 역할도 하지만, 여기저기서 무분별하게 이 상태를 참조하거나 변경하는 경우, 개발자조차 현재 프로그램이 어떻게 돌아가는지 파악하기 힘든 슬픈 상황이 발생할 수도 있다. 그래서 개발자들은 상태를 변경하는 행위에 특정한 규칙과 제약을 정해서 무분별한 상태 변화를 최대한 피하고, 이런 변화를 추적할 수 있는 상황을 선호할 수 밖에 없다. 무분별한 상태 변경 때문에 프로그램이 터지게 되는 가장 대표적인 상황은 바로 “전역 변수의 남용”이다. 자바스크립트에서 전역 변수의 사용을 아예 금지하는 컨벤션을 추천하는 것도 바로 이 이유이다. 123456789101112let greeting = 'Hi';function setName () { name = 'Evan';}setTimeout(() => { greeting = 'Hello';}, 0);setName();console.log(`${greeting}, ${name}`); greeting 변수는 전역 스코프에서 선언된 전역 변수이고, setName 함수 내부에서도 암묵적으로 전역 변수를 선언하고 있으며, setTimeout의 콜백 함수 내에서도 전역 변수인 greeting의 값을 재할당하고 있다. 이런 상황에서는 어디서 어떤 놈이 greeting이라는 전역 변수의 상태를 변경했는지 추적이 거의 불가능하며, 갑자기 콘솔에 Hi, Evan이 아닌 Get out, Evan이라고 출력된다고 해도 전혀 이상할 것이 없다. 개발자가 이런 상황을 만났을 때 야근을 하는 이유는, 슬프게도 이게 버그가 아니기 때문이다. 이 코드들은 콘솔에는 어떠한 에러도 출력되지 않는 지극히 정상적인 로직이다. (차라리 에러라도 나는 것이 디버깅은 더 쉽다) 불변성을 유지하며 순수 함수를 사용한다는 것은 함수 외부의 상태에 접근하여 이미 메모리에 할당되어 있는 값을 변경하지 않는다는 의미이므로, 이렇게 예측하지 못한 상태의 변경을 방어할 수 있다. 상태의 변경을 추적하기가 쉽다일반적으로 자바스크립트의 객체의 프로퍼티나 배열의 원소를 변경해야하는 경우, 필연적으로 불변성이 깨질 수 밖에 없다. 애초에 배열이나 객체가 처음 나왔을 때, 어딘가에 구조화된 데이터를 저장해놓고 상태를 유지하고 변경해가며 사용하고자 하는 목적을 가지고 있었기 때문이다. 12const evan = { name: 'Evan' };evan.name = 'Not Evan'; // 상태 변화! 하지만 이렇게 기존에 메모리에 저장되어있는 값을 변경하는 행위는 불변의 법칙을 정면으로 위반하는 것이기 때문에, 불변성을 유지하고 싶은 개발자는 이런 식으로 객체의 프로퍼티나 배열의 원소를 변경할 수 없다. 게다가 방금 보았던 무분별한 전역 변수의 사용과 마찬가지로 객체나 배열의 상태 변화 또한 추적할 수 없는 문제이기 때문에, 어디서 이상한 놈이 엄한 객체나 배열의 상태를 변경하여 버그가 발생하더라도 개발자가 이를 디버깅하기란 쉽지 않은 문제이다. 그렇다고 객체의 프로퍼티나 배열의 원소를 변경하지 못하도록 할 수도 없는 노릇이다. 그럼 어떻게 이 문제를 해결해야할까? 한번 객체의 프로퍼티를 변경하는 간단한 함수를 통해 객체의 프로퍼티를 변경할 때 발생하는 상태 변화의 재현과 해결 방법을 알아보도록 하자. 1234function convertToJohn(person, name) { person.name = 'John'; return person;} convertToJohn 함수는 객체를 인자로 받아, 해당 객체의 name 프로퍼티에 John이라는 문자열을 할당하는 역할을 하는 함수이다. 즉, 이 함수는 객체의 상태를 변경하는 역할을 하고 있다. 일단 결론부터 이야기하자면 이 함수는 순수 함수가 아닌데, 그 이유는 함수가 참조에 의한 호출 방식을 사용하는 객체의 프로퍼티를 직접 변경하면 함수 외부에 있는 원본 객체의 상태도 변경되기 때문이다. 12345const evan = { name: 'Evan' };const john = convertToJohn(evan);console.log(evan);console.log(john); 12{ name: 'John' } // ?{ name: 'John' } convertToJohn 함수를 사용하는 사람은 함수의 이름만 보고 “오호, 이 함수는 어떤 객체를 존 객체로 바꿔주는 함수로군?”이라고 생각하겠지만, 이 함수는 개발자 몰래 자신의 인자로 받은 객체까지 변경해버리는 나쁜 함수였다. 이렇게 의도하지않은 객체의 프로퍼티가 변경되는 것도 문제지만, 사실 더 큰 문제는 이런 상태의 변화를 전혀 추적할 수 없다는 것이다. 당장 위 예시의 evan 객체와 john 객체를 비교해보면 자바스크립트는 두 객체가 같은 객체라고 평가해버린다. 두 객체는 메모리 공간에 접근할 수 있는 변수명만 다를 뿐, 실제로는 같은 메모리 공간에 저장되어 있는 같은 객체이기 때문이다. 1console.log(evan === john); 1true 이런 상황에서 개발자는 “의도하지 않은 객체의 상태 변화”와 “상태의 변화를 추적할 수 없다”는 고약한 문제를 떠안게 된다. 그렇다면 이 문제를 어떻게 해결할 수 있을까? 이 문제는 생각보다 간단하게 해결할 수 있는데, name을 John으로 가지는 객체를 그냥 새로 생성해버리면 된다. 123456789101112function convertToJohn (person) { const newPerson = Object.assign({}, person); newPerson.name = 'John'; return newPerson;}const evan = { name: 'Evan' };const john = convertToJohn(evan);console.log(evan);console.log(john); 12{ name: 'Evan' }{ name: 'John' } 변경된 convertToJohn 함수는 더 이상 인자로 받은 person 객체에 직접 접근해서 값을 수정하지 않는다. 다만 Object.assgin 메소드를 사용하여 person 객체와 동일한 구조를 가진 새로운 객체를 생성하고 name 프로퍼티를 John으로 변경한 후 반환할 뿐이다. 이런 과정이 너무 불편하게 느껴진다면 ES6의 spread 연산자를 사용하면 더 간단한 문법으로 변경할 수도 있다. 123456function convertToJohn (person) { return { ...person, name: 'John', };} 이렇게 새로운 객체를 생성하게 되면 의도하지 않은 객체의 상태 변화도 방어할 수 있고 상태 변화를 추적할 수도 있게 된다. 왜냐하면 convertToJohn 함수가 뱉어낸 객체는 evan 객체와는 전혀 다른, 새로운 객체이기 때문이다. 1console.log(evan === john); 1false 객체의 상태를 변화시킬때, “상태가 변화된 객체”를 새로 생성한다면 우리는 이전 상태를 가진 객체와 다음 상태를 가진 객체를 비교하며 false가 나온다는 사실을 이용하며 객체의 상태가 변화되었음을 알 수 있는 것이다. 이런 원리는 웹 프론트엔드의 UI 라이브러리인 리액트(React)에서 상태의 변화를 감지하는 데에도 사용되고 있는데, 리액트는 개발자가 setState와 같은 메소드를 사용하여 상태를 변경했을 때 Object.is 메소드를 사용하여 이전 상태와 다음 상태를 비교하고 두 객체가 같지 않다고 평가되면 상태가 변이되었다고 판단하고 컴포넌트를 다시 렌더한다. 또한 상태 관리 라이브러리인 리덕스(Redux) 또한 동일한 원리로 상태의 변화를 판단하기 때문에, 리듀서를 작성할 때는 기존 state 객체의 프로퍼티를 직접 변경하지 않고 새로운 객체를 생성해서 반환해야한다. 12345678910function reducer (state, action) { switch (action.type) { case SET_NAME: return { ...state, name: action.payload, }; // ... }} 이러한 불변성의 특징들은 참조에 의한 호출을 사용하는 자료형들의 상태 변화를 쉽게 감지할 수 있도록 만들어주기 때문에 개발자가 예상하지 못하는 방향으로 버그가 발생하는 것을 어느 정도 막을 수 있다. 또한 불변성은 멀티 쓰레딩을 사용할 때도 매우 유용한데, 여러 개의 쓰레드가 한 개의 상태를 정신없이 수정하고 참조하게되면 어느 순간부터는 도대체 쓰레드가 참조한 게 어떤 값인지 파악하기가 힘들기 때문이다. 이건 마치 하나의 종이에 여러 명의 화가가 물감을 칠하면서 그림을 완성해가는 느낌이라고 할 수도 있을 것 같다. 그러나 불변성이 제대로 지켜진다면 각자 쓰레드마다 종이를 주고 그림을 그려서 제출하라는 상황과 비슷하다. 개발자는 각 쓰레드가 그림을 제출할 때마다 상태가 변경되었음을 감지할 수도 있고, 이를 이용하여 그림의 상태가 변경되는 로그를 쌓을 수도 있다. 이후 그 그림들을 어떻게 취합하던, 필요없는 그림은 버리던 그건 그 후의 문제로 분리하면 되는 것이다. 현실적인 불변성의 상황이렇게 불변성을 지키면서 프로그래밍을 하면 상태 변화를 쉽게 추적할 수 있고 관리할 수 있다는 점에서 더할 나위 없이 좋지만, 그렇다고 해서 장점만 있는 것은 아니다. 불변성의 가장 큰 문제는 기존의 객체지향 프로그래밍과의 접점을 만들기 어렵다는 것이다. 불변성을 지향한다는 함수형 프로그래밍의 특징은 기존에 우리가 익숙하게 사용하는 객체지향 프로그래밍과는 많이 다르다. 객체지향 프로그래밍은 private과 같은 접근 제한자로 상태를 외부에 노출시키지 않음으로써 사용자가 단순한 인터페이스를 접할 수 있도록 하지만, 함수형 프로그래밍은 아예 프로그램의 상태를 변경하지 않는 불변성을 지향하면서 프로그램의 동작을 예측하기 쉽고 단순하게 만든다. OOP는 변경 가능한 상태를 감추며 단순함을 만들어내지만, FP는 아예 변경 가능한 상태를 없앰으로써 단순함을 만들어낸다 즉, 객체지향 프로그래밍과 함수형 프로그래밍은 상태를 바라보는 관점 자체가 다르다는 것이다. 애초에 객체지향 프로그래밍은 상태를 “잘 변경하는 것”에 초점을 맞추는 패러다임이기 때문에 불변성과는 약간 거리가 있다. 또한 아직까지 우리가 사용하는 대부분의 API나 라이브러리들은 객체지향 프로그래밍을 기반으로 설계되고 있기 때문에 우리는 객체지향 프로그래밍에서 완벽하게 독립할 수 없는 상황이다. 문제는 이렇게 객체지향 프로그래밍을 기반으로 설계된 것들을 불변의 법칙으로 관리하려면 꽤 많은 비용이 들 수도 있다는 것이다. 쉬운 이해를 위해 웹 오디오 API를 사용하여 객체를 하나 생성하고, 이 객체의 상태를 변경해야하는 상황을 살펴보도록 하자. 1234const context = new AudioContext();let state = { node: context.createGain(),}; 상태를 표현하기 위해 간단한 객체를 생성하고 그 안에 게인 노드(GainNode)를 할당했다. 게인 노드는 오디오 신호의 크기를 키웠다 줄였다 할 수 있는 값인 gain 프로퍼티를 가지고 있고, 개발자는 이 프로퍼티의 값을 변경함으로써 간단하게 오디오 신호를 조작할 수 있다. 1state.node.gain = 1.2; 그러나 이렇게 객체에 직접 접근하여 프로퍼티를 변경하는 행위는 불변성을 위배한다. 이 상황에서 불변성을 만족하기 위해서는 gain 프로퍼티의 값을 변경할 때마다 새로운 게인 노드 객체를 생성해줘야 하는 것이다. 12345678910const setGain = (value) => { const newGain = context.createGain(); newGain.gain = value; return newGain;};state = { ...state, node: setGain(value),}; 물론 이렇게 불변성을 지켜주면 게인 노드의 상태 변화를 추적할 수 있다는 장점을 가지지만, 객체를 생성하는 비용이 클 경우에는 문제는 발생할 수 있다. 웹 오디오 API가 제공하는 게인 노드 객체는 멤버 변수와 메소드를 가지고 있는 엄연한 인스턴스이며, { gain: 1 }처럼 간단한 프로퍼티만을 가지고 있는 객체가 아니다. 만약 인스턴스가 가지고 있는 멤버 변수와 메소드가 많거나 객체를 생성할 때 무거운 작업이 동반되어야 한다면, 프로퍼티를 변경할 때마다 객체를 생성하는 것은 퍼포먼스에 상당한 부담이 될 수도 있다. 이미 생성된 객체를 복사하는 방법도 있겠지만, 저렇게 생성자를 통해 생성된 객체는 복사한 후에 프로토타입 링크도 전부 다시 연결해줘야하기 때문에, 일반 객체를 복사하는 것에 비해 그리 가벼운 작업은 아니다.(실제로 몇 번 해봤는데, 퍼포먼스가 생각보다 안 나온다) 이런 상황에서 섣불리 불변성을 유지한답시고 저런 코드를 작성하면 프로그램 전체의 퍼포먼스가 크게 저하될 수도 있기 때문에 각 상황에 맞는 현명한 판단이 필요하다. 마치며사실 이번 포스팅에서는 불변성에 대한 설명과 더불어, 일급 시민이라는 개념을 사용한 커링과 같이 기술적인 부분에 대한 이야기도 함께 하려고 했는데, 또 분량 조절에 실패해버렸다. 아무래도 요즘 관심을 많이 가지고 있는 내용이다보니 자꾸 내용이 길어지는 듯 하다. 최근 불변성이라는 키워드가 프론트엔드 쪽에서 많이 주목받고 있기는 하지만 사실 불변성이 프론트엔드에서만 주목받는 키워드는 아니다. 본래 불변성이 주목받기 시작한 이유는 변경 가능한 상태를 여러 곳에서 공유하게 됨으로써 발생하는 여러가지 문제를 해결하기 위함이었기 때문이다. 일반적으로 이런 문제는 멀티 쓰레딩과 같은 동시성 프로그래밍을 사용할 때 많이 발생했는데, 기존에는 상태에 접근할 수 있는 권한을 의미하는 일종의 락(Lock)을 걸어놓고 락이 풀린 상태에만 쓰레드가 상태에 접근할 수 있도록 허가하는 방식을 주로 사용했었다. 그러나 이러한 상태가 한두개도 아닐 뿐더러, 실수로 락을 잘못 걸어서 상태가 꼬여버려도 개발자가 알아차리기 힘든 것은 매한가지이기 때문에, 변경 가능한 상태를 아예 없애버리는 불변의 개념이 각광받기 시작한 것이다. 오히려 Erlang이나 Rust 같은 언어들은 자바스크립트보다 더 빡빡한 방법으로 불변성을 지원하고 있기 때문에, 이 키워드에 관심이 많으신 분들은 해당 언어를 한 번 체험해보는 것도 좋겠다는 생각이 든다. (필자는 Rust를 한번 해볼까 생각 중이다) 하지만 자바스크립트 또한 특유의 자유로운 언어의 성격 때문에 ES5 시절부터 무분별한 상태 관리에 많은 개발자들이 고통받았었고, 점점 더 웹 프론트엔드 어플리케이션이 고도화되고 복잡한 상태 관리를 요구하게 되면서 이런 개념을 사용하게 되었다. 실제로 필자 또한 상태의 변화를 추적할 수 없는 상황에서 발생한 버그를 디버깅하느라 고생한 적이 너무나도 많았기 때문에, 이러한 불변의 개념을 처음 알았을 때 꽤나 관심있게 지켜봤던 기억이 있다. 하지만 앞서 이야기했듯이 불변성을 유지하며 프로그래밍을 한다는 것이 모든 상황에서의 정답이 될 수는 없다. 필자도 현재 작업 중인 토이 프로젝트인 Web Audio 에디터에 아무 생각없이 리덕스를 붙혔다가 위에서 이야기했던 객체 생성 비용 문제때문에 퍼포먼스가 안 나와서 고생 중이다.(위에서 예로 든 상황은 필자의 경험담이었다) 늘 이야기하는 것이지만 모든 상황에 맞아떨어지는 절대적인 기술이라는 것은 없기 때문에, 불변성이 무조건 좋다고 이야기하기보다 그저 각 상황에 맞는 현명한 의사결정을 통해 불변성을 이용하면 된다고 생각한다. 이상으로 변하지 않는 상태를 유지하는 방법, 불변성 포스팅을 마친다.","link":"/2020/01/05/what-is-immutable/"},{"title":"서버의 상태를 알려주는 HTTP 상태 코드","text":"최근의 모던 어플리케이션은 완전히 네트워크 위에서 돌아가는 프로그램이라고 해도 과언이 아닐 정도로 프로그램의 비즈니스 로직에서 통신이 차지하는 비중이 높다. 클라이언트 어플리케이션은 백엔드에 위치한 서버와 통신하여 현재 로그인한 사용자의 정보를 받아오거나, 새로운 게시글을 생성하기도 하고, 때로는 Web Socket을 통해 서버에서 발생한 이벤트를 구독하여 푸시 메세지나 채팅과 같은 기능을 구현하기도 한다. 이 과정에서 프론트엔드와 백엔드는 어떤 방식으로 통신을 할 것인지부터 시작하여 리소스의 생성과 삭제는 어떻게 정의할 것인지, 프론트엔드에서 요청한 백엔드 작업의 성공/실패 여부는 어떻게 알려줄 것인지 등 많은 규칙들을 정의해야한다. 그래서 이러한 규칙들을 정의할 때 도움을 주는 몇 가지 가이드라인들이 존재하는데, 이때 등장하는 것들이 HTTP 메소드나 상태 코드같은 표준과 REST 같은 녀석들이다. 이번 포스팅에서는 이 중에서 프론트엔드와 백엔드 간의 통신을 할 때 조금 더 명확한 정의를 위해 필요한 요소 중 하나인 HTTP 상태 코드를 파헤쳐보는 시간을 가져보려고 한다. 굳이 이러한 가이드라인을 지켜야 하나요?사실 HTTP 메소드나 상태 코드, 그리고 REST 같은 것들은 말 그대로 가이드라인에 불과하다. 이것들을 지키지 않는다고 해서 프로그램이 작동하지 않는 것도 아니고 사용자가 프로그램을 사용하던 도중 런타임 에러가 발생하는 슬픈 일도 발생하지 않는다. 즉, 지키지 않아도 사실 프로그램을 작성하는데는 아무런 지장이 없다는 것이다. 그럼 굳이 안 지켜도 상관없는 것 아닌가요? 음, 이러한 규칙들을 지키지 않는 것은 자유지만 그로 인해 발생하는 사이드 이펙트들을 생각해보면 되도록이면 지켜주는 것이 좋다고 이야기하고 싶다. 표준 인터페이스의 존재 이유를 생각해보자산업 표준은 불특정 다수에 의해 생산되는 제품들의 호환성을 맞추고, 제품 생산자들 간의 커뮤니케이션을 원활하게 하기 위해서 제정되며, 이렇게 각자 다른 객체들을 호환하기 위해 정의하는 일련의 표준 규격을 우리는 인터페이스(Interface)라고 부른다. 이 인터페이스라는 개념은 꽤나 광범위해서 뭐든 연결해주기만 할 수 있다면 인터페이스라고 생각하면 된다. 모니터와 컴퓨터를 연결하는 HDMI, 저장 장치에 사용되는 SATA, USB와 같은 친구들도 전부 인터페이스다. 심지어 UI(User Interface)같은 경우에는 기계와 기계가 아니라 인간과 기계를 이어준다는 개념으로까지 사용된다. 그 중 개발자들에게 가장 친숙한 인터페이스는 바로 API(Application Programming Interface)이다. API는 응용 프로그램을 제작할 때 필요한 기능들을 일련의 인터페이스로 제공된 것을 의미한다. 이때 API를 사용하는 쪽에서는 API의 사용법만 알면 되고 그 이면에 어떤 거대한 로직들이 숨어있는지는 일절 관심을 끊어도 되기 때문에 굉장히 편리하다는 장점이 있다. 대표적인 API의 한 종류로는 C에서 제공하는 Windows 운영체제의 API가 있다. 123456789101112#include #include int APIENTRY _tWinMain( HINSTANCE hInstance, HINSTANCE hPrevInstance, LPTSTR lpCmdLine, int nCmdShow) { MessageBox(NULL, TEXT(\"Hello, Windows!\"), TEXT(\"App\"), MB_OK); return 0;} 개발자는 Windows 운영체제가 어떻게 저 메세지박스를 렌더하는지 모르더라도 단지 MessageBox라는 API의 함수를 사용함으로써 간단하게 메세지박스를 사용할 수 있다. 그리고 이 과정은 필자가 작성하는 C 어플리케이션과 운영체제, 전혀 다른 두 프로그램 간의 통신이기도 하다. 즉, API는 프로그램 간의 통신을 위한 인터페이스라고 할 수 있다. 마찬가지로 클라이언트가 서버에게 뭔가를 요청할 때도 특정 규칙으로 정의된 API를 사용하여 서버의 리소스를 사용하게 되는데, HTTP를 사용하여 통신하는 대부분의 모던 어플리케이션에서는 이 API를 엔드포인트(endpoint)라고 불리는 특정한 URL을 사용하여 정의하게되며, 서버는 일관된 방식으로 이 엔드포인트로 들어온 클라이언트 요청에 대한 응답을 보내줘야한다. 이때 HTTP 상태 코드는 클라이언트가 보냈던 요청의 수행 결과를 의미하는 일종의 약속이며, API를 구성하는 중요한 요소 중 하나이다. 백엔드는 잘 모르는 프론트엔드의 슬픈 사정이 섹션에서는 잘못 정의된 API를 사용하는 프론트엔드 개발자라면 한번쯤은 겪어보았음직한 일을 한번 짧게 이야기해보려고 한다. 아마 백엔드 개발자들은 프론트엔드 어플리케이션의 소스를 직접 보는 경우가 드물기 때문에 이런 상황이 있다는 사실조차 모를 수 있을 것 같다. 바로 HTTP 상태 코드를 잘못 사용하고 있는 경우인데, 이런 상황에 대한 대표적인 예시는 바로 요청이 실패했을 때에도 상태 코드를 요청 성공을 의미하는 200 Ok로 내려주는 것이다. 1GET /api/users/123 12HTTP/1.1 200 OK{ \"success\": false } 이렇게 설계한 API의 경우, 위 예시처럼 HTTP 응답 바디에 요청의 성공/실패 여부나 실패 이유를 함께 담아서 보내주는 경우가 대다수인데, 그러면 프론트엔드 어플리케이션에서는 처리가 약간 애매해지는 상황이 발생한다. 프론트엔드에서는 이런 비동기 요청을 Promise를 통해서 처리하게 되는데, 문제는 대부분의 HTTP 통신 라이브러리나 API들은 백엔드에서 보내주는 요청의 상태 코드에 따라 요청의 성공/실패 여부를 판단하고, 요청이 실패했을 경우에만 에러를 던진다는 것이다. 그래서 일반적인 경우, 프론트엔드 어플리케이션에서는 대략 이런 느낌으로 통신을 담당하는 코드를 작성한다. 123456789async function fetchUsers () { try { const response = await fetch('/api/users/123'); return response.json(); } catch (e) { alert('요청이 실패했어요!'); }} 서버로 보냈던 요청이 실패했다면 서버는 반드시 400이나 500번대의 상태 코드를 보내줄 것이고, 그렇게 되면 fetch API는 에러를 발생시킨다. 그래서 fetch를 사용할 때는 단순히 외부에서 try/catch 구문을 사용하는 것만으로도 간단하게 통신에 대한 에러를 핸들링할 수 있는 것이다. 하지만 위의 잘못된 예시처럼 백엔드 어플리케이션에서 요청이 실패했음에도 불구하고 상태 코드로 200번대 코드를 내려준다면 프론트엔드 어플리케이션의 코드에는 이런 슬픈 상황이 발생한다. 1234567891011async function fetchUsers () { try { const response = await fetch('/api/users/123'); const { success } = await response.json(); if (!success) { throw new Error(); } } catch (e) { alert('요청이 실패했어요'); }} 아까 전에는 없던 if (!success)가 생긴 것을 볼 수 있다. 즉, 불필요한 예외 처리가 한번 더 발생한 것인데, 이런 불필요한 예외처리는 코드의 가독성을 해치지만 프론트엔드 입장에서는 딱히 선택권이 없다. 그렇다고 서버가 보내주는 에러를 무시하고 핸들링을 안 할수도 없지 않은가? 게다가 백엔드 어플리케이션이 미처 핸들링하지 못한 에러가 발생하거나 서버가 아예 죽어버리기라도 하면 응답의 상태 코드에는 에러 코드인 500이나 502가 내려올 것이기 때문에 try/catch 구문을 사용하지 않을 수도 없다. 클라이언트에서 사용하는 모든 HTTP 통신 라이브러리들은 올바른 HTTP 상태 코드의 사용을 가정하고 설계되었기 때문에, 서버가 올바르지 않은 상태 코드를 사용한다면 이런 슬픈 상황이 발생할 수도 있다는 점을 이야기해두고 싶다. 그리고 사실 백엔드 어플리케이션에서 상황에 맞는 올바른 상태 코드를 내려주는 것이 그렇게 어려운 일도 아니다. (다만 조금 귀찮을 뿐이다) 클라이언트와 마찬가지로 대부분의 서버 프레임워크에서 제공하는 통신 라이브러리들도 모든 상황에 맞는 HTTP 상태 코드들을 제공하고 있으니 되도록이면 알맞은 상황에 맞는 상태 코드를 사용하는 것을 추천한다. 자, 그럼 이제 본격적으로 이 수많은 HTTP 상태 코드들이 정확히 어떤 상태를 의미하는지 알아보도록하자. 작업의 수행 상태를 알려주는 HTTP 상태 코드클라이언트가 서버에게 작업을 요청하면 서버는 요청받은 작업을 수행한 후 작업의 수행 결과를 응답으로 보내주는데, 이때 HTTP 상태 코드를 사용하여 작업의 성공/실패 여부와 작업이 실패했다면 어떤 이유로 실패했는지도 알려주게 된다. 위에서 보았던 잘못된 예시처럼 HTTP 응답 바디에 작업의 실패 여부를 담아서 응답해주는 경우도 있지만, 더 좋은 방법은 바로 올바른 HTTP 상태 코드를 사용하는 것이다. HTTP 상태 코드는 200 = 성공, 400 = 클라이언트가 요청 잘못함, 500 = 서버가 잘못함과 같이 각 상황에 맞는 코드가 표준으로 정해져있으며, 웹 상에서 돌아가는 기본적인 프로그램의 동작이나 프론트엔드, 백엔드 프레임워크들의 설계 또한 이 표준을 기준으로 만들어져 있기 때문에 되도록이면 이 표준을 지켜주는 것이 좋다. HTTP 프로토콜을 사용하는 대표적인 프로그램인 웹 브라우저 또한 이러한 상태 코드 표준을 엄격하게 지키는 녀석 중 하나인데, 실제로 브라우저는 서버가 어떤 상태 코드를 응답으로 내려주는지에 따라 이번에 자신이 보낸 요청의 성공/실패 여부를 구분하고, 이를 시각적으로 표현해주기도 한다. 브라우저는 200번대의 상태 코드와 400, 500번대의 상태 코드를 전혀 다르게 인식한다 이런 상황에서 서버가 상태 코드는 200인데 응답의 바디로만 에러를 표현한다고 하면, 브라우저는 요청이 성공했다고 생각하지만 실제로는 요청이 실패한, 요상한 상황이 발생하게 된다. 심지어 서버가 응답의 상태 코드로 301과 같은 코드를 내려준다면, 브라우저는 자동으로 사용자를 다른 페이지로 리다이렉트(Redirect)해버리기 때문에 서버가 제대로 된 상태 코드를 응답에 담아주지 않는다면 브라우저가 예측하지 못한 동작을 일으킬 수도 있다. 자, 그럼 이제 각 HTTP 상태 코드가 어떤 상태들을 의미하는 것인지 하나씩 살펴보도록 하자. HTTP 상태 코드는 100번대 부터 500번대까지로 이루어져 있으며 꽤나 다양한 상태들을 정의할 수 있지만, 이걸 다 알 필요도 없고 설명하려면 너무 길기도 하니, 필자가 단 한번이라도 사용해보았던 상태 코드들을 기준으로 설명을 진행하려고 한다. 100번대100번대 코드는 프로토콜을 교체해도 된다거나 계속 요청을 보내도 된다거나하는 식의 정보성을 띄고 있는 상태를 의미하지만, 실제로 필자가 어플리케이션을 개발하며 이 상태 코드들을 만나본 사례는 아직 단 한번도 없기 때문에 건너뛰도록 하겠다. 200번대200번대 코드들은 클라이언트가 요청한 작업을 서버가 성공적으로 수행했다는 상태라는 것을 알려주는 코드이다. 200번대 코드들은 브라우저의 콘솔의 네트워크 탭에서도 깔끔한 초록색으로 표시해준다. 물론 “요청한 작업이 성공”이라는 응답만으로도 클라이언트가 원하는 정보를 모두 만족시킬 수 있긴 하지만, 조금 더 디테일한 상태를 정의해야하는 상황이라면 이 200번대의 상태 코드를 적극적으로 사용하여 클라이언트에게 더 자세한 정보를 알려줄 수도 있다. 200 OK상태 코드 200은 단순히 작업이 성공했음을 의미한다. 대부분의 경우 클라이언트는 자신이 요청한 작업이 정확히 어떤 작업인지 알고 있기 때문에, 서버에서 “니가 보낸 요청이 성공했어”라는 정보만 알려주면 굳이 그 이상의 디테일한 정보는 알 필요가 필요없다. 그래서 이 상태 코드 하나만으로 모든 API 응답 성공 상태를 퉁치는 경우가 대다수이다. 201 Created상태 코드 201은 말 그대로 요청이 정상적으로 수행되었고, 그로 인해 리소스가 새롭게 생성되었다는 것을 의미한다. 클라이언트가 서버에게 요청을 보내서 새로운 리소스를 생성하는 상황은 굉장히 흔한데, 그 중 필자가 경험했던 대표적인 사례는 바로 “회원가입”이다. 결국 클라이언트의 회원가입 요청으로 인해 데이터베이스에 새로운 유저의 로우가 생성되었기 때문에, 이런 경우가 201 상태 코드가 아주 잘 들어맞는 케이스라고 볼 수 있다. 204 No Content상태 코드 204는 요청이 정상적으로 수행되었고, 이 요청과 관련되었던 컨텐츠 또한 더 이상 깔끔하게 존재하지 않음을 의미한다. 이 상태 코드는 클라이언트가 서버에게 요청을 보내서 뭔가를 삭제해야하는 응답으로 사용될 수 있고, 실제로 필자가 경험했던 사례 또한 게시글을 삭제하는 API였다. 참고로 이때 이 삭제 작업이 Soft Delete냐 Hard Delete냐와는 아무런 상관이 없다. 서버에서 어떤 방식으로 리소스의 삭제를 표현하던 클라이언트가 알아야할 정보는 “이 리소스는 삭제되었고, 더 이상 사용할 수 없다” 뿐이라는 사실을 명심하자. 300번대300번대 코드들은 리다이렉션에 관련된 상태들을 의미한다. 클라이언트가 요청한 리소스가 옮겨졌거나 리소스가 삭제되었거나해서 정상적인 방법으로는 더 이상 해당 리소스에 접근할 수 없고 다른 URL을 통해서 그 리소스에 접근해야하는 경우 서버는 “여기로 가면 니가 찾는 리소스가 있어!”라는 정보를 알려줄 수 있는데, 이때 사용되는 상태 코드들이 바로 300번대 코드들이다. 301 Moved Permanetly상태 코드 301은 301 Redirect라는 별칭으로 불리기도 할 만큼 리다이렉션을 위한 코드 중 가장 많이 사용되는 녀석이다. 브라우저는 자신의 대한 요청의 응답으로 301을 받으면 HTTP 헤더에 들어있는 Location 필드를 찾아보고, 해당 필드가 존재할 경우 Location 필드에 담긴 URL로 자동으로 리다이렉션한다. 12HTTP/1.1 301 Moved PermanetlyLocation: https://evan/moved-contents/1234 또한 구글과 같은 검색 엔진의 봇들은 특정 페이지에 접근했는데 응답으로 301 상태 코드를 받을 경우 자동으로 페이지 정보를 갱신하기도 하기 때문에, SEO(Search Engine Optimization) 관점에서도 이 상태 코드를 올바르게 사용하는 것은 매우 중요하다. 이런 리다이렉션 설정은 보통 서버 엔진의 설정 파일 내에서도 할 수 있고, 백엔드 어플리케이션 내에서 직접 할 수도 있다. 일반적인 경우 이 상태코드는 HTTP 프로토콜로 접속한 사용자를 HTTPS 프로토콜을 사용해야만 접근 가능한 포트로 보내버릴 때에도 많이 사용된다. 1234567891011server { listen 80; server_name evan.com; return 301 https://$host$request_uri;}server { listen 443 ssl; server_name evan.com; ...} 이 경우 80 포트로 접속한 사용자를 발견한 Nginx는 HTTPS 프로토콜을 사용해야만 접근할 수 있는 443 포트로 리다이렉트시켜서 해당 프로토콜 사용을 강제할 수 있다. 304 Not Modified상태 코드 304는 클라이언트가 요청한 리소스가 이전 요청떄와 비교해보았을 때 전혀 달라진 점이 없다는 것을 의미한다. 즉, 말 그대로 Not Modified, 수정되지 않음이다. 서버가 응답으로 이 상태 코드를 보내주면 클라이언트는 굳이 서버에게 리소스를 재전송받아야할 필요가 없기에 자신이 캐싱해놓았던 리소스를 사용하게되며, 이 과정에서 불필요한 통신 페이로드의 낭비를 줄일 수 있다. 이 과정에서 클라이언트는 서버로부터 요청된 리소스를 받은 것이 아니라 자신의 캐싱해놓았던 리소스를 사용하는 것이므로 이 또한 캐싱된 리소스로 리다이렉션되었다고 치는 것이다. 그런 이유로 304 상태 코드는 암묵적인 리다이렉션으로 불리기도 한다. 브라우저 역시 이 응답을 위한 자체 캐싱 기능을 가지고 있으며, 만약 304 상태 코드를 응답으로 받았는데 캐싱된 리소스가 없는 경우에는 빈 화면을 띄우거나 에러 화면이 노출된다. 그러니 이런 상황을 만나면 “브라우저에 Cached Resource가 없는 거 아님?”이라는 킹리적 갓심을 발휘해볼 수 있다. 400번대400번대의 코드들은 클라이언트가 서버에게 보낸 요청이 잘못된 경우를 의미한다. 만약 이 상태 코드를 발견한다면 높은 확률로 프론트엔드 개발자가 예외 처리를 제대로 안 했거나 요청에 이상한 값이 묻은 경우가 많으니, 프론트엔드 개발자의 멱살을 잡도록 하자. (낮은 확률로 백엔드의 잘못인 경우도 있다…) 400 Bad Request상태 코드 400는 가장 많이 만날 수 있는 400번대 코드 중 하나이며, 밑도 끝도 없이 “클라이언트가 요청 잘못 날림”을 의미한다. 이때 뭘 어떻게 잘못 날렸는지는 보통 HTTP 응답 바디에 담아서 알려주는 경우도 있지만, 그렇지 않은 경우에는 백엔드 어플리케이션의 로그를 까봐야하는 슬픈 상황이 펼쳐질 수도 있다. 401 Unauthorized상태 코드 401는 인증되지 않은 사용자가 인증이 필요한 리소스를 요청하는 경우에 “너 인증 필요함”이라고 알려주는 상태 코드이다. 보통 로그인이 필요한 API를 비로그인 사용자가 호출했을 때 많이 사용된다. 클라이언트에서는 서버가 401을 응답으로 보내준 경우, 로그인이 필요하다는 것으로 판단하고 로그인 페이지로 사용자를 리다이렉션하기도 한다. 403 Forbidden상태 코드 403는 클라이언트가 접근이 금지된 리소스를 요청했음을 의미한다. 이 상태 코드는 간혹 401 Unauthorized와 헷갈리고는 하는데, 상태 코드의 의미만 보면 확실히 애매모호하지만, 사실 분명한 한 가지 차이점이 있다. 401은 말 그대로 인증되지 않았다는 것을 의미하며, 인증이 되지 않았다는 것은 백엔드 어플리케이션이 현재 요청한 사용자가 누구인지 알 수가 없다는 것을 의미한다. 즉 이때 서버는 클라이언트에게 “너의 신원을 밝혀!”라고 말하고 있는 것이다. 그러나 403의 경우, 백엔드 어플리케이션은 현재 리소스를 요청한 사용자가 누구인지 전혀 신경쓰지 않는다. 클라이언트가 현재 자신이 누구인지 밝혔던 밝히지 않았던, 인증이 되었던 안 되었던 간에, 이 리소스를 요청하는 것은 무조건 금지라고 말하고 있는 것이다. HTTPS 프로토콜로만 접근해야하는 리소스에 HTTP 프로토콜을 사용하여 접근했을 경우에 서버에서 403 응답을 보내주기도 한다. 404 Not Found상태 코드 404는 말 그대로 요청한 리소스가 존재하지 않다는 것을 의미한다. 405 Method Not Allowed상태 코드 405는 현재 리소스에 맞지않는 메소드를 사용했음을 의미한다. 백엔드 프레임워크의 경우 특정 컨트롤러에 해당 메소드를 사용하는 로직이 없다면 자동으로 405를 내려주기도 한다. 406 No Acceptable상태 코드 406은 서버 주도 컨텐츠 협상을 진행했음에도 불구하고 알맞은 컨텐츠 타입이 없다는 것을 의미한다. 사실 클라이언트는 서버에게 리소스를 요청할 때, HTTP 헤더의 Accept 필드를 사용하여 어떤 컨텐츠 타입의 리소스를 원하는지도 함께 이야기해준다. 일반적으로 이 필드를 명시하지않을 경우 브라우저는 자동으로 text/html을 비롯한 몇 가지 타입들을 스스로 정의해서 헤더에 담아주고는 한다. 123GET http://evan.com/Accept: text/html,application/xhtml+xml,application/xml,*/*... 이런 요청을 받은 서버는 클라이언트가 보낸 요청의 Accept 필드를 보고 앞에서부터 하나씩 찾아가며 요청받은 리소스와 알맞은 컨텐츠 타입이 있는지 하나씩 살펴보게 되고, 이후 알맞은 컨텐츠 타입이 있다면 HTTP 응답 헤더의 Content-Type 필드에 해당 컨텐츠 타입을 명시해주게 된다. 12HTTP/1.1 200 OKContent-Type: text/html 이 과정에서 어떤 컨텐츠 타입의 리소스를 응답으로 내려줄 것인지는 전적으로 서버가 결정하게 되므로 이 과정을 “서버 주도 컨텐츠 협상”이라고 하는 것이다. 위의 예시의 경우 클라이언트가 받기를 원했던 컨텐츠 타입 중 첫 번째 우선순위를 가진 text/html를 받아왔지만, 만약 서버에 text/html 타입의 리소스가 존재하지 않는 경우, 서버는 application/xhtml+xml, application/xml 순서로 리소스를 탐색하게 된다. 만약 앞에 나열된 모든 컨텐츠 타입이 없는 경우 클라이언트가 요청했던 컨텐츠 타입 중 가장 마지막인 */* 와일드 카드에 걸리기 때문에, 서버는 리소스가 어떤 컨텐츠 타입인지 상관하지 않고 그대로 응답해줄 것이다. 그러나 만약 클라이언트가 요청한 컨텐츠 타입을 모두 탐색했는데도 불구하고 알맞은 리소스가 없을 경우 서버는 406 상태 코드와 함께 “니가 찾는 컨텐츠 타입과 맞는 리소스가 없어”라는 응답을 주는 것이다. 408 Request Timeout상태 코드 408은 클라이언트와 서버의 연결은 성사되었지만 요청의 본문이 계속 서버에 도착하지 않는 상황을 의미한다. HTTP 프로토콜을 사용하여 통신을 할 때는 반드시 클라이언트와 서버 간의 연결을 생성하고, 그 이후에 요청 본문에 해당하는 데이터를 전송하게 되는데, 408 상태 코드는 이 과정에서 연결은 제대로 생성되었지만 서버가 아무리 기다려도 클라이언트가 보냈던 요청 본문을 받지 못하는 경우에 발생하게 된다. 429 Too Many Requests상태 코드 429는 클라이언트가 서버에 너무 요청을 많이 보내는 경우에 발생한다. 너무 많이 보냈다는 것은, 너무 짧은 시간 안에 빠르게 요청을 마구 날려대서 서버가 “워워 진정해”라고 하는 경우일수도 있고, 유료 API를 사용하는 경우에는 현재 금액으로 사용할 수 있는 API 요청 횟수를 초과해서 “돈을 더 내세요”라는 의미로 사용되기도 한다. 서버에서는 429 상태 코드와 함께 응답 헤더의 Retry-After라는 필드를 사용하여 “이 시간 이후에 재요청해봐”라는 의미를 전달할 수도 있다. 500번대500번대의 코드들은 클라이언트가 아닌 서버에서 뭔가 말썽이 일어난 경우이다. 만약 이 상태 코드를 발견했다면 서버에서 뭔가 박살났다는 의미이므로 다소곳이 백엔드 개발자의 멱살을 잡아보도록 하자. 500 Internal Server Error상태 코드 500은 백엔드 어플리케이션 내에서 뭔가 알 수 없는 에러가 발생했다는 의미이다. 대부분 제대로 핸들링되지 않은 에러가 발생한 경우가 많으므로, 에러의 원인을 클라이언트에게 알려주지 않는다.(라기 보다 알려줄 수 없는 상태인 경우가 많다) 또한 이렇게 핸들링되지 않은 에러의 원인을 클라이언트에게 고스란히 알려주는 것은 보안 사고가 발생할 가능성이 너무 크므로, 500 상태 코드로 에러의 발생 자체만을 알려주는 경우가 대부분이다. 만약 이 상태 코드를 만난다면, 바로 서버 로그를 까보거나 Sentry나 Bugsnag과 같은 에러 모니터링 솔루션을 적극 활용하는 것을 추천한다. 502 Bad Gateway상태 코드 502를 만날 수 있는 가장 흔한 상황은 바로 백엔드 어플리케이션이 죽은 상황이다. 근데 왜 Server Died와 같이 직접적인 메세지가 아니라 Bad Gateway와 같은 메세지를 보내주는 것일까? 그 이유는 백엔드 아키텍처가 아무리 간단한 구조라고 해도 절대 어플리케이션 1개로만 구성되지 않기 때문이다. 여기서 말하는 게이트웨이는 어플리케이션 간의 추상적인 연결점을 의미하는데, 이 메세지가 의미하듯 백엔드의 아키텍처는 최소 2개 이상의 어플리케이션으로 구성된 경우가 대부분이다. 일반적인 경우 클라이언트가 보낸 요청은 곧바로 백엔드 어플리케이션에 전달되는 것이 아니다. 사실 백엔드 어플리케이션에 앞단에는 아파치나 Nginx 같은 서버 엔진이나 로드밸런서 같은 친구들이 대신 요청을 받아서 백엔드 어플리케이션으로 전달해주는 경우가 대부분이다. 1234567891011121314server { listen 80; server_name evan.com; location / { proxy_pass http://127.0.0.1:3000; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; }} Nginx를 사용하면 일반적으로 이런 설정을 사용하게 되는데, 이렇게 되면 Nginx는 80번 포트에서 대기하며 HTTP 프로토콜을 사용한 요청을 받아 3000번 포트에서 대기하고 있는 백엔드 어플리케이션에게 전달해주는 역할을 수행하게 된다. 이런 아키텍처를 사용하는 이유는 보안과 처리 효율 때문이다. 백엔드 어플리케이션 자체가 완전무결한 친구가 아니기 때문에 모든 요청을 안심하고 백엔드 어플리케이션에게 먹여줄 수가 없는 것이다. 그렇다고 누가 사용하는 지도 모르는 클라이언트에서 안전한 요청만 보내줄 것이라는 기대 또한 어불성설이다. 게다가 뭔가 연산이 필요한 요청이 아닌, 파일을 찾아서 보내주기만 하는 간단한 요청 같은 경우는 굳이 안 그래도 바쁜 백엔드 어플리케이션에게 시킬 필요가 없으므로 이런 서버 엔진이 대신 처리해주기도 한다. 그래서 백엔드에서는 앞 단에 아예 프록시 서버를 두어서 문지기 역할을 시키는 것이다. 이때 이 프록시 서버와 백엔드 어플리케이션 간의 연결된 추상적인 통로를 “게이트웨이”라고 부르는 것이다. 백엔드 어플리케이션이 죽어버릴 경우 앞 단의 문지기인 프록시 서버는 백엔드 어플리케이션에게 아무런 응답을 받지 못하게 되고, 클라이언트에게 502 Bad Gateway라는 응답을 보내주는 것이다. 503 Service Unavailable상태 코드 503은 서버가 요청을 처리할 준비가 되지 않았음을 의미한다. 간혹 502 Bad Gateway와 비슷한 느낌으로 사용되기는 하지만, 503은 보다 “일시적인 상황”을 의미하는 상태 코드이며, 일반적으로 서버에 부하가 심해서 현재 요청을 핸들링 할 수 있는 여유가 없는 경우에 많이 사용된다. AWS Lambda에서는 요청을 처리할 때 컨테이너의 동시 실행 갯수를 초과할 정도의 리소스가 필요하거나 어떤 작업의 처리 시간이 Lambda에 설정된 컨테이너의 최대 수명 시간을 초과했을 경우에 발생하기도 한다. 이렇듯이 503은 일시적인 상황을 의미하므로 429 Too Many Requests와 동일하게 응답 헤더의 Retry-After 필드를 사용하여 “이 시간 이후에 다시 요청해봐”라는 의미를 클라이언트에게 전달해줄 수 있다. 504 Gateway Timeout상태 코드 504는 408과 마찬가지로 요청에 대한 타임아웃을 의미한다. 그러나 504 상태 코드는 클라이언트에서 보낸 요청 때문에 타임아웃이 발생하는 것이 아니라 백엔드 아키텍처 내부에서 서버끼리 주고받는 요청에서 발생한다. 앞서 이야기했듯이 백엔드의 아키텍처는 단순히 백엔드 어플리케이션 하나로만 구성된 것이 아니기 때문에, 클라이언트의 요청이 서버에 닿은 뒤에도 백엔드 어플리케이션끼리의 통신이 발생하게 된다. 만약 프록시 서버 역할을 맡은 Nginx가 백엔드 어플리케이션에 클라이언트의 요청을 전달했는데, 백엔드 어플리케이션이 일정 시간 동안 응답을 하지 않는 경우 Nginx는 클라이언트에게 504 Geteway Timeout을 내려주게 되는 것이다. 마치며사실 이번 포스팅에서는 HTTP 상태 외에도 RESTful API에 대한 내용도 함께 이야기하려고 했지만, 다시 한번 분량 조절에 대실패하면서 포스팅을 나누어 작성하게 되었다.(점점 빈도가 잦아진다…) 앞서 이야기했듯이 이런 상태 코드와 같은 요소들은 딱히 안 지킨다고 해서 프로그램에서 에러가 발생하는 것도 아니기 때문에 가볍게 생각하고 넘어가기 쉽상이지만, 보다 명확한 인터페이스를 정의하게되면 프로그램의 작동을 예측하기도 쉬워지고, 프론트엔드와 백엔드 개발자 간의 커뮤니케이션에도 큰 도움이 되기 때문에 되도록이면 표준을 지켜주는 것을 권장한다. 이상으로 서버의 상태를 알려주는 HTTP 상태 코드 포스팅을 마친다.","link":"/2020/03/15/about-http-status-code/"},{"title":"PWA 하루 만에 도입하기(삽질기)","text":"이번 포스팅에서는 필자가 회사에서 2019년 7월 5일 금요일 하루 동안 기존 어플리케이션에 PWA(Progressive Web Application) 기능을 붙힌 삽질기를 기록하려고 한다. PWA는 지원하지 않는 브라우저에 대한 예외처리만 꼼꼼하게 해주면 UX, 성능, SEO 등에서 무조건 플러스 요인이기 때문에 예전부터 계속 해보고 싶었다. 하지만 시간이 없어서 계속 미루고 있었는데 마침 어제 간만에 필자에게 여유 시간이 주어졌다.필자가 지금 작업하고 있는 프로젝트가 회사의 비즈니스 모델과 밀접한 관련이 있는 프로젝트이고, 또 워낙 이 기능에 관련된 사내 이해관계자(Stakeholder)들이 많아서 PO가 테스트를 좀 더 꼼꼼히 하고 싶다고 했기 때문이다. 그래서 금요일 하루, 정확히 말하면 오전에는 버그 터진거 하나 핫픽스하고 점심먹고나서 자료 조사도 한시간 쯤 해본 다음에 15시 30분 쯤부터 시작해서 22시 55분까지 달렸다. 원래 필자는 근무시간에 열심히 하고 다른 시간은 나에게 투자하자는 주의라 야근은 왠만하면 안하는데, 필자에게 주어진 시간이 하루 밖에 없어서 그 안에 무조건 끝내야 했던 것도 있지만 사실 제일 큰 이유는… 네, 쉽게 보고 덤볐다가 쳐맞았습니다. 필자는 사실 PWA를 구현해본 경험이 없다. 구글에서 제공해주는 데모는 몇번 돌려본 적이 있지만 나머지는 그냥 다른 굇수분들의 블로그를 보고 오...나도 해보고 싶군 정도로만 생각했었다.근데 다른 사람들이 구현한 걸 보면 뭐 코드가 복잡한 것도 아니고 서비스 워커(Service Worker)도 작동 원리가 그렇게 생소한 느낌은 아니기에 사실 얕보고 있었다. 원래 필자의 계획원래 필자도 PWA에게 쳐맞기 전까진 그럴싸한 계획이 있었다. 물론 필자는 자세히 공부를 하고 덤비는 타입이 아니라 그냥 대충 알아본 다음에 나머지는 직접 맞아가면서 배우는 타입이라 더 그랬던 것도 있다. 어쨌든 어제 점심을 먹고 와서 필자가 사무실에 앉아서 가만히 생각을 해본 결과, PWA나 해볼까...? Manifest 넣고 서비스 워커 붙히면 뭐 나머지는 문서보고 해도 대충 될 거 같은데...?로 결론이 나왔다. 그래도 PWA의 모든 기능을 다 구현하는 것은 퇴근 시간인 19시까지는 힘들 것 같아서 작업에 대한 스코프를 잡았다. 기능을 전부 구현하는 건 힘들 것 같으니 일단 기반을 잡아놓는다고 생각하자. 서비스 워커 설치. Manifest.json 추가. 이건 남들 다하는 거니까 우리도 기본적으로 해야함. Pusher SDK와 PushManager를 사용해서 브라우저가 꺼져있더라도 사용자에게 푸시 메세지를 보여주자. 시간 되면 모바일에서 홈스크린에 어플리케이션을 추가하는 것까진 해보고 싶다.(우선순위 낮음) 오프라인 캐싱은 다음 시간에…(나름 스코프 조절) 문에반프로 계획러 자 이제 계획이 잡혔으니 PO와 테스터에게 프로젝트 테스트 하시는 동안 저는 저만의 놀이터에서 놀다 오겠습니다라고 협의(라고 쓰고 통보라고 읽는다.)를 한 뒤 마스터에서 브랜치를 하나 땄다. 브랜치 이름도 필자의 의지가 돋보이는 feature/service-worker-web-push로 딱 지어놓고 15시 30분 쯤 부터 작업을 시작했다. 그래도 이정도면 한 4~5시간 안에 충분히 가능하겠다 싶었는데… 코딩하는 내내 필자의 정신상태 결과적으로 저 중에서 달성한 제대로 달성한 목표는 1, 2, 3번 뿐이이다. 4번 목표인 푸시 메시지의 경우, Background 메세징은 삽질만 하다가 시간이 너무 많이 가서 실패하고 Notification API를 사용하여 Foreground에서만 노출되도록 구현했다. 처음에는 숨고 모바일 앱에서 이미 사용하고 있는 FCM(Firebase Cloud Messaging)을 사용하려고 했는데 그러면 웹 클라리언트의 푸시 채널이 이원화되기 때문에 일단 테스트도 해볼 겸 FCM없이 서비스 워커의 PushManager만 사용해서 구현하려고 했다. 이제 필자가 이것들을 작업하면서 어떤 문제에 봉착했고, 어떻게 해결했는지 한번 설명해보겠다. 제 1 관문, Manifest.json제일 처음 한 일은 manifest.json을 생성하는 것이다. 이건 그냥 말 그대로 생성하면 된다. 또한 manifest.json은 어차피 정적인 파일이기도 하고 업데이트도 잦지 않은 파일이기 때문에 굳이 Express에서 응답하지 않아도 된다. 그래서 프로젝트 내부의 static 디렉토리에 manifest.json을 생성하고 nginx가 바로 응답해주는 방식으로 작성했다. static/manifest.json12345678910111213141516171819202122232425{ \"name\": \"숨고\", \"short_name\": \"숨고\", \"icons\": [{ \"src\": \"https://d1hhkexwnh74v.cloudfront.net/app_icons/1x.png\", \"type\": \"image/png\", \"sizes\": \"48x48\" }, { \"src\": \"https://d1hhkexwnh74v.cloudfront.net/app_icons/2x.png\", \"type\": \"image/png\", \"sizes\": \"64x64\" }, { \"src\": \"https://d1hhkexwnh74v.cloudfront.net/app_icons/3x.png\", \"type\": \"image/png\", \"sizes\": \"128x128\" }, { \"src\": \"https://d1hhkexwnh74v.cloudfront.net/app_icons/3x.png\", \"type\": \"image/png\", \"sizes\": \"144x144\" }], \"start_url\": \"/?pwa=true\", \"display\": \"fullscreen\", \"background_color\": \"#FFFFFF\", \"theme_color\": \"#00C7AE\",} manifest.json에 대한 자세한 내용은 Google Web Developer 문서의 The Web App Manifest를 참고해서 작성했다. 그리고 저 아이콘은 지금 숨고 앱에서 사용하고 있는 아이콘들이다. 굳이 모바일 앱과 다른 아이콘을 사용할 이유도 없고, 디자이너 분들도 바빠서 멘탈나간 상황이기 때문에 필자의 기술적인 욕심 때문에 아이콘을 만들어 달라고 하기엔 너무 미안했다. 또한 앱과 웹이 같은 아이콘을 사용해야 브랜딩 측면에서도 좋을 거라 생각해서 모바일 앱 레파지토리를 클론받아서 몰래 훔쳐왔다. 그리고 저 이미지들은 굳이 프로젝트 내에 저장할 필요가 없으므로 회사에서 사용하는 S3 버킷에 업로드하고 CloudFront로 딜리버리했다. 위에서 설명했듯이 필자는 manifest.json에 대한 요청을 Express가 처리하는 것이 아니라 서버 엔진인 nginx가 처리하는 방식을 선택했는데, 이렇게 nginx가 서버 어플리케이션까지 요청을 토스해주지 않고 그냥 알아서 서빙하도록 하고 싶다면 간단한 설정을 추가하면 된다. 1234567server { ... location ~ ^/static { root /your/project/location; } ...} 이런 식으로 설정하면 바로 프로젝트 내의 static 디렉토리에 접근할 수 있다. /static으로 시작하는 요청이 들어오면 /your/project/location 경로에서 니가 알아서 찾아줘~라는 의미이다. 하지만 로컬에서 개발 서버를 사용할 때에는 nginx를 사용하지 않고 NodeJS를 사용하여 바로 개발용 서버를 띄우므로 로컬 환경에서는 Express가 직접 파일을 서빙할 수 있도록 해줘야 한다. 123if (process.env.NODE_ENV === 'local') { app.use('/static', serve('./static'));} 이제 브라우저에게 나의 Manifest 파일이 여기 있으니 가져가시오라고 알려줄 수 있는 link 태그를 하나 넣어주면 끝이다. constants/meta.constant.js12345678export default { // ... link: [{ rel: 'manifest', href: '/static/manifest.json', }], // ...}; 1 필자는 vue-meta 라이브러리를 사용하고 있기 때문에 이렇게 Object형 객체를 리턴하는 방식을 사용하면 렌더링 때 내부에 알아서 넣어준다.그 후 브라우저가 manifest.json을 제대로 가져가는지는 크롬 개발자 도구의 Application > Manifest에서 확인할 수 있다. 자, 여기까지 하는데 거의 10분 정도 걸렸던 것 같다. 아직까지는 필자의 계획대로 순탄히 흘러가고 있었다.하지만 문제는 이제부터 생기기 시작한다… 제 2 관문, Service Worker기분 좋게 커피 한잔 때리고 와서 이제 서비스 워커를 적용하는 작업을 시작했다. 서비스 워커는 브라우저에 설치하고 나면 백그라운드에서 실행되는 프로세스로, 웹 어플리케이션의 메인 로직과는 전혀 별개로 작동한다. 서비스 워커와 웹앱의 메인 로직은 서로 메세지를 주고 받는 방식으로 작동한다. postMessage로 보내면 onMessage로 받는 방식인데, WebView나 Chrome Extension과 같은 방식이기 때문에 이미 우리에겐 익숙한 방식이다. 단, 아직까지 모든 브라우저에서 지원되는 것이 아니기 때문에 반드시 navigator 전역 객체 내부에 serviceWorker 객체가 존재하는 지 확인한 후 초기화를 진행해야한다. 1if ('serviceWorker' in navigator) { /* ... */ } 뭐 이 정도 쯤이야 필자같은 프론트엔드 개발자들에게는 굉장히 익숙한 상황이기 때문에 그냥 웃고 넘길 수 있지만 이것보다 더 귀찮은 게 있었다. 바로 서비스 워커의 개발환경 세팅이다. 개발환경 세팅서비스 워커는 일반적인 웹 어플리케이션보다 많은 기능에 접근할 수 있기 때문에 보안이 굉장히 중요하다. 그래서 서비스 워커는 제한적인 환경에서만 작동할 수 있도록 만들어졌는데 그 조건은 딱 두가지이다. 웹 어플리케이션의 호스트가 localhost이거나 HTTPS 프로토콜을 사용하고 있을 것 불행히도 필자는 이 두가지 모두 다 해당이 안된다. 필자의 회사는 로컬에서 http://local.soomgo.com이라는 오리진을 사용하고 있기 때문에 서비스 워커 객체가 활성화 되지 않는다.그럼 해결 방법은 2가지로 줄어든다. 내가 localhost로 접속하거나 로컬에 HTTPS를 붙히거나. 사실 필자는 이걸 놓쳐서 아 서비스 워커 어디갔어? 왜 안돼? 죽을래?로 컴퓨터랑 실랑이하느라 한 30분 날려먹었다. 공식 문서에 버젓이 한국어로 적혀있는 내용이므로 항상 공식 문서를 꼼꼼히 읽자… 개발 서버를 HTTPS로 띄우자!근데 또 필자가 localhost로 맞추기에는 왠지 컴퓨터 따위에게 지는 기분이라 그냥 개발 환경에 HTTPS를 붙히기로 했다.Express는 내장 객체로 https 객체를 지원해주고 있기 때문에 간단하게 셋업할 수 있다. 먼저 HTTPS를 사용하기 위해서는 SSH 키쌍이 필요하므로 openssl을 사용하여 키를 만들어 주었다. 123$ openssl genrsa 1024 > private.pem # 비공개키 생성...$ openssl req -x509 -new -key private.pem > public.pem # 공개키 생성 그 다음 이 키를 프로젝트의 적당한 곳에 저장한 다음 그냥 사용하면 되는데 여기서 주의 사항. > 생성한 ssh키는 로컬 개발환경에서만 사용되어야합니다.> 이 디렉토리 내의 *.pem 파일은 gitignore에 추가되어있습니다. /keys/README.md 절대 리모트 저장소에 SSH 키를 업로드하지 말자. 보안 터진다. 물론 이 키로 뭔가 실질적인 인증을 하는 것이 아니라 로컬 개발 서버에서만 사용하기 때문에 뭔가 중요한 걸 털릴 염려는 없지만 그래도 습관이 중요하다. SSH 키는 폭탄 다루듯이 다루자. 대신 필자는 다른 개발자들이 쉽게 HTTPS 환경을 세팅할 수 있도록 프로젝트 내에 키를 저장하는 디렉토리 내부에 README.md를 작성해놓고 이 키가 어떤 용도로 사용될 것인지 주의 사항은 무엇인지 상세하게 적어놓았다.그리고 개발할 때는 HTTPS가 아니라 HTTP로 서버를 띄울 일도 생길 수 있으므로 개발 서버를 띄울 때 옵션을 사용하여 프로토콜을 선택할 수 있도록 해주었다. package.json123456{ \"scripts\": { \"serve\": \"cross-env NODE_ENV=local node server\", \"serve:https\": \"cross-env NODE_ENV=local node server --https\", }} server.js1234567891011121314151617181920212223const express = require('express');const https = require('https');// node의 옵션들은 배열 형태로 process.argv에 담겨있다.const useHttps = process.argv.some(val => val === '--https');const app = express();// ...if (isLocal) { let localApp = app; if (useHttps) { localApp = https.createServer({ // 아까 생성한 키는 여기서 사용한다! key: fs.readFileSync('./keys/private.pem'), cert: fs.readFileSync('./keys/public.pem'), }, app); } localApp.listen(port, host, () => { debug(`${isHttps ? 'https://' : 'http://'}${host}:${port}로 가즈아!!!!`); });}// ... 이런 식으로 작성해놓으면 개발 서버를 올릴 때 원하는 프로토콜을 자유자재로 변경할 수 있기 때문에, 혹시 내가 개발 서버를 HTTPS로 바꿔놔서 다른 개발자 컴퓨터에서 개발 서버가 안 올라가도 일단 시간을 벌어놓고 버그를 수정할 수 있다. 원래 진짜 고수는 맞기 전에 맞을 곳을 예상해서 미리 힘을 주고 있는 법이다.(많이 맞아본 1인) 개발 서버를 HTTPS로 띄우면 이제 navigator 객체 내부에 이쁘게 들어가있는 serviceWorker 객체를 확인할 수 있다. 12> navigator.serviceWorker< ServiceWorkerContainer {ready: Promise, controller: null, oncontrollerchange: null, onmessage: null} HTTPS를 쓰지 않고 문제 회피하기사실 필자도 처음에는 로컬에 HTTPS 붙히기가 귀찮아서 리서치하다가 찾은 얌생이인데, 로컬 서버에서도 HTTPS를 사용하지 않고도 서비스 워커를 사용할 수 있는 방법이 있다. chrome://flags/#unsafely-treat-insecure-origin-as-secure에 들어가서 해당 기능을 활성화하고 텍스트 필드에 원하는 호스트를 입력해놓으면 해당 호스트는 안전하다고 판단하여 서비스 워커를 사용할 수 있게 해준다. 이렇게 등록한 뒤 하단의 Relaunch Now 버튼을 클릭하면 크롬이 재시작되면서 설정이 변경된다. 하지만 이 방법은 스스로 보안 취약점을 만들어 내는 것이기 찜찜해서 결국 사용하지는 않았다. Service Worker 작성이제 개발환경 세팅이 다 끝났다면 이제부터는 딱히 귀찮은 건 없다. 그냥 문서보면서 쭉쭉 작성하면 된다. 일단 서비스 워커의 기능을 구현하기 전에 잘 작동하는 지 부터 확인해야 하므로 아무 내용이 없는 서비스 워커부터 만들었다. 근데 이것도 manifest.json처럼 그냥 웹 어플리케이션과 완전 분리된 static/service-worker.js로 따로 작성하면 만들 때는 편하긴 한데, 나중에 서비스 워커에서 웹 어플리케이션에서 사용하고 있는 모듈이나 데이터에 접근하고 싶을 때 굉장히 애매해질 것 같아서 그냥 Webpack을 사용해서 같이 빌드하기로 결정했다. 일단 service-worker.js 파일을 만들자!Webpack으로 빌드한다고 해도 이 친구가 무에서 유를 창조하는 친구는 아니기 때문에 당연히 소스 파일은 있어야 한다. 일단 서비스 워커가 작동하는 것을 보는 것이 목적이므로 심플하게 작성해주었다. src/service-worker.js123self.addEventListener('message', event => { console.log('저 쪽 테이블에서 보내신 겁니다 -> ', event);}); Service Worker Webpack Plugin 사용하기ServiceWorkerWebpackPlugin은 구글에 Service Worker Webpack을 검색하면 가장 상단에 나오는 플러그인이다. 깃허브 레파지토리에 가서 다들 구경 한번 해보자. README를 읽어보니 사용법이 초간단 그 자체다. 그냥 Webpack 설정에 넣어주면 끝이다. build/webpack.client.config.js123456789module.exports = merge(baseConfig, { plugins: [ // ... new ServiceWorkerWebpackPlugin({ entry: path.resolve(__dirname, '../src/service-worker.js'), }), // ... ]}) 서비스 워커 파일 경로를 찍어주고 ServiceWorkerWebpackPlugin 객체를 생성할 때 초기 인자로 넘겨주면 Webpack 설정의 output에 정의된 경로에 sw.js 파일을 생성해준다. 물론 이 파일 이름은 변경할 수 있으니 모두 각자의 취향대로 애정을 듬뿍 담은 이쁜 이름을 지어주자. 필자는 귀찮으니까 그냥 sw로 가기로 했다. 이쁘게 태어난 sw.js 근데 이 방법의 단점이 있는데, 일반적인 서비스 워커보다 파일의 크기가 커진다는 것이다. 아무래도 Webpack으로 빌드하다보니 서비스 워커의 외부에 있는 모듈이나 라이브러리를 끌어오기 때문이다.뭐 이건 애초에 필자가 조금 편하게 쓰려고 선택한 상황이니 어쩔 수 없긴하다. 어쨌든 이런 단점이 있으니, 서비스 워커의 크기를 줄이고자 하시는 독자분들은 직접 작성하시는 게 더 좋을 수도 있다. 이제 서비스 워커의 본체를 만들었으니 이걸 웹 어플리케이션이 초기화될 때 브라우저에 나 서비스 워커 가지고 있어!라고 알려주는 일만 남았다. Service Worker 설치서비스 워커를 설치하는 방법은 간단하다. 이 브라우저에 serviceWorker 객체가 지원되는 지 확인한 후 설치하면 된다.일반적인 서비스 워커의 설치 방법은 구글의 서비스 워커:소개 문서를 보고 따라하면 된다. 필자는 ServiceWorkerWebpackPlugin을 사용했기 때문에 해당 플러그인의 문서를 참조하여 작성했다. 어차피 어떤 방법을 사용하든 설치 자체는 그렇게 어렵지 않다. settings/service-worker.setting.js1234567891011121314151617export default () => { const isSupported = process.browser && 'serviceWorker' in navigator; if (!isSupported) { return; } console.log('서비스 워커가 지원되는 브라우저 입니다.'); const runtime = require('serviceworker-webpack-plugin/lib/runtime'); runtime.register().then(res => { console.log('서비스 워커 설치 성공 ->', res); }).catch(e => { console.log('서비스 워커 설치 실패 ㅜㅜ -> ', e); });} isSupported 변수에 process.browser 값을 검사하는 이유는, 숨고의 어플리케이션은 Next.js나 Nuxt.js 처럼 유니버셜 SSR 환경에서 실행되기 때문이다.유니버셜 SSR이 뭔지 궁금하신 분은 이전에 작성한 포스팅인 Universal Server Side Rendering이란?을 한번 읽어보자. 해당 모듈은 물론 클라이언트 사이드에서만 호출되지만 그래도 혹시 모르니 왠만하면 현재 실행 컨텍스트가 클라이언트인지 서버인지는 체크해주는 것이 좋다.서버 사이드 렌더링 사이클 때 브라우저 API인 navigator에 접근하려고 하면 당연히 에러가 발생한다. 서비스 워커의 설치가 성공했다면 chrome://inspect/#service-workers 또는 chrome://serviceworker-internals에 여러분의 서비스 워커가 노출될 것이다. 서비스 워커를 적용하는 방법은 사실 굉장히 심플한 편이다. 근데 사실 필자가 가장 시간을 많이 잡아먹은 부분이 바로 이 서비스 워커였는데, 그 이유는 서비스 워커 공식 문서에 적혀있다. 설치 실패 알림 기능 부족 서비스 워커가 등록되더라도 chrome://inspect/#service-workers 또는 chrome://serviceworker-internals에 표시되지 않는 경우 오류가 발생했거나 event.waitUntil()에 거부된 프라미스를 전달했기 때문에 설치하지 못했을 수 있습니다. 이 문제를 해결하려면 chrome://serviceworker-internals로 이동하여 ‘Open DevTools window and pause JavaScript execution on service worker startup for debugging’을 선택하고 설치 이벤트의 시작 위치에 디버거 문을 추가합니다. 이 옵션을 확인할 수 없는 예외 시 일시 중지와 함께 사용하면 문제를 찾을 수 있습니다. Matt Gauntcontributor to WebFundamentals 아니 이게 디버깅이 진짜 힘들다. 서비스 워커의 설치가 실패했으면 어디서 에러가 났는지, 왜 났는지 보여줘야 하는데 Uncaught DomException 하나만 딸랑 보여주고 끝낸다. 그래서 뭐가 잘못되었는지 하나하나 코드 라인에 debugger 찍고 추리해가면서 수정해야하는데 이게 진짜 힘들다. 근데 이걸 또 공식 문서에 디버깅 힘들다고 버젓이 적어놔서 괜히 더 힘든 거 같다. 도와줘요 명탐정… 범인말고 내 버그도 찾아줘… 제 3 관문, PushManager이제 서비스 워커도 설치했으니 서비스 워커의 PushManager만 연동해주면 모든 것이 끝난다! 라고 생각했지만 이 놈도 복병이었다.PushManager 또한 아직 표준이 아니라서 모든 브라우저에서 지원되는 기능이 아니기 때문에 서비스 워커 설치 시 PushManager의 존재 여부도 함께 검사해주어야한다. settings/service-worker.setting.js12const isSupported = process.browser && 'serviceWorker' in navigator && 'PushManager' in window;// ... 조건의 가독성이 조금 떨어진 것이 마음에 안들지만 일단은 구동 테스트를 하는 것이므로 그냥 넘어갔다. 딱 여기까지 하고나서 다음 스텝을 봤더니… 재앙의 시작애플리케이션 서버 키 가져오기 이 코드랩으로 작업하려면 애플리케이션 서버 키를 몇 개 생성할 필요가 있는데, 도우미 사이트인 https://web-push-codelab.glitch.me/에서 생성할 수 있습니다.여기서 공개 키 쌍과 비공개 키 쌍을 생성할 수 있습니다.다음과 같이 scripts/main.js로 공개 키를 복사하여 값을 바꾸세요. const applicationServerPublicKey = ''; 참고: 절대로 비공개 키를 웹 앱에 두면 안 됩니다! Matt Gaunt웹 앱에 푸시 알림 추가 응…? SSH 키가 필요하다고…? 예상 못하긴 했지만 HTTPS 환경에서 서버와 클라이언트 통신 채널이 하나 더 생기는 것이므로 SSH 키가 필요한 건 맞는 것 같아서 빠르게 인정했다. Google의 공식 문서에서 PushManager로 푸시 채널을 구독하는 예제를 살펴봤더니 확실히 SSH 키가 필요하긴 했다. 12345678function subscribeUser() { const applicationServerKey = urlB64ToUint8Array(applicationServerPublicKey); swRegistration.pushManager.subscribe({ userVisibleOnly: true, applicationServerKey: applicationServerKey }) // ...} 음 예상은 못했지만 이 정도는 괜찮다. 어차피 Pusher라는 푸시 솔루션을 이미 사용하고 있고 이것도 결국 같은 원리로 작동하기 때문에 내부적으로는 SSH 키쌍을 사용한 인증을 사용했을 것이다. 빠른 손절그렇게 Pusher에서 사용되는 SSH 키가 어디 있는지 찾기를 어언 20분… 결국 못 찾았다. Pusher 내부적으로 서버에서 사용하는 secret값과 클라이언트에서 사용하는 key를 사용하여 인증을 하는데, 이건 SSH 키쌍이 아니라 그냥 임의의 문자열이었다. 어차피 SSH 키를 만든다고 해도 웹 푸시에 대한 컨트롤은 백엔드가 가지고 있으므로 필자 혼자 이것저것 건드리면서 테스트 해보기에는 조금 무리가 있다. 게다가 퇴근 시간은 이미 한참 지났기 때문에 백엔드 분들은 퇴근하셨다.(사실 금요일 저녁에 이런 거 하고 있는 사람이 이상한거다.) 이때 이미 필자의 멘탈은 조금 나가있었기 때문에 월요일에 출근해서 모바일 앱에서는 어떻게 Pusher와 FCM을 연동해서 사용하고 있는 지 물어본 후 진행해야겠다고 결론을 내리고 방향을 바꿨다. 커밋 로그를 보니 이때 시간이 대략 21시 40분 쯤… 아니 그래도 퇴근은 해야하니까… 집에는 가야지… 역시 아니다 싶으면 빠른 손절이 답이다… 제 4 관문, Notification API그래서 바꾼 방향은 Background 푸시 메세지는 포기하고 Foreground 푸시 메세지라도 제대로 받게 하자였다.이렇게 되면 필자가 처음 생각했던 브라우저가 꺼져있더라도 푸시 메세지를 보여주고 싶다는 달성하지 못하지만 브라우저가 켜져있고 soomgo.com에 접속되어 있다면 창을 내려놓든 다른 일을 하고 있든 사용자에게 푸시 메세지는 보여줄 수 있으므로 어느 정도 목적 달성은 된다. 방향을 바꾸고 나니까 기존의 웹 어플리케이션에 구현되어있던 푸시 로직에 노티피케이션을 보여주는 코드만 추가하면 끝나는 간단한 일이 되었다. 어차피 Pusher 솔루션을 사용하여 인증, 이벤트 구독 등의 로직은 예전에 채팅 기능 개발할 때 다 만들어 놨기 때문이다. 기존 기능에 Notification 끼워넣기예전에 채팅 기능을 개발할 때 Pusher SDK를 한번 래핑한 헬퍼 클래스도 만들어 놨었기 때문에 나름 구조도 탄탄하다. 이제 여기에 메소드만 몇개 추가하고 웹 푸시 이벤트가 발생했을 때 알림만 보여주면 된다.우선 이 브라우저가 Notification API를 지원하는 지 확인하는 메소드가 필요하다. src/helpers/Pusher.js123isSupportNotification () { return process.browser && window && 'Notification' in window;} 그 다음 사용자에게 알림에 대한 허가를 받는 메소드를 작성한다. Notification은 내부에 permission 속성을 가지고 있고 이 속성은 granted, denied, default로 나누어 진다. src/helpers/Pusher.js123456789101112131415161718getNotificationPermission () { if (!this.isSupportNotification()) { this.isAllowNotification = false; return Promise.reject(new Error('not_supported')); } if (Notification.permission === 'granted') { this.isAllowNotification = true; return Promise.resolve(); } else if (Notification.permission !== 'denied' || Notification.permission === 'default') { return Notification.requestPermission().then(result => { if (result === 'granted') { this.isAllowNotification = true; } }); }} granted는 사용자가 이미 알림을 허용한 상태, denied는 거부한 상태, default는 아직 알림에 대한 퍼미션을 줄지말지 사용자가 결정을 하지 않은 상태이다. 따라서 우리는 퍼미션이 default 상태일 때 Notification.requestPermission 메소드를 사용하여 사용자에게 알림 노출에 대한 허가를 받아야 한다. 이제 실제로 알림창을 띄워줄 메소드를 작성해보자. Notification API 자체가 워낙 심플하다보니 그닥 어렵지 않다. 12345678910111213141516171819createForegroundNotification (title, { body, icon, link }) { const notification = new Notification(title, { body, icon: icon || `${AssetsCloudFrontHost}/app_icons/1x.png`, }); notification.onshow = () => { setTimeout(() => notification.close(), 5000); }; notification.onerror = e => { console.error(e); }; notification.onclick = event => { event.preventDefault(); if (link) { window.open(link, '_blank'); } };} new 키워드를 사용하여 Notification 객체를 생성하면 그 즉시 OSX는 화면 우측 상단에, Windows는 우측 하단에 알림 메세지가 노출된다. 그 다음 생성한 Notification객체의 onshow, onclick 등의 이벤트 리스너에 핸들러를 등록해주면 된다. 메소드 명은 Background 메세징을 실현하지 못한 필자의 슬픔을 담아 createForegroundNotification으로 결정했다. 굳이 Foreground를 강조한 이유는 언젠가 createBackgroundNotification 메소드를 만들겠다는 필자의 야망을 담았다. 이제 필요한 모든 것을 만들었으니 Pusher SDK에서 Web Socket을 통해 푸시를 보낼 때마다 알림이 작동하도록 연결만 해주면 된다. 123456789101112131415161718192021222324async subscribeNotification () { if (!this.isSupportNotification()) { return;} await this.getNotificationPermission(); if (!this.isAllowNotification) { return; } const channel = await this.getPrivateUserChannel(); channel.bind('message', response => { if (response.sender.id === this.myUserId) { return; } const targetChatRoute = !!response.sender.provider ? 'chats' : 'pro/chats'; this.createForegroundNotification(`${response.sender.name}님이 메세지를 보냈어요.`, { body: response.message, icon: response.sender.profile_image, link: `${location.origin}/${targetChatRoute}/${response.chat.id}`, }); });} Pusher SDK는 푸시 채널에 이벤트 핸들러를 바인딩할 수 있는 기능을 제공해준다. message 이벤트는 사용자가 채팅 메세지를 받았을 때 호출되는 이벤트이다. 단 자기 자신이 보낸 메세지에도 여과없이 이벤트가 호출되므로 response.sender.id === this.myUserId 조건을 통해 자신이 보낸 메세지에는 알림을 보여주지 않도록 처리하였다. 그 다음은 이제 사용자가 작은 알림 메세지만 보고도 어떤 상황이 벌어지는 것인지 쉽게 알 수 있도록 OOO님이 메세지를 보냈어요라는 형식의 제목과 메세지의 내용, 상대방의 프로필 사진을 사용하여 Notification 객체를 생성하면 끝이다. 필자의 멘탈 상태를 여과없이 보여주는 메세지 내용. 잉잉… 어쨌든 이렇게 해서 브라우저에 soomgo.com이 열려있다면 사용자들은 다른 일을 하다가 계속 페이지를 확인하거나 핸드폰을 확인할 필요없이 데스크탑 내에서 새로운 채팅 메세지를 바로 확인할 수 있게 되었다. 아직도 Background 상태에서 푸시 메세지를 바로 보여주지 못했다는 게 아쉽긴 하다. 하지만 이 정도만 해도 사용자들 입장에서는 꽤나 편할 것이라고 생각한다. 마치며사실 처음 목표했던 걸 다 이루진 못해서 찝찝했지만 너무 피곤했기 때문에 다음을 기약하기로 했다.이제 월요일에 출근해서 PO한테 이걸 보여주고 혹시 뭐 추가하고 싶은 거 없는지 물어보고 몇가지 테스트를 좀 해본 후 배포할 예정이다. 서비스 워커에 fetch 이벤트 핸들러를 추가하면 Add to Homescreen 기능도 사용할 수 있지만 사실 숨고의 프론트엔드 챕터 공식 입장은 사용자들이 모바일 웹보다는 모바일 앱을 많이 사용했으면 하는 것이기 때문에 이건 할까말까 고민 중이다.(인앱 브라우저 크로스브라우징 하기 싫…) 일단 처음 스코프를 너무 크게 잡은 것 같기도 하다. 하나하나 좀 자세히 알아보고 작업을 했으면 좋았을 것 같은데 월요일부터는 바로 또 하던 프로젝트 작업을 다시 해야해서 마음이 급했던 것도 있다. 그리고 회사에 프론트엔드 개발자가 부족하기 때문에 이런 기술적인 기능을 붙히는 건 우선 순위가 낮은 편이라서 지금 아니면 앞으로 언제 할 수 있을 지 모른다라는 마음도 컸던 것 같다. 막간을 이용해, 필자와 함께 일해주실 프론트엔드 개발자 분을 모신다는 JD를 뿌리면서 포스팅을 마무리 하겠다.PWA 외에도 다른 하고 싶은 건 많은데 프론트엔드 개발자가 모자라서 못하고 있기 때문에 경력 여하와 상관없이 그냥 재밌는 거 좋아하시는 분이면 된다.(하고 싶은 개발 다 하실 수 있도록 이 한몸 불살라 보필하겠습니다.) 이상으로 PWA 하루 만에 도입하기 포스팅을 마친다.","link":"/2019/07/06/pwa-with-notification/"},{"title":"흔한 개발랭이의 작가 입문기","text":"필자는 최근 커피 한잔 마시며 끝내는 VueJS라는 책을 집필했다. 필자는 책처럼 긴 글을 적는 게 사실 처음이라, 책을 쓰는 과정이나 책을 출판하는 과정에 대해서 무지한 상태로 집필을 시작했고 그래서 중간에 우여곡절도 꽤 많았다.그래서 이번 포스팅에서는 필자와 같이 책을 집필하고 싶어하는 분들을 위해서 2018년 8월부터 2019년 7월까지 책을 집필했던 과정과 어려움에 대해서 이야기 해보려고 한다. 책은 어떻게 쓰게 되었나?먼저 책을 집필하게 된 계기는 친구인 김영훈 개발자로부터 함께 책을 공동집필해보자는 제의를 받았기 때문이었다. 사실 필자는 대학 교양 레포트 이후로 몇 개의 블로그 포스팅와 회사의 기술 문서 작성을 제외하면 글을 써본 적이 없었다. 지금 이 블로그의 포스트 아카이브를 봐도 한눈에 알 수 있는데, 본격적으로 블로그 뽕에 취하기 전인 2019년 6월 이전에는 지금처럼 일주일에 한두번씩 글을 쏟아내는 정도가 아닌 그냥 생각날 때 간간히 쓰는 수준이었다. 게다가 저 친구의 제의를 처음 받았을 때 이걸 쓸지 말지 두 가지 이유 때문에 고민을 조금 했었는데, 그 중 첫 번째 이유는 내가 책을 쓸 실력이 되나?였다. 그리고 두 번째 이유가 조금 치명적인데… 사실 필자는 대학 교재 이후로 기술 서적을 읽어본 적이 없는 사람이다. 이렇게 또 책밍아웃을 합니다… 필자는 독서를 좋아하긴 하지만, 공부에 있어서는 인터넷이 더 편한데?라고 생각하는 쪽이라서 기술 서적보다는 소설이나 에세이, 조직 문화나 철학 같이 기술과는 관련 없는 책을 주로 읽는 편이다. 게다가 필자의 포스팅을 몇번 보신 분들은 아실 지도 모르겠지만 필자의 공부 스타일이 책을 하나 하나 읽어보면서 기초부터 하는 스타일은 아니다. 일단 뭘 만들지부터 정해놓고 그때그때 필요한 정보를 찾고 습득한 후에 바로 사용하고 넘어가는 스타일이다.(물론 그래서 지식의 깊이는 넓고 얕은 편이다.) 필자의 독서 취향은 이 쪽에 가깝다.(왠지 스파이가 하나 껴있는 것 같지만 신경쓰지말자.) 그럼 왜 책을 쓰게 되었냐? 뭐 여러가지 이유가 있겠지만 몇가지로 추려보자면 대충 이렇다. 코드 같은 무형의 작품이 아니라 내 손에 잡히고 내 눈에 보이는 유형의 작품을 만들어 내보고 싶다. 책을 쓰면서 내가 두루뭉술하게 알고 있던 지식을 정리할 수 있지 않을까? 생각보다 코딩을 배우기 위해서 책으로 입문하시는 분들이 많았다. 필자가 책 쓴다고 하니까 몇몇 분들께서 와, 에반! 이제 책으로도 돈벌겠네요?라는 드립을 간간히 쳐주셨는데 애초에 책을 팔아서 벌 수 있는 돈은 생각보다 많지 않다고 생각했기 때문에 별로 기대를 안했다. 오히려 필자에게는 책이 돈을 벌 수 있는 부수입이라기보다 책을 집필했다는 나 자신의 만족과 내가 알고 있는 무엇인가를 다른 사람들에게 알릴 수 있는 점에서 매력으로 다가왔기 때문에 집필 제의를 받아들일 수 있었던 것 같다. 그건 이 블로그도 마찬가지인데, 이 블로그로 돈을 벌어봤자 솔직히 얼마나 벌겠는가? 애초에 필자의 포스팅들은 대중적으로 소비할 수 있는 컨텐츠가 아니다. 그렇기 때문에 블로그가 진짜 흥해서 많이 벌어봤자 한달에 한 100만원 벌면 많이 버는 거다.물론 100만원이 작은 돈은 아니지만 그렇다고 내 인생이 바뀌는 돈도 아니니 별 의미는 없다고 생각한다. 그래서 필자는 그냥 필자의 생각과 지식을 다른 사람과 공유하고 싶은 마음으로 블로그를 운영하고 있다. 손에 잡히는 작품을 만들어 보고 싶다.필자는 개발자다 보니까 매일 프로그램을 만들어 낸다. 하지만 프로그램은 현실에 존재하는 물건이라는 느낌이라기 보다는 좀 더 추상적인 느낌으로 다가온다. 손으로 만질 수도 없고 냄새를 맡아볼 수도 없는 그런 무형의 개념체 같은 느낌이다. 하지만 필자에게 책이라고 하면 딱 떠오르는 이미지는 새 책을 펼쳤을 때 풍기는 종이 냄새, 책꽂이에서 책을 꺼내올 때 느껴지는 감촉, 책장을 하나하나 넘길 때 들리는 소리와 같이 여러가지 감각이 함께 하는 공감각적인 이미지이기 때문에 프로그램을 만드는 것과는 매우 다르게 다가왔던 것 같다. 결과물이 기계적인 무언가가 아니라 어떤 형태를 지닌 물건으로 나온다는 것이 굉장히 새로운 느낌이었다. 그리고 결정적으로 필자는 5살 때부터 할아버지 손 붙잡고 광화문 교보문고를 굉장히 자주 다닌 교보문고 빠돌이다. 그런 추억이 있는 광화문 교보문고에 내가 쓴 책이 진열될수도 있다는 것을 상상하니 뭔가 기분이 뭉클하면서도 묘했던 것 같다. 책을 쓰면서 내 지식도 정리하자.필자가 VueJS에 관한 책을 쓰긴 했지만, 당연히 VueJS에 대한 모든 것을 완벽하게 알고 있는 상태로 책의 집필을 시작한 건 아니다. 물론 2년 동안 VueJS를 사용했던 짬이 있으니 익숙하긴 하지만 그게 잘한다라고 할 순 없다. 그래서 필자도 다시 기초부터 VueJS를 쭉 공부한다는 생각으로 공식 문서부터 다시 읽어봤다. 가만보면 이런 점이 블로그 포스팅과 비슷한데, 예를 들어서 VueJS의 라이프 사이클에 대한 글을 작성한다고 하면 당연히 대부분의 사람들은 리서치를 한다. 왜냐면 내가 알고 있는 것이 정답이다라고 확신하지 못하기 때문이다. 이런 점에서 보면 책이든 블로그든 불특정 다수의 사람들이 내 글을 읽는다는 점에서 느껴지는 부담감은 비슷한 것 같다. 게다가 필자는 사실 VueJS를 따로 공부했던 적이 없다. 그냥 회사에 입사했는데 그 회사에서 VueJS를 이미 도입한 상태였기 때문에 필자도 VueJS를 사용한 게 시작이었고, 그 이후로는 그냥 비즈니스를 개발하면서 필요한 건 그때그때 찾아보면서 사용했었다.(그래서 입사 초반에 작성한 컴포넌트를 보면 완전 개판 오분전이다.) 이런 이유로 필자는 기왕 이렇게 된 김에 VueJS에 대한 내 지식을 제대로 한번 다져보자라고 생각하게 되었던 것이다. 생각보다 코딩을 배우기 위해서 책으로 입문하는 사람이 많았다.위에서도 얘기했듯이 필자는 공부는 책보다는 인터넷이 편하지!라고 생각하는 사람이라서 기술 서적에 대한 시각이 회의적인 편이었다. 그래서 처음 친구가 공동 집필 제의를 해줬을 때도 책을 쓴다는 것 자체가 조금은 의미 없는 일로 느껴졌고 요즘 시대에 누가 책보고 공부하냐? 다 인터넷으로 검색해서 하지.라는 말도 했었는데, 그 친구 말이 딱 이랬다. 야, 처음 공부하시는 분들은 인터넷에서 어떤 키워드로 뭘 찾아봐야 하는지도 알기 힘들수도 있어. 이 얘기를 듣고 필자가 생각이 많이 짧았다는 것을 바로 깨달을 수 있었다. 어디선가 인간은 무의식적으로 타인에게 자신의 모습을 투영하려고 한다라는 글을 본 적이 있는데, 필자가 딱 그 모양이었던 것이다. 필자가 늘 인터넷으로 공부하는 것이 익숙했기 때문에 당연히 다른 분들도 그럴 것이라 생각했던 것인데, 기본적으로 인터넷으로 공부를 한다는 것은 어떤 키워드로 구글링을 해야 내가 원하는 정보가 나오는 지 알고 있다는 전제가 필요하다는 사실을 잊고 있었다. 당연히 처음 개발을 접하거나 혹은 구글링이 익숙하지 않은 분들은 이런 점에서 진입 장벽을 느낄 수 있는데, 필자는 그 부분을 생각 안하고 있던 것이다. 그래서 친구의 저 말을 듣고 필자는 이런 마음이 들었다. 상대적으로 진입 장벽이 낮은 책을 통해 공부한 후에 혼자 구글링을 할 수 있는 역량을 만드는 게 더 나을 수 있겠다. 물론 당연히 책 한권 읽었다고 갑자기 Vue의 절대 고수가 되거나 그런건 아니겠지만, 적어도 책을 읽고 전반적인 내용을 이해한 후에 더 알아보고 싶은 것은 혼자서 키워드를 찾아내서 구글링 및 공부를 할 수 있는 능력을 독자 분들이 가질 수 있도록 도와주고 싶었다. 이 이외에도 이력서에 뭐라도 한줄 더 쓸 수 있다나 작가라니 조금 근사한 걸...?처럼 좀 깨는 이유도 있고 개발랭이들의 생태계에 조금이라도 도움이 될 수 있겠다라는 이타적인 이유도 있었지만 필자의 동기 부여에 큰 영향을 끼친 대표적인 이유는 저 위의 3가지였다. 결국 고민 끝에 2018년 8월 말, 필자는 친구 손을 잡고 고난의 길로 들어서게 되었다. 책을 쓰기 전에친구와 공동 집필이 결정된 후 제일 먼저 출판사 직원분과 역삼역 근처 카페에서 간단한 미팅을 가졌다. 내용은 뭐 대략 집필 계획서를 보내달라, 인세는 어떻게 나온다, 계약은 어떻게 진행될 것이다와 같은 간단한 내용들이었다. 이때까지만 해도 필자와 친구는 대략 이러이러한 내용을 써야겠다 정도로만 얘기가 된 상태였지만 이제 출판사에 보낼 집필 계획서를 작성하기 위해 상세한 논의를 해야했다. 이때 협의되었던 몇가지 내용들 중 핵심 포인트는 이 책을 읽고 회사에 입사했을 때 최소한 어플리케이션 개발 자체는 할 수 있어야 한다였다. 이 얘기는 지금 시중에 나와있는 다른 VueJS 책을 몇개 읽어보고 나서 나오게 된 것이다. 시중에 판매되고 있는 책들은 대부분 해당 기술에 대한 API나 기술의 원리 등을 중점으로 설명하고 있었고, 예제 또한 간단한 To Do 리스트 또는 그와 비슷한 아키텍처를 가지는 작은 어플리케이션을 만들어 보는 편이었다. 이런 책들을 읽어보며 필자와 친구가 느꼈던 아쉬운 점은 이걸 읽고 바로 실무에 투입되면 일을 할 수 있나?라는 점이었다. 아무리 책을 통해 VueJS의 API가 어떤 것이 있는지, 라이프 사이클은 어떤 순서로 실행 되는 지와 같은 지식을 익혔다고 해도 회사에 입사한 후에 신입 개발자가 마주하는 문제들은 어떤 라이프 사이클에 이 로직을 넣을까?같은 단순한 문제가 아니라 컴포넌트는 어떤 단위로 나누어야 하나?, API 통신 로직은 컴포넌트에 있어야 할까? 아니면 Vuex 스토어에 있어야 할까?, 유저가 로그인한 상태는 어떻게 관리해야하지?와 같이 한 수준 더 높은 문제들이다. 그래서 필자는 독자 분들이 이 책의 예제를 한번 작성해보고 회사에 갔을 때, 그 문제를 해결하는 건 둘째치고 어, 이거랑 비슷한거 언제 한번 해봤는데?라는 생각이 들게 하는 것만 해도 성공이라고 생각했고 친구도 필자와 동일하게 생각하고 있었기 때문에 책의 큰 방향을 정하는 건 어렵지 않았다. 어디선가 많이 본 짤이 떠오르는 장면 책의 큰 방향이 정해진 후 필자와 친구는 퇴근 후에 일주일에 한두번은 꼭 만나서 근처 카페로 간 뒤에 집필 계획서를 함께 작성하기 시작했다. 집필 계획서에는 대략 이런 정보가 포함된다. 제목(가제) 작성자 작성일 책의 핵심 컨셉 내용 요약 및 특징 이 책과 관련된 기술의 동향 예상되는 탈고 날짜 이외에도 몇가지 정보가 더 있지만 큰 틀은 대충 이렇다. 이때 작성한 집필 계획서는 향후 출판사가 책을 홍보할 때 사용하는 자료이기도 한 만큼 되도록이면 자세하게 작성하는 것이 좋다. 어쨌든 출판사에서 요구한 대로 집필 계획서를 작성해서 제출한 뒤 본격적인 집필을 시작했다. 개발자처럼 집필하자!처음에는 구글 독스(Google Docs)를 사용하여 집필을 시작했다. 구글 독스는 H1, H2과 같은 글꼴 스타일 기능을 제공해주기 때문에 미리 합의한 스타일을 정해놓고 문서를 작성할 수 있었다. 하지만 구글 독스로 챕터 3개 정도를 작성했을 때, 필자는 구글 독스가 굉장히 비생산적이라는 생각이 들기 시작했다. 글꼴을 변경하거나 이미지를 첨부하는 등의 행위를 할 때 변경하고자 하는 부분에 커서를 위치시킨 뒤 마우스로 메뉴를 클릭해서 진행해야하는 방식이 너무 번거로웠고, 그때마다 집중의 흐름이 끊겨서 불편했다. 게다가 각 챕터 별로 문서를 새로 만들때마다 글꼴 스타일을 출판사 규격에 맞게 매번 설정해줘야 한다는 점 또한 생산성을 저하시켰다. 그리고 필자와 친구는 서로 놓치고 간 부분을 잡아주거나 문체의 통일을 위해 구글 독스 문서에 코멘트를 남기는 방식으로 서로 작성한 부분을 리뷰했다. 그러나 이 방식은 필자가 생각하기에 조금 비생산적이었는데, 리뷰를 하는 입장에서는 이번에 추가된 내용이 무엇인지, 삭제된 내용이 무엇인지 알기 힘들었고, 리뷰를 받는 입장에서는 댓글이 문서의 어느 부분에 달렸는지도 한 눈에 알아보기가 힘들었다. 그래서 필자는 어떻게 하면 글쓰기 외의 다른 행동에 시간을 쓰지 않고 글쓰기에만 집중할 수 있을까? 라는 고민을 하기 시작했다. Markdown을 사용하여 책 집필하기필자가 고민한 결과로 선택한 것은 바로 마크다운(Markdown, md) 포맷이었다. 필자와 친구 둘 다 개발자이기 때문에 일단 마크다운 포맷은 우리에게 굉장히 익숙한 포맷이었고, 문서의 전체적인 스타일도 이미 정해져있기 때문에 스타일에 관한 걱정이나 별다른 작업 없이 계속 글쓰기에만 집중할 수 있을 것 같았다. 하지만 결국 출판사에 원고를 보낼때는 docx(MS Word) 파일로 보내야 했고, 다른 리뷰어 분들에게 책에 대한 리뷰를 받을 때에는 PDF 파일로 보내드려야 했기 때문에 md to docx 컨버팅 혹은 md to pdf 컨버팅을 할 수 있는 방법을 찾아야 했다. 그래서 찾아낸 게 pandoc과 markdown-pdf라는 라이브러리였다. pandoc은 홈페이지에 Universal Document Converter라고 홍보하고 있을 만큼 다양한 포맷의 문서 변환을 지원해준다. 물론 Markdown to Docx도 지원한다. 하지만 Markdown to PDF는 지원하지 않기 때문에 해당 기능을 구현하기 위해 다른 라이브러리를 찾아야했는데, 그 과정에서 markdown-pdf라는 라이브러리를 찾았다. 여기까지 리서치를 마친 필자는 친구에게 구글 독스에서 마크다운으로 갈아타자는 제안을 했고, 친구도 고민 끝에 수락하여 이후 책의 집필은 모두 마크다운과 깃(Git)을 사용하여 진행했다. Git을 이용한 문서의 버전관리구글 독스를 사용할 때 겪었던 또 다른 문제점 중 하나가 바로 문서의 버전관리가 힘들다는 것이다. 물론 구글 독스도 파일 > 버전기록 메뉴에 들어가면 각 버전에서 추가된 내용이나 삭제된 내용을 확인할 수 있지만, 그 버전이 정확히 어떤 단위로 묶인 단위인지 명확하지않다는 것이 단점이다. 하지만 깃을 사용하면 커밋을 할 때 의미있는 단위로 나누고 정확한 커밋 메세지를 작성함으로써 정확히 문서의 어떤 부분을 수정했는지 상대방에게 알릴 수 있다. 또한 일정 단위의 문서를 작성하고나면 풀 리퀘스트(Pull Request)를 통해 상대방에게 리뷰를 받는 식으로 서로 작성한 부분에 대해서 최소 1번 이상은 리뷰를 하도록 할 수 있다. 그래서 필자와 친구는 깃허브(Github)에 레파지토리를 하나 생성하고 챕터 단위로 하나씩 맡아서 집필을 진행하되, 내용을 마스터 브랜치에 머지하고 싶을 땐 반드시 풀 리퀘스트를 보내고 머지하도록 협의하였고 그 결과 서로 작성한 부분이 아니더라도 내용을 파악하는데 큰 어려움이 없이 집필을 진행할 수 있었다. 리뷰를 하면 좋은 점.jpg 물론 나중에 전체 리뷰를 할 때 한번에 고쳐야 될 부분이 많아지는 경우가 있어서 충돌(Merge Conflict) 때문에 조금 고생하기는 했지만, 구글 독스로 진행했다면 두 명이 동시에 수정한 부분이 어딘지도 모르고 넘어갔을 가능성이 컸을 것이다. 문서 컨버팅 하기이렇게 작성된 문서는 중간중간 PDF로 변경되어 부분 출판되었다. 밑에서 다시 설명하겠지만, 집필 초기에 친구가 제안했던 책의 타겟 독자층을 모집해서 우리 책으로 스터디를 진행해보자라는 아이디어를 냈었다. 그래서 매주 토요일마다 집필된 책을 참고 자료로 사용하여 스터디를 진행하고 있었기 때문에 지속적으로 md 파일을 PDF로 변경해야했었다. 그래서 markdown-pdf 라이브러리를 사용하여 간단한 출판 스크립트를 작성했고 챕터 하나의 집필이 끝날 때 지속적으로 부분 출판하여 스터디에 참여하신 분들이 끊기지 않고 진도를 나갈 수 있도록 했다. vuejs-book/scripts/publish.js12345678910111213141516171819202122glob('book/**/*.md', null, (err, mdFiles) => { const pdfFilePaths = mdFiles.map(path => { console.log(`Ready ${path}`); const newPath = path .replace('book/', `${destDir}/`) .replace('.md', '.pdf'); return newPath; }); mdpdf({ preProcessHtml: preProcessHtml(basePath), cssPath: './custom.css', remarkable: { html: true, } }).from(mdFiles).to(pdfFilePaths, () => { pdfFilePaths.forEach(pdf => { console.log(`Created ${pdf}`); }); });}); preProcessHtml는 컨버팅을 진행한 후 책에 들어간 이미지의 경로가 맞지 않아 pdf파일에서 이미지가 출력되지 않는 이슈 때문에 작성한 함수인데, 마크다운을 우선 HTML로 파싱한 후 img 엘리먼트를 찾아서 file:// 프로토콜을 사용한 링크로 바꿔주는 역할을 한다. 이 과정을 거치고 난 후 HTML to pdf 컨버팅을 진행했더니 이미지가 정상적으로 첨부되는 것을 확인할 수 있었다. 이렇게 출판한 pdf 파일들을 구글 드라이브에 업로드해서 스터디원이나 리뷰어 분들이 부분 출판된 내용을 바로바로 확인할 수 있도록 진행하였다. 그리고 출판사에 보낼 MS Word파일은 pandoc을 이용하여 컨버팅하였다. vuejs-book/scripts/deploy.js12345678910111213141516171819202122glob('book/**/*.md', null, (err, mdFiles) => { mdFiles.forEach(path => { const chapter = path.split('/')[1]; console.log(`Ready for ${chapter}`); const newPath = path .replace('book/', `${destDir}/`) .replace(`${chapter}/README.md`, `${chapter}.docx`); const args = `-f markdown -t docx -o ${newPath}`; // https://pandoc.org/MANUAL.html#syntax-highlighting 코드 하이라이팅 볼것 const originSource = fs.readFileSync(`${path}`, 'utf-8'); const source = originSource.replace(/\\(\\/assets/g, `(${basePath}/assets`); nodePandoc(source, args, (err) => { if (err) { console.error(err); } else { console.log(`Finish ${chapter}`); } }); }); }); 이렇게 작성된 문서 컨버팅 스크립트들은 npm script로 등록하여 언제든 간편하게 md 포맷을 pdf와 docx 포맷으로 뽑아낼 수 있도록 했다. package.json123456{ \"scripts\": { \"publish\": \"node scripts/publish.js\", \"deploy\": \"node scripts/deploy.js\" }} 사실 처음에 md 포맷을 pdf로만 변경할 때는 별 문제가 없었는데 나중에 pandoc을 통해 docx 포맷으로 변경할 때 글꼴 스타일에 관련된 이슈가 발생했다. pandoc을 사용하여 docx 포맷으로 컨버팅할 때 일명 레퍼런스 문서라는 것을 설정할 수 있는데, 이 문서에 커스텀 글꼴 스타일을 설정해놓으면 컨버팅 되는 문서에서 이 문서의 글꼴 스타일을 참고하여 동일한 글꼴 스타일을 사용한 문서로 만들어준다. 그래서 처음에는 이 기능만 믿고 출판사 포맷대로 글꼴 스타일을 설정해놓은 레퍼런스 문서를 하나 만들어놓으면 되겠구나했었는데 막상 레퍼런스 문서 옵션을 사용하여 스크립트를 돌려보니 문서에 첨부한 코드 블록의 스타일이 와장창나는 사태가 발생했다. 그래서 결국은 마지막에 출판사에 원고를 보낼 때에는 pandoc을 사용하여 md 포맷을 docx 포맷의 기본 글꼴 스타일로 컨버팅 한 후에 그 문서의 글꼴 스타일을 직접 수작업으로 변경하는 방법을 사용했다. 뭐 마지막에는 꽤 삽질을 하긴 했지만 그래도 md 포맷을 택함으로써 집필 과정에서 시간을 많이 줄일 수 있었고, 게다가 Git을 사용한 버전관리도 할 수 있엇으므로 결과적으로는 좋은 선택이었다고 생각한다.(라고 필자만 생각하고 있는 것 같다. 친구는 별로였던 듯…) 공동 집필 과정에서의 충돌필자는 혼자 책을 집필한 것이 아니라 친구와 함께 공동 집필을 한 것이기 때문에 서로의 의견을 일치시키고 합의점을 도출해내는 커뮤니케이션도 굉장히 중요한 일 중 하나였다. 사실 필자와 이 친구는 함께 사이드 프로젝트를 진행해본 경험도 있고 평소에도 성격이 꽤 잘맞는 편이라서 같이 해외여행도 다녀온 막역한 사이이다. 하지만 책이라는 매체의 특성 상 코드와 다르게 배포 후 수정이 불가능해서 최대한 완벽하게 끝내고 싶다라는 마음 때문에 서로 의견의 일치를 보는 것이 평소보다 더 어려웠던 것 같다. 설계에 대한 관점의 차이처음에 의견 충돌이 많이 났던 것은 예제 어플리케이션의 설계였다. 책에 들어가는 예제는 불특정 다수에게 노출되는 정보이기 때문에 최대한 주관성을 배제하고 널리 통용될 수 있는 지식을 사용하여 작성되어야한다. 그래서 필자와 친구는 예제 어플리케이션의 설계를 어떻게 가져갈 것인가?로 엄청 많은 이야기를 나누었다. 간단하게는 컴포넌트의 네이밍과 프로젝트의 디렉토리 구조에서부터 스토어의 상태의 변경을 위한 액션은 어떤 컴포넌트가 Dispatch하는 것이 맞는 것이냐와 같이 심도있는 주제까지 다양한 이야기를 나누었다. 결국 이런 끊임없는 커뮤니케이션을 거쳐서 합의점을 도출해내고 나서야 예제 어플리케이션 개발에 들어갈 수 있었다. Form 컴포넌트의 정의부터 시작해서 어떤 라이브러리를 사용할 것인지까지 일일히 다 정했었다. 일하는 스타일의 차이두 번째는 바로 일하는 스타일이었다. 사실 이게 정말 의외인 부분이었는데, 이미 필자는 이 친구와 사이드 프로젝트를 진행한 경험이 있었고, 도중에 의견 충돌이 나더라도 굉장히 서로 만족할 만한 결론을 도출해내면서 진행해왔기 때문에 이번에도 마찬가지일 것이라고 생각했다. 근데 이게 돈받고 하는 일이라서 그런지, 아니면 마감 기한이 정해져 있는 일이라서 그런지는 몰라도 사이드 프로젝트 때와는 굉장히 다른 상황이 펼쳐졌다. 일단 필자는 어떤 일을 하든 마감 기한에 쫓기면서 하는 스타일이 아니다. 이렇게 할 수 있는 이유는 필자가 일하는 중간에 생기는 변수를 최소화하고 리스크가 생길 것 같은 일을 아예 차단해버리는 스타일이기 때문이다. 그리고 결정적으로 성격 자체가 좀 느긋하다. 좋게 말하면 느긋한거고 나쁘게 말하면 게으른거긴 한데, 뭐 어쨌든 그런 성격이다. 하지만 이 친구는 뭘 하든 열정이 장난아니다. 하기 싫은 일이든 하고 싶은 일이든 항상 최선을 다하는 스타일이다. 이 친구가 이렇게 할 수 있는 이유는 기본적으로 책임감이 큰 성격이라서 그렇다고 생각한다.(뭐 아닐 수도 있다. 아무리 친구라도 모르는 부분은 있으니) 이 친구의 바로 이런 부분들이 필자에게 굉장히 자극을 주는 부분이고 친구라는 부분을 제외하고 개발자 대 개발자로 생각해도 굉장히 좋은 개발자라고 생각한다. 하지만 같이 마감기한이 있는 일을 하다보니 이 친구 입장에서는 필자가 적극적으로 참여하지 않는다는 마음이 있었던 것 같고, 필자는 필자대로 이게 이렇게까지 스트레스를 받으면서 할 일은 아닌데라는 생각이 있었던 것 같다. 그리고 이게 사람인 이상 마감기한이 다가올수록 출판사와의 약속을 지키지 못하면 어떡하나라는 불안감도 들기 마련이고, 둘 다 직장을 다니고 있기 때문에 회사 일은 또 회사 일대로 해야하는 상황이었다. 그래서 육체적인 피로감과 정신적인 피로감이 점점 쌓여서 그랬던 것 같기도 하다. 그래도 다행히 이 친구와 필자의 가장 큰 공통점은 커뮤니케이션과 같은 소프트 스킬의 중요성을 알고 있다는 점이었다. 그래서 서로 마감기한에 쫓겨 스트레스를 받고 있는 상황에서도 최대한 서로를 배려하고 서로의 의견을 많이 듣고 교환하는 과정을 통해 큰 갈등 없이 책 집필을 마칠 수 있었다. 저희 친해요^^ 책 리뷰하기책을 집필하는 동안 필자와 친구가 풀 리퀘스트를 사용한 리뷰를 계속 하긴 했지만 작가 시점의 리뷰는 주로 오탈자 수정, 문체의 통일 등의 기계적인 리뷰라면 다른 리뷰어 분들이 해주시는 리뷰는 이 내용이 정말 도움이 되는가?, 이런 내용이 있으면 더 좋을 것 같다.와 같이 작가의 시점에서는 알아차리기 힘든 정성적인 리뷰들이다. 그래서 책을 집필하는 중이나 책을 집필한 후에 리뷰어 분들의 피드백을 최대한 많이 수집하고자 했다. 사실 필자는 개발자 인맥도 별로 없고 사람 만나는 걸 좋아하는 편도 아니라서 직장 동료 외에는 리뷰를 부탁할 수 있는 사람이 많지 않았다. 하지만 친구는 필자보다 인맥도 풍부하고 사회성도 좋아서 리뷰를 부탁할 수 있는 사람도 많았다.(대단쓰…) 그런 이유로 책 리뷰는 필자 혼자서는 진행하지 못했을 일이고 진짜 친구 덕이 크다. 그리고 필자도 이 경험을 바탕으로 아 나도 네트워킹 좀 해야하나...?라는 생각이 싹트기 시작했다. 스터디원들의 리뷰위에서 한번 설명했듯이, 책을 처음 집필할 때 이런 리뷰의 중요성을 알았던 친구가 먼저 책의 타겟 독자층을 모집해서 우리 책으로 스터디를 진행해보자라는 아이디어를 제안해주었고 필자는 처음에는 고민했지만 다른 리뷰어분들이 해주시는 리뷰가 중요하다는 말에 설득당해서 스터디를 진행하게 되었다. 필자는 낯을 가리는 성격이라 처음 보는 사람들과 스터디를 한다는 것에 대해서 익숙하지가 않다. 그래서 처음에는 내가 스터디원들에게 뭘 해드려야하지...?라는 고민을 굉장히 많이 했는데 막상 스터디가 시작되고 나니 그런 걱정은 필자의 기우였다는 것을 알 수 있었다. 스터디를 지원해주신 모든 분들이 굉장히 열정적으로 질문도 먼저 건네주시고, 더 알고 싶은 부분에 있어서도 확실하게 의사 표현을 해주셔서 필자도 편한 마음으로 스터디를 진행할 수 있었다. 그리고 모두 성격이 좋으셔서 낯가림이 심한 필자도 잘 어울릴 수 있었다. 책을 집필하는 동안 실제 타겟 독자층이었던 스터디원들로부터 피드백을 받으며 책을 수정하다보니까 뭔가 책에 대한 자신감도 생겼고, 필자가 작성한 책으로 스터디하시면서 막힘없이 학습해나가시는 모습들을 보고 약간 뿌듯함도 느꼈던 것 같다. 실제로 필자는 내가 먼저 나서서 타인에게 지식을 공유하고 싶다는 욕구는 없는 편이었는데, 이번에 책도 집필하고 스터디를 진행하면서 필자는 별 거 아니라고 생각했던 지식들이 다른 분들에게는 큰 도움이 되는 것을 보고 교육에도 조금 관심이 생기기도 했고 낯가림도 조금은 나아진 것 같다.(그래도 역시 인싸의 길은 멀고도 험하다…) 현업 개발자분들의 리뷰책을 모두 집필하고 난 후에 주위의 현업 개발자 분들께도 리뷰를 부탁했는데, 이런 분들을 베타 리더라고 부른다고 한다. 필자도 이번에 처음 알았다. 베타 리더 분들은 각자의 여유 시간에 따라서 전체 리뷰와 챕터 리뷰로 나눠서 진행해주셨는데, 필자가 집필하면서 놓쳤던 부분을 되짚어 주시기도 하고 이해가 어려울 것 같은 설명에 대해서도 지적해주시는 등 정말 꼼꼼하게 피드백을 해주셔서 굉장히 든든했다. 마치며사실 처음 필자는 책? 그냥 조금 긴 포스팅 느낌일 것 같은데라는 안일한 마음으로 시작했는데, 얼마 지나지 않아서 그게 아니라는 것을 깨달았다. 이 포스팅에는 작성하지 않았지만 예제 어플리케이션을 만드는 일도 만만치 않았는데, 문제는 예제 어플리케이션을 수정하면 같은 부분을 책에서 찾아서 코드 블록을 수정하고 코드에 대한 설명도 그에 맞춰서 바꿔줘야하는 것이었다.(게다가 클라이언트 어플리케이션 2개와 API 서버까지 만들어야했다.) 예제 어플리케이션과 책의 내용이 조금만 싱크가 어긋나도 독자는 상당한 혼란을 느낄 수 있기 때문에 이런 부분을 꼼꼼히 체크해야했는데, 이게 제일 힘들었던 것 같다. 그리고 포스팅과 다르게 책은 한 주제의 호흡을 굉장히 길게 가져가야 하는 매체이다보니까 앞 쪽의 내용과 뒤 쪽이 내용의 컨셉이 달라지면 안되는데, 필자는 이렇게 긴 글을 써본 적이 없기 때문에 주기적으로 앞 쪽의 내용과 문체를 확인해가면서 집필을 해야했다. 그래도 이렇게 책을 한번 집필하고 나니까 뿌듯하기도 하고 글을 적는 행위에도 많이 익숙해져서 한 주에 한두번 포스팅 올리는 것 정도는 크게 어렵지 않아서 전체적으로는 좋은 느낌이다. 이 책을 보시는 많은 예비 독자들이 유익했다라고 느낄 수 있었으면 좋겠다. 특히 프론트엔드 개발을 처음 시작하시려는 분들이 이 책을 읽고 어 뭐야 생각보다 별 거 없네?라고 생각했으면 진짜 좋을 것 같다. 그리고 마지막으로 바쁜 개인 시간 쪼개서 책을 리뷰해주신 베타 리더분들과 기술 서적 집필이라는 좋은 기회를 제안해준 김영훈 개발자에게 압도적인 감사를 올리고 싶다. 만약 스터디 회식 때 혹시 노래방을 가게 된다면 필자가 압도적 감사의 마음으로 옆에서 탬버린을 16비트로 쳐보겠다.(준코에서 탬버린치다가 2개 부셔먹은 경력 보유 중입니다) 참고로 필자의 책은 Yes24에서 구경해볼 수 있지만, 아직 초판 인쇄가 끝나지 않았기 때문에 배송은 다음 달부터 가능하다.하지만 필자는 인터넷에서 책을 구매하는 것보다 나중에 교보문고같은 오프라인 서점에 책이 입고되면 가서 직접 한번 읽어보고 사는 것을 추천하고싶다. 이 글을 읽는 미래의 독자님들께서 교보문고에 갔을 때 민트색 표지에 커피 일러스트를 보고나서 필자를 떠올리고 한번 책을 펼쳐보게 만드는 것만으로도 필자는 행복해질 것 같다. 이상으로 흔한 개발랭이의 작가 입문기 포스팅을 마친다.","link":"/2019/07/21/vuejs-book-retrospective/"},{"title":"컴퓨터는 어떻게 소리를 들을까?","text":"이번 포스팅에서는 필자의 예전 직업이었던 사운드 엔지니어의 추억을 살려서 한번 오디오에 대한 이론을 설명해볼까 한다. 하지만 이론 설명만 하면 노잼이니까 오디오 이론을 기초로 자바스크립트의 Web Audio API를 사용하여 간단한 오디오 파형까지 그려보려고 한다. 바로 코딩에 들어가고 싶지만 오디오에 관한 기본적인 지식이 있어야 오디오 파형을 그릴 때의 과정을 이해할 수 있으므로 이론을 최대한 지루하지 않게 설명하려고 한다. 이론이 코딩보다 재미없는 건 맞지만 오디오 파형을 그리려면 최소한 기본적으로 알고 있어야 하는 것들이니까 한번 쓱 흝어보자. 소리란 무엇일까?제일 먼저 소리가 무엇인지부터 알아보자! 소리를 딱 한마디로 표현하자면 바로 진동이다. 기본적으로 우리가 소리를 듣는다는 것은 이런 순서로 일어난다. 어떤 물체가 부르르 진동을 한다. 이때 진동 주파수는 뭐 대충 440hz라고 치자. 그 물체 주변에 있는 매질이 440hz의 진동을 전달한다.(일반적인 상황에서는 주로 공기) 매질이 진동을 전달하면 우리의 고막도 440hz로 진동한다. 그 진동 신호를 달팽이관이 전기 신호로 바꿔서 청신경에 전달한다. 뇌가 신호를 받아서 해석한다. 440hz 접수완료! 이때 1번 순서에서 물체가 1초에 몇 번이나 떨렸는지 표현하기 위해 우리는 헤르츠(herz, hz) 단위를 사용한다. 10hz는 1초에 10번 진동을 했다는 의미이고 1khz는 1초에 1000번 진동을 했다는 것이다. 참고로 예시의 440hz는 도레미파솔라시도할 때 라음이다. 우리가 음악을 들을 때는 현악기면 현의 진동, 관악기면 입술이나 리드의 진동이 증폭된 것, 노래라면 성대의 진동을 듣게 되는 것이다. 아무리 감성터지는 음악도 공돌이 손에 걸리면 이렇게 분해될 수 있다. 우리가 듣는 아름다운 선율의 음악도 뜯어보면 그냥 진동 주파수 덩어리다. 이때 이 진동은 자연계에서 발생한 것이기 때문에 아날로그(Analog)의 형태로 나타난다. 사실 자연계에서 발생하는 대부분의 거시적인 신호는 아날로그 형태를 가지는데, 예를 들면 빛의 밝기가 변한다거나 바람의 세기가 변한다거나 소리의 크기가 변하는 등의 신호를 말한다. 소리는 아날로그 신호다아날로그는 신호나 자료를 연속적인 물리량으로 나타낸 것이다. 이게 연속적인 물리량이라고 하면 뭔가 전문적이고 어려워보이는데 풀어보면 사실 별 거 없다. 여기서 우리가 집중해야할 단어는 물리량이 아닌 연속적이다. 가로 축은 시간, 세로 축은 전압이다. 아날로그 신호는 아무리 쪼개도 끝이 없는 연속성을 가진다 그렇다면 연속적이라는 건 무슨 말일까?연속성의 대표적인 예는 바로 수이다. 자 우리가 이제 1부터 2까지 걸어간다고 생각해보자. 우리는 1에서 2까지 걸어가는 동안 몇개의 수를 만날 수 있을까? 우선 절반이 되는 위치에 있는 1.5가 있을 것이다. 그 절반인 1.25도 있을 것이며 또 그것의 절반인 1.125도 있을 것이다. 이런 식으로 계속 수를 쪼개다 보면 우리는 결국 이게 의미없는 삽질이라는 것을 깨닿게 된다. n/2를 계속 한들 끝이 있을리가 없기 때문이다. 만약 $10^{-10000}$ 단위까지 쪼갠다 해도 우리는 그 수를 계속 해서 무한히 쪼갤 수 있다. 우리는 이런 성질을 연속이라고 부른다. 이제 연속적이라는 말이 조금 이해가 되었길 바란다. 컴퓨터가 소리를 듣는 방법근데 여러분도 알다시피 컴퓨터는 0과 1밖에 이해하지 못하는 바보다. 이 방식을 우리는 디지털(Digital)이라고 부른다. 그래서 0과 1밖에 모르는 컴퓨터는 연속성을 가진 아날로그 신호를 이해할 수가 없다. 그렇다면 우리가 자연에서 발생한 아날로그 형태인 소리를 컴퓨터가 듣게 해주려면 어떻게 해야할까?간단하다. 아날로그를 디지털로 바꿔주면 된다. 아날로그 신호인 소리를 컴퓨터가 알아 들을 수 있게 변경하는 과정을 알고나면 Web Audio API가 우리에게 주는 정보들이 뭘 의미하는 지 알 수 있다. 그럼 지금부터 아날로그를 디지털로 변경하는 과정을 알아보자. 아날로그를 디지털로 변경하기 위해서는 몇가지 순서를 거쳐야 한다. 이 순서에서 어떤 값들을 사용하냐에 따라서 디지털로 변경된 소리의 해상도, 즉 음질이 결정된다.이때 나오는 단어가 바로 샘플 레이트(Sample Rate)와 비트 레이트(Bit Rate)이다. 인코더 프로그램 쓸 때 뭘 만져야 할지 모르게 만드는 어려운 단어들 단어는 어려워 보이지만 사실 간단하다. 결국 소리는 가로 축은 시간(Time), 세로 축은 진폭(Amplitude)으로 정의된 공간에 그려진 2차원의 진동 주파수 데이터이다. 이때 샘플 레이트는 가로 축의 해상도, 비트 레이트는 세로 축의 해상도를 의미하는 것이다. 이 값들은 아날로그를 디지털로 변경하는 첫번째 단계인 샘플링(Sampling)에서 활용된다. 샘플링(Sampling)샘플링은 아날로그 신호를 디지털 신호로 바꾸기 위한 첫번째 단계이다. 위에서 설명한 것처럼 아날로그 신호인 소리는 연속적인 신호이기 때문에 컴퓨터가 이 신호를 그대로 이해할 수가 없다. 우리가 마이크를 사용하여 소리를 녹음할 때, 이 아날로그 신호는 결국 전기 신호로 변환되어 컴퓨터에게 주어진다. 소리의 진동이 마이크 안에 있는 장치에 전달되면 이 장치가 전압을 올렸다 내렸다 하면서 변환하는 원리이다. 하지만 우리의 바보 컴퓨터는 이렇게 까지 해줘도 이 전기 신호를 이해할 수 없다. 그래서 컴퓨터는 연속적인 전기 신호를 측정하기 위해 특정 타이밍을 정해서 이 타이밍마다 내가 전압을 측정할게!라는 꼼수를 사용한다. 컴퓨터가 꼼수를 쓰는 이 과정을 그림으로 나타내보면 이런 느낌이다. 위 그림에 나타난 빨간 점이 컴퓨터가 전압을 측정한 타이밍이다. 컴퓨터는 특정 타이밍에 전기 신호를 측정하고 그 값을 저장한다. 빨간 점의 위치를 보면 저 신호는 [10, 20, 30, 27, 19, 8...] 뭐 이런 식으로 측정이 되었을 것이다.이때 저 빨간 점을 더 세밀하게, 즉 컴퓨터가 샘플 측정을 하는 간격이 짧을 수록 우리는 원래 신호에 가까운 값을 측정할 수 있다. 신호 내부의 사각형이 컴퓨터가 이해한 신호의 모양이다. 이때 이 신호를 측정하는 간격을 샘플 레이트(Sample Rate)라고 하고 신호를 측정하는 과정 자체를 샘플링(Sampling)이라고 한다. 당연히 샘플 레이트가 높을 수록 소리의 해상도, 즉 음질이 더 좋을 수 밖에 없다. 특히 높은 주파수를 가진 소리, 즉 고음의 해상도가 확연하게 좋아진다.보통 CD의 음질이 44.1kHz, TV나 라디오 방송이 48kHz의 샘플 레이트를 가지는데, 이는 약 1초에 44,100번, 48,000번 샘플을 측정한다는 것이다. 저 샘플 레이트는 어떤 기준으로 정하는 걸까? 샘플 레이트(Sample Rate)를 좀 더 자세히 알아보자방금 설명했듯이 CD의 경우 샘플 레이트가 44.1kHz이다. 그 말인 즉슨 CD에 들어가는 오디오는 컴퓨터가 아날로그 신호를 1초에 44,100번 측정한 결과물인 것이라는 것이다.하지만 인간이 들을 수 있는 영역인 가청주파수는 20hz ~ 20kHz 밖에 되지 않는다. 근데 왜 샘플 레이트는 44.1kHz, 48kHz처럼 훨씬 크게 잡는 것일까? 어차피 인간은 1초에 20,000번 진동하는 소리까지밖에 들을 수 없어서 44,100번 진동하는 소리를 녹음해도 어차피 들을 수 없는데? 이 질문에 대한 답은 소리의 진동 사이클이 어떻게 생겼는지를 보면 이해가 된다. 기본적으로 오디오 주파수는 이렇게 하나의 사이클 단위로 나누어 지는데, 이때 위로 올라가는 + 부분이 공기가 압축되는 부분이고 아래로 내려가는 - 부분이 다시 공기가 팽창하는 부분이다. 위에서 계속 설명했던 대로 소리란 곧 진동이고, 우리가 느끼는 것은 그 진동으로 인한 공기의 떨림이므로 압축 -> 팽창 -> 압축까지 모두 들어야 떨렸다!라고 느낄 수 있다는 것이다. 즉, 우리가 들을 수 있는 20,000번의 진동은 이 사이클이 1초에 20,000번 반복되는 소리라는 것이다. 압축이나 팽창 중에 하나만 주구장창 느낀다고 해서 이게 진동이구나라고 느낄 수는 없을 것이다. 그래서 오디오 신호의 한 사이클을 제대로 측정하려면 + 방향의 맨 위의 꼭지점 하나와 - 방향의 맨 밑의 꼭지점 하나를 모두 측정해야하기 때문에 최소 2번은 측정을 해야한다. 그래서 인간이 들을 수 있는 가장 높은 소리인 초당 20,000번의 떨림인 20kHz을 제대로 측정하려면 컴퓨터는 최소한 1초에 20,000 * 2 = 40,000번 측정을 해야 하는 것이다. 이게 바로 CD가 왜 44.1kHz의 샘플 레이트를 가지고 있는지에 대한 이유다. 이걸 나이퀴스트 이론(Nyquist Theorem)이라고 한다. 즉, 나이퀴스트 이론을 한마디로 정리하자면 측정하고 싶은 오디오 주파수있지? 오디오 신호 제대로 다 살리고 싶으면 최소한 그 주파수보다 두배는 더 빠르게 측정해야된다.그러니까 최소한 측정하고 싶은 오디오 주파수의 두배 사이즈의 샘플 레이트를 준비하렴. 인 것이다. 근데 여기서 또 의문이 생긴다. 저 이론에 따르면 인간의 가청주파수는 20,000hz니까 딱 40,000번만 측정하면 인간이 들을 수 있는 소리는 다 녹음할 수 있는데 왜 44,100번이나 48,000번까지 측정하는 걸까? 자연에는 인간만 있는 것이 아니라 이런 친구들도 있기 때문이다. 안녕하세요 자연계 고음의 절대강자입니다. 엠씨더맥스 사계 정도는 밥먹으면서도 부를 수 있슴다. 사실 자연에는 우리가 듣지 못하는 훨씬 높은 소리들도 존재한다. 단지 우리가 20kHz까지밖에 못 들을 뿐이다. 뭐 박쥐나 돌고래 같은 친구들은 훨씬 고음역대의 소리를 내지 않는가? 근데 이 소리가 40kHz의 샘플 레이트를 준비한 그릇에 들어오면 어떻게 될까? 컴퓨터는 1초에 20,000번의 사이클을 도는 소리를 제대로 측정하려고 1초에 40,000번 전압을 측정하려고 했는데 만약 1초에 30,000번의 사이클을 도는 훨씬 더 높은 주파수의 소리가 들어와버린다면? 정답. 점을 이상한데다가 찍는다. 그림을 보면 컴퓨터가 점을 찍는 간격, 즉 전압을 측정하는 간격보다 들어온 신호의 사이클이 더 짧다. 그래서 컴퓨터가 찍은 점을 보면 신호의 꼭지점이 아닌 어중간한 어딘가에 찍힌 것을 볼 수 있다. 이것이 바로 나이퀴스트 이론의 가지고 있는 함정이다. 그리고 저 어중간한데 찍힌 점들을 이어본 파란색 선을 보면 결국 낮은 주파수가 된 것을 알 수 있다. 그러면 어떻게 될까? 우리 귀에 아주 잘 들린다. 참 소름돋는 순간이다! 녹음할 때는 분명히 아무것도 안들렸는데 녹음한 걸 들어보니 이상한 소리가 녹음되어있으니 말이다. 그래서 이 현상을 고스트 주파수(Ghost Frequency)라고 부른다. 아니 그러면 샘플 레이트를 팍팍 올리면 되잖아! 높은 소리도 제대로 녹음되면 문제 해결 아닌가? 하지만 이 디지털 오디오 기술이 처음 사용되기 시작한게 1970년대이기 때문에 무작정 샘플 레이트를 올리기에는 하드웨어 용량이 못 따라갔었다. 그래서 이 문제를 해결하기 위해 사용한 방법이 바로 LPF(Low Pass Filter)이다. 이 필터는 전기 쪽 공부하신 분들은 매우 익숙할텐데, 말 그대로 낮은(Low) 주파수만 통과(Pass)시키는 필터이다.오디오 녹음을 할때 LPF를 사용해서 인간의 가청주파수보다 높은 소리는 다 잘라버리고 인간의 가청주파수 영역의 소리만 통과시키면 방금 얘기한 고스트 주파수가 생길 일도 없기 때문이다. 근데 또 아날로그 신호라는 게 그렇게 무 자르듯이 싹뚝! 잘리는 게 아니다. LPF를 써도 결국 잘린 부분이 저렇게 비스듬하게 꺾이면서 약간 아쉬운 부분이 남게된다. 이때 그림에 표시된 허용 범위인 -3db 밑으로 떨어지기 시작하는 곳을 Cut Off라고 부른다. 저기에서부터 신호가 잘렸다고 치겠다는 것이다. 그럼 Cut Off되는 부분을 20kHz보다 조금 더 밑으로 내리면 20kHz 언저리에서 신호가 사라지게 만들 수 있지 않을까? 라는 생각도 들지만 그 문제 때문에 20kHz의 주파수 영역을 전부 활용하지 못하는 건 너무 아깝다고 판단이 들었나보다. 그래서 Cut Off를 딱 20kHz에 맞추고 남는 부분은 그냥 감수하자고 합의가 된 것이다. 당시 기술로 저 남는 부분을 줄이고 줄여서 맞춘게 딱 2,050hz였다. 그럼 이제 남는 부분과 가청주파수를 합쳐보면 22,050hz가 된다. 나이퀴스트 이론에 따르면 우리는 최소한 2배의 샘플 레이트를 준비해야 이 신호를 제대로 측정할 수 있으므로 결국 CD의 표준 샘플 레이트가 44,100hz = 44.1kHz가 된 것이다.뭐 그 외에도 당시 기술의 한계 외에도 기업들끼리 싸우기도 하고 어른의 사정도 있는 등 국제 표준을 정할 때 늘 발생하는 여러가지 이슈가 있었지만 대표적인 기술적인 이슈는 이 이유였다. 그 후 48kHz, 96kHz, 192kHz 등의 높은 샘플 레이트는 그냥 디바이스가 발전하면서 기술적인 제한이 없어졌으니까 샘플 레이트는 클수록 좋지! 뿜뿜하면서 늘린 것이다. 비트 레이트(Bit Rate)비트 레이트는 샘플링에 비하면 초 간단하다. 특히 우리 같은 개발랭이들에게 익숙한 이름인 Bit가 붙어있지 않은가? 샘플 레이트가 소리의 가로 해상도 역할을 한다면 비트 레이트는 세로 해상도 역할을 한다. 샘플 레이트와 마찬가지로 비트 레이트도 CD를 기준으로 설명하는 게 보편적이기 때문에 다시 CD를 기준으로 설명하겠다. CD의 비트 레이트는 16bit인데 이건 말 그대로 세로로 16bit 만큼의 해당하는 값을 표현할 수 있다는 얘기이다. 아까 위에서 설명한 샘플링을 진행할 때 전압을 측정했었다. 이때 컴퓨터가 측정한 이 전압의 값을 얼마나 섬세하게 표현할 수 있느냐를 비트 레이트가 결정하는 것이다. 16bit면 16자리의 이진법을 사용할 수 있다는 것이고 $2^{16} = 65536$이니까 0~65536까지 총 65537개의 값을 사용할 수 있는 것이다. 비트 레이트가 높을 수록 아날로그 신호 내부의 막대가 더 꼼꼼하게 채워지는 것을 볼 수 있다. +와 -를 합쳐서 비트를 세는 signed이기 때문에 그림에는 50% 씩만 표현되어있는 것이다. 물론 아날로그인 소리 신호가 변환된 전압 값이 123 같이 딱 떨어지는 정수일리가 없으므로 우리가 사용할 수 있는 0~65536 중 근사치를 찾아서 바꿔주는데 이 과정을 양자화(Quantizing)이라고 부른다. (양자역학에서 나오는 그 양자랑 같은 의미 맞다.) 이후 0~65536의 값으로 변경된 전압을 컴퓨터가 이해할 수 있는 이진수(Binary)로 변경하는 과정을 부호화(Coding)라고 한다. 이런 것들은 우리같은 개발랭이들은 워낙 익숙한 문제이기 때문에 이 정도만 설명하고 넘어가겠다. Web Audio API로 파형 그려보기자, 드디어 길고 길었던 소리 이론이 끝났다. 필자는 디지털로 변환된 오디오를 가지고 파형을 그려보는 것이 목적이기 때문에 Analog to Digital만 다뤘고 Digital to Analog는 이 포스팅에서 다루지 않겠다. 근데 이것도 나름 재밌으므로 따로 찾아보길 강추한다. 필자는 오디오 파일을 업로드해서 해당 오디오 파일을 Web Audio API로 분석하여 필요한 데이터를 뽑아내고 svg를 사용하여 파형을 그릴 것이다.참고로 프로젝트는 webpack4와 babel7을 사용하여 초간단하게 구성했다. 자세한 코드는 깃허브 레파지토리에서 확인 해볼 수 있다. 기본 틀 잡기그럼 먼저 HTML을 간단하게 작성하자. index.html123456 HTML은 이게 끝이다. 오디오 파일을 업로드할 input 엘리먼트 하나와 파형을 그릴 svg 엘리먼트 하나만 있으면 된다. 어차피 테스트 용도라서 UI가 중요한게 아니기 때문에 기능 구현에 충실했다.(라고 포장을 해봅니다) 이제 파일이 업로드되면 실행될 이벤트 핸들러를 작성하자. index.js1234567891011(function () { const inputDOM = document.getElementById('audio-uploader'); inputDOM.onchange = e => { const file = e.currentTarget.files[0]; if (file) { const reader = new FileReader(); reader.onload = e => console.log(e.target.result); reader.readAsArrayBuffer(file); } }})(); FileReader.prototype.readAsArrayBuffer 메소드는 바이너리 형태인 파일을 한번에 반환하는 것이 아니라 일정 단위의 청크(Chunk)로 반환하는 메소드이다. 보통 뭐 서버로 파일 업로드 한다거나 할 때 많이 사용한다. 원래는 이 ArrayBuffer를 처리해주기 위한 별도의 로직을 작성해야하지만 이번에 사용할 Web Audio API가 내부적으로 ArrayBuffer를 알아서 처리해주기 때문에 걱정하지 않아도 된다. AudioAnalyzer 클래스 작성이제 본격적인 Web Audio API를 사용해볼 차례이다. 필자는 별도로 AudioAnalyzer라는 싱글톤 클래스를 만들었다. lib/AudioAnalyzer.js1234567891011121314151617181920212223242526class AudioAnalyzer { constructor () { if (!window.AudioContext) { const errorMsg = 'Web Audio API 지원 안돼유 ㅜㅜ'; alert(errorMsg); throw new Error(errorMsg); } this.audioContext = new (AudioContext || webkitAudioContext)(); this.audioBuffer = null; this.sampleRate = 0; this.peaks = []; this.waveFormBox = document.getElementById('waveform'); this.waveFormPathGroup = document.getElementById('waveform-path-group'); } reset () { this.audioContext = new (AudioContext || webkitAudioContext)(); this.audioBuffer = null; this.sampleRate = 0; this.peaks = []; }}export default new AudioAnalyzer(); SVG 뷰박스를 설정하자이렇게 대충 기본 틀을 잡아주고 나서 오디오 파형을 그릴 svg 엘리먼트의 뷰박스를 설정할 수 있는 메소드를 하나 선언할 것이다. 소리 신호는 샘플 레이트에 따라서 들어온 데이터의 길이가 천차만별이기 때문에 뷰박스의 width를 오디오 데이터의 크기에 맞게 동적으로 변경해줘야지 모든 신호를 뷰박스에 딱 맞게 그릴 수 있다. 123updateViewboxSize () { this.waveFormBox.setAttribute('viewBox', `0 -1 ${this.sampleRate} 2`);} svg 엘리먼트의 viewBox 속성은 앞에서부터 min-x, min-y, width, height를 의미한다. 비트 레이트를 설명할 때 얘기 했듯이 오디오 신호는 부호를 가지는 signed이기 때문에 0, 0이 아닌 0, -1에서 뷰박스를 설정해야한다. 그리고 width는 오디오의 샘플 레이트로 잡아줘서 오디오 신호가 뷰박스의 처음부터 끝까지 꽉 차도록 설정하고 height는 -1 ~ 1까지 잡아줘야하니까 2로 설정했다. 이렇게 선언한 updateViewboxSize메소드는 어플리케이션이 초기화될때 뷰박스 사이즈도 함께 초기화되도록 constructor와 reset 메소드에 적당히 추가해주었다. 업로드된 AudioBuffer 확인하기이제 input 엘리먼트를 통해 업로드된 오디오 파일을 디코딩하는 귀여운 setter를 하나 선언하면 된다. 12345setAudio (audioFile) { this.audioContext.decodeAudioData(audioFile).then(buffer => { console.log(buffer); });} AudioContext.prototype.decodeAudioData 메소드는 ArrayBuffer를 받아서 AudioBuffer로 변환하는 메소드이다. 아까 위에서 설명했던 대로 readAsArrayBuffer 메소드가 반환한 ArrayBuffer는 한번에 모든 바이너리 데이터를 불러오는 것이 아니라 청크 단위로 불러오기 때문에 decodeAudioData도 변환을 동기적으로 해주지는 않는다. 그래서 이 메소드를 사용할 때는 콜백함수를 사용하거나 Promise를 사용해야 한다. 필자는 콜백 혐오자이기 때문에 Promise를 사용했다. 이제 실제로 오디오 파일이 어떻게 변환되는지 보기 위해 파일을 업로드하면 AudioAnalyzer에게 오디오 파일을 넘기도록 변경해주자. index.js12345678910111213141516import AudioAnalyzer from './lib/AudioAnalyzer';(function () { const inputDOM = document.getElementById('audio-uploader'); inputDOM.onchange = e => { const file = e.currentTarget.files[0]; if (file) { // AudioAnalyzer 초기화 AudioAnalyzer.reset(); const reader = new FileReader(); // AudioAnalyzer에게 파일 토스 reader.onload = e => AudioAnalyzer.setAudio(e.target.result); reader.readAsArrayBuffer(file); } }})(); 이 쯤 했으면 이제 메인 함수에는 더 이상 뭘 작성할 필요가 없을 것 같다. 이제 AudioAnalyzer만 가지고 놀면 된다. 데이터가 어떻게 나오는 지 확인하기 위해 필자가 좋아하는 가수인 거미의 그대 돌아오면.mp3 파일을 업로드 해보았다. decodeAudioData123456AudioBuffer { length: 12225071, duration: 277.2124943310658, sampleRate: 44100, numberOfChannels: 2} 그러면 AudioAnalyzer의 setAudio 메소드 내부에서 실행된 decodeAudioData가 ArrayBuffer를 받아서 AudioBuffer로 변환한 뒤 반환해준다. 이걸 뜯어보면 유용한 정보들이 들어있다. sampleRate: 당연히 샘플 레이트를 의미하고 이 곡의 샘플 레이트는 44,100hz이다. numberOfChannels: 이 오디오가 몇개의 채널을 가지고 있는지를 나타내는데 이 파일은 두 개의 채널이 있는 스테레오 채널 오디오 파일이다. length: 피크(Peak)들의 개수를 의미한다. 피크는 컴퓨터가 샘플링을 진행할 때 전압을 측정한 값을 의미한다. duration: 이 오디오 파일의 재생 길이를 초단위로 표시해준다. 그대 돌아오면은 약 277초동안 재생되나보다. 음 여기서 한가지 결정해야할 것이 생겼다. 채널이 $n$개인 오디오 데이터가 들어왔을 때 그 $n$개의 채널의 오디오 파형을 모두 표현해주거나 채널을 하나로 합쳐서 파형을 표현해주어야한다. 필자는 $n$개의 채널을 가진 오디오 데이터가 들어오더라도 그 채널을 모두 한개의 채널로 머지해서 파형을 그릴 예정이다. 하지만 이 포스팅에서 채널을 머지하는 것까지 모두 이야기하면 너무 복잡해지기 때문에 그냥 하나의 채널만 사용하여 진행하도록 하겠다. 오디오 데이터 분석하고 정제하기이제 필자는 기본적인 오디오 데이터인 AudioBuffer를 얻었다. 이제 이 데이터만 있어도 오디오 파형을 그릴 수 있다.이제 AudioBuffer에서 필요한 데이터만 뽑아서 클래스의 멤버변수들에 할당해주자. 12345678910setAudio (audioFile) { this.audioContext.decodeAudioData(audioFile).then(buffer => { // AudioBuffer 객체를 멤버변수에 할당 this.audioBuffer = buffer; // 업로드된 오디오의 샘플 레이트를 멤버변수에 할당 this.sampleRate = buffer.sampleRate; // 샘플 레이트에 맞춰서 svg 엘리먼트 크기 조정 this.updateViewboxSize(); });} 후, 여기까지 왔으면 파형을 그릴 준비가 모두 끝났다. 우선 AudioBuffer에서 오디오 신호를 하나하나 뽑아보자.먼저 오디오 신호가 들어있는 배열부터 한번 살펴보자. 아까 필자가 AudioBuffer의 데이터를 간략하게 봤을 때 numberOfChannels라는 프로퍼티가 있었고 이 값이 2인 스테레오 채널이었다. 이 채널 데이터는 AudioBuffer.getChannelData 메소드를 통해서 가져올 수 있다. 123456for (let i = 0; i < this.audioBuffer.numberOfChannels; i++) { console.log(this.audioBuffer.getChannelData(i));}// Float32Array(12225071) [0, 0, 0, …]// Float32Array(12225071) [0, 0, 0, …] 아닛 어마무시한 길이의 Float32Array가 나왔다. 지금 필자에게 보이는 값은 전부 0이지만 보통 음악이 시작하자마자 쿠과과광! 하진 않기 때문에 그런거지 뒤쪽 인덱스의 엘리먼트에는 값이 제대로 들어가 있다. 그렇다고 이 값들을 그대로 사용하긴 조금 힘들고 조금 손질을 해줘야 한다. 우선 저 배열에 들어있는 원소의 정체가 뭔지부터 생각해보자. 배열의 길이인 12225071를 사용하면 이 친구의 정체를 파악할 수 있다. 아까 우리가 AudioBuffer를 통해 이 곡의 재생길이(Duration)를 봤을 때 약 277초 였다. 이 곡의 샘플 레이트와 재생길이를 곱해보면 12225071이 나온다. 즉, 저 원소들은 컴퓨터가 샘플 레이트에 따라 측정한 전압인 피크이다. 저 하나하나의 피크들이 전부 Float32Array의 원소로 들어가있다. 일단 저 원소의 정체가 피크라는 것을 알았으니 이 친구들을 조금 정제해보자. 왜 이걸 정제하느냐? 12,225,071개의 피크를 전부 렌더한다는 게 과연 효율적인 시각화인가…? 라는 합리적 의심 때문이다. 솔직히 피크가 천만개가 찍히던 오백만개가 찍히던 어차피 우리는 조그만 모니터로 볼 것이기 때문에 굳이 이걸 하나하나 다 표현해준다는 것 자체가 큰 의미는 없다. 그래서 필자는 피크들을 적당히 압축할 것이다. 모든 피크를 수집하는 것이 아니라 적당한 길이의 샘플을 만들고 해당 샘플 안의 최대값과 최소값만 수집할 것이다. 위에서 설명했듯이 나이퀴스트 이론에 따르면 어차피 하나의 사이클을 표현할때 최대값 한개와 최소값 한개만 알면 아무 문제가 없다. 파형의 해상도는 조금 떨어지겠지만 이건 뭐 상용 툴도 아니기 때문에 큰 의미는 없다. 12345678910111213141516171819202122232425262728const sampleSize = peaks.length / this.sampleRate;const sampleStep = Math.floor(sampleSize / 10);// 예시로 0번 채널만 가져옴const peaks = this.audioBuffer.getChannelData(0);const resultsPeaks = [];// 딱 샘플 레이트 길이인 44100개의 피크만 수집할 것이다.Array(this.sampleRate).fill().forEach((v, newPeakIndex) => { const start = Math.floor(newPeakIndex * sampleSize); const end = Math.floor(newPeakIndex + sampleSize); let min = peaks[0]; let max = peaks[0]; for (let sampleIndex = start; sampleIndex < end; sampleIndex += sampleStep) { const v = peaks[sampleIndex]; if (v > max) { max = v; } else if (v < min) { min = v; } } resultPeaks[2 * newPeakIndex] = max; resultPeaks[2 * newPeakIndex + 1] = min;}); 좀 복잡해보이지만 이 코드를 한마디로 표현하면 다음과 같다. 샘플레이트 만큼 이터레이션을 돈다. 특정 길이의 2차 샘플 구간을 정하고 그 구간 내에서 최대값과 최소값을 찾는다. resultPeaks 배열의 짝수 인덱스에 최대값을, 홀수 인덱스에는 최소값을 삽입한다. resultsPeaks배열에 값을 저장할 때 2 * newPeakIndex와 같이 배열의 크기를 2배로 해주는 이유는 피크를 한번 이터레이션 돌 때 max와 min 최대 2개의 값을 저장해야하기 때문이다. 이렇게 해서 필자는 적당히 압축된 88200 길이의 피크 배열 하나를 얻게 되었다. 이제 진짜 그려보자!그리는 방법은 생각보다 단순하다. 위에서 뽑아온 피크 배열을 순회돌면서 그려주기만 하면 된다. SVG의 path 엘리먼트는 d 속성에 담긴 문자열을 분석하여 선을 그려준다. 이때 M은 Move, 그림을 그릴 포인터를 옮기는 명령어이고 L은 Line, 지정한 곳까지 선을 긋는 명령어이다. 즉, 우리는 포인터를 옮기고 선을 긋고를 반복하면 되는 것이다. 아까 필자가 짝수 인덱스에는 최대 값을 담고 홀수 인덱스에는 최소값을 담아준 이유가 바로 여기에 있다. 이터레이션을 돌면서 짝수 인덱스 일때는 M 명령어를 사용하여 포인터를 옮기고 홀수 인덱스 일때는 L 명령어를 사용하여 선을 그리려고 한 것이다. 123456789101112131415161718192021draw () { if (this.audioBuffer) { const peaks = this.peaks; const totalPeaks = peaks.length; let d = ''; for(let peakIndex = 0; peakIndex < totalPeaks; peakIndex++) { if (peakNumber % 2 === 0) { d += ` M${Math.floor(peakNumber / 2)}, ${peaks.shift()}`; } else { d += ` L${Math.floor(peakNumber / 2)}, ${peaks.shift()}`; } } const path = document.createElementNS('http://www.w3.org/2000/svg', 'path'); path.setAttributeNS(null, 'd', d); this.waveFormPathGroup.appendChild(path); }} Math.floor(peakNumber / 2)는 $x$축, peaks.shift()는 $y$축을 의미한다. 이 코드는 대략 이런 식으로 작동할 것이다. 앞에 붙은 숫자는 피크의 인덱스라고 생각하자. (0, 100)으로 포인터 이동 (0, -25)으로 선을 긋는다. (1, 300)으로 포인터 이동 (1, -450)으로 선을 긋는다. 쭉쭉 반복 저 과정을 반복하면 대략 이런 모양의 작대기들이 그려진다 우리가 그리려는 오디오 파형은 디지털 신호를 표현한 것이기 때문에 위에서 우리가 봤던 오디오 신호의 예시처럼 연속적인 아날로그로는 표현이 불가능 하다. 그래서 이렇게 작대기를 쭉쭉 그어가면서 표현해야하는 것이다. 이렇게 보면 상당히 허접한 그림이 나올 것 같지만, 저 선이 88,200개가 겹쳐있으면 꽤 그럴싸해보인다. 작대기도 잘 그으면 예술이 된다 자, 이렇게 오디오 파형을 심플하게 그려보았다. 여기까지 해놓고 하는 말인데… 사실 WaveSurfer라고 이거 해주는 라이브러리가 있다. 기능도 더 많다. 그냥 내 손으로 한번 직접 그려보고 싶었다. 음악 주세요 DJ파형만 그리기는 좀 아쉬우니 막간을 이용해서 메소드 하나만 더 만들어보자. 바로 이 음악을 재생하는 기능이다.원래 이 메소드를 만들 때 필자의 의도는 음악이라도 좀 들으면서 만들자였는데 이거 만들면서 그대 돌아오면을 너무 많이 들어서 조금 질려버렸다.(거미 누나 미안…) 123456play (buffer) { const sourceBuffer = this.audioContext.createBufferSource(); sourceBuffer.buffer = buffer; sourceBuffer.connect(this.audioContext.destination); sourceBuffer.start();} AudioContext.prototype.createBufferSource 메소드는 AudioNode 중 하나인 AudioBufferSourceNode라는 친구를 생성하는 메소드인데, 이 AudioNode에게 우리가 지금까지 가지고 놀았던 AudioBuffer를 넘겨주면 진짜 오디오적인 컨트롤을 할 수가 있게 도와주는 음…껍데기 같은 느낌이다. Web Audio API는 이런 AudioNode들을 서로 연결해서 소리를 변조할 수 있는 기능을 제공하기 때문에 이걸 사용해서 컴프레서나 리버브 같은 오디오 이펙터도 만들 수 있다. 다음에는 한번 오디오 이펙터를 만들어보는 것을 목표로 해야겠다. 완성된 샘플의 소스는 필자의 깃허브 레파지토리에서, 라이브 데모는 여기서 확인할 수 있다. 이상으로 컴퓨터는 어떻게 소리를 들을까? 포스팅을 마친다.","link":"/2019/07/10/javascript-audio-waveform/"},{"title":"Git 뉴비를 위한 기초 사용법 - 버전 관리","text":"이번 포스팅에서는 저번 포스팅인 Git 뉴비를 위한 기초 사용법 - 시작하기에서 설명했던 기본적인 명령어보다 좀 더 나아가서 몇 가지 개념과 명령어를 더 공부해보려고 한다. 저번 포스팅에서는 리모트 서버에서 소스를 클론하고 수정한 후 다시 리모트 서버에 업데이트하는 과정에 대해 집중해서 설명했다면 이번 포스팅에서는 Git의 메인 주제인 버전 관리에 대해 더 다뤄볼 예정이다. 그럼 저번 포스팅과 마찬가지로 간단한 용어와 개념에 대한 설명한 후, 그 개념들을 사용하기 위한 명령어를 설명하도록 하겠다. 용어와 개념 알아보기Git은 버전을 효율적으로 관리하기 위해 몇 가지 개념을 제시하고 있다. 현재 나의 버전 상태를 의미하는 HEAD, 작업 공간인 브랜치(Branch), 브랜치를 합치는 머지(Merge)와 리베이스(Rebase)등이 그렇다. 그리고 이런 기능들을 사용하다보면 가끔 Git의 에러와 마주하게 되는데, 필자같은 경우는 처음 개발자로 일을 시작했을 때 코딩하다가 발생하는 에러보다 Git에서 발생하는 에러가 더 무서웠던 기억이 있다. 솔직히 코딩하다가 나는 에러는 고치는 과정이 쉽든 어렵든간에 그냥 필자가 고치면 해결되지만, Git에서 나는 에러는 고친답시고 이것 저것 건드리다가 잘못 건드리면 왠지 소스가 가루가 되어 날아갈 것 같아서 무서웠다. 하지만 Git을 사용한 지 6년쯤 지난 지금 다시 그때를 생각해보면 Git에 대해서 잘 몰랐기 때문에 더 무서웠던 것 같다. 내가 어떤 기능을 사용했을 때 소스가 어떻게 되는 지 정확히 알고 있지 않은 상태였기 때문에 그런 걱정도 들었던 것이다. 혹시 이 글을 읽는 독자 분들 중에서도 필자와 같은 경험이나 생각을 하신 분들이 있을 것이기 때문에 필자가 알고 있는 Git의 개념들을 최대한 알기 쉽게 풀어보려고 한다. Merge Conflict필자 생각에 Git을 사용하다가 가장 많이 마주치는 에러는 아무래도 머지 컨플릭트(Merge Conflict)인 것 같다. 에러라고 하기에는 조금 애매하긴 하지만 어쨌든 정상은 아닌 상황이기 때문에 처음 마주하면 굉장히 당황스럽고 뭐가 뭔지 헷갈린다. 컨플릭트는 말 그대로 소스의 충돌이 발생한 상황이기 때문에 주니어든 시니어든 가리지 않고 평등하게 발생하고, 또한 평등하게 당황하게 된다. 왜냐면 컨플릭트는 논리적인 에러가 아니라 내 작업물과 다른 사람의 작업물이 충돌한 상황이기 때문에 스스로 혼자 해결하기 힘든 경우가 많기 때문이다. 그럼 컨플릭트, 즉 충돌이 정확히 어떤 상황을 말하는 지 알기 위해 실제 협업 상황에서 발생할 수 있는 예시를 함께 살펴보자. 철수와 영희는 사장님으로부터 지각한 사람의 명단을 만들어서 관리해 달라는 부탁을 받았다. 그래서 철수와 영희는 다음과 같은 텍스트 파일을 만들어서 지각자를 관리하기 시작했다. 지각자.txt1234567월 25일 지각자 명단나연채영사나쯔위 자, 이제 철수와 영희는 사장님으로부터 지각자를 관리할 수 있는 권력을 부여받았고, 매일 이 파일에 지각자를 입력해야한다. 하지만 철수와 영희는 별로 사이가 안 좋기 때문에 이 둘은 서로 커뮤니케이션을 하지 않고 각자 맘대로 파일을 수정하는 방식으로 지각자 입력 작업을 진행하게 된다. 그러던 중 철수와 영희는 사무실에 심어놓은 자신들의 정보원으로부터 7월 25일의 세번째 지각자인 사나가 사실 지각이 아니였다는 정보를 입수했다. 근데 문제는 이 정보원들이 가져온 정보가 서로 달랐던 것이다. 철수의 정보원: 야 7월 25일에 지각한 사람 있잖아, 사나가 아니고 미나래!영희의 정보원: 영희야, 7월 25일에 사나가 지각한게 아니고 지효가 지각한거라는데? … 이 둘은 자신의 정보원을 100% 신뢰하기 때문에 바로 각자 지각자 명단 파일을 수정하기 시작했다. 시간 철수 영희 1 지각자.txt의 사나를 미나로 변경 지각자.txt의 사나를 지효로 변경 2 리모트 저장소에 변경 사항 커밋 후 푸쉬 계속 작업 중 3 철수 퇴근 >_ FETCH_HEADAuto-merging 지각자.txtCONFLICT (content): Merge conflict in 지각자.txtAutomatic merge failed; fix conflicts and then commit the result. 이렇게 다른 사람과 내가 같은 부분을 수정하게 되면, Git은 어떤 것이 맞는 소스인지 알 방법이 없다. 이런 상황에서 Git은 어떤 부분이 충돌났는지 표시하여 사용자에게 알려주기만 하고 나머지는 사용자가 알아서 수정하라고 맡겨버리는데, 이런 상황이 바로 병합 충돌(Merge Conflict)이다. 철수와 영희 처럼 같은 브랜치에서 작업한다는 것은 소스의 변경 사항을 계속 히스토리를 공유한다는 것이다. 즉, 주기적으로 리모트 저장소로부터 상대방이 작업한 것을 Pull로 가져와서 내 로컬 브랜치에 병합해야한다는 것인데, 이 과정에서 충돌이 발생할 가능성이 높다. 123456789107월 25일 지각자 명단나연채영> 35058b46325bb61112efd52f4019f907c561328d쯔위 이때 > 커밋 해쉬사이의 내용은 어떤 커밋에서 수정된 내용과 충돌이 발생했는지 알려준다. 이 예시에서는 철수가 사나를 미나로 수정한 부분이 될 것이다. Git은 그냥 버전 관리만 해주는 친구이기 때문에 이런 상황에서 사나는 사실 지각자가 아니였기 때문에 다른 사람으로 변경해야했다와 같은 비즈니스 히스토리는 모른다. 그렇기 때문에 Git은 둘 중에 어떤 것이 맞는 소스인지도 당연히 모를 수 밖에 없다. 그래서 사용자에게 선택을 맡기는 것이다. 이 상황에서 영희는 다음 세가지 선택지를 가질 수 있다. 철수의 변경 사항을 무시 자신의 변경 사항을 무시 두 변경 사항 모두 반영 보통 이런 상황에서는 철수를 불러서 사나 대신 미나를 추가한 이유가 무엇인지 물어본 다음 결정해야하지만 영희는 철수와 사이가 좋지 않으니 그냥 철수의 커밋을 날려버릴 수도 있겠다.(실제 상황에서 함부로 이러면 혼납니다) 여러 개의 Branch를 사용하는 이유브랜치(Branch)는 저번 포스팅에서 한번 간단하게 설명했다. 이미 저번 포스팅에서 기초적인 브랜치의 개념에 대해서 한번 언급하고 넘어갔기 때문에 이번에는 왜 여러 개의 브랜치를 사용해야 하는가?에 대한 이야기를 해보려고 한다. 기본적으로 Git은 혼자 만의 작업이 아닌 여러 명이 함께 작업하는 협업 상황을 상정하고 만들어졌다. 아무리 Git이 리모트 레파지토리와 로컬 레파지토리로 소스를 분산해서 관리하는 분산 버전 관리 시스템이지만 여러 사람이 한번에 같은 어플리케이션의 코드를 수정하고 있는 상황에서는 방금 위에서 설명한 머지 컨플릭트가 자주 발생하게된다. 그래서 보통 사용자들은 브랜치로 주제에 맞는 작업 공간을 따로 나누어서 히스토리를 관리하는 것이다. 이렇게 브랜치를 나누어도 결국 언젠가 소스를 병합해야 하기 때문에 컨플릭트가 발생할 확률은 있지만 적어도 작업 중간중간에 계속 해서 컨플릭트를 수정해야하는 일은 많이 줄일 수 있다. 그래서 개발자들은 어떻게 해야 효율적으로 여러 개의 브랜치를 관리할 수 있을까?라는 고민을 하게 되는데, 이때 나온 것이 바로 브랜치 전략이다. 브랜치 전략 중 대표적인 것은 Git flow가 있는데, 이건 그냥 유명한 전략 중 하나일 뿐이기 때문에 어떤 브랜치 전략을 가져갈 것인지는 그 조직이 결정하면 된다. 그럼 브랜치 전략이 어떤 것인지 알아보기 위해 대표적인 브랜치 전략인 Git flow를 한번 간략하게 살펴보도록 하자. 전략적인 브랜치 관리, Git flowGit flow는 기본적으로 master와 develop 브랜치를 가지고 시작하게 된다. 이때 master는 항상 운영되고 있는 소스의 상태를 가지고 있어야하며, 절대 master 브랜치에는 바로 커밋을 할 수 없다. 그리고 develop 브랜치는 팀이 현재 개발을 진행하고 있는 브랜치이다. 그리고 develop 브랜치에서 각자 개발을 맡고 있는 기능 별로 feature 브랜치를 생성해서 실제 개발을 진행하게 된다. 모바일에서는 이 그래프의 내용이 잘 안보일 수 있으니 브랜치 이름에 색을 입혀서 설명하도록 하겠다. 이 그래프를 보면 master 브랜치에 프로젝트의 시작을 의미하는 커밋이 찍힌 후 develop 브랜치가 생성되었고, develop에서부터 기능 개발을 담당하는 브랜치들이 분기하고 있는 모습을 볼 수 있다. 에반은 feature/add-typescript 브랜치를 생성한 후 어플리케이션에 타입스크립트를 붙히는 작업을 하고, 다니엘은 feature/social-login 브랜치를 생성한 후 소셜 로그인 연동 작업을 하고 있다. 그 후 개발이 끝나는 대로 develop 브랜치에 해당 브랜치들을 차례로 머지하고 있는 모습을 볼 수 있다. 다니엘이 에반보다 develop 브랜치에 머지한 시점이 늦기 때문에 만약 에반과 다니엘이 같은 부분을 변경했다면 이때 컨플릭트가 발생하게 된다. 하지만 적어도 에반과 다니엘이 각자 기능을 개발하고 있을때는 컨플릭트가 발생하지 않기 때문에 좀 더 기능 개발에 집중할 수 있게 되는 것이다. 이렇게 개발을 쭉쭉 진행하다가 배포를 해야할 시점이 오면 master로부터 release브랜치를 생성한다. 필자의 직장같은 경우는 release/release-1.0.0과 같이 배포 버전을 브랜치 이름에 표기하는 네이밍 컨벤션을 따르고 있다. 이 release 브랜치는 온전히 배포 만을 위한 브랜치이기 때문에 해당 버전의 배포가 끝나면 버려진다. 개발이 종료되고 1.0.0 버전을 배포하기 위해 release/release-1.0.0이라는 노란색 브랜치를 master로부터 하나 생성했다. 그 후 다음 버전에 배포될 기능들을 가지고 있는 develop 브랜치를 release/release-1.0.0 브랜치에 머지하고 스테이징 서버에 배포하는 등 최종 테스트를 한 다음, 조직원 모두가 해당 버전의 배포에 동의한다면 master 브랜치에 해당 브랜치를 머지하고 버전명으로 태그를 단다. 이때 이런 궁금증이 생기는 분이 있을 것이다. 그럼 핫픽스는 어떻게 하나요? develop 브랜치에서 브랜치를 분기하면 현재 버전의 기능 개발이 끝날 때까지 기다려야하는데… 이런 경우 develop 브랜치에서 hotfix 브랜치를 분기하여 master로 머지 후 긴급 배포를 하게되면 develop 브랜치에 들어있는 아직 배포되지 말아야할 기능들까지 배포되기 때문에, 핫픽스는 예외적으로 master에서 분기해서 다시 master로 머지할 수 있다. 그래프가 조금 복잡해졌지만 검정색 라인인 hotfix/fix-main-page에만 집중해보자. master 브랜치로부터 갈라져나와서 한 개의 커밋이 찍히고 다시 master로 머지되는 것을 볼 수 있다. 이때는 정식 릴리즈가 아닌 핫픽스 릴리즈이므로 Sementic Version 룰에 따라 0.0.1 버전의 태그를 달아주었다. hotfix/fix-main-page 브랜치가 머지되고 배포가 되었다는 것은 현재 운영 환경에서 돌아가고 있는 소스가 변경되었다는 것이므로 develop 브랜치에도 해당 변경 사항을 반영해줘야한다. 그렇기 때문에 핫픽스 담당자는 배포를 하고 나면 develop 브랜치를 직접 최신화하는 것이 좋다. 그리고 동료들에게 지금 핫픽스 배포가 끝났으니 각자 작업하시는 브랜치에서 develop 브랜치를 Pull하여 최신화 해주세요~라고 알려주면 더더욱 좋을 것이다. 물론 이런 복잡한 브랜치 전략 없이도 그냥 잘 돌아가는 조직도 있다. 하지만 같은 소스를 만지는 개발자가 많아지면 많아질수록 어느 정도의 룰조차 없다면 원활한 협업이 진행되기는 힘들 수도 있기 때문에 대부분의 규모있는 조직에서는 각자의 상황에 맞는 브랜치 전략을 세워서 버전 관리를 진행하고 있다. 브랜치를 왜 하나로만 운영하지 않는지 이해가 되었다면, 이제 이 브랜치와 버전 히스토리들을 가지고 놀 수 있는 몇 가지 유용한 기능들을 더 살펴보도록 하자. 두 개의 브랜치를 합쳐보자각자의 브랜치에서 작업을 계속 진행하다보면 언젠가 두 브랜치를 합쳐야 하는 날이 다가온다. 이때 두 개의 브랜치를 합치는 행위를 브랜치 병합(Branch Merge)이라고 한다.Git은 Merge, Merge and Squash, Rebase 각각 특색있는 3개의 브랜치 병합 기능을 제공한다. 결국 이 3개의 명령어 모두 두 개의 브랜치를 합친다는 행위는 같지만, 합치는 방법도 다르고 버전 히스토리도 다르게 남기 때문에 적재적소에 이 기능들을 잘 이용한다면 팀원들에게 이쁨받는 깃쟁이가 될 수 있을 것이다. Merge 머지(Merge)는 제일 기본적인 브랜치 병합 기능으로, 합치려고 하는 대상 브랜치의 변경 사항을 타겟 브랜치에 모두 반영하면서 머지 커밋(Merge commit)을 남긴다. 12$ git checkout master$ git merge feature 일반적인 머지는 이미 많은 분들이 알고 있을테니 깊게 설명하지 않고 빠르게 넘어가겠다. Merge squash 이번에는 두 개의 브랜치를 병합할 때 사용하는 머지 명령어의 --squash 옵션을 한번 알아보자. --squash 옵션은 해당 브랜치의 커밋 전체를 통합한 커밋을 타겟 브랜치에 머지하는 옵션이다. 12$ git checkout master$ git merge --squash feature 일반 머지는 머지가 되는 대상 브랜치의 모든 커밋이 남아있는 상태에서 타겟 브랜치로 합쳐지지만 머지 스쿼시는 대상 브랜치의 모든 커밋을 모아서 하나의 커밋으로 합치고 타겟 브랜치에 머지하는 방식이다. 사실 이 기능의 정확한 이름은 Merge \"and\" Squash이다. 즉, 스쿼시도 머지와 같이 독립된 하나의 개념이라는 것이다. 스쿼시는 커밋을 여러 개 합친다는 개념이기 때문에 하단에 후술할 rebase 명령어와 함께 사용하여 현재 브랜치의 커밋을 합칠 때도 사용한다. 1$ git rebase -i HEAD~~ 위 명령어는 HEAD부터 HEAD의 ~~(전전) 커밋까지의 히스토리를 변경하겠다는 의미이다. 이 명령어를 입력하면 vim이 실행되고 아래와 같은 내용이 표시된다. 12345678910111213141516pick 9a54fd4 commit의 설명 추가pick 0d4a808 pull의 설명을 추가# Rebase 326fc9f..0d4a808 onto d286baa## Commands:# p, pick = use commit# r, reword = use commit, but edit the commit message# e, edit = use commit, but stop for amending# s, squash = use commit, but meld into previous commit# f, fixup = like \"squash\", but discard this commit's log message# x, exec = run command (the rest of the line) using shell## If you remove a line here THAT COMMIT WILL BE LOST.# However, if you remove everything, the rebase will be aborted.# 위의 텍스트에 표시된 커밋들의 맨 앞에 있는 pick 문자를 s또는 squash로 변경하면 두 개의 커밋이 합쳐진다. Rebase 리베이스(Rebase)도 머지(Merge)와 마찬가지로 브랜치를 다른 브랜치로 합칠 수 있는 기능이다. 단 머지와 차이가 있다면 바로 합치는 방식이다. 머지는 말 그대로 두 개의 브랜치를 하나로 합치는 기능이기 때문에 A 브랜치의 변경 사항 전부를 B 브랜치에 푸쉬하는 것과 동일하다. 그렇기 때문에 머지를 사용하여 브랜치를 합치게 되면 반드시 머지 커밋(Merge commit)이 남게 된다. 12$ git checkout feature$ git merge master [출처] Merging vs. Rebasing 그렇기 때문에 머지는 어느 시점에 어떤 브랜치가 머지 되었는 지 커밋을 통해 알기 쉽다는 장점이 있다. 그러나 단점은 불필요한 커밋이 생성된다는 것이다. 이 단점은 작업 중인 브랜치가 별로 많지 않을 때는 나타나지 않지만 브랜치가 많아지면 나중엔 커밋 로그가 머지 커밋으로 뒤덮혀있는 광경을 볼 수도 있게 된다. 반면 리베이스는 단순히 합치는 것이 아니라 말 그대로 브랜치의 베이스를 변경하는 것이다. 방금 전 예시의 feature 브랜치를 master로 리베이스하게 되면 마치 feature 브랜치의 변경 사항들이 master의 변경 사항이었던 것처럼 히스토리가 기록된다. 12$ git checkout feature$ git rebase master [출처] Merging vs. Rebasing 리베이스의 장점은 바로 깔끔한 커밋 히스토리를 만들어 준다는 것이다. 머지 커밋이 남지 않고 애초에 master에서 수정한 것 마냥 히스토리가 남기 때문에 깔끔하게 일자로 쭉 떨어지는 이쁜 히스토리를 볼 수 있다. 하지만 리베이스의 단점은 바로 이 커밋 끼워넣기 때문에 발생하는 문제이다. [출처] Merging vs. Rebasing 필자가 만약 feature 브랜치를 master로 리베이스했다고 가정해보자. 이때 필자가 feature 브랜치를 생성한 이후에 master에 반영된 커밋들은 모두 맨 끝으로 이동하고 중간에 feature 브랜치의 커밋들을 끼워넣게 된다. 즉, 필자가 보고 있는 master의 상태는 feature의 변경 사항들이 반영되어 있는 히스토리를 가지고 있지만 다른 사람의 master는 아직 예전 master의 히스토리와 함께 일하고 있다는 것이다. 그럼 두개의 master를 강제로 병합해줘야하는데 병합 자체는 푸쉬할때 --force 옵션을 주면 되지만 문제는 이게 굉장히 혼란스러운 상황이라는 것이다. 쉽게 말해서 커밋 히스토리가 꼬이게 되고 사무실의 여기저기서 어? 이거 왜 이래? 왜 푸시 안돼?라는 소리가 들려오기 딱 좋은 상황이다. 그래서 master로의 병합은 머지 스쿼시를 사용하고 develop으로의 병합 때 리베이스를 사용하거나 하는 경우도 있다. 머지는 머지 커밋을 발생시키며 히스토리가 미래로 나아가기 때문에 이런 문제가 발생할 확률이 적지만, 리베이스는 과거를 변경하는 것이기 때문에 문제가 생기기 쉬운 것이다. 뭐 여러모로 둘 다 장단점이 있으니 잘 골라서 사용하도록 하자. Cherry Pick체리픽(Cherry Pick)은 다른 브랜치에서 어떤 하나의 커밋만 내 브랜치로 가져오는 기능이다. 체리픽이 하는 일을 보면 대상 브랜치의 커밋 하나를 가져와서 현재 브랜치에 병합하는 행위라고 느껴지지만 히스토리를 보면 병합되는 그림이 아니라 그냥 해당 커밋을 그대로 복사해와서 내 브랜치에 커밋되는 형태로 기록된다. 12$ git checkout master$ git cherry-pick 35058b4 # 가져올 커밋 해쉬 물론 체리픽을 사용할 때도 현재 브랜치의 소스와 충돌이 날 가능성은 있기 때문에 가져오기 전에 충돌을 수정할 수도 있다는 마음의 준비는 필요하다. 체리픽은 잘 쓰면 은근히 꿀 기능인데, 바로 이런 상황 때문이다. A 브랜치에서 철수가 기능 개발 중 B 브랜치에서 영희가 기능 개발 중 디자이너가 영희에게 리뷰 별점 아이콘과 디자인을 변경해달라고 요청 영희가 B 브랜치에서 디자이너의 요구 사항을 반영 근데 B 브랜치보다 A 브랜치가 먼저 배포되야 함 디자이너가 철수에게 A 브랜치에 왜 리뷰 별점 디자인 반영안됐냐고 물어봄 제일 좋은 상황은 철수가 디자이너의 요구 사항을 반영하는 것이겠지만, 막 정신없이 일을 하다보면 그렇게 술술 풀리는 경우만 있는 게 아니기 때문에 이런 문제가 발생하긴 한다. 이때 철수는 영희가 작업하고 있는 B 브랜치에서 리뷰 별점 아이콘이 수정된 커밋을 A 브랜치로 체리픽함으로써 이 상황을 쉽게 해결할 수 있게 된다. 이런 사람 애매해지게 만드는 상황은 생각보다 자주 발생하기 때문에 체리픽에 익숙해지는 것을 추천한다. 사실 이 상황은 필자가 얼마 전에 겪은 상황인데 철수가 필자이고 영희가 동료 프론트엔드 개발자였다. 그래서 동료 개발자분과 B 브랜치의 커밋 로그를 봤는데, 리뷰 별점 아이콘만 수정된 커밋이 아니라 다른 변경 사항도 함께 묻어있는 커밋 밖에 없어서 디자이너와 딜을 할 수 밖에 없었고, 그럼 B 브랜치가 배포될 때 한꺼번에 같이 반영해주세요~라는 결론으로 무사히 넘어갈 수 있었다.(디자이너님 감사감사…) 작업하던 사항을 임시로 저장해보자Stash스태쉬(Stash)는 현재 작업 중인 변경 사항들을 잠시 스택에 저장할 수 있는 명령어이다. 이 명령어는 아직 마무리되지 않은 작업이 있는데 다른 브랜치로 체크아웃 해야하는 경우에 유용하게 사용할 수 있다. 123$ git stash # 현재 변경 사항들을 스택에 저장$ git stash list # 스태쉬 목록을 확인$ git stash apply # 가장 최근의 스태쉬를 다시 불러온다 또는 직접 스태쉬 이름을 지정할 수도 있다. 스태쉬의 이름을 지정하지 않으면 스택에 들어간 순서(First In Last out)대로만 스태쉬를 가져올 수 있으므로 왠만하면 이름을 지정하는 것을 추천한다. 필자는 주로 스태쉬 이름을 브랜치 이름과 동일하게 지정하는 편이다. 12$ git stash branch-name # 스태쉬 이름을 branch-name으로 지정하고 스택에 저장$ git stash apply branch-name # branch-name 이름을 가진 스태쉬를 불러온다 실제로 회사에서 개발을 하다보면 갑자기 긴급한 버그 픽스 건이 들어온다거나 아니면 PO들이 이슈의 우선 순위를 다시 정리하면서 기존에 작업을 하고 있던 브랜치에서 다른 브랜치로 건너가야하는 경우는 꽤나 빈번하게 발생한다. (특히 버그 픽스…) 이때 다른 브랜치로 넘어가기위해 작업하던 것을 그대로 커밋하게 되면 해당 브랜치에서 함께 개발하고 있는 다른 팀원들에게 피해가 갈 수 있으니 반드시 변경 사항을 스태쉬하도록 하자. 이미 커밋한 내용 되돌리기개발을 진행하다보면 가끔 커밋을 다시 되돌려야 하는 경우도 생긴다. 보통 실수로 인해서 이런 상황이 발생하는 것을 많이 봤는데, 배포되지 말아야 할 기능이 release 브랜치에 껴서 들어간 경우를 제일 많이 본 것같다. 이런 상황에서 그 기능의 코드를 일일히 찾아 손으로 지우는 것은 너무 위험하기 때문에 Git을 사용하여 커밋을 되돌리게 된다. 이때 사용하는 기능이 바로 Reset과 Revert이다. Reset리셋(Reset)은 지정한 커밋 당시로 돌아가는 것이다. 아예 시간을 되돌린다고 생각하면 된다. 즉, 리셋을 사용하게되면 지정한 커밋 이후의 히스토리는 모두 사라지게 된다. 예를 들어 이런 히스토리가 있다고 생각해보자. 123456789* 19061e7 - 맛없는 식당을 찾은 죄로 여자친구한테 이별 통보를 받았다.|* e50aff9 - 여자친구가 맛이 없다고 한다.|* 2d57c29 - 알리오 올리오를 주문했다.|* c04f8f6 - 찾아본 식당에 방문했다.|* 7d9d953 - 여자친구와 함께 갈 좋은 식당을 찾았다! 필자는 여친과 함께 방문할 좋은 식당을 찾아서 기대감을 안고 알리오 올리오를 주문했지만 너무 느끼하고 맛이 없어서 결과적으로 여친한테 차이고 말았다.(실제 아니고 가상입니다) 그래서 필자는 너무 슬픈 나머지 기억을 지우고 싶어서 알리오 올리오를 주문하기 전으로 돌아가려고 한다. 이때 사용할 수 있는 명령어가 reset이다. 돌아가고 싶은 커밋을 지정하면 해당 커밋 이후의 히스토리는 모두 삭제하고 과거로 돌아갈 수 있다. 123456$ git reset --hard c04f8f6# 식당을 방문했을 때로 돌아갔다!* c04f8f6 - 찾아본 식당에 방문했다.|* 7d9d953 - 여자친구와 함께 갈 좋은 식당을 찾았다! 우리는 reset 명령어를 사용할 때 3개의 옵션을 사용할 수 있는데, 바로 hard, soft, mixed이다. 이 옵션들은 히스토리를 삭제한다는 것은 전부 동일하지만 삭제된 내용을 처리하는 방식이 조금씩 다르다. hard: 지정한 커밋 이후의 히스토리가 삭제되고 삭제된 내용들은 그대로 사라진다.soft: 지정한 커밋 이후의 히스토리가 삭제되고 삭제된 내용들은 스테이지로 이동한다.(add한 상태로 변경)mixed: 지정한 커밋 이후의 히스토리가 삭제되고 삭제된 내용들은 스테이지에 올라가지 않은 상태가 된다.(다시 add 해줘야 함) 필자는 방금 위에서 hard 옵션을 사용했기 때문에 식당을 방문했던 커밋 이후의 잊고 싶었던 기억을 모두 깔끔하게 삭제할 수 있었다. 만약 옵션을 지정하지 않고 reset 명령어를 사용하면 mixed 옵션으로 작동한다. 그리고 만약 이미 되돌리고자 하는 히스토리가 리모트 저장소에 푸쉬까지 된 상태라면 리셋 후 히스토리를 푸쉬할 때 --force 옵션을 사용해야한다. Revert리벗(revert) 또한 리셋처럼 히스토리를 다시 되돌리고 싶을 때 사용하는 명령어이다. 리셋이 지정한 커밋 이후의 모든 히스토리를 없애버렸다면 리벗은 특정 커밋의 변경 사항을 되돌리는 기능이다. 이때 해당 커밋을 되돌린다고 해서 히스토리에서 그 커밋을 삭제하는 것이 아니라, 되돌리고자 하는 커밋의 내용을 반전시키는 것이다. 123$ git revert 35058b4 # 특정 커밋을 되돌린다$ git revert 35058b4..c04f8f6 # 커밋의 범위를 지정하여 되돌린다$ git revert HEAD # 현재 헤드가 위치한 커밋을 되돌린다 만약 35058b4 커밋에서 A.js의 2번 라인에 a라는 글자가 추가되었다고 하면 git revert 35058b4를 사용했을때 A.js의 2번 라인에서 a를 다시 삭제하는 것이다. 즉, 추가된 사항은 제거하고 제거된 사항은 다시 추가한다. 말 그대로 지정한 커밋의 변경 사항을 반전하고 다시 커밋하는 것이다. 그렇기 때문에 리벗은 리셋과 다르게 히스토리를 삭제하지 않고 하나의 커밋이 추가되는 형태로 히스토리가 남는다. 1234567891011* 35058b4 - Revert 맛없는 식당을 찾은 죄로 여자친구한테 이별 통보를 받았다. # 여친한테 차인 히스토리만 리벗하자|* 19061e7 - 맛없는 식당을 찾은 죄로 여자친구한테 이별 통보를 받았다.|* e50aff9 - 여자친구가 맛이 없다고 한다.|* 2d57c29 - 알리오 올리오를 주문했다.|* c04f8f6 - 찾아본 식당에 방문했다.|* 7d9d953 - 여자친구와 함께 갈 좋은 식당을 찾았다! 위의 예시에서 필자는 여친한테 차인 커밋을 다시 리벗했지만 히스토리 상에는 필자의 흑역사가 고스란히 남아있다.(다시 말하지만 실제 상황 아닙니다) 보통 필자는 리벗을 자주 사용하지는 않지만 가끔 테스트용으로 넣었던 console.log가 껴서 들어가거나 할 때 해당 커밋을 리벗 해본 적은 있다. 리셋과 리벗 둘 다 변경 사항을 되돌리는 기능이지만, 되돌리는 방법은 완전 다르니 적재적소에 잘 사용해보도록 하자. 마치며지금 Git에 대한 포스팅을 두 편 연속으로 작성했는데도 아직 Git에 대해서 전부 설명하지 못했다. 그 만큼 Git은 정말 다양한 기능들로 사용자가 효과적으로 버전 관리를 할 수 있도록 도와주는 도구라고 할 수 있다. 위에서 한번 언급했듯이 어플리케이션의 버전을 관리한다는 특성 때문에 Git이 어떻게 작동하는 지, 내가 이 기능을 사용하면 버전이 어떻게 되는 지 알지 못한다면 사실 두려울 수 밖에 없다. 내가 손가락 하나 잘못 놀려서 다른 사람들이 작성한 코드가 날아가면 어떡하지...?라는 생각은 필자도 해봤고 지금 Git을 겁나 잘쓰시는 많은 개발자 분들도 한번씩은 다 해본 생각일 것이다.(사실 맘 먹고 리셋하고 강제로 푸쉬하지않는 이상 그럴 일이 별로 없다) 하지만 Git에서 말하는 개념이나 기능의 이름들이 생소해서 그렇지 알고보면 Git의 기능들이 작동하는 방법 자체는 그렇게 복잡하지 않다. 그리고 이 복잡성은 제대로 관리되지 않았던 버전 히스토리 그래프도 한 몫 한다고 생각한다. 솔직히 이리저리 꼬여있는 그래프를 보면 오 이거 해볼만 한데?라는 생각은 별로 안드는 것 같다. 포스팅을 작성하면서 최대한 쉽게 써보려고 했는데 독자분들이 이해가 잘 가셨는 지 모르겠다. 사실 Git은 필자 또래의 개발자 분들보다 나이가 조금 있으신 선배들이 더 어려워하시는 것 같다. 지금까지 SVN을 주로 사용해서 버전을 관리하다가 갑자기 처음보는 개념들이 우수수 떨어지는 Git이 대세라고 하니 공부하기도 쉽지 않을 것 같다고 하시는 선배들의 이야기를 몇 번 들어본 적 있다. 사실 모든 프로그래밍이 그렇듯, Git도 글로 읽는 것보다는 직접 몇번 해본 다음에 히스토리 그래프가 어떻게 변했는 지도 보고 소스가 어떻게 변했는 지 보는 게 제일 이해가 잘 된다. Github처럼 무료 저장소를 제공해주는 서비스도 많으니, 연습용 레파지토리를 하나 만든 다음 그 안에서 간단한 코드나 텍스트 파일을 변경해보면서 직접 연습해보시는 걸 추천한다. 이상으로 Git 뉴비를 위한 기초 사용법 - 버전 관리 포스팅을 마친다.","link":"/2019/07/28/git-tutorial-advanced/"},{"title":"비전공 개발자가 전공자보다 정말 불리할까?","text":"이번 포스팅에서는 많은 분들이 질문해주신 컴퓨터 공학을 전공하지 않은 개발자가 과연 전공자에 비해 불리한가에 대해서 한번 이야기해보려고 한다. 어떻게 보면 예민한 주제일수도 있지만 주변에 이와 같은 질문을 주시는 분들도 꽤 있는데다가, 심지어 컴퓨터 공학을 전공하지 않았다는 이유로 자기 자신을 낮게 평가하시는 분도 계셨다. 필자는 이런 것들이 어떤 특정 개인에게 국한된 것이 아니라고 생각되어 이에 대한 필자의 생각을 조심스럽게 한번 적어보려고 한다. 이 포스팅은 비전공 개발자가 많은 분야인 백엔드, 프론트엔드, 모바일 앱 개발자 등에 대해서 이야기할 예정이다. 어플리케이션 레이어보다 더 로우한 레이어에서 개발을 하는 분야는 애초에 비전공자가 들어가기 힘든 분야이기 때문이다. 필자는 지난 1년 간 꽤 많은 사람의 면접을 진행했었는데, 그 중에는 컴퓨터 공학 전공자도 있었고 IT와 전혀 관련없는 전공을 가지신 분도 있었으며, 대학교를 갓 졸업한 신입부터 경력이 10년 넘은 시니어 개발자까지 다양한 사람들이 있었다. 일단 결론부터 말하자면 경력은 경우에 따라 고려되는 경우가 있겠지만, 사실 전공 자체는 컴퓨터 공학 전공이든 파리 뒷다리 분해학 전공이든 그게 그렇게 중요한 건 아니라고 생각한다. 다만 컴퓨터 공학을 전공한 사람이 전공하지 않은 사람에 비해서 여러모로 유리한 점이 있다는 것 또한 사실이기에, 컴퓨터 공학 전공이라는 타이틀 자체가 어떤 이점을 가지고 있는 지는 한번쯤 생각해봄직 하다. 또한 본인이 컴퓨터 관련 학과를 졸업하지 않았다면, 막연한 두려움을 가지기 보다는 내가 전공자와 어떤 차이가 있는지를 명확하게 인지하고 그에 따른 공부를 하면 그만이다. 대학교에서 가르치는 과목이라고 해서 독학하지 못할 것도 없기 때문이다. 그리고 컴퓨터 공학을 전공 했다고 해서 전공자들이 무슨 넘사벽 계층도 아니고, 공부하면 충분히 앞지를 수 있다. 참고로 필자는 인서울 4년제 대학교를 나오지도 않았으며, 컴퓨터 공학 전공도 아니다. 필자는 학점은행제인 동국대학교 전산원에서 멀티미디어 공학을 전공했지만 전공 선택 과목으로 대부분 컴퓨터 공학과의 과목을 선택해서 수강했기 때문에 전공과 비전공의 애매한 경계에 걸쳐있는 혼종이라고 할 수 있다.(원래 멀티미디어과 커리큘럼 자체가 잡캐 양성소다) 그래서 필자는 혼종의 입장에서 컴퓨터 공학을 전공하신 분들의 고민도 들어보고 비전공이신 분들의 고민도 들어볼 수 있었는데, 이에 대한 몇가지 생각을 이번 포스팅에서 한번 적어보려고 한다. 회사에서는 전공보다 실력을 본다사실 모두 알다시피 개발자와 같은 전문기술직은 학력보다는 실력이 최우선으로 우대된다. 게다가 개발자들의 실력은 천차만별이라 좋은 대학교의 컴퓨터 공학과를 졸업한 10년차 개발자라고 해도 프로그래밍 실력은 형편 없을 수도 있고, 반대로 대학을 안가고 고등학교만 졸업한 신입 개발자라고 해도 엄청난 실력을 가진 개발자일 수도 있는 게 이 쪽 세계의 특징이다. 하지만 많은 사람들이 대학을 가라고 하는 데는 다 이유가 있다. 대학을 가서 더 깊은 공부를 해라 뭐 이런 이유도 있겠지만 사실은 좀 더 현실적인 이유 때문에 추천을 하는 것이다. 이 이유를 알기 위해서 우리는 입사 지원자가 아니라 채용 담당자의 입장에서 생각해볼 필요가 있다. 엄마가 대학을 가라고 했던 이유사실 필자도 대한민국에서 살면서 그 놈의 지긋지긋한 인서울 대학에 대한 집착을 많이 겪어본 터라, 학력주의사회에 대한 어느 정도 염증을 가지고 있는 사람이다. 하지만 사람들이 학력이나 전공이 중요하다고 말할 수 밖에 없는 이유는 따로 있다. 바로 채용 담당자가 나의 이력서만 보고 얻을 수 있는 정보는 굉장히 단편적이기 때문이다. 이력서에 적혀있는 정보는 단편적이고 진위 여부도 알기 힘들다 자, 우리가 새로운 개발자를 뽑는 채용 업무를 맡았다고 생각해보자. 참고로 작은 기업에서는 이 업무를 해당 업무를 하는 실무자에게 맡기기도 하기 때문에 여러분의 경력과 상관없이 상황에 따라서 언제든지 맡을 수 있는 업무이다. 우리는 이제 채용 담당자로써 하루에도 수십 통 또는 수백 통 씩 쏟아지는 입사지원자들의 이력서를 살펴보고 이 중에서 2차 기술 면접을 진행할 사람을 필터링 해야한다. 이력서에 적힌 정보는 여러 가지가 있겠지만 지원자의 경력과 기술 스펙 정도가 제일 핵심 정보가 될 것이다.(성별과 얼굴을 보시는 분은 없을거라 믿는다) 어쨌든 입사 지원자의 이력서를 보고 우리가 알고 싶은 것은 이 사람이 회사가 요구하는 최소한의 실력을 만족하는 사람인지에 대한 것이다. 조금 더 냉정하게 말하자면, 면접관으로 들어갈 팀원들의 리소스를 투자하면서 이 지원자를 더 자세히 알아보고 싶은 가치가 있는 것인지 고민하는 것이다. 그리고 이 단계에서 이력서라는 몇 장의 종이 쪼가리만 보고 이 사람의 가치관이나 성격 같은 것을 알기는 힘들기 때문에 일반적으로 기술 스펙을 중점으로 확인하게된다. 지원자가 시니어일 경우는 경력을 보고 이 분이 지금까지 어떤 길을 걸어왔는지 대략 유추해볼 수 있지만, 신입이나 주니어의 경우는 경력이라고 할게 거의 없기 때문에 경력에서는 실력에 대한 정보를 거의 얻을 수 없는 경우가 많다.(물론 신입이라도 가끔 눈에 확 띄는 굇수 분들이 있긴 하다) 그렇다면 다음으로 볼 수 있는 것이 이력서에 첨부된 블로그나 깃허브인데, 이 마저도 없는 경우에는 무엇을 보고 이 사람의 실력을 유추해볼 수 있을까? 이런 상황에서는 사실 학교와 전공을 볼 수 밖에 없다. 이 지원자의 실력을 판가름할 수 있는 정보가 더 이상 없기 때문이다. 물론 이후 기술 면접을 진행하면 좀 더 상세히 알 수 있겠지만, 기술 면접 자체도 기존 팀원들의 리소스를 어느 정도 잡아먹는 일이기 때문에 무작정 모든 지원자의 기술 면접을 진행할 수도 없는 노릇이다. 그래서 사람들이 학력 또는 전공도 나름 중요하다고 하는 것이다. 즉, 내가 아무것도 가지고 있지 않은 상황에서 최소한의 보험이라고 할 수 있다. 내 실력을 증명하는 것이 중요하다우리가 방금 위에서 예시로 보았던 상황을 다시 되짚어보면 회사에게 중요한 것은 이 사람이 회사에 입사해서 일을 할 수 있는 최소한의 실력이 되는가이지, 학교나 전공 자체가 아니다. 고졸이든 인서울대학 컴공을 졸업했든 부트캠프를 수료했든 실력이 좋은 개발자는 어디서든 모셔가기 마련이다. 자, 만약 다음과 같은 두 사람이 있다면 회사는 어떤 사람을 선택할까? SKY 컴퓨터 공학과를 졸업했지만 깃허브도 없고 어떤 프로젝트를 진행했었는지도 알 수가 없다. 고졸이지만 오픈 소스인 유명 프레임워크의 핵심 컨트리뷰터이다. 조금 극단적인 예시긴 하지만 아무래도 2번 개발자가 선택될 확률이 더 높을 것이다. 결과적으로 회사가 원하는 개발자는 일을 잘하는 개발자이지 단순히 좋은 학교의 컴퓨터 공학과를 졸업한 개발자는 아니기 때문이다. 당연히 학벌 이외에 실력을 검증할만한 요소가 없는 1번 개발자보다는 오픈 소스에 컨트리뷰팅을 함으로써 실력이 어느 정도 검증된 2번 개발자를 선택하는 것이 회사 입장에서도 리스크가 적다. 그러나 만약 2번 개발자의 실력을 유추할 수 있는 무언가가 없었다면 그때는 1번 개발자가 선택될 확률이 높다. 즉, 내 실력을 증명할 수만 있다면 학력이나 전공은 뒤집을 수 있다는 것인데, 이때 회사가 원하는 실력이 어떤 것인지 파악하는 것이 중요하다. 어떤 회사는 특정 프레임워크에 해박한 사람을 원할 수도 있고, 어떤 회사는 프레임워크는 그냥 와서 공부하면 되지만 탄탄한 기초 지식이 있어야한다고 하는 등 회사마다 또는 그 회사가 처한 상황마다 원하는 인재의 기술 스펙은 달라질 수 있기 때문이다. 결과적으로 우리는 전공 여부나 학벌과 관계없이 이 스펙만 맞춰주면 되는 것이다. 근데 회사에서 원하는 기술 스펙과 실력만 맞춰도 취업에 지장이 없다면, 컴퓨터 공학 전공자가 비전공자에 비해서 유리한 점은 없는걸까? 아니, 솔직히 말해서 그렇지는 않다. 실력만 있다면 전공 여부는 중요하지않다라는 말이 비전공자가 더 유리하다라는 말은 아니기 때문이다. 전공자가 비전공자에 비해 유리한 점컴퓨터 공학을 전공한 사람은 어쨌든 간에 4년 정도 전문적인 커리큘럼을 통해 컴퓨터에 대한 공부를 꾸준하게 해온 사람이다. 그래도 4년 동안 교육을 받은 사람들인데 비전공자와 아무런 차이가 없다면 거짓말일 것이다. 하지만 전공자와 비전공자 간의 차이는 프로그래밍 실력 자체는 아니다. 요즘에는 짧은 기간 안에 부트캠프를 수료하신 분들도 최신 트렌드 기술을 사용하여 어플리케이션을 훌륭하게 만들어 내시는 분들이 많기 때문이다. 그렇다면 회사가 컴퓨터 공학을 전공한 사람에게 기대하는 것은 무엇일까? 지피지기면 백전백승이다참고로 필자가 지금 이 이야기를 하는 이유는 전공자가 이런 면에서 더 유리하기 때문에 이 부분은 그냥 포기하라는 의미로 하는 말이 아니다. 포스팅의 서두에서 한번 이야기 했지만, 내가 비전공 개발자라면 전공자들에 비해서 어떤 점이 부족한지 알고 있어야 그 점을 보완할 수 있다. 그래야 공부의 방향도 잡을 수 있고 나만의 강점을 만들 수도 있기 때문에 우선 필자가 생각했을 때 컴퓨터 공학 전공자가 가지는 장점에 대해서 한번 이야기 해보려고 한다. 컴퓨터 공학과는 코딩을 배우는 곳이 아니다솔직히 말해서 둘 다 신입인 경우에는 전공이든 비전공이든 프로그래밍을 하는 실력 자체에는 큰 차이가 없다고 생각한다. 물론 이 기준은 일반적으로 어플리케이션 레이어에서 비즈니스 어플리케이션을 개발하는 개발자를 이야기하는 것이다. 애초에 이보다 더 로우한 레이어의 분야는 비전공자가 들어갈 틈 자체가 넓지 않다. 여기서 많은 분들이 오해하시는 부분이 있는데, 컴퓨터 공학을 전공했으면 당연히 프로그래밍도 비전공자보다 잘할 것이라고 생각하는 것이다. 하지만 컴퓨터 공학도가 학교에서 배우는 것은 함수형 프로그래밍이나 Git을 사용한 버전 관리나 Flux 패턴 같은 것이 아니다. 어디까지나 컴퓨터 공학도가 궁극적으로 추구하는 것은 프로그래밍보다 더 근본적인 컴퓨터 시스템의 원리와 구조적인 사고를 하는 방법이다. 가령 OS 과목을 수강하게 되면 배우는 것 중에서 프로세스 스케줄링(Process Scheduling)라는 것이 있다. 프로세스 스케줄링은 단일 프로세서 시스템에서 여러 개의 프로세스를 어떤 방식으로 실행해야 효율적으로 프로세스를 처리할 수 있을 지에 대한 이론과 알고리즘을 의미한다. 이때 SJF, FCFS, RR 등 여러가지 알고리즘을 배우게 되는데, 사실 왠만큼 로우 레벨로 내려가지 않는 이상 일반적인 어플리케이션을 만들 때 이런 개념을 활용해야 경우는 드물다. 필자도 위 개념을 학교에서 배웠기 때문에 알고는 있었지만 실제로 사용해본 적은 한번도 없고, 대신 Node.js의 이벤트 루프를 이해할 때 약간의 도움은 되었던 것 같다. 컴퓨터 공학과에서 배우는 학문은 이런 느낌이다. 즉, 컴퓨터 공학과를 졸업했다고 해서 바로 비즈니스 어플리케이션을 잘 만드는 것은 약간 거리가 있다는 것이다. 비즈니스 어플리케이션을 잘 만드려면 MVC, MVVM, Flux와 같은 상태 관리 방법이나, 디자인 패턴, 적절한 개발 방법론 등 여러가지 요소가 필요한데 이런 건 학교에서 따로 알려주지 않는다. 심지어 면접보러가서 Git이라는 걸 처음 알았다는 사람도 있었다. 그래서 컴퓨터 공학을 전공한 사람들도 졸업 후 다시 학원에 들어가서 배우는 경우도 왕왕 있다. 컴퓨터 공학을 전공한 사람이 비전공인 사람보다 프로그래밍 실력 자체가 그렇게 월등하게 높지 않다면, 전공자가 비전공자에 비해 유리한 점은 도대체 무엇일까? 기초 지식이 탄탄할 확률이 높다바로 컴퓨터에 대한 기초 지식을 보유한 사람의 비중이 높다는 것이다. 이때 필자가 말하는 기초 지식은 자료구조, 알고리즘, 네트워크, 수학, 암호학 등 컴퓨터 관련 기술의 근본을 이루는 지식을 말하는 것이다. 사실 이런 지식이 중요하다고 하는 사람도 있고 아닌 사람도 있지만, 필자는 초반에는 몰라도 경력이 쌓이면 쌓일 수록 이런 지식들이 빛을 발한다고 생각한다. 사실 요즘에는 워낙 좋은 프레임워크들도 많은데다가 대부분의 기술들이 사용하기 쉽게 추상화되어 있기 때문에 기초 지식을 모르더라도 프로그램을 만드는 데는 큰 지장이 없다. 하지만 이런 최신 트렌드는 2~3년 만에 패러다임이 완전히 바뀌기도 하는 등 굉장히 빨리 변화하기 때문에 이렇게 쏟아져 나오는 기술을 빠르게 따라갈 수 있는 개발자가 시장에서 양질의 경쟁력을 가질 수 밖에 없다. 이런 새로운 기술들은 얼핏 보면 굉장히 혁신적이고 새로운 개념인 것 같지만, 대부분의 경우 기존에 존재하던 개념을 기반으로 발전한 것들이 많다. 그렇다는 얘기는 결국 기반 기술 자체는 크게 변하지 않는다는 것이다. 예를 들어 HTTP/1.1에서 HTTP/2로 바뀐 것은 얼핏 보면 많은 기능이 추가되었기 때문에 기존과 전혀 다른 새로운 무언가라고 생각할 수 있지만, 결국은 기존의 HTTP 프로토콜에서 이미 사용하고 있던 TCP 커넥션을 좀 더 효율적으로 사용할 수 있게 바뀐 것이다. 이때 기반 기술에 대한 키워드는 TCP가 될 것이다. Docker 또한 기존에 이미 존재하던 VM 등이 사용하던 가상화 기법이 너무 무거우니까 가상머신마다 각각 Guest OS 레이어를 올리는 것이 아니라 어플리케이션 구동에 필요한 부분만 떼어내서 공통으로 사용할 레이어로 올리고 그 위에 프로세스 레이어만 분리하자는 개념에서 출발한 것이다. 이때 기반 기술에 대한 키워드는 가상화, 레이어에 대한 개념 정도가 될 것이다. 기존의 가상화 기법과 Docker의 차이는 결국은 이 레이어를 어떻게 쌓느냐이다 즉, 이렇게 완전히 새로운 패러다임인 것 같은 기술도 그 밑바닥에는 기존의 기반 기술들이 자리잡고있다. 그렇기 때문에 이런 기반 지식들을 어느 정도 알고 있는 사람은 기반 지식을 모르는 사람보다 새로운 기술이 나왔을 때 학습 곡선을 상대적으로 완만하게 그려나갈 수 있다는 것이다. 그러나 이러한 기반 지식이 없다면 새로운 기술이 나올 때마다 해당 프레임워크의 사용법만을 공부하거나, 다른 개발자들이 이미 정리해놓은 정보들을 통해 학습할 수 밖에 없다. 이런 학습 방식은 이해라기 보다는 암기에 가깝기 때문에 몇 달 뒤에 비슷한 기술이 또 출시되면 같은 루프를 반복하며 학습해야한다. 즉, 컴퓨터 관련 학과를 졸업한 사람은 4년 동안 학교 생활을 어떻게 했던 간에 이런 기초 지식을 학교에서 대부분 배우긴 했다는 것이고, 대체로 이런 기반 지식을 활용해 새로운 기술에 높은 적응력을 보일 가능성이 높다. 이런 이유로 많은 개발자들이 이런 적응력과 잠재력을 컴퓨터 관련 학과를 졸업한 신입이나 주니어에게 기대하게 되는 것이다. 내 실력을 키우고 어필하자4년 동안 학교에서 구르면서 기초 지식을 좋든 싫든 강제로라도 배워서 나온 전공자에 비해서 비전공자는 학원이나 부트캠프를 통해 짧은 기간동안만 프로그래밍을 배운 경우가 많기 때문에 전공자에 비해 상대적으로 기초에 대한 지식이 적을 수 밖에 없다. 이는 학원이라는 교육 기관의 특성 상 짧은 기간 안에 수강생을 프로그래밍이 가능한 인재로 만들어 내기 위해 발생한 어쩔 수 없는 결과이다. 학원의 경우 대부분 6개월에서 9개월 정도의 학습 기간을 가지는데, 사실 이렇게 짧은 기간 안에 컴퓨터에 대한 기초 이론부터 가르쳐서 취업을 시킨다는 건 거의 불가능한 미션이기 때문이다. 물론 부트캠프를 수료하신 몇몇 분들의 이야기를 들어보면 비록 짧은 기간이지만 그 기간 내내 오전 8시부터 밤 10시까지 프로젝트를 진행하는 소름돋는 스케줄을 소화해낸다고 한다.(다들 열정이 대단하시다) 하지만 아무래도 전공자와 비교 했을 때 절대적인 학습 시간의 차이는 있을 수 밖에 없고, 학원에서 가르치는 것은 코딩 또는 특정 프레임워크의 사용법이지 컴퓨터의 기초 개념이 아니기 때문에 연차가 쌓일 수록 이런 기초 지식의 부재로 인해 새로운 지식을 학습하는 것에 부담을 느끼고 경쟁에서 밀리는 경우가 왕왕 보인다. 그러나 실제로 취업하고 있는 수많은 비전공 출신 개발자 분들을 보면 컴퓨터 공학을 전공하지 않은 개발자들이 시장에서 경쟁력을 가지고 있다는 것 또한 분명히 사실이다. 위에서 이야기했듯이 개발자라는 직업은 결국 실력이 깡패기 때문에 내 실력을 얼마나 잘 어필할 수 있냐가 중요한 포인트다. 필자도 일단 이력서에는 멀티미디어 공학이라고 찍혀있기 때문에, 면접을 볼 때 비록 컴퓨터 공학을 전공하지는 않았지만 필자만의 장점이 있다는 점을 어필하고는 한다. 블로그와 깃허브는 중요하다블로그는 채용 담당자 입장에서 굉장히 도움이 많이 되는 정보이다. 이 사람이 평소에 생각하는 것과 관심있는 기술, 공부하는 방법, 문제를 해결했던 경험 등을 정제된 형태의 데이터로 한 눈에 볼 수 있는 창구이기 때문이다. 비전공 개발자는 이력서를 통해 채용 담당자에게 내 실력을 인증할 수 있는 데이터가 전공자에 비해 상대적으로 부족하기 때문에 이렇게 적극적으로 자신의 실력과 프로그래밍에 대한 관심을 어필할 수 있는 수단을 사용하는 것이 좋다. 또한 많은 기업들이 내가 알고 있는 지식을 남에게 공유하는 행위에 대해서 긍정적으로 생각하고 있기 때문에 블로그를 꾸준히 작성하면 분명히 도움이 된다. 도저히 뭘 써야할 지 감이 안온다면 자신이 재밌게 읽었던 외국 포스팅의 번역부터 시작해보자. 필자 같은 경우는 크게 두가지 주제를 가지고 글을 쓰는 편인데, 하나는 지금 이 포스팅처럼 그냥 필자의 생각을 적는 것이고, 다른 하나는 기술 관련 포스팅이다. 특히 기술 관련 글을 작성할 때는 잘못된 지식을 전하지 않으려는 두려움 때문에라도 강제로 엄청난 리서칭을 하게 될 수 밖에 없고, 그 리서치 결과를 글로 정리하는 과정에서 다시 한번 정제되기 때문에 그냥 읽기만 하는 것보다 오히려 더 공부가 되고 기억에도 오래 남는 경향이 있다. 또한 이렇게 자신의 생각을 글로 정리해서 적는 행위 자체는 논리력 향상에 굉장히 많은 도움을 준다고 한다. 말로 하면 어느 정도 논법이 맞지 않아도 들어줄만 하지만 이렇게 글을 작성할 때 논법과 주제가 흐려지기 시작하면 그냥 딱 봐도 굉장히 어색한 글이 되버리기 때문이다. 개발자에게 논리력이 얼마나 중요한 요소인지는 굳이 말하지 않아도 다들 알고있으리라 생각한다. 그리고 깃허브는 두 번 말하면 입 아플 정도로 엄청 중요하다. 하지만 잔디를 꾸준히 심지않는 빈 계정과 다름 없는 상태에서는 별로 임팩트가 없기 때문에 사이드 프로젝트를 하던 오픈 소스에 기여를 하던 꾸준히 커밋을 하는 것을 추천한다. 솔직히 어떤 사람의 깃허브 계정을 봤을 때 진한 초록색 잔디가 가득 심어져 있다면 오 대박 쩌는걸이라는 감탄사가 나올 수 밖에 없다. 필자의 깃허브 잔디. 빡빡하진 않지만 적어도 얘가 뭔가를 하고 있구나 정도는 알 수 있다 게다가 깃허브 레파지토리를 살펴보거나 어떤 프로젝트에 스타를 찍었는지를 보면 최근 어떤 기술에 관심을 가지고 있는 지도 알 수 있다. 하지만 무엇보다 제일 큰 장점은 이 사람이 작성한 코드를 직접 볼 수 있다는 것에 있다. 코딩을 시작한지 얼마 안된 사람일수록 자신의 코드를 공개된 장소에 오픈한다는 것을 부끄럽다고 생각할 수 있지만, 생각보다 남들이 여러분의 코드를 그렇게까지 자세히 분석하고 점수 매기지는 않으니까 그냥 작은 걸 만들더라도 왠만하면 Public으로 깃허브에 올리자. 깃허브 레파지토리를 보면 어떤 기술에 관심이 있는지 대략 알 수 있다 최근에는 코드스테이츠와 같은 부트캠프에서도 수강생들에게 블로그나 깃허브 계정을 생성하는 것을 추천하고 있는 걸로 봐서 취업에 이런 것들이 도움이 된다는 사실을 수강생들에게도 알려주고 있는 것 같다. 나만의 전문성을 확보하자개발자는 현실의 다양한 문제를 컴퓨터를 통해 해결하는 사람들이다. 이때 현실의 문제란 어떤 단일 카테고리의 문제가 아니라 굉장히 다양한 카테고리의 문제라는 것을 잊어서는 안된다. 소프트웨어는 의료, 금융, 예술 등 다양한 분야에서 사용되고 있고, 결국 그 소프트웨어도 개발자가 만드는 것이다. 이때 이런 다양한 분야의 문제를 해결하기 위해서는 컴퓨터 관련 지식 뿐만 아니라 다른 분야의 지식도 함께 요구될 수 밖에 없다. 자신이 만드는 프로그램이 어떤 일을 하는지도 모르면서 좋은 프로그램을 만들 수는 없지 않을까? 비전공 개발자는 자신의 원래 전문 분야가 있는 사람들이 많기 때문에 이 점을 잘 살려보면 자신만의 특별한 무기가 될 수도 있다고 생각한다. 어쨌든 개발자로 일을 하고 있다는 것은 개발에도 어느 정도 전문성을 가지고 있다는 것이기 때문에, 자신의 원래 전문 분야가 있던 사람은 전문 분야가 최소 2개 이상이라는 것이기 때문이다. 만약 더존과 같은 회계관리 프로그램을 만드려면 개발자도 어느 정도 회계에 대한 지식이 있어야 하고, Logic Pro X와 같은 오디오 편집 프로그램을 만드려면 개발자도 오디오와 음악에 대한 지식이 있어야 하기 때문이다. 그러나 4년 동안 컴퓨터 공학만을 공부한 사람의 경우에는 따로 찾아서 공부하지 않는 이상 다른 분야의 지식을 습득하기 쉽지 않기 때문에 다른 전공을 공부하고 개발자가 된 사람은 이 점에서 나름 강점을 가지고 있다. 오디오와 음악에 대한 지식이 없다면 Logic Pro X와 같은 프로그램은 만들 수 없다 필자 같은 경우는 예전에 연예 기획사에서 사운드 엔지니어로 일을 했었기 때문에 오디오와 레코딩에 지식을 가지고 있다. 또한 어릴 때부터 꾸준히 음악 교육을 받았기 때문에 음악에 대한 지식도 어느 정도 가지고 있다. 그래서 필자는 Web Audio API와 같은 생소한 API를 꽤나 단기간에 이해할 수 있었고, 이 API에서 제공해주는 오실레이터의 프로퍼티들을 조절하여 원하는 악기의 소리를 만들어 낼 수도 있었다. 이런 음악과 오디오 이론에 대한 지식은 컴퓨터 공학과 마찬가지로 하나의 전문 분야이기 때문에 반짝 공부한다고 단기간에 습득할 수 있는 것은 아니다. 많은 비전공 개발자 분들은 컴퓨터 공학에 대한 지식이 전공자보다 부족하다고 생각하기 때문에 자신이 부족한 개발자라고 생각하지만, 조금만 다르게 생각해보면 컴퓨터 공학 지식은 부족할지 몰라도 다른 분야에 대한 전문 지식을 가지고 있는, 오히려 더 특별한 능력의 소유자들이라고도 할 수 있다. 물론 지금 회사에서 맡고 있는 일이 자신의 전공과 전혀 다른 일이라면 딱히 도움은 안되겠지만, 필자는 오히려 자신의 전공을 살려서 틈새 시장을 노려볼 수도 있지 않나라는 생각을 해보게 된다. 그래도 컴퓨터 기초 지식은 중요하다하지만 전공이든 비전공이든 어쨌든 개발자는 개발자이고, 개발자의 기본적인 능력은 컴퓨터에 대한 이해에서부터 출발하는 것이기 때문에 이 점을 무시할 수는 없다. 필자가 위에서 이야기했듯이 컴퓨터 공학을 전공한 사람이 비전공자에 비해서 강점을 가지는 부분은 바로 컴퓨터에 대한 기초 지식이다. 하지만 이런 기초 지식은 당장 취업에 도움이 되는 게 아니기 때문에 학원에서는 비중있게 가르쳐 주지 않고, 이론의 비중 또한 높기 때문에 독학으로 하기엔 너무 재미가 없다. 그러나 필자는 개인적으로 다른 건 몰라도 자료구조, 알고리즘, 네트워크 이 3개는 왠만하면 공부를 하는 것이 좋다고 생각한다. 이 3가지 과목은 실무에서 알게 모르게 도움이 많이 된다. 자료구조예를 들어 어떤 상품들의 정보를 서버로부터 받아와서 클라이언트에 저장해야하는 상황이라고 생각해보자. 이때 각각의 상품들은 정수 자료형으로 표현되는 ID 값을 가지고 있다. 12345678910111213141516{ \"goods\": [ { \"id\": 1, \"name\": \"맥북프로\" }, { \"id\": 22, \"name\": \"청바지\" }, { \"id\": 100, \"name\": \"말린 오징어\" } ]} 자, 서버로부터 응답이 이렇게 내려온다고 생각해보자. 이때 우리는 이 데이터를 어떻게 가공하고 저장할 것인지 선택해야한다. 지금 필자는 대충 3가지 정도를 떠올렸다. 그냥 저대로 배열 안에 담아 놓고 쓴다. 상품 ID를 인덱스로 사용해서 새로운 배열에 담는다. 맵을 사용한다. 이 중에서 각 방법이 어떤 장점이 있고 어떤 단점이 있을까? 1번 방법은 서버에서 내려준 응답 상태의 구조를 그대로 사용하기 때문에 추가적인 이터레이션을 돌지 않아도 된다. 대신 어떤 특정 상품에 접근하고 싶다면 매번 배열을 탐색해야하기 때문에 O(n)의 시간 복잡도가 소요될 것이다. 2번 방법은 어떨까? 이 방법은 상품의 ID를 배열의 인덱스로 사용하기 때문에 값에 대한 접근을 O(1)만에 할 수 있다고 생각할 수 있지만, JavaScript에서는 아니다. 이 경우 배열에 들어가는 원소는 Object 자료형이므로 원시 자료형처럼 원소들이 동일한 자료형으로 평가되지 않는다. 결국 저 배열은 리스트로 생성될 것이고, 리스트는 특정 원소에 접근하기 위해 Head부터 차례대로 탐색해야하므로 접근 시간은 결국 O(n)이다. 이에 대한 자세한 내용은 JavaScript 배열(Array)의 발전과 성능에 대해서 자세히 알아보기 포스팅을 참고하자. 게다가 저 배열이 리스트가 아니라 실제 배열이라고 하더라도 문제는 여전히 존재한다. 상품의 ID는 대부분 데이터베이스에서 사용하는 Primary Key일 것이기 때문에 새로운 상품이 데이터베이스에 추가될수록 할당되는 ID의 값도 점점 증가할 것이다. 그러면 메모리에 할당해야하는 배열의 길이도 점점 길어질 것이라는 사실을 예상 해볼 수 있다.(배열은 메모리에 연속적으로 할당된다) 그래서 이런 경우라면 필자는 3번 방법을 사용할 것 같다. ID는 상품을 구분하는 고유한 값이기 때문에 중복될 가능성이 없다고 봐도 무방하다. 즉 그냥 맵에다가 데이터를 막 때려박아도 충돌이 날 가능성이 없다는 것이다. 맵의 특성 상 접근 자체도 O(1)으로 해결할 수 있고, 늘 서버로부터 받아온 상품 만큼만 메모리를 할당하면 되므로 합리적이다. 배열에 대한 이해가 없다면 잘못된 선택을 할 수도 있다 지금 이 예시는 굉장히 간단한 예시지만 실무에서 충분히 자주 접하는 상황이다. 기본적으로 배열이라는 자료구조가 메모리에 어떻게 할당되는지, 배열과 맵의 장단점이 무엇인지, 이 자료구조에서 값에 어떻게 접근하는지와 같은 자료구조에 대한 기본적인 지식이 없다면 저 3가지 보기 중에 현재 상황에 적합한 답을 찾기 힘들 것이다. 알고리즘알고리즘의 경우에는 하노이의 탑같은 문제를 푸는 방법을 외우라는 것이 절대 아니다. 알고리즘은 단순히 어떤 문제를 풀기 위한 방법을 외우는 것이 아니라 어떤 문제를 효율적으로 풀기 위한 방법을 찾는 것이다. 주어진 행렬 안에서 가장 큰 정사각형을 찾는 문제를 푸는 것이 중요한 게 아니라, 내가 삼중 for문으로 작성한 코드에서 어떻게 하면 for문을 하나라도 줄일 수 있을 지를 고민하는 것이 중요한 것이다. 최소한 자신이 작성한 코드가 어느 정도의 시간 복잡도를 가졌는지는 계산할 수 있어야 한다. 필자가 방금 위의 예시에서 배열의 요소에 접근하는데 소요되는 시간복잡도와 리스트의 요소에 접근하는데 소요되는 시간복잡도를 계산한 것도 알고리즘의 범주 안에 속한다. 또한 알고리즘은 굉장히 넓고 방대한 분야이기 때문에, 어떤 것부터 시작해야 하는지 감이 오지 않을 수도 있다. 그럴 때는 정렬과 탐색부터 한번 도전해보는 것을 추천한다. 기본적인 정렬 알고리즘인 Bubble, Merge, Quick 같은 알고리즘이나 탐색 알고리즘인 Binary Search와 같은 알고리즘은 꽤 직관적이기 때문에 이해하기도 그렇게 어렵지 않다. 또한 이런 알고리즘을 공부하다보면 자연스럽게 자료구조와도 엮이기 때문에 자료구조와 알고리즘을 함께 공부하는 것을 추천한다. 프로그래머스와 같은 알고리즘 문제 사이트에서 문제를 푸는 것도 물론 재밌고 좋지만, 이런 문제는 기초라기보다 응용 문제에 가깝기 때문에 기초적인 개념을 먼저 잡고나서 도전하는 것을 추천한다. 그리고 이런 사이트들은 다른 사람들이 이 문제를 어떻게 풀었는지 볼 수 있는 기능을 제공하는 경우가 많은데, 많은 분들이 극단적으로 코드 라인을 줄이는 것에 초점을 맞추고 있는 것 같아서 그런 코드를 보고 공부하는 것을 딱히 추천하고 싶지는 않다. 아마 그런 사이트에서 그렇게 코드 라인을 줄여서 작성하시는 분들도 실무에서까지 그렇게 작성하시진 않을 것이다. 네트워크네트워크의 경우, 웹이나 앱 개발자인 이상 대부분 클라이언트와 서버가 통신하는 기능을 만질 수 밖에 없다. 특히 웹의 경우는 네트워크 위에서 개발하는 것이라고 봐도 무방할 정도로 네트워크와 밀접한 관련이 있기 때문에 네트워크에 대한 기본적인 지식은 필수다. 또한 웹 어플리케이션을 개발하다보면 네트워크와 관련된 다양한 문제들이 발생하는데, 네트워크에 대한 기본적인 지식이 없다면 해결하기 힘든 경우가 많다. 아마 프론트엔드 개발자가 겪는 대표적인 네트워크 이슈로는 CORS(Cross-Origin Resource Sharing) 위반을 예로 들 수 있을 것 같다. 이 이슈는 a.com이 b.com처럼 서로 다른 오리진을 가진 곳으로 리소스를 요청하는 경우 보안을 침해할 가능성이 있기 때문에 브라우저가 서버의 응답을 무시해버리는 이슈다. 클라이언트는 본 요청을 서버로 보내기 전에 Preflight라는 예비 요청에 사용하고 싶은 커스텀 헤더와 메소드 등을 담아서 미리 보내게 되는데 이때 서버가 이 예비 요청을 받고 응답 헤더 내에 Access-Control-Allow-Origin라는 키를 사용하여 올바른 값을 내려주지 않는다면 CORS 위반이 되어 통신을 할 수 없는 이슈가 발생하는 것이다. CORS의 통신 과정 이때 이런 일련의 통신 과정과 보안에 대한 기본적인 개념이 없다면 인터넷에 많이 나와있는 해결책인 Access-Control-Allow-Origin: *을 운영환경에서 사용하여 서버의 보안을 망가트릴수도 있다. 게다가 이 문제를 주로 접하는 것은 프론트엔드 개발자이지만 실질적인 해결은 백엔드 쪽에서 해줘야 하기 때문에 뭐가 문제인지 모른다면 혼자 부여잡고 끙끙대다가 아예 해결을 못할 수도 있다. 마치며필자가 이 포스팅을 작성하게 된 이유는 자신이 비전공 개발자라는 이유로 자신을 낮게 평가하는 분들 때문이었다. 필자는 부트캠프나 학원을 다녀본 적이 없기 때문에 모든 것을 공감하지는 못하지만, 부트캠프나 학원 출신 개발자 분들이 전공에 대한 선입견 때문에 답답한 감정을 느꼈다고 하는 이야기를 들으면 기분이 좋지는 않았다. 전공 여부를 가지고 뭐라고 하는 사람들도 문제지만, 본인들 스스로도 그 프레임에 갖혀서 내가 비전공이라 그런가라는 이야기를 하는 경우가 종종 있기에, 필자는 그럴때마다 그냥 개발자는 개발자일 뿐이다라고 얘기를 해주고는 한다. 사실 학원에서 6개월 동안 공부한 개발자가 학교에서 4년 동안 공부한 개발자에 비해 컴퓨터에 대한 기초 지식이 모자른 것은 어찌보면 당연하다. 그러나 어떠한 개발자에 대한 평가는 컴퓨터에 대한 지식만으로 평가되는 것이 아니라는 사실을 잊지 않았으면 좋겠다. 그리고 포스팅의 주제 상 어쩔 수 없이 전공과 비전공이라는 단어를 많이 사용했지만, 개발에 있어서 전공자냐 비전공자냐는 절대 중요한 게 아니다. 그냥 개발자는 개발자일 뿐이고 컴퓨터 이론에 특화된 개발자와 다른 분야의 지식도 함께 섭렵한 개발자처럼 각 개발자마다의 특징만 있을 뿐이다. 만약에 누군가가 여러분에게 컴공 출신 개발자는 역시 잘하네, 역시 비전공자 출신은 안돼 같은 말을 한다면 그건 그 사람의 마인드 셋이 잘못된 것이지, 여러분이 잘못된 것이 아니다. 하지만 위에서 설명한대로 4년 동안 컴퓨터 공학 공부를 하고 졸업한 사람들에 비교했을 때, 그렇지 않은 사람들이 컴퓨터에 대한 기초 지식이 부족한 것은 사실이기에 기초에 대한 공부는 꾸준히 하는 것이 좋을 것 같다는 생각이 든다. IT 시장의 스펙트럼은 굉장히 넓고, 각 분야에서 요구되는 개발자들의 스펙도 그만큼 다양하다. 그 말인 즉슨, 전공 여부와 상관없이 여러분이 가진 능력들을 잘 조합해서 자신만의 무기로 만들면 시장에서 충분히 경쟁력을 가질 수 있다는 소리다. 꾸준히 공부하고 재밌게 코딩하다보면 일은 알아서 잘 풀릴 거라고 생각하자. 이상으로 비전공 개발자가 전공자보다 정말 불리할까? 포스팅을 마친다.","link":"/2019/09/09/major-is-not-important/"},{"title":"HTTP/3는 왜 UDP를 선택한 것일까?","text":"HTTP/3는 HTTP(Hypertext Transfer Protocol)의 세 번째 메이저 버전으로, 기존의 HTTP/1, HTTP/2와는 다르게 UDP 기반의 프로토콜인 QUIC을 사용하여 통신하는 프로토콜이다. HTTP/3와 기존 HTTP 들과 가장 큰 차이점이라면 TCP가 아닌 UDP 기반의 통신을 한다는 것이다. 필자는 최근에 다른 분들이 공유해주시는 포스팅을 보고 나서 HTTP/3가 나왔다는 것을 처음 알게 되었다. 그 포스팅은 HTTP/3: the past, the present, and the future라는 포스팅이었는데, 솔직히 처음 딱 제목만 보고나서 이런 생각을 했었다. 아니, HTTP/2가 공개된지 4년 정도 밖에 안 지났는데 무슨 HTTP/3가 벌써 나와? 그냥 설계하고 있다는 거 아니야? 그런데 포스팅을 읽어 보니 이미 Google Chrome은 HTTP/3를 지원하는 카나리 빌드도 배포되어 있어서 실제로 사용까지 해볼 수 있는 단계에 도달했다는 사실을 알게 되어 놀랐다. HTTP/1에서 HTTP/2로 가는 데만 해도 대략 15년 정도의 시간이 걸렸는데, 고작 4년 만에 바로 사용해볼 수 있는 정도의 완성도인 다음 메이저 버전이 배포되었다는 것이다. 게다가 아직 전 세계의 HTTP/2 점유율을 보면 40% 정도 밖에 안된다. 그 정도로 HTTP/2가 나온지도 얼마 되지 않았다는 것이다. W3Techs.com에서 조사한 2019년 8월 HTTP/2 사용률 프로그래밍 언어나 프레임워크같은 친구들은 배포하는 쪽에서 업데이트를 쫙 해버리고 유저들이 업데이트를 하면 그만이지만, 프로토콜은 일종의 규약이기 때문에 소프트웨어 제조사 간 합을 맞추는 기간이 필요하므로 이렇게 단기간 안에 급격한 변화가 자주 발생하지 않을 것이라고 생각했다. 아무리 요즘 기술의 변화가 빠르다지만, HTTP는 나름 웹의 근간이 되는 프로토콜인데 꼴랑 4년 만에 이런 급격한 변화가 일어났다는 게 놀라울 따름이다.(몇 달 전에 HTTP/2를 처음 도입해본 웹 개발자는 웁니다) 그리고 또 한가지 놀랐던 점은 HTTP/3는 TCP가 아닌 UDP를 사용한다는 것이었다. 뭐 딱히 웹 프로토콜이 무조건 TCP만 사용해야 한다는 법이 있는 건 아니긴 하지만, 학교에서 배울 때도 그렇고 실무에서도 실제로 사용할 때도 그렇고 HTTP는 TCP 위에서 정의된 프로토콜이라는 사실이 너무 당연하게 인식되어 있었기 때문에 UDP를 사용한다는 점이 신기하기도 했고 “왜 멀쩡히 잘 돌아가는 TCP를 냅두고 UDP를 사용하는거지?”라는 의문도 들었다. 사실 HTTP/3는 정식으로 배포된 프로토콜이라기보다 아직 테스트를 거치고 있는 단계라고 보는 게 맞다. 하지만 위에서 이야기 했듯이 Google Chrome은 이미 HTTP/3를 지원하는 카나리 빌드를 배포한 상태이고, Mozila Firefox도 조만간 Nightly 버전에서 HTTP/3를 지원할 예정이며, cURL에서도 HTTP/3를 실험적 기능으로 제공하고 있는 만큼 가까운 미래 안에 HTTP/3가 메인 프로토콜이 될 가능성이 높은 것도 사실이다. 만약 Google Chrome에서 HTTP/3 프로토콜을 사용해보고 싶다면 터미널에서 --enable-quic과 --quic-version=h3-23 인자를 사용하여 실행하면 된다. 1$ open -a Google\\ Chrome --args --enable-quic --quic-version=h3-23 http/2+quic/46이라고 되어있는 녀석들이 HTTP/3 프로토콜을 사용한 연결이다 필자는 웹 개발자이기 때문에 HTTP가 메이저 업데이트 된다는 사실을 그냥 넘기기는 힘들었고, UDP를 사용한다는 것이 뭘 의미하는지도 궁금하기도 해서 결국 HTTP/3에 대한 조사를 하게 되었다. 그래서 이번 포스팅에서는 필자가 여기저기 쑤셔보면서 알아본 HTTP/3에 대한 내용을 정리해볼까 한다. HTTP/3에 대한 간단한 소개사실 HTTP/3는 처음에는 HTTP-over-QUIC이라는 이름을 가지고 있었는데, IETF(Internet Engineering Task Force) 내 HTTP 작업 그룹과 QUIC 작업 그룹의 의장인 마크 노팅엄이 이 프로토콜의 이름을 HTTP/3로 변경할 것을 제안했고, 2018년 11월에 이 제안이 통과되어 HTTP-over-QUIC이라는 이름에서 HTTP/3으로 변경되게 되었다. 즉, HTTP/3는 QUIC이라는 프로토콜 위에서 돌아가는 HTTP인 것이다. QUIC은 Quick UDP Internet Connection의 약자로, 말 그대로 UDP를 사용하여 인터넷 연결을 하는 프로토콜이다.(참고로 발음은 그냥 퀵이라고 한다) HTTP/3는 QUIC을 사용하고, QUIC은 UDP를 사용하기 때문에 결과적으로 HTTP/3는 UDP를 사용한다 라고 이야기 할 수 있는 것이다. 그렇다면 QUIC이 도대체 뭐길래 기존의 TCP보다 더 빠른 전송 속도를 가질 수 있다는 것일까? 그 이유를 알려면 먼저 TCP가 왜 느리다고 하는지, UDP를 사용함으로써 어떤 이득을 얻을 수 있는지 부터 알아야 한다. TCP가 왜 느리다고 하는 걸까?필자는 학교에서 네트워크 강의를 들을 때 TCP와 UDP의 차이에 대한 내용을 처음 배웠었는데, 교수님이 이건 반드시 시험에 나온다길래 이런 표를 보면서 열심히 외웠던 기억이 난다. TCP UDP 연결 방식 연결형 서비스 비연결형 서비스 패킷 교환 가상 회선 방식 데이터그램 방식 전송 순서 보장 보장함 보장하지 않음 신뢰성 높음 낮음 전송 속도 느림 빠름 위 표를 보면 대략 TCP는 신뢰성이 높고 느리다, UDP는 신뢰성이 낮고 빠르다 정도로 정리가 되는데, 여기서 말하는 신뢰성이란 전송되는 데이터 패킷들의 순서, 패킷 유실 여부 등을 검사하여 송신 측이 보낸 모든 데이터가 수신 측에 온전하게 전달이 될 수 있느냐를 말하는 것이다. TCP는 클라이언트와 서버가 서로 신뢰성있는 통신을 할 수 있도록 몇 가지 방법을 사용하게되는데, 이 방법들 또한 결국은 클라이언트와 서버 간의 통신이기 때문에 레이턴시가 발생할 수 밖에 없다. 게다가 이 과정은 TCP라는 프로토콜이 생길 때부터 정의된 표준이므로 무시할 수도 없다. 그렇다면 레이턴시를 줄이기 위해서는 TCP에서 정의한 기능 외에 다른 부분들을 건드려야 한다는 것인데, 여러모로 제한 사항이 많다. 아무리 회선의 대역폭을 늘린다고 해도 기술이 발전하면서 전송해야하는 데이터의 크기도 점점 커지기 때문에 결국 언젠가는 또 느려질 것이고, 회선의 전송 속도 자체를 높힌다고 해도 결국은 빛의 속도 보다 빠르게 전송할 수 없기 때문에 한계가 있다. HTTP/3이 UDP 기반인 QUIC 프로토콜을 사용하는 이유가 바로 이런 제약 조건을 뛰어넘기 위해 프로토콜 자체를 손보는 방법을 택한 것이다. 하지만 TCP는 워낙 오래된 프로토콜이기도 하고 커널까지 내려가는 로우 레벨에서 정의되어 있기 때문에 이걸 뜯어고치는 것도 만만치 않은 대작업이라 UDP를 선택한 것이다. 그럼 한번 TCP가 신뢰성 있는 통신을 위해 사용하는 방법들이 왜 느리다고 하는지 알아보자. 3 Way HandshakeTCP는 굉장히 친절한 프로토콜이다. 통신을 시작할 때와 종료할 때 서로 준비가 되어있는지를 반드시 먼저 물어보고 패킷을 전송할 순서를 정하고 나서야 본격적인 통신을 시작하기 때문이다. 이때 통신을 시작할 때 거치는 과정을 3 Way Handshake, 통신을 마칠 때 거치는 과정을 4 Way Handshake라고 한다. 이 포스팅의 목적은 이 과정을 자세히 다루는 것이 아니므로, TCP를 사용하여 통신을 시작할 때 거치는 과정인 3 Way Handshake가 어떤 원리로 작동하는지만 설명하겠다. 통신을 시작할 때 3 Way Handshake를 하는 과정 위 그림을 보면 클라이언트가 처음 서버와 통신을 하기 위해 TCP 연결을 생성할 때 SYN와 ACK이라는 패킷을 주고 받고 있다. 이 패킷 내부에 들어있는 값들을 사용하여 클라이언트와 서버가 서로 보낸 패킷의 순서와 패킷을 제대로 받았는 지를 확인할 수 있는 것이다. 그리고 이 과정에는 총 3번의 통신이 필요하다. OSX나 Linux를 사용하고 있는 분이라면 터미널에서 tcpdump 유틸리티를 사용하여 이 과정을 직접 눈으로 확인해볼 수 있다. 단, tcpdump를 아무 옵션 없이 사용하면 디바이스의 모든 패킷을 감시하고 출력하기 때문에 원하는 정보를 찾기 힘들다. 그래서 필자는 루프백에서 동작하고 있는 블로그 서버와의 통신만 캡쳐해보았다. 1234$ sudo tcpdump host localhost -i lo0IP localhost.53920 > localhost.terabase: Flags [S], seq 1260460927, win 65535IP localhost.terabase > localhost.53920: Flags [S.], seq 3009967847, ack 1260460928, win 65535IP localhost.53920 > localhost.terabase: Flags [.], ack 3009967848, win 6379 보낸 놈 > 받은 놈: Flags [플래그 종류], 헤더의 값들 원래는 이것보다 더 많은 정보가 나오지만, 여기에 전부 기재하기에는 양이 너무 많으니 설명에 필요한 정보만 추려보았다. 이 로그에서 중요한 키워드는 Flag, seq, ack 정도이다. 한번 하나하나 뜯어보도록 하자. 먼저 localhost.53920은 클라이언트, localhost.terabase는 서버를 의미한다. 각 라인의 첫번째 필드는 보낸 놈 > 받은 놈을 의미하고 있으니, 첫 패킷은 클라이언트가 서버에게, 두 번째 패킷은 서버가 클라이언트에게 보낸 것이라고 할 수 있다. 그리고 각 라인에는 Flag라는 것이 붙어있는데, 플래그는 이 패킷이 어떤 타입의 패킷인지를 알려주는 역할을 한다. Flag 이름 의미 S SYN 연결을 생성할 때 클라이언트가 서버에 시퀀스 번호를 보내는 패킷 S. SYN-ACK 시퀀스 번호를 받은 서버가 ACK 값을 생성하여 클라이언트에게 응답하는 패킷 . ACK ACK 값을 사용하여 응답하는 패킷 이 통신 과정을 거치고 나면 클라이언트와 서버는 신뢰성 있는 TCP 연결을 생성할 수 있고, 이때 총 3회의 통신을 하기 때문에 3 Way Handshake라고 하는 것이다. 그렇다면 이 과정에서 어떤 일이 벌어지길래 신뢰성 있는 연결을 생성할 수 있다는 것일까? 조금 더 자세히 들여다보면 클라이언트와 서버는 3 Way Handshake를 할 때 대략 이런 과정을 거치고 있다. 1번 라인: 클라이언트가 서버로 시퀀스 번호를 seq 필드에 담아 보냄2번 라인: 서버는 클라이언트가 보내준 시퀀스 번호를 1 증가시켜서 ack 필드에 담아 보냄3번 라인: 클라이언트는 다시 서버로부터 받은 시퀀스 번호를 1 증가시켜서 자신의 ack 필드에 담아 보냄 새로운 TCP 연결을 생성하고자 할 때 클라이언트가 서버에게 랜덤한 시퀀스 번호를 전송함으로써 3 Way Handshake가 시작된다. 이때 생성된 시퀀스 번호는 이후 송신 측이 전송한 패킷을 수신 측이 재조립할때 패킷의 조립 순서를 알려주는 역할을 한다. 이때 클라이언트와 서버는 상대방이 보내준 seq(시퀀스 번호)를 1 증가 시킨 후 자신의 ack(승인 번호) 필드에 담아서 보내는데, “지금 이 패킷이 니가 전에 보낸 시퀀스 번호의 다음으로 이어지는 패킷이야”라고 말하고 있는 것이다. 이 3회의 통신이 바로 3 Way Handshake이다. 이 과정을 통해 클라이언트와 서버는 데이터를 주고 받을 준비가 되었다는 것을 서로에게 알려주고 이후 데이터 전송에 필요한 시퀀스 번호를 알 수 있게 된다. 연결을 끊을 때도 마찬가지로 이와 비슷한 과정인 4 Way Handshake를 거치고 나서야 세션을 종료할 수 있으며, 이때는 총 4회의 통신을 통해 연결을 종료한다. 즉, TCP를 사용하는 이상 본격적인 통신을 시작하기 전에 무조건 저 번거로운 통신 과정을 거쳐야한다는 것이다. HTTP/1은 하나의 TCP 연결에 하나의 요청만 처리하고 연결을 끊어버렸기 때문에 매 요청마다 이 번거로운 핸드쉐이크를 거쳐야 했다. 그래서 HTTP/2에서는 핸드쉐이크를 최소화하기 위해서 단일 TCP 연결을 유지하면서 여러 개의 요청을 처리할 수 있도록 변경된 것이다. 결국 HTTP/1에서 HTTP/2로 넘어갈 때도 핸드쉐이크 과정 자체는 건드리지 않았고 단지 핸드쉐이크가 발생하는 횟수를 최소화함으로써 레이턴시를 줄인 것이다. 이는 TCP를 사용하는 이상 핸드쉐이크가 반드시 필요한 과정이기 때문에 건드리지 못한 것이다. 그러나 HTTP/3는 UDP를 사용함으로써 이 핸드쉐이크 과정 자체를 날려버리고 다른 방법으로 연결의 신뢰성을 확보함으로써 레이턴시를 줄이는 방법을 택했다. HOLB(Head of line Blocking)그 외에도 TCP를 사용하는 기존의 HTTP에는 한 가지 문제가 더 있는데, 바로 HOLB(Head of Line Blocking)이라고 하는 문제이다. 사실 HTTP 레벨에서의 HOLB와 TCP 레벨에서의 HOLB는 다른 의미이기는 하나 결국 어떤 요청에 병목이 생겨서 전체적인 레이턴시가 늘어난다는 맥락으로 본다면 동일하다고 할 수 있다. TCP를 사용한 통신에서 패킷은 무조건 정확한 순서대로 처리되어야 한다. 수신 측은 송신 측과 주고받은 시퀀스 번호를 참고하여 패킷을 재조립해야하기 때문이다. 그래서 통신 중간에 패킷이 손실되면 완전한 데이터로 다시 조립할 수 없기 때문에 절대로 그냥 넘어가지 않는다. 무조건 송신 측은 수신 측이 패킷을 제대로 다 받았다는 것을 확인한 후, 만약 수신 측이 제대로 패킷을 받지 못했으면 해당 패킷을 다시 보내야 한다. 또한 패킷이 처리되는 순서 또한 정해져있으므로 이전에 받은 패킷을 파싱하기 전까지는 다음 패킷을 처리할 수도 없다. 이렇게 패킷이 중간에 유실되거나 수신 측의 패킷 파싱 속도가 느리다면 통신에 병목이 발생하게 되는 현상을 HOLB라고 부르는 것이다. 이건 TCP 자체의 문제이므로 HTTP/1 뿐만 아니라 HTTP/2도 가지고 있는 문제이다. 이런 문제들을 해결하기 위해 HTTP/3는 UDP를 기반으로 만들어진 프로토콜인 QUIC 위에서 작동하는 것을 선택한 것이다. 그럼 이제 QUIC가 정확히 어떤 프로토콜인지, UDP를 사용한다는 것이 TCP에 비해서 어떤 장점이 있다는 것인지를 알아보자. HTTP/3가 UDP를 사용하는 이유HTTP/3는 QUIC을 기반으로 돌아가는 프로토콜이기 때문에 우리가 HTTP/3를 이해하려면 QUIC에 초점을 맞춰야 한다. QUIC은 TCP가 가지고 있는 이런 문제들을 해결하고 레이턴시의 한계를 뛰어넘고자 구글이 개발한 UDP 기반의 프로토콜이다. QUIC은 처음부터 TCP의 핸드쉐이크 과정을 최적화하는 것에 초점을 맞추어 설계되었고, UDP를 사용함으로써 이를 실현해낼 수 있었다. UDP는 User Datagram Protocol이라는 이름에서도 알 수 있듯이 데이터그램 방식을 사용하는 프로토콜이기 때문에 애초에 각각의 패킷 간의 순서가 존재하지 않는 독립적인 패킷을 사용한다. 또한 데이터그램 방식은 패킷의 목적지만 정해져있다면 중간 경로는 어딜 타든 신경쓰지 않기 때문에 종단 간의 연결 설정 또한 하지 않는다. 즉, 핸드쉐이크 과정이 필요없다는 것이다. 결론적으로 UDP는 TCP가 신뢰성을 확보하기 위해 거치던 많은 과정을 거치지 않기 때문에 속도가 더 빠를 수 밖에 없다는 것인데, 그렇다면 UDP를 사용하게되면 기존의 TCP가 가지던 신뢰성과 패킷의 무결함도 함께 사라지는 걸까? 아니 그렇지 않다. UDP를 사용하더라도 기존의 TCP가 가지고 있던 기능을 전부 구현할 수 있다. UDP의 진짜 장점은 바로 커스터마이징이 용이하다는 것이기 때문이다. UDP는 하얀 도화지 같은 프로토콜이다필자는 학교에서 UDP와 TCP의 가장 큰 차이점으로 UDP는 TCP보다 신뢰성이 없는 대신 빠르다라고 배웠었는데, 사실 이 말은 반은 맞고 반은 틀리다. 왜냐면 애초에 UDP는 데이터 전송을 제외한 그 어떤 기능도 정의되어 있지 않은 프로토콜이기 때문에 프로토콜 자체적으로 신뢰성을 보장하지 않는 것은 맞지만, 다르게 말하자면 데이터 전송 기능을 제외한 아무 기능이 없는 백지 상태의 프로토콜이라고도 할 수 있기 때문이다. TCP가 신뢰성있는 연결과 혼잡 제어 등을 위해 얼마나 많은 기능을 가지고 있는 지는 TCP의 헤더를 보면 대충 각이 나온다. 이미 정보들이 뚱뚱하게 들어찬 TCP의 헤더 TCP의 경우 워낙 오래 전에 설계되기도 했고, 이런 저런 기능이 워낙 많이 포함된 프로토콜이다보니 이미 헤더가 거의 풀방이다. TCP에 기본적으로 정의되어 있는 기능 외에 다른 추가 기능을 구현하고 싶다면 가장 하단에 있는 옵션(Options) 필드를 사용해야 하는데, 옵션 필드도 무한정 배당 해줄 수는 없으니 최대 크기를 320 bits로 정해놓았다. 그러나 TCP의 단점을 보완하기 위해 나중에 정의된 MSS(Maximum Segment Size), WSCALE(Window Scale factor), SACK(Selective ACK) 등 많은 옵션들이 이미 옵션 필드를 차지하고 있기 때문에 실질적으로 사용자가 커스텀 기능을 구현할 수 있는 자리는 거의 남지도 않았다. 반면 UDP는 데이터 전송 자체에만 초점을 맞추고 설계되었기 때문에 헤더에 진짜 아무 것도 없다. TCP와 비교해보면 확실히 휑한 UDP의 헤더 UDP의 헤더에는 출발지와 도착지, 패킷의 길이, 체크섬 밖에 없다. 이때 체크섬은 패킷의 무결성을 확인하기 위해 사용되는데, TCP의 체크섬과는 다르게 UDP의 체크섬은 사용해도 되고 안해도 되는 옵션이다. 즉, UDP 프로토콜 자체는 TCP보다 신뢰성이 낮기도 하고 흐름 제어도 안되지만, 이후 개발자가 어플리케이션에서 구현을 어떻게 하냐에 따라서 TCP와 비슷한 수준의 기능을 가질 수도 있다는 것이다. 물론 TCP가 신뢰성을 확보하기위해 이런 저런 기능을 제공해주는 것이 개발자 입장에서는 편하고 좋지만, 한가지 슬픈 점은 이 기능들이 프로토콜 자체에 정의된 필수 과정이라서 개발자가 맘대로 커스터마이징 할 수 없다는 것이다. 결국 여기서 발생하는 레이턴시들을 어떻게 더 줄여볼 시도조차 하기 힘들다. TCP에 TLS까지 사용한다면 통신을 시작하기도 전에 이렇게 많은 과정을 거쳐야 한다 결국 레이턴시를 줄이려면 프로토콜 외적인 것들을 건드려야 하는데, 위에서 이야기 했듯이 일반적인 개발자가 통신 과정에서 건드릴 수 있는 영역은 한계가 있기 때문에 이 또한 어려운 것이 사실이다.(통신 업계의 큰 손 형님들이 인프라를 깔아주시는 걸 기다리자) 아직 TCP와 UDP의 차이가 잘 와닿지 않는다면 좋은 기능이 다 들어있는 무거운 라이브러리와 필요한 기능만 들어있는 가벼운 라이브러리로 비교해보면 조금 더 이해가 빠를 것 같다. 예를 들어 JavaScript 진영에서 많이 사용하는 lodash와 같은 라이브러리는 기능은 무궁무진하고 사용자에게 큰 편리함을 주지만, 보통 lodash의 모든 메소드를 다 사용하는 사람은 많지 않을 것이다. 결국 편하긴 하지만 내가 사용하지 않는 기능까지 전부 내 JS 번들에 포함시켜야 한다는 부담이 있다. 반면 단순한 하나의 기능을 제공하는 라이브러리는 lodash보다 기능은 많지 않아도 내가 원하는 부분만 쏙쏙 골라서 사용할 수 있다는 장점이 있다. 하지만 해당 라이브러리에서 지원하지 않는 기능은 직접 구현해야하는 번거로움이 있을 수도 있다. 이때 lodash와 같은 만능 라이브러리가 TCP, 하나의 기능만 제공하는 작은 라이브러리가 UDP인 것이다. 이렇듯 구글이 QUIC을 만들 때 UDP를 선택한 이유에는 기존의 TCP를 수정하기가 어려운데다가, 백지 상태나 다름 없는 UDP를 사용함으로써 QUIC의 기능을 확장하기 쉽다고 생각했기 때문이라는 것도 있다. HTTP/3가 UDP를 사용함으로써 기존 프로토콜보다 나아진 점지금까지 HTTP/3의 뼈대로 사용되는 QUIC이 왜 TCP가 아닌 UDP를 사용했는지 간략하게 알아보았다. 그렇다면 실제로 UDP를 사용함으로써 얻는 이득에는 무엇이 있을까? 진짜로 HTTP/3는 UDP를 사용함으로써 기존의 HTTP+TCP+TLS를 사용했던 방법보다 더 좋아진 것 일까? 그에 대한 해답은 Chromium Projects의 QUIC Overview라는 문서에서 찾을 수 있었다. 한번 구글이 이야기하는 QUIC의 장점에 대해서 살펴보자. 연결 설정 시 레이턴시 감소QUIC은 TCP를 사용하지 않기 때문에 통신을 시작할 때 번거로운 3 Way Handshake 과정을 거치지 않아도 된다. 클라이언트가 보낸 요청을 서버가 처리한 후 다시 클라이언트로 응답해주는 사이클을 RTT(Round Trip Time)이라고 하는데, TCP는 연결을 생성하기 위해 기본적으로 1 RTT가 필요하고, 여기에 TLS를 사용한 암호화까지 하려고 한다면 TLS의 자체 핸드쉐이크까지 더해져 총 3 RTT가 필요하다. 반면 QUIC은 첫 연결 설정에 1 RTT만 소요된다. 클라이언트가 서버에 어떤 신호를 한번 주고, 서버도 거기에 응답하기만 하면 바로 본 통신을 시작할 수 있다는 것이다. 즉, 연결 설정에 소요되는 시간이 반 정도 밖에 안된다. 어떻게 이게 가능한 걸까? 그 이유는 생각보다 간단하다. 첫번째 핸드쉐이크를 거칠 때, 연결 설정에 필요한 정보와 함께 데이터도 보내버리는 것이다. TCP+TLS는 데이터를 보내기 전에 신뢰성있는 연결과 암호화에 필요한 모든 정보를 교환하고 유효성을 검사한 뒤에 데이터를 교환하지만, QUIC은 묻지도 따지지도 않고 그냥 바로 데이터부터 꽂아버리고 시작한다. 이 과정에 대해서는 2015년 IEEE Symposium에서 발표된 How Secure and Quick is QUIC?이라는 세션에서 자세히 들어볼 수 있다. 한 손 주머니에 꽂고 발표하는 모습에서 스웩이 넘친다 결국 이 영상에서 말하고자 하는 것은 TCP+TLS는 서로 자신의 세션 키를 주고 받아 암호화된 연결을 성립하는 과정을 거치고 나서야 세션 키와 함께 데이터를 교환할 수 있지만, QUIC은 서로의 세션 키를 교환하기도 전에 데이터를 교환할 수 있기 때문에 연결 설정이 더 빠르다는 것이다. 단, 클라이언트가 서버로 첫 요청을 보낼 때는 서버의 세션 키를 모르는 상태이기 때문에 목적지인 서버의 Connection ID를 사용하여 생성한 특별한 키인 초기화 키(Initial Key)를 사용하여 통신을 암호화 한다. 이 과정에 대한 자세한 설명은 QUIC 작업 그룹의 Using TLS to Secure QUIC 문서에서 확인 해볼 수 있다. 그리고 한번 연결에 성공했다면 서버는 그 설정을 캐싱해놓고 있다가, 다음 연결 때는 캐싱해놓은 설정을 사용하여 바로 연결을 성립시키기 때문에 0 RTT만으로 바로 통신을 시작할 수도 있다. 이런 점들 때문에 QUIC은 기존의 TCP+TLS 방식에 비해 레이턴시를 더 줄일 수 있었던 것이다. 참고로 이 세션이 발표될 당시에는 TLS 1.3이 나오기 전이라 따로 언급이 되지 않았지만, 지금은 TCP Fast Open과 TLS 1.3을 사용하여 QUIC와 비슷한 과정을 통해 연결을 설정함으로써 TCP를 사용하더라도 동일한 이점을 가져갈 수도 있긴하다. 그러나 TCP SYN 패킷은 한 패킷당 약 1460 Byte만 전송할 수 있도록 제한하지만 QUIC은 데이터 전체를 첫 번째 라운드 트립에 포함해서 전송할 수 있기 때문에 주고 받아야할 데이터가 큰 경우에는 여전히 QUIC가 유리하다고 할 수 있다. 패킷 손실 감지에 걸리는 시간 단축QUIC도 TCP와 마찬가지로 전송하는 패킷에 대한 흐름 제어를 해야한다. 왜냐면 QUIC든 TCP든 결국 본질적으로는 ARQ 방식을 사용하는 프로토콜이기 때문이다. 통신과정에서 발생한 에러를 어떻게 처리할 것인지를 이야기하는 것인데, ARQ 방식은 에러가 발생하면 재전송을 통해 에러를 복구하는 방식을 말하는 것이다. TCP는 여러 ARQ 방식 중에서 Stop and Wait ARQ 방식을 사용하고 있다. 이 방식은 송신 측이 패킷을 보낸 후 타이머를 사용하여 시간을 재고, 일정 시간이 경과해도 수신 측이 적절한 답변을 주지 않는다면 패킷이 손실된 것으로 판단하고 해당 패킷을 다시 보내는 방식이다. 우선 2017년 구글에서 발표한 QUIC Loss Detection and Congestion Control에 따르면, QUIC은 기본적으로 TCP와 유사한 방법으로 패킷 손실을 탐지하나, 몇 가지 개선 사항을 추가한 것으로 보인다. TCP에서 패킷 손실 감지에 대한 대표적인 문제는 송신 측이 패킷을 수신측으로 보내고 난 후 얼마나 기다려줄 것인가, 즉 타임 아웃을 언제 낼 것인가를 동적으로 계산해야한다는 것이다. 이때 이 시간을 RTO(Retransmission Time Out)라고 하는데, 이때 필요한 데이터가 바로 RTT(Round Trip Time)들의 샘플들이다. 한번 패킷을 보낸 후 잘 받았다는 응답을 받을 때 걸렸던 시간들을 측정해서 동적으로 타임 아웃을 정하는 것이다. 즉, RTT 샘플을 측정하기 위해서는 반드시 송신 측으로 부터 ACK를 받아야하는데, 정상적인 상황에서는 딱히 문제가 없으나 타임 아웃이 발생해서 패킷 손실이 발생하게 되면 RTT 계산이 애매해진다. 패킷 전송 -> 타임 아웃 -> 패킷 재전송 -> ACK 받음!(근데 이거 첫 번째로 보낸 패킷의 ACK야? 두 번째로 보낸 패킷의 ACK야?) 이때 이 ACK가 어느 패킷에 대한 응답인지 알기 위해서는 타임스탬프를 패킷에 찍어주는 등 별도의 방법을 또 사용해야하고, 또 이를 위한 패킷 검사도 따로 해줘야 한다. 이를 재전송 모호성(Retransmission Ambiguity)이라고 한다. 이 문제를 해결하기 위해 QUIC는 헤더에 별도의 패킷 번호 공간을 부여했다. 이 패킷 번호는 패킷의 전송 순서 자체만을 나타내며, 재전송시 동일한 번호가 전송되는 시퀀스 번호와는 다르게 매 전송마다 모노토닉하게 패킷 번호가 증가하기 때문에, 패킷의 전송 순서를 명확하게 파악할 수 있다. TCP의 경우 타임스탬프를 사용할 수 있는 상황이라면 타임스탬프를 통해 패킷의 전송 순서를 파악할 수 있지만, 만약 사용할 수 없는 경우 시퀀스 번호에 기반하여 암묵적으로 전송 순서를 추론할 수 밖에 없지만, QUIC는 이런 불필요한 과정을 패킷마다 고유한 패킷 번호를 통해 타파함으로써 패킷 손실 감지에 걸리는 시간을 단축할 수 있었다. 이 외에도 QUIC는 대략 5가지 정도의 기법을 사용하여 이 패킷 손실 감지에 걸리는 시간을 단축시켰는데, 자세한 내용은 QUIC Loss Detection and Congestion Control의 3.1 Relevant Differences Between QUIC and TCP 챕터를 한번 읽어보는 것을 추천한다. 멀티플렉싱을 지원멀티플렉싱(Multiplexing)은 위에서 TCP의 단점으로 언급했던 HOLB(Head of Line Blocking)을 방지하기 때문에 매우 중요하다. 여러 개의 스트림을 사용하면, 그 중 특정 스트림의 패킷이 손실되었다고 하더라도 해당 스트림에만 영향을 미치고 나머지 스트림은 멀쩡하게 굴릴 수 있기 때문이다. 참고로 멀티플렉싱은 여러 개의 TCP 연결을 만든다는 의미가 아니라, 단일 연결 안에서 몇 가지 얌생이를 사용하여 여러 개의 데이터를 섞이지 않게 보내는 기법이다. 이때 각각의 데이터의 흐름을 스트림이라고 하는 것이다. HTTP/1의 경우는 하나의 TCP 연결에 하나의 스트림만 사용하기 때문에 HOLB 문제에서 벗어날 수 없었다. 또한 한번의 전송이 끝나게 되면 연결이 끊어지기 때문에 다시 연결을 만들기 위해서는 번거로운 핸드쉐이크 과정을 또 겪어야 했다. 비록 keep-alive 옵션을 통해 어느 정도의 시간 동안 연결을 유지할 수는 있지만 결국 일정 시간 안에 액세스가 없다면 연결이 끊어지게 되는 것은 똑같다. 그리고 HTTP/2는 하나의 TCP 연결 안에서 여러 개의 스트림을 처리하는 멀티플렉싱 기법을 도입하여 성능을 끌어올린 케이스이다. 이 경우 한번의 TCP 연결로 여러 개의 데이터를 전송할 수 있기 때문에 핸드쉐이크 횟수도 줄어들게 되어 효율적인 데이터 전송을 할 수 있게 된다. HTTP/3도 HTTP/2와 같은 멀티플렉싱을 지원한다. QUIC 또한 HTTP/2와 동일하게 멀티플렉싱을 지원하기 때문에, 이런 이점을 그대로 가져가고 있다. 혹여나 하나의 스트림에서 문제가 발생한다고 해도 다른 스트림은 지킬 수 있게 되어 이런 문제에서 자유로울 수 있다. 클라이언트의 IP가 바뀌어도 연결이 유지됨TCP의 경우 소스의 IP 주소와 포트, 연결 대상의 IP 주소와 포트로 연결을 식별하기 때문에 클라이언트의 IP가 바뀌는 상황이 발생하면 연결이 끊어져 버린다. 연결이 끊어졌으니 다시 연결을 생성하기 위해 결국 눈물나는 3 Way Handshake 과정을 다시 거쳐야한다는 것이고, 이 과정에서 다시 레이턴시가 발생한다. 게다가 요즘에는 모바일로 인터넷을 사용하는 경우가 많기 때문에 Wi-fi에서 셀룰러로 전환되거나 그 반대의 경우, 혹은 다른 Wi-fi로 연결되는 경우와 같이 클라이언트의 IP가 변경되는 일이 굉장히 잦아서 이 문제가 더 눈에 띈다. 반면 QUIC은 Connection ID를 사용하여 서버와 연결을 생성한다. Connection ID는 랜덤한 값일 뿐, 클라이언트의 IP와는 전혀 무관한 데이터이기 때문에 클라이언트의 IP가 변경되더라도 기존의 연결을 계속 유지할 수 있다. 이는 새로 연결을 생성할 때 거쳐야하는 핸드쉐이크 과정을 생략할 수 있다는 의미이다. 마치며사실 HTTP/3와 QUIC을 제대로 설명하기 위해서는 네트워크에 대한 기본 개념들이 필수적으로 동반되야하기 때문에 이 짧은 포스팅 만으로 세부적인 설명을 하기 힘든 부분이 있었다. 최대한 자세하게 작성해보려고 했지만 생각보다 글이 너무 길어지게 되어서 분량 조절을 조금 하려고 한다. 이번에 HTTP/3를 공부해보고 여러가지 자료를 찾아보면서 느낀 점은 “뭐가 이렇게 많이 바뀌었어?” 였던 것 같다. 일단 TCP부터 갖다 버렸으니 뭐가 많이 바뀔만 하긴 했지만, HTTP/2를 사용해본지도 몇 달 밖에 되지 않은 필자의 입장에서는 조금 당황스럽기는 했다.(HTTP를 만든다면서 TCP를 갖다 버린 건 아직도 신기하다) 사실 개발자들이 HTTP/2를 사용하든 HTTP/3를 사용하든 한국에서 인터넷을 사용하고 있는 사용자는 별로 큰 차이를 못 느낄 것이다. 한국은 워낙 땅덩이도 작고 통신 인프라도 좋다보니 핸드쉐이크 레이턴시고 나발이고 그냥 인프라로 대충 커버칠 수 있지만, 그래도 상대적으로 통신 인프라가 빈약한 나라의 경우에는 꽤 큰 차이가 느껴질 수도 있을 것 같다. 필자가 이 포스팅에서는 HTTP/3와 UDP의 장점만을 이야기했지만, 사실 많은 사람들이 TCP를 버리고 UDP로 갈아타는 것에 대해서 걱정하고 있다. 당연히 완벽한 기술이란 없으니 문제도 있을 것이다. 그러나 기존의 HTTP와 TCP가 가지고 있는 한계를 돌파하기 위한 시도로는 굉장히 좋은 것 같다. 마치 엔비디아의 RTX 시리즈 같은 느낌이랄까. 이상으로 HTTP/3는 왜 UDP를 선택한 것일까? 포스팅을 마친다. 참고 링크 The QUIC Transport Protocol: Design and Internet-Scale Deployment QUIC vs TCP+TLS - and why QUIC is not the next big thing HTTP/3: the past, the present, and the future QUIC과 HTTP/3 - 1.UDP기반 전송 프로토콜의 대두 QUIC, a multiplexed stream transport over UDP Using TLS to Secure QUIC The Road to QUIC","link":"/2019/10/08/what-is-http3/"},{"title":"기존의 사고 방식을 깨부수는 함수형 사고","text":"최근 많은 언어들이 함수형 프로그래밍 패러다임을 도입하며, 이에 대한 개발자들의 관심 또한 나날히 높아지고 있다. 필자 또한 함수형 사고라는 책을 읽으면서 기존의 패러다임과 사뭇 다른 함수형 프로그래밍에 대해 많은 관심을 가지게 되었던 기억이 있다. 재밌어서 여러 번 읽고 있는 귀여운 다람쥐 책 물론 이 책을 읽었다고 해서 함수형 프로그래밍을 자유롭게 할 수 있는 것은 아니다. 함수형 프로그래밍에서 사용하는 커링, 모나드, 고계 함수와 같은 개념와 기법들은 열심히 공부해서 이해하고 많이 써보면 금방 익숙해질 수 있는 것들이지만, 앞서 이야기했듯이 어떤 패러다임을 사용하여 프로그램을 제대로 설계하기 위해서는 말 그대로 사고 방식 자체를 바꿔야하기 때문이다. 그런 이유로 이번 포스팅에서는 함수형 프로그래밍의 스킬들보다는 함수형 프로그래밍이 왜 이렇게 각광받는지, 또 이 패러다임이 어떤 개념들을 사용하여 프로그램을 바라보고 있는지에 대한 이야기를 해보려고 한다. 함수형 프로그래밍을 왜 알아야하나요?사실 객체지향적 사고와 명령형 프로그래밍을 사용하기만 해도 왠만한 프로그램을 설계하고 작성하는 데는 무리가 없다. 우리는 이미 몇십 년간 이러한 사고방식으로 거대한 프로그램을 만들어왔지 않은가? 그러나 함수형 프로그래밍이 이렇게 주목받는 이유는 분명 기존의 그것들과는 분명한 차이가 있기 때문이고, 그 차이로 인해 개발자들이 조금 더 생산성있는 프로그래밍을 할 수 있기 때문일 것이다. 그렇다면 도대체 함수형 프로그래밍의 어떤 점이 개발자들의 마음을 움직였던 것일까? 뭐 개발자마다 각자 다른 이유들을 가지고 있겠지만, 일단 필자가 느꼈던 함수형 프로그래밍의 대표적인 장점은 바로 이런 것들이다. 높은 수준의 추상화를 제공한다 함수 단위의 코드 재사용이 수월하다 불변성을 지향하기 때문에 프로그램의 동작을 예측하기 쉬워진다 아마 함수형 프로그래밍에 대해 설명하는 다른 블로그 포스팅이나 책들을 봐도 비슷한 점을 함수형 프로그래밍의 장점으로 이야기하고 있을 것이다. 이 장점들 중에서 불변성에 대한 것은 함수형 프로그래밍 자체의 장점이라기보다는 순수 함수(Pure Functions)를 사용함으로써 자연스럽게 따라오는 장점에 가깝기 때문에, 여기에 대한 이야기는 추후 순수 함수에 대한 설명을 할 때 다시 이야기하도록 하고, 이번 포스팅에서는 높은 수준의 추상화와 함수 단위의 재사용라는 키워드에 대해 초점을 맞춰서 이야기해보려고 한다. 높은 수준의 추상화기본적으로 함수형 프로그래밍은 선언형 프로그래밍의 특성을 함수들의 조합을 사용하여 구현하는 패러다임이고, 선언형 프로그래밍의 대표적인 장점 중 하나가 바로 명령형 프로그래밍이나 객체지향 프로그래밍에서 사용하는 방법들보다 높은 수준의 추상화를 통해 개발자가 문제 해결에만 집중할 수 있게 해준다는 점이다. 선언형 프로그래밍이 제공한다는 높은 수준의 추상화라는 것이 정확히 무엇을 의미하는 것이고 어떤 장단점이 있는지 이해하기 위해 먼저 추상화에 대한 개념적인 정리가 먼저 필요할 것 같다. 추상화의 정확한 개념아마 객체지향 프로그래밍을 알고 있는 분들이라면 추상화가 현실에 존재하는 무언가의 특징을 뽑아내어 정리하는 행위라고 이야기할지도 모르겠다. 뭐 이것도 틀린 말은 아니고, 객체지향 프로그래밍을 처음 공부할 때는 이렇게 이해하는 것이 더 편하기도 하다. 하지만 사실 추상화라는 단어의 근본적인 의미는 단순히 객체의 특징을 정리하여 클래스를 정의하는 것보다 훨씬 넓은 범위의 개념이다. 추상화의 근본적인 의미는 어떤 작업을 수행할 때 그 이면에 존재하는 복잡한 것들을 간단한 것처럼 보이게 만들어주는 것들에 가깝다. 객체지향 프로그래밍에서 객체를 추상화하여 클래스를 정의하는 행위 또한 근본적으로는 이 정의에 부합한다. 자, 이름을 가지고 있고 자신의 이름을 말할 수 있는 사람을 추상화한 Human 클래스를 사용하여 객체를 생성하고 메소드를 사용한다고 생각해보자. 1234567891011121314class Human { private name: string = ''; constructor (name: string) { this.name = name; } public say (): string { return `Hello, I am ${this.name}.`; }}const me = new Human('Evan');console.log(me.say()); 1Hello, I am Evan. 여기서 중요한 사실이 하나 있다. 대부분의 경우 우리가 직접 클래스도 만들고 객체도 만들기 때문에 쉽게 지나치는 사실이지만, 사실 Human 클래스가 어떻게 구현되었는지 전혀 모르는 상태라도 이 클래스를 사용하여 객체를 생성하고 메소드를 사용하는데는 아무 문제가 없다는 것이다. 그냥 이 클래스가 외부로 노출하는 기능이 무엇인지만 알고 있으면 객체를 생성하고 메소드를 호출하여 원하는 동작을 이끌어낼 수 있다. 1234interface { constructor: (string) => Human; say: () => string;} 객체지향 프로그래밍에서는 사용자에게 높은 수준의 추상화를 제공하기 위해 public, private과 같은 접근 제한자를 사용하여 클래스 외부로 노출시키고 싶은 것만 노출시키는 캡슐화라는 기법을 사용한다. 또한 클래스 외에 일반적으로 우리가 사용하는 라이브러리들도 일종의 추상화된 모듈이라고 할 수 있다. 만약 우리가 React나 RxJS와 같은 라이브러리를 사용할 때 해당 라이브러리의 구현체를 전부 파악해야만 사용할 수 있다면 굉장히 힘들것이다. 그러나 우리는 라이브러리의 구현 소스를 일일히 파악하지 않더라도 공식 문서를 통해 기능을 파악하기만 한다면 일단 사용하는 데는 별 지장이 없다. (잘 쓰려면 봐야한다는 게 함정) 그리고 우리가 프로그램을 작성할 때 유용하게 사용하고 있는 OS API나 브라우저 API와 같은 API들도 일종의 추상화된 기능 리스트이다. 이런 추상화된 기능 리스트가 있기에 우리는 어떤 API가 어떤 동작을 하는 지만 알고 있다면, 로우 레벨의 동작을 직접 다루지 않더라도 컴퓨터에게 편하게 명령을 내릴 수 있는 것이다. 추상화된 OS API만 안다면 하드웨어 구조를 모르더라도 프로그램을 만들 수 있다 만약 개발자가 아닌 분이라면 여러분이 평소에 사용하는 일반적인 프로그램을 생각해보면 된다. 예를 들어 여러분이 포토샵을 사용하여 컬러 사진을 흑백 사진으로 변경한다고 생각해보자. 포토샵은 이미지 프로세싱이라는 복잡한 연산을 수행하는 프로그램이지만, 우리는 포토샵이 제공하는 여러가지 기능들을 사용하여 사진 보정이라는 행위에만 집중할 수 있다. 실제로 컬러 사진을 흑백 사진으로 변경할 때는 행렬로 이루어진 사진의 픽셀 데이터를 순회하며 RGB 값의 평균을 내거나하는 등의 과정을 수행해야한다. 하지만 그런 복잡하고 귀찮은 과정이 추상화되어있기 때문에 사용자는 그저 포토샵이 외부로 노출해준 기능인 Image > Adjustments > Desaturate을 사용하면 되는 것이다. 즉, 추상화란 복잡한 무언가에서 핵심적인 개념이나 기능을 간추려내어 단순하게 만드는 것을 의미하며, 추상화가 잘 되어있는 프로그램을 사용하는 사용자는 자신과 맞닿은 추상 계층 밑에 무엇이 있고 어떻게 작동하는지 모르더라도 해당 기능을 편하게 사용할 수 있다. 추상화의 수준이 높으면 좋은 건가요?방금 알아본 바와 같이 추상화란, 복잡한 무언가를 단순해보이도록 만들어주는 행위를 의미한다. 즉, 추상화의 수준이 높다는 것은 복잡한 것을 단순해보이도록 만드는 행위 자체의 수준이 높아졌다는 것을 의미하는 것이다. 그런 의미에서 보면 확실히 추상화의 수준이 높을 수록 사용자가 편하긴 하다. 이 편하다는 의미가 잘 감이 안오시는 분들을 위해 더 쉽게 이야기해보면, 우리가 사용하는 프로그래밍 언어를 예시로 들어볼 수 있겠다. 우리는 프로그램을 만들 때 0과 1로 이루어진 기계어를 사용할 수도 있고, 어셈블리를 사용할 수도, 자연어에 가까운 고급 언어인 자바를 사용할 수도 있다. 만약 길가는 개발자를 붙잡고 “기계어로 코딩할래? 어셈블리로 코딩할래?”라고 묻는다면 기계어로 코딩하고 싶다는 변태는 거의 없을 것이다. 기계어는 거의 추상화되지 않은 날 것이나 마찬가지이기 때문에 사람이 이해하기 너무 어렵기 때문이다. 또한 “어셈블리로 코딩할래? 자바로 코딩할래?”라고 물어본다면 어셈블리라고 대답하는 변태 또한 그렇게 많지 않을 것이다. 이 경우에는 어셈블리가 자바에 비해 추상화 수준이 매우 낮기 때문에, 자바로 코딩하는 것이 사람에게는 더 편한 것이다. 이게 바로 대표적인 추상화 수준의 차이이다. 어셈블리로 거대한 프로그램을 작성하게 되면 개발자가 신경써줘야하는 것이 너무나도 많지만, 자바를 사용하면 for, if 등의 문법과 다양한 API를 사용하고 조합하여 프로그램을 작성함으로써 어셈블리보다 좀 더 편하게 프로그래밍 할 수 있다. 메모리에 값을 할당하고 레지스터로 모았다가 다시 빼내고하는 잡다한 일은 JRE(Java Runtime)가 다 알아서 해줄 것이기 때문이다. 이렇게 예전의 기술보다 높은 수준의 추상화를 제공하며 사용자에게 편의를 안겨주는 경우는 함수형 프로그래밍이 처음이 아니고, 오히려 컴퓨터 공학 역사에서 굉장히 빈번하게 발생되었던 일이다. 이렇게 기술이 발전하며 추상화 수준을 높혀감으로써 점점 더 복잡한 프로그램을 만들 수 있게 되는 것이다. 그러나 추상화의 수준이 높다는 것이 장점만 있는 것은 아니다. 이런 높은 추상화를 장점으로 내세우는 기술이 발표되면 높은 확률로 “성능이 안 좋다”, “이렇게 추상화해버리면 최적화는 어떻게 하냐”와 같은 논란에 휩싸이게 되는데, 이런 논란에 휩쓸렸던 대표적인 친구들이 바로 자바나 가비지 컬렉터이다. (그리고 어셈블리가 있다…) 존 폰 노이만 (1903 ~ 1957) 괜히 어셈블리 같은 걸 만들어서 컴퓨터 성능 낭비하지 말라고 하신 분 자바의 경우에는 바이트 코드와 JVM(Java Virtual Machine)이라는 개념을 사용하여 한번만 코드를 작성해도 모든 OS에서 동작하는 프로그램을 작성할 수 있다는 점이 굉장한 강점이었다. OS나 CPU에 종속되어있던 기존의 프로그래밍 언어들에 비해 추상화 수준이 높아진 것이다. 그러나 처음 자바가 나왔을 당시에는 C에 비해서 너무 느려서 못 써먹을 물건이라고 상당히 많이 까였다. 또한 가비지 컬렉터도 개발자가 일일히 메모리를 할당하고 해제하지 않아도 되는 편리함을 제공하지만 GC가 객체의 메모리 해제 시점을 매번 추적하고 있어야하는 성능 문제나, 개발자가 객체가 메모리에서 해제되는 시점을 정확히 알기 어렵다던가, 참조 횟수 계산 방식을 사용할 때 순환 참조 객체를 해제하지 못하는 등 문제들이 여전히 존재하기 때문에 늘 완벽하게 동작하지는 않는다. 하지만 그렇다고 자바나 가비지 컬렉터나 자바를 사용할 수 있는 환경에서 굳이 C를 사용하고 수동으로 메모리를 관리하고 싶어하는 사람이 많지는 않을 것이다. 애초에 이런 높은 수준의 추상화를 제공하는 기술이 가지는 성능 상의 단점을 머신이 어느 정도 커버할 수 있기 때문에 사람들이 많이 사용하는 것이기도 하다. 함수형 프로그래밍도 높은 수준의 추상화를 지향하는 패러다임인 만큼 어느 정도 성능 면에서 불리한 점이 있긴 하다. 사실 함수 단위로 프로그램을 추상화하는 개념은 1958년에 LISP가 발표되었을 때부터 시작되었지만, 당시에는 아마 이런 개념이 사치로 느껴졌을 것이다. 메모리가 너무 부족해서 당장 작업 하나 하기도 빡센 데 추상화는 무슨 추상화란 말인가. 그러나 요즘 머신의 성능은 함수형 프로그래밍이 지향하는 추상화 레벨을 충분히 커버할 수 있을 정도로 예전에 비해 많이 좋아졌기 때문에 성능에 관한 문제는 크게 중요하지 않다고 느껴진다. 그리고 만약 성능이 진짜 중요한 프로그램을 작성할 때는 굳이 함수형 프로그래밍을 고집하지 않아도 된다. 애초에 이런 패러다임은 어떤 정답이라고 할 게 없기 때문이다. 지금도 최적화를 위해 C나 어셈블리를 사용하는 경우가 있는 것처럼, 함수형 프로그래밍도 상황에 따라 적재적소에 잘 사용하면 된다. 그럼 함수형 프로그래밍이 제공하는 높은 수준의 추상화가 우리에게 어떤 형태로 나타나는지 알아보기 위해, 기존에 우리에게 익숙한 패러다임인 명령형 프로그래밍과 함수형 프로그래밍의 상위 개념인 선언형 프로그래밍을 한번 비교해보도록 하자. 명령형과 선언형은 사고 방식이 다르다명령형 프로그래밍은 문제를 어떻게 해결해야 하는지 컴퓨터에게 명시적으로 명령을 내리는 방법을 의미하고, 선언형 프로그래밍은 무엇을 해결할 것인지에 보다 집중하고 어떻게 문제를 해결하는 지에 대해서는 컴퓨터에게 위임하는 방법을 의미한다. 처음 프로그래밍이라는 개념이 등장했을 때부터 비교적 최근까지도 우리는 컴퓨터에게 명시적으로 명령을 내리는 방법인 명령형 프로그래밍을 주로 사용해왔지만, 함수형 프로그래밍은 문제를 해결하는 방법에 더 집중하고 사소한 작업은 컴퓨터에게 넘겨버리는 선언형 프로그래밍의 일종이다. 이처럼 컴퓨터에게 사소한 작업들을 위임해버리는 패러다임의 특성 상, 선언형 프로그래밍에는 필연적으로 높은 수준의 추상화라는 키워드가 따라오는 것이다. 추상화 수준이 낮다면 저 사소한 작업들을 개발자가 일일히 다 컨트롤해줘야한다는 이야기니 말이다. 자, 그럼 명령형 프로그래밍과 선언형 프로그래밍을 사용하여 문제를 해결하는 과정을 비교해보며 이 두 패러다임 간의 추상화 수준의 차이에 대해서 한번 살펴보도록 하자. 우리가 해결해야하는 문제는 바로 이것이다. 1const arr = ['evan', 'joel', 'mina', '']; 배열을 순회하며 빈 문자열을 걸러내고, 각 원소의 첫 글자를 대문자로 변경해라. 어떻게 문제를 해결할까? (명령형 프로그래밍)명령형 프로그래밍은 말 그대로 컴퓨터에게 어떻게 작업을 수행할 지에 대한 자세한 명령을 내리는 것이다. 더 정확히 말하자면, 어떻게 문제를 해결하는지 일일히 명령을 내리면서 내가 원하는 결과를 만들어나가는 방식이라고 할 수 있다. 사실 오늘날 현업에서 일하고 있는 대부분의 개발자는 처음 프로그래밍을 접할 때 전통적인 패러다임인 명령형 프로그래밍으로 공부를 시작했던 경우가 많기 때문에, 필자를 포함한 많은 개발자들에게 익숙한 방법이라고도 할 수 있다. 대부분의 개발자는 이런 문제를 만났을 때, for문과 같은 반복문을 사용하여 순차적으로 배열의 원소를 탐색하고 작업하는 코드를 떠올릴 것이다. 12345678const newArr = [];for (let i = 0; i < arr.length; i++) { if (arr[i].length !== 0) { newArr.push( arr[i].charAt(0).toUpperCase() + arr[i].substring(1) ); }} 이렇게 for문을 사용하여 특정 행위를 반복하는 코드를 작성하는 일은 눈 감고도 작성할 수 있을 정도로 개발자들에게 익숙한 로직이지만, 너무 익숙한 나머지 이 짧은 코드 안에도 많은 명령이 들어가 있다는 사실을 간과하고는 한다. 필자가 위 코드를 작성할 때 필자가 떠올렸던 생각을 대충 정리해보자면 이런 느낌이다. 변수 i를 0으로 초기화 i가 arr 배열의 길이보다 작다면 구문을 반복 실행 for문 내부의 코드의 실행이 종료될 때마다 i에 1씩 더함 arr 배열의 i 번째 원소에 접근 만약 원소의 길이가 0이 아니라면 원소의 첫 번째 글자를 대문자로 변경 이렇게 합쳐진 문자열을 newArr 배열에 삽입 필자가 코드를 작성할 때 생각했던 사고의 흐름은 필자가 컴퓨터에게 내려야하는 명령과 정확하게 매칭된다. 즉, 필자는 i라는 상태를 직접 관리해야하며, 매 루프 때마다 i에 1을 더 해가면서 배열의 어느 인덱스까지 탐색했는지도 신경써줘야 하는 상황인 것이다. 게다가 배열의 원소에 접근할 때도 i를 사용하여 직접 접근 명령을 내려야하고 이 원소가 빈 문자열인지 아닌지 여부도 검사해줘야한다. 이렇게 컴퓨터에게 일일히 명령을 내려서 자신이 원하는 결과를 만들어가는 과정을 통해 프로그램을 작성하는 방식을 명령형 프로그래밍이라고 하는 것이다. 자 이 쯤에서 우리가 해결해야하는 문제를 다시 한번 보자. 배열을 순회하며 빈 문자열을 걸러내고, 각 원소의 첫 글자를 대문자로 변경해라. 저 명령들을 쭉 읽어보고 바로 이 문제를 떠올릴 수 있을까? 물론 이 문제는 굉장히 간단한 문제이기 때문에 바로 알아챌 수도 있겠지만, 이 문제보다 더 복잡한 문제라면 아마 몇 번은 읽어보고 그림도 그려봐야하지 않을까? 즉, 명령형 프로그래밍은 사람이 생각하는 방식보다 컴퓨터가 생각하는 방식에 가깝기 때문에 그리 인간 친화적인 방식은 아니다. 그렇기 때문에 개발자들은 알고리즘 문제 풀이 등을 통해 이런 방식의 사고를 하는 것을 꾸준히 연습하기도 한다. 자, 그럼 선언형 프로그래밍으로 같은 일을 수행하는 코드를 작성하면 어떻게 바뀔까? 무엇을 해결할까? (선언형 프로그래밍)반면 선언형 프로그래밍은 컴퓨터에게 나 이거 할거야!라고 알려주기만 하는 느낌이다. 잡다한 일 처리는 컴퓨터가 알아서 하도록 위임해버리고 개발자는 문제 해결을 위해 무엇을 할 지만 신경쓰면된다. 즉, 함수형 프로그래밍은 이런 선언형 프로그래밍의 특성을 함수를 통해 구현하게되는 패러다임이라고 할 수 있는 것이다. 그럼 방금 전과 같은 문제를 선언형 프로그래밍을 사용하여 구현해보도록 하자. 배열을 순회하며 빈 문자열을 걸러내고, 각 원소의 첫 글자를 대문자로 변경해라. 1234567function convert (s) { return s.charAt(0).toUpperCase() + s.substring(1);}const newArr2 = arr .filter(v => v.length !== 0) .map(v => convert(v)); 이 코드에서 사용된 filter 메소드는 배열을 순회하며 콜백 함수의 반환 값이 참이 아닌 원소를 걸러낸 새로운 배열을 생성 후 반환하는 역할을, map 메소드는 배열을 순회하며 콜백 함수의 반환 값을 사용한 새로운 배열을 생성한다. 이 코드를 작성할 때 필자의 사고의 흐름은 다음과 같았다. 인자로 받은 문자열의 첫 글자만 대문자로 변경하는 함수를 선언 arr 배열에서 원소의 길이가 0이 아닌 것들을 걸러냄 걸러진 배열을 순회하면서 1번에서 선언한 함수를 사용하여 원소의 첫글자를 대문자로 변경 분명 같은 작업을 수행하는 코드를 작성했지만 사고의 흐름이 많이 다른 것을 볼 수 있다. 물론 내부적으로는 아까 명령형 프로그래밍을 사용할 때의 필자 사고 방식과 유사한 방법으로 처리되겠지만, 적어도 필자가 자잘한 인덱스 변수의 선언이나 관리에 대해서 생각할 필요는 없어졌다. 그리고 더 중요한 점은 필자가 해결해야하는 문제와 사고 방식의 흐름이 비슷해졌다는 것이다. 배열을 순회하며 빈 문자열을 걸러내고(filter), 각 원소의 첫 글자를 대문자로 변경해라(convert + map). 이렇게 선언형 프로그래밍은 개발자가 문제의 본질에 집중할 수 있게 만드는 것에 초점을 맞추고 있고, 결국 함수형 프로그래밍은 이런 선언형 프로그래밍의 패러다임을 함수를 사용하여 구현하는 것이다. 하지만 이렇게 사소한 제어를 컴퓨터에게 맡겨버린다는 것이 장점만 있는 것은 아니다. 여기서 발생하는 대표적인 트레이드오프는 바로 추상화를 설명할 때 이야기했던 성능이다. 필자가 명령형 프로그래밍을 사용하여 이 작업들을 수행한 경우 필자는 단 한 번의 루프 안에서 여러가지 작업을 수행했지만, 선언형 프로그래밍으로 작성한 예시는 filter와 map 메소드가 각각 전체 배열을 순회하기 때문에 성능 상 손해가 발생할 수 있는 것이다. 아무리 요즘 머신의 성능이 좋아져서 저 정도의 추상화 레벨을 커버할 수 있다지만 만약 탐색해야하는 원소의 개수가 10억개라면 이런 사소한 차이가 전체 프로그램의 성능에 지대한 영향을 끼칠 수도 있다. 그래서 필자가 함수형 프로그래밍도 상황에 따라 적재적소에 잘 사용해야한다고 이야기 했던 것이다. 객체로 이루어진 프로그램과 함수로 이루어진 프로그램자, 여기까지 명령형 프로그래밍을 사용할 때와 선언형 프로그래밍을 사용할 때 발생하는 사고의 차이에 대해 알아보았다. 위에서 이야기했듯이 함수형 프로그래밍은 선언형 프로그래밍이라는 패러다임을 함수들의 집합과 연산으로 구현한 것이기 때문에 명령형 프로그래밍과도 많이 비교당하지만, 프로그램을 객체들의 집합과 관계로 정의하는 객체지향 프로그래밍과도 많이 비교를 당하게 된다. 명령형 프로그래밍과 선언형 프로그래밍을 다루며 이야기했던 것과 마찬가지로, 객체지향 프로그래밍과 함수형 프로그래밍 간에도 어떠한 우위는 없다. 다만 서로의 차이에 따른 각기 다른 장단점이 있을 뿐이다. 그렇다면 객체지향 프로그래밍을 사용하지 않고 함수형 프로그래밍을 사용함으로써 가져갈 수 있는 장점은 무엇일까? 물론 여기에도 여러가지 장단점이 있겠지만 필자는 개인적으로 함수 단위의 코드 재사용이 더욱 쉬워진다는 것이 가장 큰 장점이라고 생각한다. 더 작게 쪼개어 생각할 수 있다기존의 객체지향 프로그래밍에 익숙한 개발자는 어떤 프로그램의 요구 사항을 들었을 때 머릿 속에 객체의 설계도가 떠오르게 된다. 그리고 이러한 객체들의 관계를 정의하여 거대한 프로그램을 만들기 위한 기반을 다져나간다. 객체지향패턴에서 객체란 멤버 변수(상태)와 메소드(행위)로 이루어진, 프로그램을 구성하는 최소 단위이기 때문에, 객체지향패턴을 사용할 때 우리는 이 객체보다 더 작은 무언가를 사용하여 프로그램을 설계할 수 없다. 우리가 객체지향패턴을 사용할 때는 객체를 생성하기 위해, 객체를 추상화하여 일종의 설계 도면 역할을 하는 클래스를 사용하여 객체가 가질 상태와 행위를 정의하게 된다. 123456789101112131415161718class Queue { private queue: T[] = []; // 내부 상태 // 메소드로 표현된 큐의 행위 public enqueue (value: T): T[] { this.queue[this.queue.length] = value; return this.queue; } public dequeue (): T { const head = this.queue[0]; const length = this.queue.length; for (let i = 0; i < length - 1; i++) { this.queue[i] = this.queue[i + 1]; } this.queue.length = length - 1; return head; }} 12const myQueue = new Queue();myQueue.enqueue(1); Queue 클래스는 하나의 배열을 내부 상태로 가지고 이 배열에 원소를 추가하고 제거하는 큐의 기능을 구현한 클래스이다. 이때 이 클래스의 메소드를 통하지않고 클래스의 내부 상태인 queue 배열을 외부에서 맘대로 접근해서 수정하는 행위를 막기위해 private 접근제한자를 사용하여 외부에서의 접근을 막아주었다. 사실 이 정도만 해도 일반적인 프로그래밍을 할 때 딱히 불편하거나 어려운 점은 없다. 하지만 여기에서 필자가 Stack이라는 클래스를 새로 만들면 어떻게 될까? 간단히 생각해보면 큐의 동작인 dequeue는 그대로 사용하고 enqueue의 동작만 반대로 바꿔줘도 훌륭한 스택이 구현될 것 같다. 그러나 이미 클래스의 메소드로 구현되어버린 dequeue를 다른 클래스에서 가져다 자신의 메소드처럼 사용할 수 있는 방법은 상속 밖에 없는데, 그렇다고 큐를 상속한 스택을 만들어버리면 객체 간의 관계가 꼬이기 시작할 것이다. 즉, 객체지향 프로그래밍에서 어떤 존재를 추상화하여 표현하고 재사용할 수 있는 최소 단위는 객체이기 때문에 그 이상 작게 쪼개기 힘들어지는 것이다. 하지만 함수형 프로그래밍에서는 객체로 표현된 큐나 스택이 아닌, 이 존재들이 자료를 다루는 동작에만 집중한다. 배열의 꼬리에 원소를 추가하는 동작 (push) 배열의 머리에서 원소를 빼오고 남은 원소를 앞으로 한 칸씩 당겨주는 동작 (shift) 이처럼 자바스크립트의 빌트인 메소드인 push와 shift를 사용하면 굳이 클래스나 객체를 선언하거나, 필자가 위에서 구현한 것처럼 명령형 프로그래밍으로 큐의 동작을 구구절절 작성하지 않더라도 큐의 동작을 완벽하게 구현할 수 있다. 또한 추가적으로 스택을 구현하고 싶다면, shift 메소드를 사용하여 원소를 빼오고 push 메소드 대신 unshift 메소드를 사용하여 원소를 추가하면 된다. 1234567const queue: number[] = [1, 2, 3];queue.push(4);const head = queue.shift();const stack: number[] = [1, 2, 3];stack.unshift(0);stack.shift(); 굉장히 당연하다고 느껴지겠지만, 이것이 객체 단위로 요소를 구성하는 것과 함수 단위로 요소를 구성하였을때 누릴 수 있는 근본적인 장점이다. 어떤 요소를 재사용할 수 있는 범위가 넓어지는 것이다. 다만 이렇게 작은 단위의 함수를 넓은 범위로 재사용하게 되면 프로그램의 복잡성이 빠르게 증가하기 때문에 이를 방어하기 위해서 함수가 함수 외부에 있는 값을 수정하면 안된다거나, 동일한 인자를 받은 함수는 동일한 값을 반환해야 한다거나 하는 몇 가지 제약 조건이 필요하게 된다. 이런 제약을 가진 함수를 바로 순수 함수(Pure Functions)라고 부르는 것이다. 근데 보통 순수 함수라는 개념을 설명하다보면 방금 필자가 이야기한 것처럼 “몇 가지 제약 조건이 있는 함수”라는 개념으로 설명하게 되는데, 이거 근본적으로 그냥 수학에서 사용하는 함수랑 거의 동일한 개념이다. 컴퓨터 공학의 함수는 수학의 함수에서 유래되기는 했지만, 이 두 개의 학문이 추구하는 방향과 발전되어온 과정이 꽤나 다르기 때문에 함수라는 개념도 이름만 똑같을 뿐, 사실은 서로 다른 부분이 많다. 즉, 순수 함수는 수학에서 이야기했던 함수의 본질 그 자체로 회귀하여 단순함을 확보하자는 개념에서 시작하는 것이다. 그런 이유로 필자는 개인적으로 순수 함수를 프로그래밍적인 관점에서 접근하여 이해하는 것보다 수학적인 관점에서 접근하여 이해하는 것이 더 쉽고 빠르다고 생각한다. 일단 이 포스팅의 주제는 함수형 프로그래밍보다는 함수로 사고하는 방식의 장단점과 특징에 대한 이야기이므로, 순수 함수나 불변성에 대한 이야기는 다음에 함수형 프로그래밍의 특징과 스킬을 설명할 때 조금 더 자세히 이야기해보도록 하겠다. (이것도 은근히 꿀잼이다) 마치며필자는 사실 얼마 전까지만 해도 함수형 프로그래밍에 대한 관심이 깊은 편이 아니었다. 명령형 프로그래밍과 객체지향적인 사고만으로도 대부분의 어플리케이션은 충분히 설계할 수 있다고 생각했기 때문이다. 하지만 이건 필자가 학교에서 처음 배웠던 패러다임이 명령형 프로그래밍과 객체지향 프로그래밍이었기 때문에 몸에 더 익어서 그랬던 것이다. 사실 어떤 패러다임을 자유자재로 다룰 수 있다는 것은 해당 패러다임이 요구하는 사고 방식에 이미 익숙해졌다는 이야기이기 때문에, 새로운 설계 패턴이나 패러다임을 익힌다는 것은 이런 기존의 사고 방식을 깨야하는 상황이기도 하다. 개인적으로 커링이나 고계 함수, 모나드 같은 것들을 익히는 것보다 이런 사고 방식을 바꾸는 것이 훨씬 더 어렵다는 생각을 한다. 솔직히 말하면 어느 정도 함수형 프로그래밍에 대해 공부하고 마음이 열린 상태인 지금도 어떤 요구사항을 들었을 때 명령형과 객체지향을 먼저 떠올리고 있기도 하고 말이다. 물론 포스팅에서 여러 번 이야기했듯이 함수형 프로그래밍이 객체지향 프로그래밍이나 명령형 프로그래밍을 대체하는 패러다임도 아닐 뿐더러, 이 패러다임들간에는 어떠한 우위도 없다. 각자의 장단점만 있을 뿐이고 상황에 따라 적당히 골라쓰면 되는 것이다. 게다가 아직까지 많은 라이브러리들이 객체지향적인 개념으로 설계되었고, 사용 방법 또한 대부분 멤버 변수와 메소드를 내장한 객체를 생성하여 사용하는 방법을 채택하고 있기 때문에, 이런 라이브러리와 내 프로그램을 연동하려면 프로그램의 전체적인 아키텍쳐 또한 객체지향으로 설계하는 것이 편하다는 점도 무시할 수는 없다. 일례로 자바스크립트 진형에서 상태관리 라이브러리로 자주 사용하는 Redux의 경우 순수 함수와 불변성을 기반으로 하여 상태 변경을 감지하게 되는데, 필자는 Web Audio API를 사용한 토이 프로젝트를 진행할 때 AudioNode 객체들의 상태를 Redux로 관리하는 것에 상당히 애를 먹고 있기도 하다. (이건 그냥 필자가 Redux를 못 쓰는 걸 수도…) 하지만 함수형 프로그래밍이 가져다주는 높은 수준의 추상화나 더 작은 수준의 코드 재사용과 같은 장점들은 분명히 복잡한 프로그램을 작성할 때 크게 도움이 되는 것들이다. 결국 함수형 프로그래밍을 잘 사용한다는 것은 단순히 이 패러다임을 깊게 이해하는 것보다는 이 패러다임이 어떤 상황에 적합한지 판단할 수 있는 능력 또한 포함하는 이야기라고 생각한다. 다음 포스팅에서는 함수형 프로그래밍에서 빼놓을 수 없는 키워드인 순수 함수, 불변성, 지연 평가와 같은 개념들과 프로그램의 복잡도를 낮추기 위해 사용하는 다양한 스킬들을 소개하는 포스팅을 작성할 예정이다. 이상으로 기존의 사고 방식을 깨부수는 함수형 사고 포스팅을 마친다.","link":"/2019/12/15/about-functional-thinking/"},{"title":"[JavaScript로 오디오 이펙터를 만들어보자] 나만의 소리 만들기","text":"이번 포스팅에서는 저번 포스팅에 이어 HTML5 Audio API를 사용하여 실제로 오디오 이펙터를 만드는 과정에 대해서 포스팅 하려고 한다. 저번 포스팅에서 이미 이야기 했듯이 Audio API는 여러 개의 노드를 연결하여 오디오의 흐름을 만들어 내는 것을 기본 개념으로 가지고 있고, 이펙터를 만들기 위해 필요한 몇 개의 추상화된 노드들을 기본적으로 제공해주기 때문에 그렇게 어려울 건 없다. 우리는 단지 우리가 만드려고 하는 이펙터들이 각각 어떤 역할을 하며, 어떤 원리를 가지고 있고, 어떤 용도로 사용되는지만 알고 있으면 된다. 오디오에 사용하는 이펙터는 그 종류가 굉장히 많기 때문에 모든 이펙터를 만들어 볼 수는 없고, 필자가 생각했을 때 가장 대표적으로 많이 사용되는 기본적인 이펙터 5개 정도를 구현해볼 생각이다. 기본적으로 오디오를 로드하여 소스 노드(Source Node)를 생성하는 과정은 이미 저번 포스팅에서 설명했기 때문에 따로 설명하지 않겠다. 이번 포스팅에서는 바로 이펙터를 구현하는 내용부터 설명한다. 모든 이펙터는 먼저 해당 이펙터가 하는 일과 원리에 대해서 간략하게 설명하고 이후 묻지도 따지지도 않고 바로 구현 들어가도록 하겠다. 자, 그럼 하나하나 뜯어보도록 하자. Compressor 컴프레서(Compressor)는 소리가 일정 크기 이상으로 커질 경우에 이를 꾹꾹 눌러서 다시 작은 소리로 만드는 일종의 압축기 역할을 하는 이펙터이다. 이렇게 소리의 크기를 조절하는 이펙터를 다이나믹 이펙터라고 한다. 기본적으로 오디오 소스를 사용할 때 기본적으로 컴프레서를 걸어놓고 믹싱을 시작하는 경우가 많은데, 이는 오디오 신호가 일정 크기 이상으로 갑자기 커졌을 때 발생하는 클리핑(Clipping) 현상을 방어하기 위해서이기도 하다. 그럼 여기서 한가지 의문이 들 수 있는데, 아니 단순히 클리핑을 막는 거면 그냥 Gain을 줄이면 해결되는 거 아니야? 맞다. 사실 게인을 줄여도 어느 정도 클리핑을 방어할 수는 있다. 하지만 일반적으로 음악이란 셈여림이 존재하기 때문에 무작정 게인을 낮추면 작은 소리는 아예 입력되지도 않는 슬픈 상황이 발생하게 된다. 예를 들어 여러분이 노래방에 갔을 때를 생각해보자. 일반적으로 발라드를 부른다면 노래의 도입부에서는 잔잔한 느낌으로 조용히 부르다가 후렴에서는 고음을 내기위해 성대를 통과하는 공기의 압력이 올라가며 음량이 커진다. 이때 무작정 게인을 낮춰서 녹음하는 방향으로 접근한다면 필연적으로 가장 큰 소리인 후렴의 빵빵 지르는 소리의 크기에 게인을 맞출 수 밖에 없고, 그러면 도입부의 잔잔한 부분은 거의 입력되지 않을 것이다. 창법에 따라 조금씩 다르지만 이 음량 차이는 생각보다 크다. 이때 컴프레서로 입력 게인을 적당한 수준으로 높혀주고 너무 큰 소리는 압축하여 노래 도입부의 작은 소리와 후렴부의 큰 소리의 격차를 좁혀 전체적인 소리의 크기를 맞추기 위해서 사용하는 것이다. Threshold를 넘어선 세기의 신호를 압축해서 Threshold 밑으로 들어가도록 만든다 또한 필자가 컴프레서 소리를 압축한다고 했는데, 소리를 압축한다는 것이 뭔지 잘 이해가 안갈 수 있다. 대표적인 예로 우리가 일반적인 음원에서 듣고 있는 퍽!, 탁! 하는 깔끔한 드럼소리가 바로 압축된 소리이다.(보통 이렇게 팍팍치는 소리를 Damping이라고 한다.) 일반적으로 드럼을 녹음하면 드럼 특유의 통이 울리는 잔향이 남는데, 이 소리를 컴프레서로 압축하면 우리가 일반적으로 듣는 깔끔한 드럼소리로 만들 수 있다. 그 외에도 베이스에 컴프레서를 사용하여 단단한 느낌을 부여하거나 멀리 있는 소리를 가까이로 끌어오거나 그 반대 역할도 할 수 있는 등, 컴프레서만 잘 사용해도 소리에 굉장히 많은 느낌을 부여할 수 있다. 그래서 필자에게 사운드 엔지니어닝을 알려주셨던 선생님도 컴프레서의 중요성을 굉장히 강조하셨던 기억이 난다. 컴프레서는 몇가지 값들을 사용하여 신호를 언제부터 압축할 것인지, 어느 정도의 속도로 압축할 것인지와 같은 세팅을 할 수 있도록 설계되었다. HTML5 Audio API에서 제공하는 DynamicsCompressorNode도 이 값들을 동일하게 제공하고 있으므로 우리는 이 값들이 어떤 의미를 가지고 있는지 알아야 올바른 방법으로 이 노드를 사용할 수 있다. ThresholdThreshold는 소리를 어느 크기부터 압축할 것인지를 정하는 임계점을 의미한다. 단위는 DB(데시벨)을 사용한다. RatioRatio는 Threshold를 넘은 소리가 어느 정도의 비율로 줄어들 것인지를 정하는 값이다. 이 값은 입력:출력의 비를 의미하기 때문에 일반적으로는 2:1, 5:1와 같은 비율로 이야기한다. 하지만 HTML5 Audio API의 속성에서는 단위가 조금 다르다. 공식 문서에는 출력 값의 1db를 변경하기 위해 필요한 db값이라고 적혀있는데 그냥 이 속성에 12를 할당하면 압축 비율이 12:1인거라고 생각하면 된다.(공돌이들 특징인 어렵게 말하기가 발동했다) 보통 컴프레서를 적당히 걸었다라고 하면 4:1 정도의 비율을 말하기 때문에 해당 속성의 기본 값인 12:1은 상당히 하드한 압축 비율이라고 할 수 있다. AttackAttack은 소리를 어느 정도의 빠르기로 압축할 것인지를 정하는 값이다. Threshold를 넘은 값을 얼마나 빠르게 때려서 눌러 담을 지를 정하면 된다고 생각하자. 많은 분들이 여기서 정해주는 어택 타임이 Attack이 시작되는 시간으로 잘못 알고 있는 데, 사실 신호의 크기가 Threshold를 넘으면 Attack 자체는 바로 시작된다. 우리가 정해주는 어택 타임은 정해진 Ratio로 정해준 비율까지 도달하는 데 걸리는 시간이다. 단위는 보통 밀리초(ms)를 사용하지만 Audio API에서는 초(seconds)를 사용한다. ReleaseAttack이 소리를 누르는 빠르기였다면 Release는 압축한 소리를 어느 정도의 빠르기로 다시 풀어줄 것인가를 정하는 값이다. 이때 풀어주는 값은 소리의 원래 크기가 아니라 표준 음량인 10db에 도달하는 시간을 목표로 한다. Release도 Attack과 마찬가지로 단위는 보통 밀리초(ms)를 사용하지만 Audio API에서는 초(seconds)를 사용한다. KneeKnee는 사실 대부분의 하드웨어 컴프레서에는 없는 기능이지만 소프트웨어 컴프레서에서는 꽤 자주 볼 수 있는 기능이다. 이 값은 컴프레서가 얼마나 자연스럽게 적용될 것인지를 결정한다. 위 그림의 그래프의 꺾이는 정도가 컴프레서가 얼마나 서서히 적용되는지를 보여주고 있다. 이때 빠르게 팍! 적용하는 컴프레션을 Hard하다고 하고 천천히 적용하는 컴프레션을 Soft하다고 한다. Compressror 구현해보기사실 위에서 이야기 했듯이 HTML5 Audio API는 자체적으로 DynamicsCompressorNode를 제공하기 때문에 우리가 소리를 압축하는 알고리즘을 직접 구현할 필요가 없다. 단지 노드를 생성한 후 연결해주기만 하면 될 뿐이다. 이번에는 사용자가 업로드한 오디오 파일에서 오디오 버퍼를 추출하여 소스 노드를 생성하는 것이 아니라 태그에서 추출하여 소스 노드를 생성하는 방식으로 진행하도록 하겠다.(이렇게 하면 코드가 훨씬 간단해진다) 지금 생성한 소스노드는 앞으로 다른 이펙터를 구현할 때도 계속 사용할 것이다. 123456789101112131415161718192021222324const audioContext = new (AudioContext || webkitAudioContext)();const audioDOM = document.getElementById('my-audio');const sourceNode = audioContext.createMediaElementSource(audioDOM);const threshold = -24;const attack = 0.003;const release = 0.25;const ratio = 12;const knee = 30;const compressorNode = audioContext.createDynamicsCompressor();compressorNode.threshold.setValueAtTime(threshold, audioContext.currentTime);compressorNode.attack.setValueAtTime(attack, audioContext.currentTime);compressorNode.release.setValueAtTime(release, audioContext.currentTime);compressorNode.ratio.setValueAtTime(ratio, audioContext.currentTime);compressorNode.knee.setValueAtTime(knee, audioContext.currentTime);const inputGainNode = audioContext.createGain();const outputGainNode = audioContext.createGain();sourceNode.connect(inputGainNode);inputGainNode.connect(compressorNode);compressorNode.connect(outputGainNode);outputGainNode.connect(audioContext.destination); 필자는 소스 -> 게인 -> 컴프레서 -> 게인의 순서로 오디오 소스의 흐름을 생성했는데, 사실 이건 개인의 취향이다. 하지만 일반적으로 대부분의 컴프레서는 인풋 게인과 아웃풋 게인을 모두 가지고 있으므로 필자도 이와 동일하게 구현했다. 이후 소스노드를 재생해보면 압축된 소리를 들을 수 있긴 한데, 사실 사운드 엔지니어가 아닌 일반인이 소리의 미세한 압축의 정도를 느끼기는 힘드므로 위의 값들을 조금 극단적으로 바꿔보는 것을 추천한다. Reverb 리버브(Reverb)는 소리에 울림을 통해 공간감을 부여하는 공간계 이펙터이다. 소리에 울림을 통해 공간감을 부여한다는 게 어떤 의미일까? 사실 우리는 소리를 듣고 현재 있는 공간이 넓은지 좁은지, 이 공간이 거친 벽면으로 이루어져 있는지, 아니면 유리같은 맨들맨들한 공간으로 이루어져 있는지를 대략적으로 파악할 수 있다. 그 차이가 워낙 미세해서 훈련되지 않은 사람이라면 알아채기 힘들 뿐이다. 어떻게 이런 일이 가능할까? 바로 소리의 반사에 의한 잔향 때문이다. 먼저, 소리를 듣고 공간의 크기를 감지하는 원리는 간단하다. 필자가 어떤 방 안에서 소리를 왁!하고 지른 뒤 얼마 후에 첫번째 반사음이 들리는지를 감지하면 된다. 하지만 이 첫번째 반사음은 ms 단위의 굉장히 빠른 속도로 다시 필자에게 돌아오기 때문에 1초, 2초 이렇게 세는 것이 아니라 그냥 느껴야하는 것이다. 이때 이 반사음을 초기 반사음(Early Reflection)이라고 한다. 하지만 여기서 끝이 아니다. 소리가 한번 반사되어 여러분의 귀로 전달된 뒤에도 반사는 계속 될 것이다. 이때 이 잔향들은 공간의 사방팔방으로 부딫히고 반사되어 여러분의 귀로 다시 돌아올 것이다. 초록색 선이 초기 반사음, 사방팔방 부딫히는 파란색 선이 바로 잔향이다 이때 이 잔향이 얼마나 오래 들리는가, 얼마나 선명하게 들리는가와 같은 특성이 방의 재질을 결정한다. 이야기만 들으면 이렇게 소리를 듣고 공간을 판별한다는 것이 불가능한 것 같지만 여러분이 이미 평소에 듣고 음악에는 모두 이 원리를 적용한 공간적 설계가 함께 담겨있다. 이렇게 리버브는 말 그대로 잔향을 만들어내기만 하면 되기 때문에 하드웨어 리버브 중에서는 스프링이나 철판 등의 재료를 장비 내부에 넣어놓고 오디오를 재생하여 재료가 떨리며 발생한 잔향을 증폭하는 방식을 사용하는 것도 있다. 즉, 뜯어보면 장비 내부에 스프링이나 철판 하나 딸랑 들어있다는 것이다.(이런 단순한 구조로 좋은 소리를 뽑는다는 게 더 무섭…) 그러나 리버브를 소프트웨어로 구현할 때는 이야기가 조금 다르다. 컴퓨터는 스프링이나 철판의 떨림과 같은 자연적인 아날로그 신호를 생성할 수 없으므로 직접 계산을 통해 구현해야한다. 이때 소프트웨어 리버브는 크게 두 가지 종류로 나누어지는데 바로 Convolution Reverb와 Algorithm Reverb이다. 하지만 이 포스팅에서 두 리버브를 모두 구현하기에는 글이 너무 길어질 것 같으므로 아쉬운대로 컨볼루션 리버브에 초점을 맞춰 진행하겠다.(알고리즘 리버브만 해도 포스팅 하나 분량이다.) Convolution Reverb컨볼루션 리버브(Convolution Reverb)는 실제 공간의 잔향을 녹음한 후에 잔향 오디오 소스와 원본 오디오 소스를 실제 공간의 울림을 원본 오디오 소스에 합성하는 방법이다. 이때 실제 공간의 잔향을 녹음하는 대표적인 방법을 간단하게 설명하자면, 녹음하고자 하는 공간에 순수한 사인파(Sine Wave)의 소리를 낮은 주파수부터 높은 주파수까지 쭈우우욱 이어서 틀고 그때 발생하는 잔향을 녹음하는 것이다. 공간의 IR을 녹음하는 모습 - Alan JS Han 블로그 이때 이 잔향 신호를 Impulse Response(IR)이라고 부르기 때문에 컨볼루션 리버브는 IR 리버브라는 이름으로도 불린다. 이렇게 녹음한 IR은 원본 소스에 컨볼루션(Convolution), 또는 합성곱이라고 불리우는 연산을 통해 합쳐지게 된다. 이 컨볼루션이라는 개념을 수학적으로 접근하기 시작하면 머리도 아프고 또 포스팅이 길어지니까 간단하게 정의해보자면, 그냥 서로 다른 정보들을 섞는 것이라고 표현할 수 있다. 이 포스팅을 읽는 분들은 아마 개발자 분들이 많을 테니 우리에게 좀 더 친숙한 머신러닝을 사용하여 컨볼루션을 설명하자면 학습 알고리즘 중 하나인 CNN(Convolution Neural Network)을 예로 들어볼 수 있겠다. CNN에서도 첫번째 레이어의 이미지를 두번째 레이어로 보낼 때 행렬로 구현한 커널(또는 필터)와 이미지를 섞어서 피처맵을 생성한 후 다음 레이어로 보내게된다. 이때 첫번째 레이어의 이미지와 커널의 정보가 섞인 것이라고 생각할 수 있다. 원본 이미지와 커널을 섞어서 새로운 정보인 피처맵을 만들어낸다 오디오에서의 컨볼루션 리버브도 이와 마찬가지다. 이 경우에는 섞어야하는 정보가 원본 소스와 IR이 된 것 뿐이다. 컨볼루션은 원본 소스와 IR이라는 두 오디오 소스의 주파수 스펙트럼을 곱하는 과정이기 때문에 이를 통해 두 소스 간에 겹치는 주파수는 강조되고 겹치지 않는 주파수는 감쇠된다. 이렇게 원본 소스와 IR 간 겹치는 주파수가 강조되면 원본 소스는 IR의 음질의 특성을 띄게 되는데, 이게 바로 컨볼루션 리버브의 원리이다. 원본 신호와 녹음한 IR 신호를 컨볼루션 연산한 모습 사실 HTML5 Audio API는 컨볼루션 연산을 대신 수행해주는 ConvolverNode를 제공하기 때문에 컨볼루션이 무엇인지 몰라도 컨볼루션 리버브를 만드는 데는 아무 문제가 없다. 그러나 적어도 이 이펙터가 2개의 신호 정보를 곱해서 새로운 신호를 만들어내는 원리를 가지고 있다는 것을 알아야 필자가 왜 이런 코드를 작성하는지도 알 수 있기 때문에 대략적인 설명을 하는 것이다. 어쨌든 컨볼루션 리버브의 대략적인 원리를 파악했다면 이제 바로 만들어보도록 하자. Convolution Reverb 구현해보기먼저 HTML5 Audio API는 ReverbNode 같은 건 제공하지 않는다. 하지만 위에서 설명했듯이 컨볼루션 연산을 지원하는 ConvolverNode를 제공해주고 있기 때문에 우리는 잔향 소스인 IR(Impulse Response)만 직접 만들어주면 된다. 그리고 일반적으로 리버브는 wet과 dry라는 수치로 원본 소스와 잔향 소스를 비율에 맞게 섞을 수 있도록 제작되므로 필자도 동일하게 코드를 작성하겠다. 123const mix = 0.5;const time = 0.01;const decay = 0.01; 리버브의 사용할 3개의 변수를 먼저 설명하자면, mix는 wet/dry의 비율을 의미하고, time은 잔향의 길이, decay는 잔향이 감소하는 빠르기를 의미한다. 그럼 이제 이 값들을 사용하여 직접 IR을 생성해보자. 123456789101112131415function generateImpulseResponse () { const sampleRate = audioContext.sampleRate; const length = sampleRate * time; const impulse = audioContext.createBuffer(2, length, sampleRate); const leftImpulse = impulse.getChannelData(0); const rightImpulse = impulse.getChannelData(1); for (let i = 0; i < length; i++) { leftImpulse[i] = (Math.random() * 2 - 1) * Math.pow(1 - i / length, deacy); rightImpulse[i] = (Math.random() * 2 - 1) * Math.pow(1 - i / length, deacy); } return impulse;} 뭔가 복잡해보이지만 뜯어보면 별 거 없다. sampleRate는 우리가 생성하고자 하는 IR의 샘플레이트, 즉 음질을 의미하고 length는 sampleRate * time, 즉 time초 만큼의 잔향을 표현하기 위한 버퍼의 길이를 의미한다. 그리고 그냥 버퍼 노드를 하나 생성한 다음 -1 ~ 1의 무작위 값을 생성한 후 1 - i / length에 decay를 제곱한 후 방금 생성한 난수에 곱해준다. 이러면 i값이 커질수록 값이 작아질 것이고, deacy 값이 커질수록 더 빠르게 작아질 것이다. 이는 잔향의 감쇠를 표현 해준 것이다. 이후 이 샘플을 방금 만든 버퍼 노드에 쭈르륵 담아주면 끝이다. 이렇게 생성된 IR 버퍼를 파형으로 표현해보면 대략 다음과 같은 모양을 가질 것이다. 쨘, 이렇게 간단하게 IR를 생성해보았다. 이제 ConvolverNode를 사용하여 원본 소스와 이 IR을 합성해주는 것만 남았다. 리버브 이펙터의 오디오 흐름을 만들기 위해서 필요한 노드들을 먼저 생성해보자. 12345const inputNode = audioContext.createGain();const wetGainNode = audioContext.createGain();const dryGainNode = audioContext.createGain();const reverbNode = audioContext.createConvolver();const outputNode = audioContext.createGain(); 위에서도 설명했듯이 일반적인 리버브 이펙터는 wet/dry라는 수치를 사용하여 원본 소스와 리버브가 적용된 소스를 섞어서 출력하는 기능을 제공한다. 이때 dry한 소스는 리버브 이펙터를 거치지 않고 바로 outputNode로 연결되서 출력되어야 하며, wet한 소스는 우리가 만든 reverbNode를 한번 거치고 outputNode로 출력되어야 한다. 1234567891011121314151617sourceNode.connect(inputNode);// Dry 소스 노드 연결inputNode.connect(dryGainNode);dryGainNode.connect(outputNode);dryGainNode.gain.value = 1 - mix;// IR을 생성하여 Convolver의 오디오 버퍼에 입력해준다.reverbNode.buffer = generateImpulseResponse();// Wet 소스 노드 연결inputNode.connect(reverbNode);reverbNode.connect(wetGainNode);webGainNode.connect(outputNode);wetGainNode.gain.vaule = mix;outputNode.connect(audioContext.destination); 이렇게 컨볼루션 리버브를 간단하게 구현해보았다. 사실 컨볼루션 리버브의 퀄리티에 가장 큰 영향을 끼치는 것은 IR의 퀄리티인데, 우리는 대충 만든 샘플 오디오로 IR을 만들었으므로 이 리버브의 퀄리티는 좋을 수가 없다. 그러나 소스 노드를 재생해서 들어보면 신기하게도 소리에 공간감이 부여된 것을 들어볼 수 있다. 만약 기회가 된다면 다음에는 알고리즘 리버브의 구현체도 한번 포스팅 해보도록 하겠다. 알고리즘 리버브는 실제 공간의 잔향을 녹음하여 사용하는 컨볼루션 리버브와는 다르게 100% 알고리즘으로만 구현된 리버브이다. 그렇기 때문에 약간 인위적인 느낌이 나기는 하지만 컨볼루션 리버브와는 또 색다른 느낌을 부여할 수 있으므로 사운드 엔지니어들은 이 두가지 리버브의 특성을 파악하고 적재적소에 사용한다. 그렇기 때문에 개발자들에게는 오히려 컨볼루션 리버브보다 알고리즘 리버브 쪽이 더 이해가 잘될 수 있으나, ConvolverNode 하나와 대충 만든 IR만 있으면 나머지는 알아서 다 연산해주는 컨볼루션 리버브와는 다르게 알고리즘 리버브는 진짜 밑바닥부터 만들어야한다. 그래서 아쉽지만 알고리즘 리버브는 다음에 포스팅 하도록 하겠다. 만약 알고리즘 리버브의 구현체가 궁금하신 분은 필자의 깃허브 레파지토리에서 확인해볼 수 있다. Delay 딜레이(Delay)는 리버브와 같은 공간계 이펙터이고 소리를 반복해서 들려준다는 점이 같기 때문에 비슷하다고 생각할 수 있지만 그 원리와 용도는 많이 다르다. 먼저, 딜레이는 단순히 소리를 반복하는 효과이지만 리버브는 공간 내에서의 복잡한 반사음을 흉내내는 것이므로 딜레이만 사용하면 리버브와 같은 자연스러운 공간감을 표현하기가 힘들다. 방금 만들어봤던 리버브 이펙터는 사실적인 공간 표현이 목적이기 때문에 컨볼루션이나 복잡한 알고리즘을 사용하지만 딜레이는 그냥 원본 소스를 잠깐 지연시켰다가 n초 후에 다시 틀어주면서 조금씩 소리를 작게 해주면 끝이다. Echo(Delay)와 Reverb의 차이 딜레이는 이렇게 간단한 원리를 가지고 있기 때문에 만드는 것 역시 그렇게 어렵지 않다. Delay 구현해보기HTML5의 Audio API는 인풋으로 받은 신호를 지연시켜서 다시 출력하는 Delay Node를 제공해주기 때문에 우리는 이 노드를 사용하여 간단하게 딜레이 이펙터를 구현할 수 있다. 그러나 단순히 DelayNode만 사용한다면 단 한번의 지연만 발생시킬 수 있기 때문에 우리는 한가지 얌생이를 사용하여 딜레이를 구현할 것이다. 먼저 딜레이에 필요한 변수들을 선언해보도록 하자. 123const mix = 0.5;const feedback = 0.5;const time = 0.3; 리버브와 마찬가지로 대부분의 딜레이 이펙터도 wet/dry 값을 사용하여 원본 소스와 딜레이된 소스를 섞어서 출력해주는 기능을 가지고 있기 때문에 필자도 동일하게 구현해줄 것이다. 그리고 feedback 변수는 원본 소스가 한번 딜레이될 때 감소시킬 음량이고 time 변수는 메아리들의 간격을 의미한다. 딜레이에 사용할 변수들을 모두 선언했다면 이제 노드들을 만들 차레이다. 123456const inputNode = audioContext.createGain();const wetGainNode = audioContext.createGain();const dryGainNode = audioContext.createGain();const feedbackNode = audioContext.createGain();const delayNode = audioContext.createDelay();const outputNode = audioContext.createGain(); webGainNode와 dryGainNode는 리버브와 동일하기 때문에 그냥 넘어가고, 새로운 노드인 feedbackNode와 delayNode에 집중해보자. 사실상 딜레이 이펙터는 이 두개의 노드가 핵심이다. 먼저, 딜레이 이펙터가 하는 일에 대해서 다시 한번 살펴보자. 입력 -> 지연 -> 감소된 신호 출력 -> 입력 -> 지연 -> 감소된 신호 출력… 딜레이 이펙터가 하는 일은 이게 전부다. 신호를 조금씩 지연시키고 감소된 신호를 다시 출력하는 일을 반복한다. 그래서 필자는 delayNode와 feedbackNode를 서로 연결해주는 방법으로 이 이펙터를 구현하려고 한다. 이렇게 간단한 연결만으로 딜레이를 구현할 수 있다 이렇게 노드들을 연결하게되면 DelayNode를 통해 입력된 오디오 신호가 지연된 후 FeedbackNode와 OutputNode로 출력되고, FeedbackNode를 통해서 게인이 감소된 소리는 다시 DelayNode로 입력되어 지연된 후 OutputNode로 출력될 것이다. 그럼 위 그림대로 한번 노드들을 연결해보도록 하자. 123456789101112131415161718sourceNode.connect(inputNode);// Dry 소스 노드 연결inputNode.connect(dryGainNode);dryGainNode.connect(outputNode);dryGainNode.gain.value = 1 - mix;// Delay 루프 생성delayNode.connect(feedbackNode);feedbackNode.connect(delayNode);// Wet 소스 노드 연결inputNode.connect(delayNode);delayNode.connect(wetGainNode);wetGainNode.connect(outputNode);wetGainNode.gain.vaule = mix;outputNode.connect(audioContext.destination); 이제 소스 노드를 재생해보면 딜레이 이펙터를 통해 메아리가 치는 듯한 효과가 적용된 소리를 들어볼 수 있다. Filter 필터(Filter)는 무언가를 걸러내는 도구 혹은 개념을 의미한다. 우리는 이미 필터라는 개념을 평소에 많이 사용하고 있기 때문에 그렇게 이해하기 어려운 개념은 아닐 것이다. 그리고 오디오 이펙터에서의 필터는 바로 주파수를 걸러내는 역할을 한다. 쉽게 말하면 필터는 오디오의 음역대 중 특정한 음역대만 콕 집어내어 없애버릴 수 있는 이펙터인 것이다. 그래서 필터는 주로 소리에 섞여있는 노이즈를 걸러내거나 너무 낮거나 너무 높아서 쓸데없는 울림을 생성하는 주파수를 걸러내는데 많이 사용된다. 이러한 필터의 특성을 잘 사용하면 상당히 재미있는 짓을 많이 할 수 있는데 대표적인 예를 두개 정도 들자면, 바로 전화기에서 나오는 목소리를 만들거나 클럽에서 나는 음악 소리와 같은 소리를 만드는 경우가 있다. 먼저, 전화기에서 나오는 목소리는 전체 주파수 중에서 특정한 대역의 주파수만 통과시키는 Bandpass 필터를 사용하여 만들어 낼 수 있다. 전화기가 전송할 수 있는 주파수 대역에 한계가 있다는 것을 이용하여 인간의 목소리 대역인 100 ~ 250hz의 주파수를 제외한 나머지 주파수를 모두 잘라내는 것이다. 위 영상의 5:00 경 “Hey, kitty~” 하는 부분이 필터가 적용된 목소리이다. 그러므로 사람의 목소리 소스에 필터를 사용하여 100 ~ 250hz 대역을 제외하고 나머지 주파수를 모두 날려버리게되면 우리가 일반적으로 전화를 할때 들리는 목소리로 만들어낼 수 있는 것이다. 클럽에서 나는 음악 소리도 비슷한 원리로 만들어내는 것이다. 클럽의 특성 상 보통 지하에 위치하고 좁은 입구를 가지고 있는 경우가 많다. 그런 상황에서 클럽에서 노래를 틀게되면 소리가 밖으로 빠져나올 수 있는 통로가 거의 없기 때문에 우리가 지상에서 클럽에서 틀고 있는 노래를 들어보면 굉장히 묵직한 붐~ 붐~하는 소리가 들리게 된다. 클럽 음악의 특성 상 강한 드럼과 베이스로 인해 저음이 부각되는 경우가 많고, 고음보다는 저음의 물체 투과율이 높기 때문에 클럽 외부에서는 상대적으로 고음에 비해 많이 투과된 저음을 위주로 듣게 되는 것이다. 이러한 파동의 특성은 소리에 한정되는 것은 아니고 빛과 같은 다른 파동또한 고주파의 에너지 손실률이 저주파보다 높다. 낮은 주파수만 통과시키는 Lowpass 필터를 사용한 소리 이렇듯 사운드 엔지니어들은 특정 상황의 소리가 어떻게 들리는지 분석하고 필터를 포함한 여러가지 이펙터들을 사용하여 그 상황의 현장감을 부여하기도 한다. 다행히도 HTML5 Audio API는 이런 필터를 만들 수 있는 BiquadFilterNode를 제공해주고 있기 때문에 우리가 직접 오디오 버퍼를 까서 주파수를 분석해야하는 슬픈 상황은 피할 수 있다. 우리는 이 노드가 제공하는 값들이 어떤 것을 의미하는지만 알고 있으면 된다. 그럼 BiquadFilterNode가 제공하는 속성들이 무엇을 의미하는지 하나하나 살펴보도록 하자. FrequencyFrequency는 어떤 대역의 주파수를 걸러낼 것인지를 정하는 값이다. 단위는 hz(헤르츠)를 사용하며, 10hz부터 오디오의 샘플레이트의 절반까지의 값을 할당할 수 있다. 만약 오디오 소스의 샘플레이트가 44,100hz라면 22,050까지를 할당할 수 있다는 의미이다. Q신호를 걸러낸다는 것은 기본적으로 칼처럼 딱! 자를 수 있는 것이 아니다. 소리 자체는 아날로그 신호이기 때문에 네모 반듯하게 잘라낼 수 없고 어느 정도 바운더리를 가지고 걸러낼 수밖에 없는데, 이때 Q는 특정 주파수를 걸러낼 때 얼마나 예민하게 걸러낼 수 있는 지를 의미한다. Q에는 0.0001 ~ 1000 사이의 값을 할당할 수 있으며, Q의 값이 높을 수록 잡아낸 주파수를 더 예민하게 걸러낼 수 있다. 그러나 오디오 신호를 필터링 할 때 Q가 너무 높다면 자연스럽게 들리지 않고 인위적으로 들릴 수 있기 때문에 적당한 값을 찾는 것이 중요하다. TypeBiquadFilterNode로는 여러가지 타입의 필터를 만들어 낼 수 있는데, 크게는 주파수를 아예 걸러내버리는 타입과, 특정 주파수를 증폭시키거나 감소시킬 수 있는 타입으로 나눠진다. 주파수를 걸러내는 타입 lowpass(highcut): 지정한 주파수보다 높은 주파수를 모두 걸러낸다. highpass(lowcut): 지정한 주파수보다 낮은 주파수를 모두 걸러낸다. bandpass: 지정한 주파수를 제외한 모든 주파수를 걸러낸다. notch: 지정한 주파수를 걸러낸다. 주파수를 증폭/감소하는 타입 lowshelf: 지정한 주파수보다 낮은 주파수를 증폭/감소 시킨다. highshelf: 지정한 주파수보다 높은 주파수를 증폭/감소 시킨다. peaking: 지정한 주파수를 증폭/감소 시킨다. 이 중 주파수를 증폭/감소시키는 타입은 밑에서 후술할 EQ(Equalizer)에서도 사용할 수 있다. 이번에는 단순히 주파수를 걸러내는 필터를 만들 예정이므로 필자는 주파수를 걸러내는 타입만을 사용하여 필터를 구현할 것이다. 필자는 특정 주파수보다 낮은 주파수를 모두 걸러내는 Lowpass 필터와 특정 주파수보다 높은 주파수를 모두 걸러내는 Highpass 필터를 구현할 것이다. 그럼 한번 간단하게 필터를 구현해보도록 하자. Filter 구현해보기우선 AudioContext 객체의 createBiquadFilter 메소드를 사용하여 BiquadFilterNode를 생성한다. 필자가 가지고 있는 오디오 샘플은 44,100hz의 샘플레이트를 가지고 있으므로 Lowpass 필터의 주파수는 1,000hz로 Highpass 필터의 주파수는 20,000hz로 설정하겠다. 1234567const lowpassFilterNode = audioContext.createBiquadFilter();lowpassFilterNode.type = 'lowpass';lowpassFilterNode.frequency.setValueAtTime(1000, audioContext.currentTime);const highpassFilterNode = audioContext.createBiquadFilter();highpassFilterNode.type = 'highpass';highpassFilterNode.frequency.setValueAtTime(20000, audioContext.currentTime); Q값을 따로 설정해주지 않았는데, 그래도 사실 상관없다. BiquadFilterNode의 Q는 기본 값으로 350을 가지고 있고 이 값은 너무 과하지도 부족하지도 않은 적당한 값이기 때문에 그냥 기본 값을 사용할 것이다.(조금 귀찮기도 하다.) 이제 생성한 필터 노드들을 오디오 소스와 연결해주면 1,000hz보다 낮고 20,000hz보다 높은 주파수가 제거된 오디오 샘플을 들어볼 수 있다. 123sourceNode.connect(lowpassFilterNode);lowpassFilterNode.connect(highpassFilterNode);highpassFilterNode.connect(audioContext.destination); 여기까지 읽으신 분들은 슬슬 느끼기 시작했겠지만 사실 HTML5 Audio API가 워낙 잘 만들어져있어서 개발자가 만져야할 부분이 거의 없다. 필자는 BiquadFilterNode의 존재를 알기 전에는 와 이거 필터는 어떻게 만들지...?라고 고민했었는데 사실 제일 고민할 필요가 없는 놈이었다.(그래서 약간 허무하기도 했다.) EQ Equalizer(EQ, 이퀄라이저)는 이름에서도 알 수 있듯이 일종의 주파수 평탄화 작업(Frequancy Equalizing)을 하는 목적으로 사용하는 이펙터이다. EQ는 컴프레서와 함께 오디오 믹싱의 기본으로 깔고 들어가는 이펙터인데, 원본 소스에서 쓸데없는 소리를 없애고 다른 소리들과의 조화를 이루도록 하는 용도로 주로 사용한다. EQ는 결국 주파수를 컨트롤하는 이펙터이므로 필터를 사용하여 구현하는데, 이미 우리는 필터를 한번 만들어봤기 때문에 EQ 정도는 뚝딱 만들 수 있다. EQ는 크게 파라메트릭 이퀄라이저(Parametric EQ)와 그래픽 이퀄라이저(Graphic EQ) 두 가지 종류로 나누어지는데, 필자는 이 중 그래픽 이퀄라이저를 구현할 예정이다. 참고로 상단의 그림은 파라메트릭 EQ인데, 그냥 이미지가 더 멋있어서 넣었다. 참고로 그래픽 EQ는 이렇게 생겼다. 짙은 아날로그의 향수가 풍겨오는 비주얼 두 개의 EQ 모두 장단점이 존재하는데, 일단 그래픽 EQ의 장점은 파라메트릭 EQ에 비해 조절할 수 있는 주파수 대역의 수가 많고 직관적인 인터페이스를 가지고 있다는 것이다. 여러분은 이 단락의 맨 위에 첨부된 파라메트릭 EQ 이미지를 보고 엥? 파라메트릭도 나름 직관적인데?라고 하실 수 있는데, 원래 하드웨어 파라메트릭 EQ는 이렇게 생겼다. 까만 것은 노브요. 하얀 것은 숫자로다. 그래서 그래픽 EQ는 보통 빠른 대응이 필요한 공연장같은 곳에서 많이 사용되고 있으며, 짬이 많이 쌓인 시니어 사운드 엔지니어들은 공연장에서 하울링(노래방에서 삐- 하며 가끔 나는 날카로운 소리)이 발생하면 바로 해당 주파수를 캐치해서 그래픽 EQ로 죽여버리는 무서운 스킬을 가지고 있다. 그래픽 EQ의 단점은 조절할 수 있는 주파수 대역이 정해져 있다는 것과 주파수의 세밀한 조정이 힘들다는 것이다. 반면 파라메트릭 EQ는 그래픽 EQ와 다르게 조절할 수 있는 주파수 대역까지 모두 정해줄 수 있다. 그러나 한번에 조절할 수 있는 주파수의 개수는 그래픽 EQ에 비해 크게 부족하다. 일반적인 파라메트릭 EQ가 3~5개의 주파수 대역을 조절할 수 있는 반면에 그래픽 EQ는 한번에 조절 가능한 주파수가 40개가 넘어가는 굇수도 존재한다. 필자 생각에 하드웨어 파라메트릭 EQ의 최대 단점은 바로 직관적이지 않은 인터페이스를 가지고 있다는 것인데, 이 단점은 소프트웨어로 구현하면 UI로 커버할 수 있는 영역이기도 하고 대부분의 녹음실에서는 즉각적인 대응보다는 계속 소리를 들어보면서 이퀄라이징을 할 수 있는 상황이 대부분이기 때문에 많은 소프트웨어 EQ가 주파수 대역 조절에 자유도가 높은 파라메트릭 EQ로 구현된다. 하지만 필자가 만드는 데모처럼 간단히 구현해보는 상황에서는 저 위에 하드웨어 파라메트릭 EQ와 비슷한 UI로 구현될 것이 뻔하므로 필자는 상대적으로 UI 만들기가 쉬운 그래픽 EQ를 선택했다.(이 말이 잘 이해가 안된다면 EQ 챕터의 가장 위에 첨부한 파라메트릭 EQ를 한번 보고 오자) 위에서 한번 이야기했듯이 EQ는 필터를 사용하여 구현하므로 그렇게 복잡하지 않다. 그럼 이제 한번 간단하게 뚝딱 만들어보도록 하자. 그래픽 EQ 구현하기위에서 그래픽 EQ 이미지를 봤다면 알겠지만 이 친구는 조절할 수 있는 주파수 대역의 개수가 정해져 있는 장비이다. 그렇기 때문에 필자도 조절할 수 있는 주파수가 들어있는 배열을 하나 선언하고 이 배열을 이터레이션하면서 필터들을 생성할 것이다. 12345const frequencies = [ 25, 31, 40, 50, 63, 80, 100, 125, 160, 200, 250, 315, 400, 500, 630, 800, 1000, 1250, 1600, 2000, 2500, 3150, 4000, 5000, 6300, 8000, 10000, 12500, 16000, 20000]; 이때 주의해야할 점이 있다. EQ는 여러 개의 필터를 사용하기 때문에 각 필터를 서로 체이닝해서 연결해주어야한다. 이때 필터의 게인이 1보다 조금이라도 높다면 한번 필터를 통과할 때마다 소리가 조금씩 증폭되어 여러분의 귀에 들어올 때 쯤이면 엄청 큰 소리가 되어 여러분의 고막을 영원히 이별시킬 수도 있다. 그렇기 때문에 반드시 필터들의 게인을 0이하로 설정해주어야 한다. 12345678910111213141516171819202122232425262728const inputNode = audioContext.createGain();sourceNode.connect(inputNode);const filters = frequencies.map((frequency, index, array) => { const filterNode = audioContext.createBiquadFilter(); filterNode.gain.value = 0; filterNode.frequency.setValueAtTime(frequency, audioContext.currentTime); if (index === 0) { filterNode.type = 'lowshelf'; } else if (index === array.length - 1) { filterNode.type = 'highshelf'; } else { filterNode.type = 'peaking'; } return filterNode;});filters.reduce((prev, current) => { prev.connect(currentNode); return currentNode;}, inputNode);const outputNode = audioContext.createGain();filters[filters.length - 1].connect(outputNode);outputNode.connect(audioContext.destination); map 메소드 내부의 if 문을 보면 해당 첫번째 필터와 마지막 필터의 타입만 다르게 주고 있는 것을 볼 수 있는데, 이는 Shelf 타입의 필터를 사용하여 첫번째 필터의 주파수보다 낮은 주파수와 마지막 필터의 주파수보다 높은 주파수까지 모두 커버하기 위해서이다.(필터 타입이 잘 기억나지 않는다면 Filter 부분을 다시 보고 오자) 그 후 생성된 필터를 reduce 메소드를 통해 모두 체이닝해주고 outputNode와도 연결해주었다. 여기까지 작성한 후 sourceNode를 재생시켜보면 아무 변화도 없는 것을 알 수 있다. 당연히 모든 필터의 게인이 0이기 때문에 아무런 변화가 없는 것이다. 이때 저 필터들의 값을 -1 ~ 1사이의 랜덤한 난수로 할당하면 소리가 조금씩 변하는 것을 들어볼 수도 있다. 필자는 개인적으로 input[type=\"range\"] 엘리먼트와 연동하여 필터들의 게인을 조절할 수 있도록 만들고 직접 이것저것 만져보는 것을 추천한다. 또한 가장 낮은 주파수와 높은 주파수의 필터를 Shelf 타입으로 설정했기 때문에 이 필터들의 게인을 낮추면 Lowpass 필터나 Highpass 필터와 같은 효과도 낼 수 있다. 마치며자, 여기까지 대표적으로 많이 사용하는 이펙터들인 컴프레서, 리버브, 딜레이, 필터, EQ를 만들어보았다. 사실 이 5개 외에도 재밌는 여러가지 이펙터가 있지만 분량조절 대실패로 인해 여기까지만 노는 것으로 하겠다. 필터를 만들 때 한번 이야기 했듯이 HTML5 Audio API는 굉장히 높은 수준의 추상화된 노드를 제공해주기 때문에 사실 개발자가 직접 구현할 것들이 별로 없다. 이는 다르게 말하면 세밀한 수준의 구현이 힘들다는 뜻이기도 하지만 필자가 무슨 오디오 이펙터 회사를 차릴 것도 아니기 때문에 단순 재미로 만들어보기에는 충분한 것 같다. 이렇게 여러가지 이펙터를 구현해보며 필자도 예전에 사운드 엔지니어로 일할 때의 추억이 새록새록 떠오르기도 했고 또 이펙터에 대해서 새롭게 알게된 내용도 있어서 굉장히 재밌게 작업을 했다. 필자는 포스팅에 작성한 이펙터 외에도 여러가지 이펙터를 계속 구현해볼 예정이므로 관심있는 분들은 필자의 깃허브에서 한번 쓱 둘러보고 PR을 날려도 된다.(좋은 건 나누면 배가 되는 법이다.) 이상으로 JavaScript로 오디오 이펙터를 만들어보자 - 나만의 소리 만들기 포스팅을 마친다.","link":"/2019/08/21/javascript-audio-effectors-practice/"},{"title":"알고 보면 재밌는 객체 지향 프로그래밍, OOP 흝어보기","text":"이번 포스팅에서는 객체 지향 프로그래밍(Object-Oriented Programming), 줄여서 흔히들 OOP라고 부르는 설계 방법론에 대해서 이야기해보려고 한다. OOP는 프로그래밍의 설계 패러다임 중 하나로, 현실 세계를 프로그램 설계에 반영한다는 개념을 기반으로 접근하는 방법이다. OOP는 90년대 초반부터 유명해지기 시작했지만 아직까지도 전 세계의 많은 프로그래머들이 사용하고 있는 설계 패턴 중 하나이기 때문에 알아둬서 나쁠 건 없다. 객체 지향 프로그래밍을 왜 알아야 하나요?사실 OOP가 오랜 기간동안 전 세계에서 사랑받고있는 설계 패턴인 것은 맞지만 최근에는 OOP의 단점을 이야기하며 함수형 프로그래밍과 같은 새로운 설계 패러다임이 각광받기도 했다.(함수형 프로그래밍도 사실 꽤 오래된 패러다임이다) 사실 OOP니 함수형 프로그래밍이니 하는 이런 것들은 결국 프로그램을 어떻게 설계할 것인가?에 대한 방법이기 때문에 당연히 장단점 또한 존재하기 마련이고 시대나 용도에 맞게 개선된 패러다임이 제시되는 것은 자연스러운 흐름이다. 필자는 개인적으로 아직까지 OOP가 괜찮은 설계 패턴이라고 생각하고 있지만, 여러분은 함수형 프로그래밍이 OOP보다 더 효율적이고 괜찮다고 생각할 수도 있다. 당연히 어떤 패러다임을 선호하는지는 개인의 자유기 때문에 다르게 생각할 수 있지만, 어떤 기술을 선택할 때는 해당 기술의 장단점과 그 기술을 선택했을 때 얻을 수 있는 것과 잃을 수 있는 것을 제대로 파악하고 있어야 올바른 선택을 할 수 있기 때문에 여러분이 함수형 프로그래밍을 선택한다고 하더라도 OOP가 무엇인지 알고 있어야 하는 것은 마찬가지다. 또한 OOP는 1990년대 초반부터 2019년인 현재까지도 모던 프로그래밍 설계에 중요한 역할을 하고 있는 개념이다. 아무리 함수형 프로그래밍과 같은 새로운 패러다임이 주목받기는 했지만 아직까지는 OOP가 대부분의 프로그램 설계에 사용되고 있다는 사실은 부정할 수 없는 현실이며, 이게 바로 우리가 OOP를 좋은 싫든 알고 있어야 하는 현실적인 이유 중의 하나이다.(참고로 Java, Python, C++ 등 메이저 언어들도 전부 OOP를 지원하는 언어이다.) 그래서 이번 포스팅에서는 OOP가 추구하는 것이 무엇인지, 또 OOP를 이루고 있는 개념들은 무엇이 있는지 간략하게 살펴보려고 한다. 객체 지향이라는 것은 무엇을 의미하나요?OOP의 의미인 Object-Oriented Programming의 Object-Oriented를 한국말로 그대로 직역하면 객체 지향이다. 여기서 말하는 객체는 현실 세계에 존재하는 하나 하나의 독립된 무언가를 의미한다. 보통 OOP를 배울 때 가장 처음 접하는 개념이 바로 이 객체라는 개념인데, 사실 한번 이해하고나면 꽤 간단한 개념이지만 우리가 평소에 살면서 잘 생각해보지 않는 개념이기 때문에 잘 이해가 되지 않을 수도 있다. 객체를 설명하기 위해서는 클래스라는 개념을 함께 설명해야하는데, 용어가 직관적이지 않아서 그렇지 조금만 생각해보면 누구나 다 이해할 수 있는 개념이다. 일반적으로 이걸 설명할 때 붕어빵과 붕어빵 틀과 같은 비유를 들며 설명하지만 필자는 일반적인 설명과 다르게 클래스는 무엇이고, 객체는 무엇이다라는 방식으로 접근하기보다는 일단 OOP의 포괄적인 설계 개념을 먼저 설명하는 방식으로 접근하도록 하겠다. 재미없고 복잡한 용어는 일단 제쳐두고 일단 예시를 보면서 의식의 흐름대로 따라와보자. 클래스와 객체필자는 이 포스팅의 서두에서 OOP란 현실 세계를 프로그램의 설계에 반영하는 것이라고 이야기했다. 이 말이 뜻하는 의미를 먼저 이해하고 나면 클래스나 객체 같은 것은 자연스럽게 이해할 수 있으니 먼저 OOP가 왜 현실 세계를 반영한 설계 방식이라고 하는 지를 먼저 알아보도록 하자. 뭐 여러가지 예시가 있겠지만 우리가 일상적으로 사용하고 있는 물건을 예로 드는 것이 좀 더 와닿을테니 필자는 스마트폰을 예로 들어서 설명을 진행하려고 한다. 필자는 애플에서 만든 아이폰7이라는 기종을 사용하고 있기 때문에 아이폰7을 예시로 설명을 시작하겠다. 먼저, 우리가 아이폰7이라는 것을 프로그램으로 구현하고 싶다면 제일 먼저 아이폰7이 무엇인지부터 정의해야한다. 너무 어렵게 생각할 필요없다. 진짜로 프로그램을 짜는 것이 아니기 때문에 대충 정의해도 된다. 필자가 지금 바로 생각해낸 아이폰7은 약간 동글동글한 바디를 가지고 있고 햅틱 엔진이 내장된 홈 버튼을 가지고 있으며, 시리즈 최초로 3.5mm 이어폰 단자가 없어진 아이폰 시리즈라는 것이다.(개인적으로 이어폰 단자 좀 다시 넣어줬으면 한다…) 우리는 여기서 한발짝 더 나아가서 아이폰7의 상위 개념인 아이폰에 대해서도 정의해볼 수 있다. 결국 아이폰7은 아이폰이라는 개념을 기반으로 확장된 개념이기 때문이다. 그럼 아이폰은 무엇일까? 아이폰은 애플에서 제조한 스마트폰으로, iOS를 사용하고 있는 스마트폰 시리즈의 명칭이다. 이때 아이폰은 아이폰7 외에도 아이폰X, 아이폰8, 아이폰 SE 등 수많은 아이폰 시리즈의 제품들을 포함하는 좀 더 포괄적인 개념이다. 일상 속에서 우리가 친구한테 너 핸드폰 뭐 써?라고 물어봤을 때 친구가 아이폰 또는 갤럭시라고 대답하는 경우를 생각해보자. 이때 친구는 자신이 사용하는 스마트폰이 아이폰X든 갤럭시 S10이든 간에 무의식적으로 아이폰이나 갤럭시라는 좀 더 포괄적인 개념을 떠올리고 하위 개념들을 그룹핑한 것이다. 그 정도로 이런 접근 방법은 우리에게 이미 일상적이고 익숙한 방법이다. 어렵게 생각하지 말자. 아이폰7의 상위 개념인 아이폰은 모든 아이폰을 포괄할 수 있는 개념이 된다. 여기서 가장 중요한 점은 하위 개념인 아이폰7은 상위 개념인 아이폰의 특징을 모두 가지고 있다는 것이다. 마찬가지로 아이폰의 다른 하위 개념인 아이폰X이나 아이폰 SE와 같은 아이폰 시리즈들도 아이폰의 모든 특징을 가지고 있을 것이다. 여기서 끝내면 아쉬우니 한번만 더 해보도록 하자. 아이폰의 상위 개념은 무엇일까? 아이폰은 애플에서 제조하고 iOS를 사용하는 스마트폰의 명칭이다. 즉, 아이폰의 상위 개념은 스마트폰이라고 할 수 있다. 이때 스마트폰이라는 개념은 아이폰 뿐만 아니라 갤럭시, 샤오미, 베가와 같은 다른 스마트폰들까지 모두 포괄하는 개념일테고, 마찬가지로 이 스마트폰이라는 개념의 하위 개념들은 모두 스마트폰의 특징을 그대로 가지며 자신들만의 고유한 특징을 추가적으로 가질 수 있을 것이다. 스마트폰이라는 개념은 아이폰, 갤럭시, 샤오미 등 모든 스마트폰을 포괄할 수 있는 개념이 된다. 이런 식으로 우리는 아이폰7이라는 개념에서 출발하여 계속해서 상위 개념을 정의해나갈 수 있다. 아이폰7 -> 아이폰 -> 스마트폰 -> 휴대전화 -> 무선 전화기 -> 전화기 -> 통신 기기 -> 기계… 결국 이렇게 상위 개념을 추적해나가면서 설계하는 것이 OOP의 기초이고, 이때 아이폰7, 아이폰과 같은 개념들을 클래스(Class)라고 부르는 것이다. 그리고 방금 했던 것처럼 상위 개념을 만들어나가는 행위 자체를 추상화(Abstraction)라고 한다. 추상화는 밑에서 다시 한번 설명할테니 일단 지금은 클래스라는 개념만 기억하도록 하자. 그럼 객체(Object)는 무엇일까? 필자는 방금 클래스를 설명하면서 개념이라는 단어를 굉장히 많이 사용했다. 말 그대로 클래스의 역할은 어떠한 개념을 의미하는 것이다. 하지만 개념이라는 것 그 자체 만으로는 현실의 물건이 될 수는 없는 법이다. 잘 생각해보면 아이폰7이라는 것 또한 그냥 어떠한 제품 라인의 이름이다. 어떤 고유한 물건의 이름이 아니라는 것이다. 여기서 필자가 말하는 고유하다라는 의미는 전 세계에 단 한개만 존재하는 수준의 고유함이다. 당장 내 아이폰7과 친구의 아이폰7만 봐도 실제로는 다른 아이폰7이지 않은가? 즉, 아이폰7이라는 클래스는 어떠한 실체가 있는 게 아니라는 것이다. 아이폰7 클래스에는 CPU, 디스플레이 해상도, 메모리와 같은 사양이 정의되어 있을 것이고 이를 기반으로 공장에서 실제 아이폰7을 찍어내고 일련번호를 부여한 후 출고하고나면 그제서야 우리 손에 잡을 수 있는 물건인 아이폰7이 되는 것이다. 이때 생산된 아이폰7에는 고유한 ID인 일련번호가 부여되었기 때문에 우리는 전 세계에 일련번호가 1234인 아이폰7은 단 하나밖에 없다는 사실을 알 수 있다. 이때 이렇게 생산된 아이폰7들을 객체라고 할 수 있다. 즉, 클래스는 일종의 설계도이고 이것을 사용하여 우리가 사용할 수 있는 실제 물건으로 만들어내는 행위가 반드시 필요하다. 그리고 객체는 클래스를 사용하여 생성한 실제 물건이다. 이러한 OOP의 설계 접근 방식으로 우리의 일상 속에 보이는 대부분의 개념들을 추상화할 수 있는데, 그냥 평소에 보이는 모든 것들을 이렇게 추상화해보는 연습을 하면 나름 재미도 있다. 몇가지 예를 들어보겠다. 소나타 -> 중형 세단 -> 세단 -> 자동차 -> 이동수단 문동욱 -> 남자 -> 인간 -> 영장류 -> 포유류 -> 동물 오버워치 -> 블리자드가 만든 FPS 게임 -> FPS 게임 -> 게임 실제로 우리 일상 속에 존재하는 거의 대부분의 개념은 이런 추상화 기법으로 어느 정도 정리할 수 있다. 눈에 보이는 생활 속의 물건들을 추상화 해보는 것은 따로 시간을 내지 않아도 일상 속에서 할 수 있는 좋은 연습 방법이니 한번 해보기를 추천한다. 이 방법이 익숙해지면 카페에 가서 커피를 마시면서도 머릿 속에서 작은 카페를 만들어 볼 수도 있다.(데이트 할때는 하지 맙시다) 결국 객체 지향이라는 말의 의미는 이렇게 클래스를 사용하여 추상적인 개념들을 정의하고, 그 클래스를 사용하여 실제로 사용할 객체를 만들어냄으로써 현실 세계의 모든 것을 표현할 수 있다는 것에서 출발하는 것이다. 추상화에 대해서 조금 더 깊이 생각해보자방금 우리는 아이폰7부터 시작해서 상위 개념을 이끌어내는 간단한 추상화를 경험해보았다. 하지만 우리가 방금 저 예시를 진행할 때는 그렇게까지 깊은 고민이 없었을 것이다. 왜냐면 아이폰이나 스마트폰 같은 개념은 이미 우리에게 상당히 친숙한 개념이기 때문에 깊이 고민할 필요없이 이미 여러분의 머릿속에 어느 정도 추상화가 되어 정리된 상태였기 때문이다. 하지만 실제로 프로그램 설계에 OOP를 사용할 때에는 우리에게 친숙한 아이폰과 같은 개념을 사용하는 것이 아니라 개발자가 이 개념 자체부터 정의해야하는 경우가 많다. 이때 추상화가 어떤 것인지 정확히 이해하고 있지 않다면 자칫 이상한 방향으로 클래스를 설계할 수 있기 때문에 정확히 추상화가 무엇인지 짚고 넘어가도록 하겠다. 추상이라는 단어의 뜻부터 한번 생각해보자. 추상은 어떠한 존재가 가지고 있는 여러가지의 속성 중에서 특정한 속성을 가려내어 포착하는 것을 의미한다. 대표적인 추상파 화가 중 한명인 피카소가 소를 점점 추상화하며 그려가는 과정을 한번 살펴보면 추상화가 어떤 것인지 조금 더 이해가 된다. 피카소가 소를 추상화하는 과정 이렇듯, 추상화라는 것은 그 존재가 가지고 있는 가장 특징적인 속성들을 파악해나가는 것을 의미한다. 우리가 방금 전 아이폰7의 상위 개념인 아이폰을 떠올리게 되는 과정은 꽤나 직관적으로 진행되었지만 사실 추상화를 그렇게 직관적으로 접근하려고 하면 더 방향을 잡기가 힘들다. 원래대로라면 아이폰이라는 상위 개념을 만들고자 했을 때 아이폰7 뿐만이 아니라 다른 아이폰 시리즈들까지 모두 포함할 수 있는 아이폰들의 공통된 특성을 먼저 찾는 것이 올바른 순서이다. 이렇게 만들어진 올바른 상위 개념의 속성은 그 개념의 하위 개념들에게 공통적으로 적용할 수 있는 속성이 된다. 상위 개념아이폰: 애플에서 만든 iOS 기반의 스마트폰 아이폰 클래스 기반의 하위 개념아이폰X: 애플에서 만든 iOS 기반의 스마트폰이며, 홈 버튼이 없고 베젤리스 디자인이 적용된 아이폰아이폰7: 애플에서 만든 iOS 기반의 스마트폰이며, 햅틱 엔진이 내장된 홈 버튼을 가지고 있는 아이폰.아이폰 SE: 애플에서 만든 iOS 기반의 스마트폰이며, 사이즈가 작아서 한 손에 잡을 수 있는 아이폰. 이 예시에서 볼 수 있듯이 하위 개념들은 상위 개념이 가지고 있는 모든 속성을 그대로 물려받는데, 그래서 이 과정을 상속(Inheritance)이라고 한다. 이 상속에 관해서는 밑에서 다시 자세하게 살펴보도록 하겠다. 객체 지향 프로그래밍의 3대장방금까지 설명한 클래스, 객체, 추상화는 OOP를 이루는 근본적인 개념들이다. 필자는 여기서 좀 더 나아가서 OOP를 지원하는 언어들이 기본적으로 갖추고 있는 몇가지 개념을 더 설명하려고 한다. OOP는 그 특성 상 클래스와 객체를 기반으로 조립하는 형태로 프로그램을 설계하게 되는데 이때 이 조립을 더 원활하게 하기 위해서 나온 유용한 몇가지 개념들이 있다. 하지만 이 개념들은 JavaScript에는 구현되지 않은 개념도 있으므로 이번에는 Java를 사용해서 예제를 진행하도록 하겠다. 단편적인 문법만 보면 그렇게 이질감 느껴질 정도로 차이가 크지 않기 때문에 JavaScript만 하셨던 분들도 아마 금방 이해할 수 있을 것이다. 참고로 TypeScript도 OOP를 지원하기는 하지만 이거 세팅하는 것보다 그냥 Java 컴파일하는게 편하기 때문에 Java로 간다. 그럼 이제 객체 지향의 3대장이라고 불리는 상속과 캡슐화, 그리고 다형성에 대해서 간단하게 알아보도록 하자. 상속상속(Inheritance)은 방금 전 추상화에 대한 설명을 진행하면서 한번 짚고 넘어갔던 개념이다. OOP를 제공하는 많은 프로그래밍 언어에서 상속은 extends라는 예약어로 표현되는데, 하위 개념 입장에서 보면 상위 개념의 속성을 물려받는 것이지만 반대로 상위 개념 입장에서 보면 자신의 속성들이 하위 개념으로 넘어가면서 확장되는 것이므로 이 말도 맞다. 그럼 이제 상속이 어떻게 이루어지는지 코드로 살펴보도록 하자. 123456789101112131415161718class IPhone { String manufacturer = \"apple\"; String os = \"iOS\";} class IPhone7 extends IPhone { int version = 7;} class Main { public static void main (String[] args) { IPhone7 myIPhone7 = new IPhone7(); System.out.println(myIPhone7.manufacturer); System.out.println(myIPhone7.os); System.out.println(myIPhone7.version); }} 123appleiOS7 IPhone7 클래스를 생성할 때 extends 예약어를 사용하여 IPhone 클래스를 상속받았다. IPhone7 클래스에는 manufacturer와 os 속성이 명시적으로 선언되지 않았지만 부모 클래스인 IPhone 클래스의 속성을 그대로 물려받은 것을 볼 수 있다. 마찬가지로 이 상황에서 IPhoneX 클래스를 새로 만들어야 할때도 IPhone 클래스를 그대로 다시 사용할 수 있다. 123class IPhoneX extends IPhone { int version = 10;} 즉, 추상화가 잘된 클래스를 하나만 만들어놓는다면 그와 비슷한 속성이 필요한 다른 클래스를 생성할 때 그대로 재사용할 수 있다는 말이다. 그리고 만약 아이폰 시리즈 전체에 걸친 변경사항이 생겼을 때도 IPhone7, IPhoneX와 같은 클래스는 건드릴 필요없이 IPhone 클래스 하나만 고치면 이 클래스를 상속받은 모든 하위 클래스에도 자동으로 적용되므로 개발 기간도 단축시킬 수 있고 휴먼 에러가 발생할 확률도 줄일 수 있다. 하지만 여기서 만약 요구사항이 변경되어서 갤럭시 시리즈를 만들어야한다면 어떻게 될까? 갤럭시 시리즈는 iOS가 아니라 Android를 사용하고, 제조사도 애플이 아니라 삼성이기 때문에 우리가 방금 만든 IPhone 클래스를 사용할 수는 없다. 이때 우리는 IPhone 클래스를 그대로 냅두고 그냥 Galaxy 클래스를 새로 만들 수도 있지만 SmartPhone이라는 한단계 더 상위 개념을 만드는 방향으로 가닥을 잡을 수도 있다. 123456789101112131415161718192021222324class SmartPhone { SmartPhone (String manufacturer, String os) { this.manufacturer = manufacturer; this.os = os; }}class IPhone extends SmartPhone { IPhone () { super(\"apple\", \"iOS\"); }}class Galaxy extends SmartPhone { Galaxy () { super(\"samsung\", \"android\"); }} class IPhone7 extends IPhone { int version = 7;}class GalaxyS10 extends Galaxy { String version = \"s10\";} 위의 코드에서 super 메소드는 부모 클래스의 생성자를 호출하는 메소드이다. 부모 클래스를 Super Class, 자식 클래스를 Sub Class라고 부르기도 하기 때문에 부모와 관련된 키워드 역시 super를 사용하는 것이다. 그리고 이때 자식 클래스인 IPhone7이나 GalaxyS10 클래스가 부모 클래스의 manufacturer나 os 속성을 덮어쓰게 할 수도 있는데, 이러한 작업을 오버라이딩(Overriding)이라고 한다. 안드로이드 개발을 하다보면 밥먹듯이 쓰는 @Override 데코레이터도 부모 클래스의 메소드를 덮어쓰는 방식으로 세부 구현을 진행하는 것이다. 이러한 OOP의 클래스 의존관계는 클래스의 재사용성을 높혀주는 방법이기도 하지만, 너무 클래스의 상속 관계가 복잡해지게 되면 개발자가 전체 구조를 파악하기가 힘들다는 단점도 가지고 있으므로 개발자가 확실한 의도를 가지고 적당한 선에서 상속 관계를 설계하는 것이 중요하다.(근데 이 적당한 선의 기준이 개발자마다 다 다르다는 게 함정) 캡슐화캡슐화(Encapsulation)는 어떠한 클래스를 사용할 때 내부 동작이 어떻게 돌아가는지 모르더라도 사용법만 알면 쓸 수 있도록 클래스 내부를 감추는 기법이다. 클래스를 캡슐화 함으로써 클래스를 사용하는 쪽에서는 머리 아프게 해당 클래스의 내부 로직을 파악할 필요가 없어진다. 또한 클래스 내에서 사용되는 변수나 메소드를 원하는 대로 감출 수 있기 때문에 필요 이상의 변수나 메소드가 클래스 외부로 노출되는 것을 방어햐여 보안도 챙길 수 있다. 이렇게 클래스 내부의 데이터를 감추는 것을 정보 은닉(Information Hiding)이라고 하며, 보통 public, private, protected 같은 접근제한자를 사용하여 원하는 정보를 감추거나 노출시킬 수 있다. Capsulation.java1234567891011class Person { public String name; private int age; protected String address; public Person (String name, int age, String address) { this.name = name; this.age = age; this.address = address; }} 자 이렇게 간단한 클래스를 하나 만들어보았다. Person 클래스는 생성자의 인자로 들어온 값들을 자신의 멤버 변수에 할당하는데, 이 멤버 변수들은 각각 public, private, protected의 접근제한자를 가지고 있는 친구들이다. 그럼 한번 객체를 생성해보고 이 친구들의 멤버 변수에 접근이 가능한지를 알아보자. Capsulation.java12345678class CapsulationTest { public static void main (String[] args) { Person evan = new Person(\"Evan\", 29, \"Seoul\"); System.out.println(evan.name); System.out.println(evan.age); System.out.println(evan.address); }} 자, 여기까지 직접 작성해보면 알겠지만 Java는 컴파일 언어이기 때문에 굳이 실행시켜보지 않더라도 IDE에서 이미 알아서 다 분석을 끝내고 빨간줄을 쫙쫙 그어주었을 것이다. 에러가 난 부분은 private 접근제한자를 사용한 멤버변수인 age이다. 이처럼 private 접근제한자를 사용하여 선언된 멤버 변수나 메소드는 클래스 내부에서만 사용될 수 있고 외부로는 아예 노출 자체가 되지 않는다. public과 protected를 사용하여 선언한 멤버 변수인 name과 address는 정상적으로 접근이 가능한 상태이다. public 같은 경우는 이름만 봐도 클래스 외부에서 마음대로 접근할 수 있도록 열어주는 접근제한자라는 것을 알 수 있지만, protected가 접근이 가능한 것은 조금 이상하다. 이름만 보면 왠지 이 친구도 private처럼 접근이 막혀야할 것 같은데 왜 외부에서 접근이 가능한 것일까? protected 접근제한자는 해당 클래스를 상속받은 클래스와 같은 패키지 안에 있는 클래스가 아니면 모두 접근을 막는 접근제한자인데, 위의 예시의 경우 필자는 Person 클래스와 CapsulationTest 클래스를 같은 파일에 선언했으므로 같은 패키지로 인식되어 접근이 가능했던 것이다. 그럼 Person 클래스를 다른 패키지로 분리해내면 어떻게 될까? 테스트 해보기 위해 먼저 MyPacks라는 디렉토리를 생성하고 그 안에 Person.java 파일을 따로 분리하여 별도의 패키지로 선언해주겠다. MyPacks/Person.java12345678910111213package MyPacks;public class Person { public String name; private int age; protected String address; public Person (String name, int age, String address) { this.name = name; this.age = age; this.address = address; }} Capsulation.java123456789import MyPacks.Person;class CapsulationTest { public static void main (String[] args) { Person evan = new Person(\"Evan\", 29, \"Seoul\"); System.out.println(evan.name); System.out.println(evan.address); }} 이렇게 Person 클래스를 별도의 패키지로 분리하면 이제 evan.address에도 빨간 줄이 쫙 그어진다. 이렇게 외부 패키지로 불러온 클래스 내부 내의 protected 멤버 변수나 메소드에는 바로 접근할 수 없다. 그러나 Person 클래스를 상속한다면 외부 패키지인지 아닌지 여부와 상관 없이 자식 클래스 내에서는 protected 멤버에 접근이 가능하다. Capsulation.java123456789101112131415import MyPacks.Person;class CapsulationTest { public static void main (String[] args) { Evan evan = new Evan(); }}class Evan extends Person { Evan () { super(\"Evan\", 29, \"Seoul\"); System.out.println(this.address); System.out.println(super.address); }} 12SeoulSeoul 접근제한자는 Java 뿐만 아니라 TypeScript, Ruby, C++ 등과 같이 OOP를 지원하는 많은 프로그래밍 언어들도 가지고 있는 기능이므로 이 개념을 잘 알아두면 클래스를 설계할 때 원하는 정보만 노출시키고 원하지 않는 정보는 감추는 방법을 사용하여 보안도 지킬 수 있고 클래스를 가져다 쓰는 사용자로 하여금 쓸데없는 고민을 안하게 해줄 수도 있다. 다형성다형성(Polymorphism)은 어떤 하나의 변수명이나 함수명이 상황에 따라서 다르게 해석될 수 있는 것을 의미한다. 다형성은 어떤 한가지 기능을 의미하는 것이 아니라 개념이기 때문에 여러가지 방법으로 표현할 수 있다. Java에서 다형성을 위한 대표적인 기능은 바로 추상 클래스(Abstract Class)와 인터페이스(Interface), 그리고 Overloading이 있다. 추상 클래스와 인터페이스는 사실 그 용도가 조금 다르지만 필자가 예로 들 간단한 예시에서는 그 차이를 크게 느끼기 힘들기도 하고 무엇보다 이 포스팅은 Java 포스팅이 아니라 단순히 다형성을 설명하기 위함이므로 필자는 이 중 추상 클래스만을 사용할 것이다. 그럼 이 기능들이 어떤 역할을 하는 지 살펴보면서 다형성이 무엇인가를 좀 더 자세히 알아보도록 하자. 먼저, 추상 클래스를 사용하여 다형성을 만족시키는 예시를 먼저 설명할텐데, 사실 다형성이라는 단어를 모르고 있던 분들이라도 자신도 모르게 이런 설계 패턴을 사용하고 있었을 수도 있을 정도로 기본적인 예시이다. 추상 클래스를 사용한 다형성 구현추상 클래스는 Java에서 다형성을 만족시키기 위해 자주 사용되는 대표적인 기능이다. 말로만 설명하면 재미가 없으니 한번 코드를 직접 눈으로 보는 것이 좋은데, 필자는 오버워치를 좋아하기 때문에 추상 클래스에 대한 예시도 오버워치를 가져와서 설명하겠다. 갓겜 고오급 시계 자, 필자는 이제 오버워치의 여러 영웅들을 클래스로 만드려고 한다. 오버워치의 영웅들은 공통적으로 궁극기 게이지가 찼을 때 Q 버튼을 누르면 궁극기가 발동된다라는 기능을 가지고 있다. 하지만 오버워치의 영웅들은 각자 특색에 맞게 다른 궁극기를 가지고 있는데, 라인하르트는 망치를 내리치며 다른 영웅들을 기절시키고 맥크리는 시야에 보이는 여러 영웅에게 동시에 헤드샷을 날릴 수 있으며 메이는 로봇을 던져서 일정 범위 안의 영웅들을 얼린다. 이런 경우 다형성을 가지지 않은 오버워치 영웅 클래스는 다음과 같은 모습을 보일 것이다. 12345678910111213141516171819202122232425262728293031323334class Hero { public String name; Hero (String name) { this.name = name; }}class Reinhardt extends Hero { Reinhardt () { super(\"reinhardt\"); } public void attackHammer () { System.out.println(\"망치 나가신다!\"); }}class McCree extends Hero { McCree () { super(\"mccree\"); } public void attackGun () { System.out.println(\"석양이 진다. 빵야빵야\"); }}class Mei extends Hero { Mei () { super(\"mei\"); } public void throwRobot () { System.out.println(\"꼼짝 마! 움직이지 마세요!\"); }} 이때 만약 우리가 Hero 클래스를 상속받은 영웅 클래스들의 궁극기를 발동시키고 싶다면 어떻게 해야할까? 안봐도 뻔하겠지만 눈물나는 if문 또는 switch문의 향연이 펼쳐질 것이다. 모든 영웅들의 궁극기 발동 메소드의 이름이 다르기 때문에 달리 방도가 없다. 그리고 추가적으로 Hero 클래스에는 궁극기 발동 메소드가 없기 때문에 객체를 해당 영웅의 클래스로 형변환 해줘야하는 불편한 작업도 해야한다. 1234567891011121314151617181920212223242526class Main { public static void main (String[] args) { Mei myMei = new Mei(); Reinhardt myReinhardt = new Reinhardt(); McCree myMcCree = new McCree(); Main.doUltimate(myMei); Main.doUltimate(myReinhardt); Main.doUltimate(myMcCree); } public static void doUltimate (Hero hero) { if (hero instanceof Reinhardt) { Reinhardt myHero = (Reinhardt)hero; myHero.attackHammer(); } else if (hero instanceof McCree) { McCree myHero = (McCree)hero; myHero.attackGun(); } else if (hero instanceof Mei) { Mei myHero = (Mei)hero; myHero.throwRobot(); } }} 123꼼짝 마! 움직이지 마세요!망치 나가신다!석양이 진다. 빵야빵야 여기에 영웅이 더 추가된다면 영웅의 종류 만큼 분기의 개수도 늘어날 것이고, 무엇보다 Mei myHero = (Mei)hero처럼 굳이 새로운 변수를 선언하면서 사용하고 있는 걸 보자니 마음이 한켠이 먹먹해져온다. 다형성은 바로 이럴 때 우리를 행복하게 만들어 줄 수 있는 단비와 같은 개념이다. 자, 아까 위에서 필자는 다형성의 개념을 어떤 하나의 변수명이나 함수명이 상황에 따라서 다르게 해석될 수 있는 것이라고 했다. 그렇다면 이 경우 우리는 영웅들의 궁극기 호출 메소드명을 ultimate로 통일하되, 이 메소드를 호출했을 때 실행되는 코드는 영웅에 따라 달라지도록 만들면 다형성을 만족시킬 수 있는 것이다. 이런 경우 그냥 Hero 클래스를 상속받은 영웅 클래스들에게 직접 하나하나 ultimate라는 메소드를 선언할 수도 있지만, 그렇게 되면 개발자가 실수할 확률이 존재한다.(특히 오타로 인한 실수가 가장 많을 것이다) 그래서 Java는 개발자가 특정 메소드를 강제로 구현하도록 만들어주는 기능을 제공한다. 그 기능이 바로 추상 클래스(Abstract Class)와 인터페이스(Interface)인 것이다. 필자는 위에서 한번 이야기 했듯이 이 중 추상 클래스만을 사용하여 예제를 진행할 것이다. 그래도 혹시 이 두 기능이 뭐가 다른지 궁금하신 분이 있을 것 같으니 최대한 간단히만 설명하고 넘어가자면, 추상 클래스는 어떤 클래스의 기능을 그대로 사용하면서 그 기능을 확장하고 싶을 때 사용하는 것이고 인터페이스는 아무런 구현체가 없는 그냥 껍데기만 구현하는 것이다. 그렇기 때문에 인터페이스에는 자세한 메소드의 구현체가 들어갈 수 없지만 추상 클래스는 자체적인 메소드의 구현체를 가질 수도 있다.(Java 8부터는 default 키워드를 사용하여 인터페이스에도 메소드 구현체를 넣을 수 있게 변경되긴했다. 덕분에 구분이 더 애매해짐.) 이 예제의 Hero 클래스는 name 멤버 변수를 생성자로부터 받아서 자신의 멤버 변수로 추가하는 기능을 가지고 있기 때문에 추상 클래스를 사용하는 것이 더 적절하다. 그럼 이제 추상 클래스를 사용하여 ultimate 메소드의 구현을 강제해보도록 하자. 12345678910111213141516171819202122232425262728293031323334353637abstract class Hero { public String name; Hero (String name) { this.name = name; } // 내부 구현체가 없는 추상 메소드를 선언한다. public abstract void ultimate ();}class Reinhardt extends Hero { Reinhardt () { super(\"reinhardt\"); } public void ultimate () { System.out.println(\"망치 나가신다!\"); }}class McCree extends Hero { McCree () { super(\"mccree\"); } public void ultimate () { System.out.println(\"석양이 진다. 빵야빵야\"); }}class Mei extends Hero { Mei () { super(\"mei\"); } public void ultimate () { System.out.println(\"꼼짝 마! 움직이지 마세요!\"); }} 이렇게 추상 클래스인 Hero를 상속받은 영웅 클래스들은 무조건 ultimate 메소드를 구현해야한다. 이렇게 메소드명이 통일되면 영웅 클래스를 가져다 쓰는 입장에서는 궁극기를 발동시키고 싶을 때 어떤 메소드를 호출해야할지 이제 더 이상 고민할 필요가 없다. 1234567891011121314151617class Main { public static void main (String[] args) { Mei myMei = new Mei(); Reinhardt myReinhardt = new Reinhardt(); McCree myMcCree = new McCree(); Main.doUltimate(myMei); Main.doUltimate(myReinhardt); Main.doUltimate(myMcCree); } public static void doUltimate (Hero hero) { // Hero 클래스를 상속받은 클래스는 // 무조건 ultimate 메소드를 가지고 있다는 것이 보장된다. hero.ultimate(); }} 어떤가? 코드가 훨씬 심플해지지 않았는가? 추상 메소드를 사용하여 클래스 내부의 ultimate라는 메소드의 구현을 강제했기 때문에 Hero 클래스를 상속받은 영웅 클래스에 해당 메소드가 없을 확률은 0%이다. 그렇기 때문에 사용하는 입장에서는 깊은 고민없이 안심하고 ultimate 메소드를 호출할 수 있다. 또한 ultimate 메소드는 모든 영웅 클래스들이 가지고 있는 메소드이지만 내부 구현은 전부 다르기 때문에 발동하는 스킬 또한 영웅 별로 다르게 나올 것이다. 이런 것을 바로 다형성이라고 하는 것이다. 오버로딩을 사용한 다형성 구현이번에는 오버로딩(Overloading)을 사용한 다형성의 예시를 한번 살펴보도록 하자. 위의 상속 챕터에서 잠깐 언급하고 넘어간 오버라이딩(Overriding)과 헷갈리지 말자. 오버라이딩은 부모 클래스의 멤버 변수나 메소드를 덮어 씌우는 것이고, 오버로딩은 같은 이름의 메소드를 상황에 따라 다르게 사용할 수 있게 해주는 다형성을 위한 기능이다.(필자는 학교에서 시험볼 때 자주 헷갈렸다) 오버로딩은 생각보다 단순한 개념이지만, 만약 오버로딩을 지원하지 않는 언어인 JavaScipt나 Python을 주로 사용하는 개발자들에게는 나름 충공깽일 수 있다. 그 이유는 바로 오버로딩이 메소드의 인자로 어떤 것을 넘기냐에 따라서 이름만 같은 다른 메소드가 호출되는 기능이기 때문이다. 이게 뭔 개소리야? 어떤 클래스가 sum이라는 메소드를 가지고 있다고 생각해보자. 이때 sum은 두 개의 인자를 받은 후 이 두 값을 합쳐서 리턴하는 내부 구조를 가지고 있다. 근데 만약 3개를 합치고 싶다면 어떻게 해야할까? 이런 경우에 JavaScript와 같이 오버로딩을 지원하지 않는 언어에서는 편법을 사용할 수 밖에 없다. 1234567class Calculator { sum (...args) { return args.reduce((prev, current) => prev + current); }}const c = new Calculator();c.sum(1, 2, 3, 4, 5); 115 뭐 어쨌든 되긴 되니까 상관없다고 생각할 수 있지만, 이건 객체의 다형성이라기보다 그냥 JavaScript의 언어적인 특성을 사용하여 우회한 것에 불과하다. 이렇게 작성하면 두 개의 인자를 더해서 반환하는 메소드에서 n개의 인자를 더해서 반환하는 메소드로는 만들 수 있지만 객체의 다형성을 만족할 수는 없다. 이 메소드의 더한다라는 기능 자체도 변경할 수 있어야 그제서야 다형성을 만족한다고 할 수 있는 것이다. 반면, Java나 C++과 같은 언어에서는 제대로 다형성을 만족시킬 수 있는 오버로딩을 지원한다. 1234567891011class Overloading { public int sum (int a, int b) { return a + b; } public int sum (int a, int b, int c) { return a + b + c; } public String sum (String a, String b) { return a + b + \"입니다.\"; }} 쨘, 간단한 클래스를 하나 선언하고 sum이라는 메소드를 여러 개 선언했다. 만약 JavaScript에서 이렇게 선언했다가는 위에 선언된 두개의 sum은 무시되고 맨 아래의 sum 메소드로 덮어씌워지기 때문에 오버로딩을 할 수가 없다. 그리고 문자열을 인자로 받는 sum 메소드의 경우에는 문자열 맨 뒤에 입니다도 붙히는 센스를 발휘하도록 만들어주었다. JavaScript에서는 이 동작을 구현하려면 반드시 타입을 체크하는 조건 분기문이 필요하지만 Java는 오버로딩을 지원하기 때문에 그럴 필요가 없다. 그럼 이제 한번 이 메소드들이 잘 작동하나 호출해보도록 하자. 12345678class Main { public static void main (String[] args) { Overloading o = new Overloading(); System.out.println(o.sum(1, 2)); System.out.println(o.sum(1, 2, 3)); System.out.println(o.sum(\"자\", \"바\")); }} 12336자바입니다. 위의 예시에서 볼 수 있듯이 Overloading 클래스는 여러 개의 sum 메소드를 가지고 있고, 메소드의 인자가 무엇인지에 따라서 이름만 동일한 다른 메소드들을 호출해주고 있다. 이것이 오버로딩이며, Java에서 제공해주는 대표적인 다형성 지원 기능 중 하나이다.(오버라이딩이랑 헷갈리지 말자!) 마치며사실 이 포스팅을 작성할 때 생각했던 타겟 독자층은 컴퓨터 공학을 전공한 개발자들이 아니였다. 애초에 컴퓨터 공학을 전공하거나, 타 과라도 컴퓨터 공학 전공 수업을 들었던 사람들은 대부분 학교에서 객체 지향 프로그래밍이라는 수업을 들어보았을 것이기 때문에 이 개념에 대해서 어느 정도 알고 있을 것이다. 필자가 이 포스팅의 타겟으로 한 독자 층은 바로 학원이나 부트캠프에서 코딩을 처음 배우신지 얼마 안된 분들이다. 학원이나 부트캠프에서는 Java를 가르치는 경우가 아니라면 OOP에 대한 내용을 거의 언급하지 않고 넘어가는 경우가 많은 것으로 알고 있다. 사실 학교와 다르게 학원은 짧은 기간 안에 실무를 할 수 있는 인재를 양성하여 취업시키는 것이 목적인 기관이라는 점을 생각해보면 이해가 안가는 것도 아니지만, OOP는 Java에만 국한된 개념이 아니라 어떤 언어를 사용하더라도 적용할 수 있는 범용적인 프로그래밍 패러다임이기 때문에 이에 대한 내용을 가르치지 않는 것이 안타깝긴 하다. 참고로 필자는 OOP가 좋은 패러다임이니까 배워야 한다라고 이야기하는 것이 아니다. 이 포스팅의 서두에서 한번 언급했듯이 전 세계에서 상당한 점유율을 차지하고 있는 메이저 언어인 Java, Python, C++과 같은 언어들이 대부분 OOP를 기반으로 설계되었거나 OOP를 지원하기 때문에 2019년에 프로그래밍을 하는 개발자라면 좋든 싫든 OOP를 알고는 있어야 한다고 생각하기 때문에 OOP를 추천하는 것이다. TIOBE의 2019년 8월 전 세계 언어 순위 C와 JavaScript, SQL을 제외한 모든 언어가 OOP를 사용한다. 어차피 프로그래밍 패러다임에는 정답이 없다. 선언적 프로그래밍이 좋은 것이냐, 명령적 프로그래밍이 좋은 것이냐라고 물어보면 쉽사리 대답할 수 없는 것 처럼 말이다. 그냥 우리는 어떤 패러다임이 어떤 방향을 추구하는지, 거기서 파생된 개념은 어떤 것들이 있는 지를 학습하고 각기 상황에 맞는 패러다임을 도입해서 사용하면 되는 것이다. 어쨌든 이 포스팅을 통해 혹시나 OOP를 모르고 있었거나, 아니면 너무 어렵게 느끼고 있던 분들이 좀 더 OOP를 친숙하게 받아들일 수 있기를 바라는 마음이다. 이상으로 알고 보면 재밌는 객체 지향 프로그래밍, OOP 흝어보기 포스팅을 마친다.","link":"/2019/08/24/what-is-object-oriented-programming/"},{"title":"어떻게 하면 안전하게 함수를 합성할 수 있을까?","text":"함수형 프로그래밍에서 코드를 작성한다는 것은 프로그램에서 수행해야하는 여러가지 행위들을 함수로 표현하고, 또 그 함수들을 요리조리 잘 합성해가며 거대한 프로그램을 만들어나가는 패러다임이다. 결국 함수형 프로그래밍에서 함수를 합성하는 행위라는 것은 이 패러다임의 근간이 되는 개념이기 때문에 굉장히 큰 의미를 가질 수 밖에 없는데, 문제는 이렇게 함수를 합성하는 과정에서 크고 작은 현실적인 문제들이 빵빵 터진다는 것이다. 이런 문제가 빵빵 터지는 가장 큰 이유는 간단하다. 아무리 우리가 순수 함수를 사용한다고 해도 수학의 함수와 완벽하게 똑같을 수는 없기 때문이다. 애초에 프로그래밍과 수학은 비슷하지만 엄연히 다른 학문이다. 그래서 전 세계의 똑똑이들은 이런 문제들을 해결하기 위해 펑터(Functor)나 모나드(Monad)와 같은 수학의 개념들을 끌고 와서 사용하기 시작했는데, 문제는 이 개념들이 직관적으로 이해하기에는 너무나도 추상적이고 난해한 녀석들이라는 것이다. 보기만 해도 눈물이 절로 나오는 펑터 설명 다이어그램… 필자가 펑터나 모나드에 대한 공부를 하면서 구글링을 하면서 찾아본 많은 자료들은 대략 두 가지 정도로 나누어졌는데, 바로 “겁나 어려운 수학적인 설명”과 “코드로 된 예시”였다. 문제는 이 수학적인 설명과 코드로 된 예시 사이를 이어주는 자료가 별로 없었다는 것이다. 즉, 펑터나 모나드가 정확히 프로그래밍의 어떤 문제를 해결하기 위해 도입된 것인지 쉽게 풀어서 설명해놓은 자료가 별로 없었다. (남들은 다 이해하는데 필자가 멍청해서 이해를 못한 것일수도 있다) 하지만 그렇다고해서 제대로 된 이유도 모르고 펑터나 모나드를 사용하고 싶지는 않았기에 직접 조사해보고 조져보기로 했다. 그래서 이번 포스팅에서는 함수형 프로그래밍에서 별 생각없이 함수를 조합하면 어떤 문제들이 발생하는지, 그리고 그 문제들을 어떤 방식으로 해결할 수 있는 지에 대한 이야기를 해보려고 한다. 모든 것은 함수의 합성으로 이루어진다다시 한번 이야기하지만 함수형 프로그래밍은 프로그램에서 수행해야하는 어떠한 행위들을 함수로 표현하고, 또 그 함수들을 이렇게 저렇게 잘 합성하여 거대한 프로그램을 만들어나가는 패러다임이다. 즉, 이러한 함수형 프로그래밍의 정의에서 가장 중요한 키워드는 역시 “함수의 합성”이라고 말할 수 있다. 함수형 프로그래밍에서 그토록 사이드 이펙트를 경계하는 이유도 결국 함수를 안전하게 합성하기 위해서는 함수의 입력과 출력을 예측할 수 있어야하기 때문이다. 함수형 프로그래밍의 세계에서는 프로그램 내부에서 발생하는 모든 행위들을 함수로 표현하고 있기 때문에 변수에 값을 할당하거나 간단한 사칙연산 조차도 함수로 표현된다. 123456789// 명령형 프로그래밍const foo: number = 1;foo + 2;// 함수형 프로그래밍const foo = ((): number => 1)();const add2 = (x: number): number => x + 2;add2(foo); 이 프로그램은 number 타입의 변수를 선언하고, 그 값에 2를 더하는 초 간단한 프로그램이다. 명령형 프로그래밍으로 작성된 코드에서는 단순히 foo = 1과 같이 표현할 수 있었던 변수의 할당은 1을 반환하는 함수로, foo + 2로 표현하던 연산은 add2(foo)와 같은 함수로 표현되었다. 우리가 이 코드에서 주목해야할 부분은 바로 가장 마지막 줄의 add2(foo)이다. add2(foo)라는 것은 foo 변수에 할당되었던 익명 함수의 출력 값인 1을 add2 함수의 입력 값으로 사용하겠다는 의미이며, 이러한 행위가 바로 함수의 합성이다. 123// 조금 더 간단하게 표현한 모습은 이렇다const add2 = x => x + 2;add2( (() => 1)() ); 변수에 값을 할당하고 더하는 간단한 연산 조차도 함수로 표현해야하는 함수형 프로그래밍의 세계에서 거대한 프로그램을 견고하게 만든다는 것은 여러가지 복잡한 함수들을 어떻게 잘 합성해서 사용할 수 있는지에 따라 좌지우지될 수 있다는 뜻이다. 이렇게 보면 굉장히 간단한 개념이지만, 사실 아무 함수끼리나 막 합성할 수 있는 것은 아니다. 함수의 합성에는 아주 중요한 규칙이 한 가지 정해져있는데, 바로 합성하려하는 함수들의 “정의역과 치역이 서로 일치해야한다는 것”이다. 정의역과 치역이 일치해야 함수를 합성할 수 있다이전에 작성했던 수학에서 기원한 프로그래밍 패러다임, 순수 함수 포스팅에서 한 차례 이야기한 적이 있지만, 함수형 프로그래밍에서는 함수의 사이드 이펙트를 최대한 없애버리기 위해 순수 함수를 사용한다. 대표적인 순수 함수의 특징은 대략 이 두 가지이다. 함수 외부의 상태를 변경하거나 참조하지 않는다! 동일한 입력을 넣었으면 항상 동일한 출력을 반환해야 한다! 이런 순수 함수를 사용하면 개발자가 함수의 행동을 예측하기 쉬워지기 때문에 디버깅이 편리하다는 장점도 있지만, 사실 애초에 저 규칙들이 지켜지지 않는다면 함수를 합성할 수 없기 때문에 모든 행위를 함수로 표현하고 조합해서 프로그래밍을 만드는 짓을 할 수 조차 없다. 왜 저 규칙들을 준수해야 함수의 합성이 가능하다는 것일까? 일단 순수 함수는 수학의 함수를 프로그래밍으로 구현한 개념이니, 한번 수학의 함수가 어떤 느낌으로 작동하는 녀석인지부터 살펴보도록 하자. 일단 수학의 함수는 함수의 입력으로 사용할 수 있는 값들의 집합인 정의역과, 함수의 출력으로 사용할 수 있는 값들의 집합인 치역을 가지고 있다. 그리고 정의역에 있는 원소 하나와 치역에 있는 원소 하나는 무조건 1:1로 매칭되어야한다. 즉, 동일한 입력을 함수에 넣었으면 항상 동일한 출력을 반환해야 한다는 말이다. 만약 이 규칙이 깨져버리면 그건 더 이상 함수라고 부를 수 없는 변태같은 무언가가 되어버린다. 쉽게 말해 함수의 입력으로 사용할 수 있도록 정해져 있는 값들 중에 하나를 뽑아서 함수에 던지면, 반드시 출력으로 사용할 수 있도록 정해져 있는 값들 중에 하나가 튀어나온다는 것이다. 그렇다면 이 개념을 그대로 프로그래밍으로 구현한 순수 함수에게도 정의역과 치역이라고 부를 수 있을만한 무언가가 있다는 말인데, 프로그래밍의 세계에서 살고 있는 순수 함수의 정의역과 치역은 무엇이 될 수 있을까? ... 바로 타입(Type)이다. 사실 프로그래밍에서 사용하는 타입이라는 녀석도 잘 생각해보면 일종의 집합이라고 볼 수 있다. number라는 집합은 {-1, 0, 0,1, 1, 2, NaN, Infinity...}과 같이 모든 숫자 값을 원소로 가지고 있는 집합이고, boolean이라는 집합은 {true, false}를 원소로 가지는 집합, string이라는 집합은 프로그래밍으로 만들어 낼 수 있는 모든 문자열들을 가지고 있는 집합이라는 이야기이다. 위에서 예시로 들었던 add2 함수를 다시 한번 가져와서 살펴보면, 이 함수는 number 타입을 가진 값을 받아서 다시 number 타입의 값을 반환하고 있다는 것을 알 수 있다. 1const add2 = (x: number): number => x + 2; 이때 add2 함수는 number 집합을 정의역과 치역으로 가지고 있다고 볼 수 있는 것이다. 여기에 더 나아가서 다른 형태의 함수들의 정의역과 치역도 모두 동일한 규칙으로 정의해볼 수 있다. 123type f = (): number => string; // 정의역: number, 치역: stringtype g = (): Array => boolean; // 정의역: Array, 치역: booleantype h = (): string => boolean; // 정의역: string, 치역: boolean 이제 슬슬 수학의 함수와 프로그래밍의 순수 함수 간의 공통점이 조금 더 보이기 시작한다. 그럼 이제 원래 본론이었던 함수의 합성에 대해 한번 이야기해보자. 사실 수학의 세계에서 함수를 합성하는 상황은 굉장히 흔한 일이며, 심지어 함수의 합성을 나타내는 전용 기호도 준비되어있다. $f$ 함수와 $g$ 함수를 합성한 합성함수 $h$는 이런 간단한 수식으로 나타낼 수 있다. h=g∘f\\begin{aligned} h = g\\circ f \\end{aligned}​h=g∘f​​ 갑자기 수식이 나와서 머리가 아프다면 그냥 $f$ 함수는 밥먹기, $g$ 함수는 그릇 치우기, 합성함수 $h$는 밥먹고 그릇 치우기 정도로 이해해도 아무 문제가 없다. 원래 함수란 그렇게 추상적인 느낌이다. 이 식에서 함수의 실행 순서는 오른쪽에서 왼쪽이다. 즉, 합성된 함수인 $h$ 함수를 $h(x)$와 같이 사용한다는 것은 사실 $g(f(x))$와 같이 함수를 합성해서 사용하는 것과 동일하다는 것이다. 하지만 이런 식으로 함수를 겹쳐가면서 합성을 표현한다면 많은 함수를 합성하는 공식을 보았을 때 괄호만 보일 게 뻔하므로 저 동그란 연산자를 사용하여 합성된 함수들을 펼쳐서 읽을 수 있도록 해주는 것이다. (콜백과 async/await의 차이를 생각해보자) 12// 함수 합성 연산자가 없다면 대충 이런 느낌이 되어 버리지 않을까...?foo(b(a(h(f(g(x)))))); 이때 함수를 합성하기 위해서는 첫 번째 함수인 $f$의 치역과 그 다음 함수인 $g$의 정의역이 동일해야 한다는 중요한 원칙이 있다. 방금 위에서 순수 함수의 정의역과 치역은 타입이라고 했으니, 첫 번째 함수의 출력 값의 타입과 그 다음 함수의 입력 값의 타입이 동일해야한다고 말할 수도 있을 것 같다. 1234567// 합성이 가능하다!f: number => numberg: number => number// 이건 합성이 불가능...f: number => stringg: number => number 정의역과 치역 어쩌고하면 조금 복잡해보일지 몰라도 타입으로 바꿔보니 굉장히 당연하기 짝이 없는 이야기가 되어버렸다. 그렇다면 순수 함수를 사용하면서 이 규칙만 잘 적용해주면 아무 문제가 없을까? 음, 대부분의 경우에는 가능하겠지만 슬프게도 모든 케이스를 커버할 수는 없다. 프로그래밍의 세계에는 함수의 실패라던가 불확실성과 같이 수학의 세계에는 없는 케이스들이 존재하기 때문이다. 수학의 함수를 프로그래밍적으로 구현한 순수 함수라 할 지라도 프로그래밍의 세계에 존재하는 이상 이런 케이스들을 모두 피해갈 수는 없다. 결국 아무리 순수 함수를 사용한다고 해도 이런 문제점들이 여전히 존재하기 때문에 전세계의 똑똑이들이 “도대체 어떻게 하면 안전하게 함수를 합성할 수 있을까?”라는 고민을 하게 된 것이고, 그 고민을 통해 도입된 것이 바로 펑터나 모나드와 같은 수학의 개념들인 것이다. 순수 함수에도 사이드 이펙트는 존재한다사이드 이펙트(Side-Effect)라는 단어는 한국어로 직역하면 부수 효과이다. 즉, 함수에게 기대하고있는 행위 외에 발생하는 모든 부수 효과들을 우리는 사이트 이펙트라고 하는 것이다. 함수가 외부 상태에 영향을 받는 것은 대표적인 사이드 이펙트 중 하나에 불과하다. 사실 순수 함수를 수학의 함수와 비교해보면 “같은 값을 입력받으면 늘 같은 출력을 반환한다”라는 규칙이 보장되는 것 외에는 허술하기 짝이 없는 함수이다. 예를 들어 문자열을 입력받은 후 그 문자열의 가장 첫번째 글자를 반환하는 함수가 있다고 생각해보자. 123function getFirstLetter (s: string): string { return s[0];} 일단 이 함수도 순수 함수는 맞다. 함수의 출력 값은 인자에만 영향을 받고 있고, 늘 같은 입력 값에는 같은 출력을 반환하고, 외부 상태에 전혀 영향도 받고 있지 않기 때문이다. getFirstLetter 함수는 주어진 문자열의 첫 글자를 반환하는 순수 함수이지만, 만약 빈 문자열이 인자로 주어질 경우 string형이 아닌 undefined를 반환할 것이다. 우리가 과연 이 함수를 사용할 때 “반드시 string 타입이 반환될꺼야”라고 장담할 수 있을까? 만약 이렇게 getFirstLetter 함수가 반드시 string 타입을 반환할 것이라고 장담하고 함수를 합성했다면 아마 이런 타입 에러를 만날 수 있을 것이다. 12345function getStringLength (s: string): number { return s.length;}getStringLength(getFirstLetter('')); 1Uncaught TypeError: Cannot read property 'length' of undefined 사실 이 에러 조차 사이드 이펙트라고 할 수 있다. 어찌되었든 우리의 순수 함수에게 기대했던 효과가 아니라 부수적으로 발생하고 있는 효과이기 때문이다. 이렇게 여러 개의 함수가 합성되어 있는 상황에서 단 하나의 함수라도 에러가 발생하면 합성 함수로 구성된 연산 전체가 망해버리기 때문에 우리는 이 사이드 이펙트를 반드시 관리해줘야 한다. 사실 getFirstLetter 함수의 치역은 string이 아니라, string 집합과 undefined 집합이 합쳐져 있는 string|undefined 집합이다. 그러니 우리는 이 두 함수의 정의역과 치역을 다시 설정해주고 예외 처리를 추가함으로써 이 문제를 해결할 수 있다. 123456789function getFirstLetter (s: string): string|undefined { return s[0];}function getStringLength (s: string|undefined): number { if (!s) { return -1; } return s.length;} 하지만 이렇게 어떤 함수가 여러 개의 집합이 합쳐진 치역을 가지기 시작하면 이 함수와 합성하기 위한 모든 함수의 정의역도 여러 개의 집합이 합쳐진 치역을 가져야하기 때문에, 결국 type|undefined처럼 암 걸리는 타입이 모든 함수에 적용되어야 할 것이다. 게다가 이런 상황이 발생할 때마다 함수 내부에서 매번 조건 검사를 통해 값의 유무를 검사하는 것은 너무나도 귀찮은 일이고, 여기저기서 동일한 코드가 계속 커플링되기 때문에 이 방법이 근본적인 해결책은 아닌 것 같다. 이렇게 함수에서 어떤 타입이 반환될지 장담할 수 없다는 불확실성 또한 결국 정의역과 치역을 일치시켜야하는 함수의 합성 과정에서 명확한 타입의 사용을 저해하는 요소가 되기 때문에 반드시 믿고 걸러야하는 사이드 이펙트라고 할 수 있다. 그럼 사이드 이펙트를 어떻게 관리해야할까?이런 상황이 발생하는 이유는 그냥 “컴퓨터는 수학이 아니니까”라고 말할 수 밖에 없다. 어쨌든 프로그램에서 돌아가는 모든 함수는 저런 문제들을 가지고 있다. 심지어 순수 함수라고 할지라도 말이다. 즉, 근본적으로 이 문제는 함수들 간의 합성 과정에서 어쩔 수 없이 발생하는 사이드 이펙트를 어떻게 하면 잘 관리해가면서 합성할 수 있을지에 대한 고민이다. 함수를 합성할 때 중간에 껴있는 함수에서 에러가 발생하더라도 합성된 함수의 연산을 안전하게 끝낼 수 있을 지, 불확실한 함수의 출력을 어떻게 하면 명확하게 만들어서 다음 함수로 전달할 수 있을 지 말이다. 그렇다면 함수를 다른 함수로 한번 감싸서 안전하게 예외처리를 진행하거나, 혹은 중간에 이상한 값이 나오면 그대로 다음 함수를 지나치게 만들면 되지 않을까? 12345678type StringFunction = (s: string) => number;function safety (x: string|undefined, fn: StringFunction) { return x ? fn(x) : x;}safety(getFirstLetter('Hi'), getStringLength);safety(getFirstLetter(''), getStringLength); 121undefined 하지만 이런 방식은 수많은 타입의 입출력을 가진 함수들에게 모두 적용하기에는 약간 무리가 있어보이니 제네릭 타입을 사용하여 조금 더 유연하게 만들어 보자. 123456function safety (x: T|undefined, fn: (x: T) => U) { return x ? fn(x) : x;}safety(getFirstLetter('Hi'), getStringLength);safety(getFirstLetter(''), getStringLength); 121undefined 오호 조금 그럴싸해졌다. 결국 safety 함수는 T또는 undefined의 값을 인자로 받은 후 이 인자가 undefined이라면 그대로 undefined을 반환하고, 만약 아니라면 T 타입을 인자로 받아서 U 타입을 반환하는 함수에게 인자를 넘겨주고 그 함수의 실행 결과를 반환한다. 12값이 있다: T -> fn값이 없다: T -> undefined 결국 우리는 getFirstLetter 함수의 치역과 getStringLength 함수의 정의역을 바로 연결해버리는 것이 아닌, x ? fn(x) : x라는 로직을 통해 함수의 사이드 이펙트를 한번 감싸준 다음 함수를 안전하게 합성한 것이다. 그렇다면 이 개념을 조금 더 확장해서 함수가 출력한 값을 사용할 때 일종의 안전장치 역할을 하는 함수가 늘 값을 감싸고 있다면 어떨까? 이런 느낌으로 함수의 정상적인 결과와 사이드 이펙트를 감싸줄 수 있는 무언가를 만들 수 있으면 이 문제를 해결할 수 있지 않을까? 만약 저 추상적인 무언가가 함수의 사이드 이펙트를 관리해주면서 다른 함수와의 합성을 진행할수만 있다면, 함수를 합성하는 과정에서 일일히 저런 예외 처리를 해주지 않아도 되고, 함수들의 입출력에 대한 타입 안정성도 가져가며 마음 놓고 합성을 쭉쭉 해나갈수 있을 것 같다. 123// 이런 느낌으로 말이다!f: Something -> Somethingg: Something -> Something 함수는 제대로 된 치역에 해당하는 값을 반환할 수도 있고 사이드 이펙트를 일으킬 수 있는 null이나 undefined 같은 값을 반환할 수도 있지만, 뭐가 되었든 저 Something이라는 녀석이 알아서 예외를 핸들링할 수 있도록 만들기만 한다면 우리는 그런 자잘한 건 신경쓰지 않고 함수를 쭉쭉 합성할 수 있기 때문이다. 그리고 이런 느낌이라면 null이나 undefined를 관리하는 것 외에도, 다양한 로직을 값에다가 감싸서 사용하면 되니까 나름 확장성도 좋은 개념인 것 같다. 뭐 대충 이런 느낌으로 말이다. 123Maybe = 값이 있을 수도 있고 없을 수도 있다Promise = 지금은 값이 없는데 나중에 값이 생기면 값을 준다List = 같은 속성의 값을 여러 개 가지고 있을 수도 있다 그리고 이렇게 값을 감싸고 있는 무언가를 효율적으로 사용하려면 내부에 있는 값을 자유롭게 변경할 수 있어야 하므로 Maybe -> Maybe와 같은 동작을 수행할 수 있는 무언가도 필요할 것 같다. 이런 고민 끝에 프로그래머들은 이런 비슷한 역할을 수행하는 수학의 한 개념을 차용하게 되는데, 그 개념이 바로 펑터(Functor)이다. 펑터란 무엇일까?펑터는 보통 값을 품고 있는 어떠한 박스의 형태로 설명되고는 한다. 방금 위에서 설명한 것과 같이 함수의 정상적인 결과와 사이드 이펙트를 감싸서 처리할 수 있는 무언가를 설명하기에는 박스가 적절한 예시이기 때문이다. [출처] Functors, Applicatives, And Monads In Pictures 저 박스는 결국 안전하게 값을 사용할 수 있도록 도와주는 로직을 가지고 있거나, 혹은 여러 개의 값을 처리할 수 있는 로직을 가지고 있거나, 아직은 값이 결정되지 않았지만 나중에 값이 결정되고 나면 값을 사용할 수 있는 로직을 가지고 있는 등, 값을 사용할 때 도움을 주는 여러가지 로직을 담고 있는 마법의 박스라고 할 수 있다. 이때 이 박스는 역할이 고정된 것이 아니라 Maybe, Promise 등 다양한 기능을 가질 수도 있기 때문에 문맥(Context)이라는 이름으로 불리기도 한다. 우리가 방금 만들었던 safety 함수도 값을 감싸고 있는 일종의 박스라고 생각해볼 수 있다. 123function safety (x: T|undefined, fn: (x: T) => U) { return x ? fn(x) : x;} x 값이 있으면 fn(x)를 실행하고, 없으면 그대로 x를 반환하는 박스 즉, x라는 값을 바로 사용하는 것이 아니라 safety 함수에 x라는 값을 넣어서 사용하고 있으므로 safety 함수를 일종의 박스라고 생각하자는 것이 저 설명의 취지이다. 사실 펑터의 개념적인 내용은 이게 전부라고 할 수 있고, 이후 펑터를 구현하는 방법만 익혀도 펑터를 사용함에 있어서는 아무런 지장이 없다. 그러나 이번 포스팅의 목적은 펑터가 무엇인지 조금 더 깊숙하게 조져보는 것이므로 필자는 조금 더 근본적인 펑터의 개념에 대해서 이야기해볼까한다. 카테고리(Category)펑터(Functor)는 수학의 카테고리 이론(Category Theory)에 등장하는 개념이며, 동일한 구조를 가지고 있는 카테고리들의 관계를 정의할 수 있는 구조체라고 정의된다. 그렇기 때문에 펑터가 본질적으로 무엇인지, 왜 Something이라는 것을 통해 함수를 안전하게 합성하기위해 펑터가 필요하다는 것인지 알기 위해서는 카테고리라는 개념에 대해 알고 있어야 한다. 사실 수학에서 이야기하는 카테고리라는 개념은 우리가 일상 생활 속에서 사용하는 카테고리의 의미와 크게 다르지 않다. 뭐 비슷한 것들을 묶어놓은 그런 개념이랄까…? 이렇게 마음을 가볍게 먹은 후 카테고리 이론을 위키피디아에 검색해보면 아래와 같은 검색 결과를 만날 수 있게 된다. 범주 $\\mathcal {C}$는 다음과 같은 데이터로 구성된다. 대상(對象, 영어: object)들의 모임 $ob(C)$. 이 모임의 원소를 $C$의 “대상”이라고 한다. 임의의 두 대상 $a,b \\in ob(C)$에 대하여, $a$를 정의역으로, $b$를 공역으로 하는 사상(寫像, 영어: morphism)들의 모임 $f\\colon a\\to b$로 쓰고, $f$를 $a$에서 $b$로 가는 사상’이라고 한다. $C$의 사상의 모임을 $\\hom(C)$로 나타낸다. 임의의 세 대상 $a,b,c\\in ob(C)$에 대하여, 이항 연산 $\\hom(a,b)\\times \\hom(b,c)\\to \\hom(a,c)$. 이를 사상의 합성(合成, 영어: composition)이라고 한다. $f\\colon a\\to b$와 $g\\colon b\\to c$의 합성은 $g\\circ f$ 또는 $gf$ 등으로 나타낸다.… 위키피디아범주(수학) 읭…? 사실 카테고리 이론의 개괄적인 내용은 누구나 다 간단하게 이해할 수 있는 수준의 내용이다. 단지 추상적인 학문인 수학의 특성 상 일상적인 언어로 풀어서 설명하면 너무 길어지고 복잡해지니 간단하게 축약할 수 있는 단어와 기호들로 표현한 것 뿐이다. (사실 이게 수포자가 생기는 원인 중 하나) 일단 위에서 이야기 했듯이 수학에서 이야기하는 카테고리는 쇼핑몰 사이트에 있는 그 카테고리가 맞다. 다만 수학의 카테고리는 조금 더 추상적인 개념이기 때문에 물건으로 구성되는 카테고리일수도 있고 자연수로 구성된 카테고리일수도 있으며, 때로는 함수로 구성된 카테고리가 될 수도 있다는 차이점이 있다. 위의 수학적 정의에서 카테고리는 대상(Object)과 사상(Morphism)이라는 것으로 구성된다고 이야기하고 있다. 대상이라는 것은 그냥 카테고리 안에 있는 하나의 객체이다. 만약 패션 쇼핑몰의 상품 카테고리라면 대상은 셔츠, 맨투맨, 아우터, 코트가 될 것이고, 자연수로 이루어진 카테고리라면 1, 2, 3과 같은 수가 될 것이다. 여기까지는 우리가 일상적으로 사용하는 카테고리라는 단어와 비슷한 느낌이기 때문에 이해가 그리 어렵지 않다. 그러나 수학에서의 카테고리는 대상 외에도 사상이라는 한 가지 데이터를 더 가지고 있다. 위의 수학적 정의를 다시 보면 사상은 임의의 두 대상 $a,b \\in ob(C)$에 대하여, $a$를 정의역으로, $b$를 공역으로 하는 무언가라고 한다. 사실 $a,b \\in ob(C)$라는 말은 $a$와 $b$라는 대상이 카테고리 $C$안에 있다는 것을 의미하는 것이니 그냥 넘어가도록 하고, 우리가 집중해야할 단어는 정의역과 공역이다. 정의역과 공역이라는 단어를 듣고 가장 먼저 생각나는 단어가 무엇일까? 바로 함수이다. $a$를 정의역으로, $b$를 공역으로 한다는 이야기는, $a$라는 대상에 어떤 사상(함수)를 적용하면 $b$가 된다는 것을 이야기하고 있는 것이다. 12345const 대상A = 1;const 사상add1 = x => x + 1;// 대상 A에게 add1이라는 사상을 적용하면...사상add1(대상A); 12 // 대상 B가 된다 즉, 사상이라는 것은 대상과 대상 간의 관계를 나타낼 수 있는 일종의 함수라고 생각하면 된다. 그래서 수학적 표현으로는 $f: a \\to b$라고 표현할 수 있는 것이고 프로그래밍적으로는 그냥 람다 함수로 (a) => b이라고 표현할 수 있는 것이다. (대상의 종류에 따라 사상이 함수가 아닌 경우도 있지만, 거기까진 생각하지 말자) 여기까지 이해했다면 보다 쉬운 설명을 위해 간단한 카테고리를 하나 가져와서 가지고 놀아보도록 하겠다. 대충 이런 구조를 가진 카테고리 $\\mathcal {C}$가 있다고 생각해보자. 이 카테고리에서 대상은 자전거, 자동차, 비행기이고 사상은 객체들 사이에 있는 빠름이라는 화살표이다. 즉, 이 카테고리에서 자전거에 빠름이라는 사상을 적용하면 자동차가 되고, 자동차에 다시 빠름라는 사상을 적용하면 비행기가 된다는 뜻이다. 사상으로 객체들 간의 관계를 표현할 수 있다는 말은 이런 의미이다. 자전거가 빨라지면 자동차가 되고, 자동차가 빨라지면 비행기가 되는 것이니 말이다. 그리고 사상을 적용한다는 것은 함수를 적용한다는 말과 같으므로 간단한 코드로 이 카테고리의 구조를 표현해볼 수도 있겠다. 123456789const 카테고리 = ['자전거', '자동차', '비행기'];function 빠름 (카테고리, 대상) { const index = 카테고리.findIndex(v => v === 대상); return 카테고리[index + 1];}빠름(카테고리, '자전거');빠름(카테고리, '자동차'); 12'자동차''비행기' 그리고 자전거에서 비행기로 바로 그어진 빠름 $\\circ$ 빠름 사상은 빠름 사상 두 개를 합성한 것을 의미하니까, 코드로는 함수 두 개를 합성한 형태인 빠름(빠름(자전거)) === '비행기'로 표현할 수 있다. 결국 사상의 합성이라는 것을 프로그래밍으로 표현하면 그냥 함수를 합성하는 것 그 이상도 이하도 아니다. 이 간단한 걸 수학적인 정의로 이야기하면 이렇게 복잡해보이는 이야기가 되는 것이다. 임의의 세 대상 $a,b,c\\in ob(C)$에 대하여, 이항 연산 $\\hom(a,b)\\times \\hom(b,c)\\to \\hom(a,c)$. 이를 사상의 합성(合成, 영어: composition)이라고 한다. $f\\colon a\\to b$와 $g\\colon b\\to c$의 합성은 $g\\circ f$ 또는 $gf$ 등으로 나타낸다. 이 정의에서 이야기하는 임의의 세 대상은 각각 위 카테고리의 자전거, 자동차, 비행기라고 생각하면 된다. 그리고 $hom(a,b)$라는 말에서 나오는 $hom$이라는 녀석은 여러 개의 사상을 가지고 있는, 사상의 집합을 의미한다. 이 정의에서 사상이 하나가 아닌 여러 개라고 이야기하는 이유는 간단하다. 위의 카테고리만 보더라도 자전거와 자동차 간의 관계가 단지 빠름이라는 것만 있지는 않을 것이니 말이다. 자전거 -빠름-> 자동차 자전거 -비쌈-> 자동차 자전거 -크기가 큼-> 자동차 자전거 -엔진이 달림-> 자동차 뭐 이런 식으로 어떤 대상과 대상 사이에는 여러 개의 사상이 존재할 수 있기 때문에, 이 사상들의 집합을 퉁쳐서 $hom$라고 표현한 것이다. 뭐 이딴 것까지 하나하나 다 신경쓰고 있냐고 할 수도 있지만, 수학은 분명히 정답이 존재해야하고, 절대 예외를 허용하지 않는 논리적인 학문이기 때문에 이렇게 모든 케이스를 전부 고려한 정의를 만들어줘야한다. 그리고 $f\\colon a\\to b$ 라는 표현은 $hom$에 있는 여러 개의 사상 중에서, $a$ 대상과 $b$ 대상에 단 하나의 사상만 적용한 경우를 말한다. 저 사상들 중 무슨 사상을 적용할지는 모르겠는데, 어쨌든 적용할 때는 한번에 하나만 적용해야하기 때문이다. (여러 개를 동시에 적용할거야!는 양자 컴퓨터가 아니면 불가능하다) 그리고 마지막으로 우리가 자전거에다 빠름 사상을 두 번 적용한 것과 같이 사상을 합성한 것을 $f: a\\to b$ 사상과 $g: b\\to c$ 사상이 합성된 $g\\circ f$, 사상의 합성이라고 표현하는 것이다. 즉, 자전거라는 대상에 사상 $f$를 적용하고, 다시 사상 $g$를 적용하면 비행기라는 대상이 된다는 것을 이야기 하는 것이며, 이것도 그냥 코드로 표현하면 그냥 g(f(자전거)) === '비행기'라고 할 수 있겠다. 이렇듯이 카테고리 이론은 굉장히 추상적인 이론이라 프로그램 안에서 벌어지는 일을 전부 저런 카테고리 모델로 표현해낼 수 있으며, 마찬가지로 우리가 함수형 프로그래밍을 하면서 어떤 값에 함수를 적용하고 합성하는 과정 또한 일종의 카테고리 모형으로 표현할 수 있는 것이다. 프로그램 내에서 벌어지는 모든 일은 이렇게 카테고리로 표현할 수 있다 필자는 개인적으로 여기까지가 펑터를 조금 더 쉽게 이해하기위한 카테고리 이론의 전부라고 생각한다. 프로그램에서 발생하는 일들을 일종의 카테고리로 정의할 수 있다는 사실까지 받아들이고 나면 펑터를 이해하는 것이 간단해지기 때문이다. 펑터(Functor)자, 이제 방금 만들었던 간단한 카테고리를 이제 조금 추상적인 모델로 바꿔보도록 하자. 비행기, 자동차, 자전거와 같은 이름은 변수 x, y, z로 변경하고 사상 빠름 역시 변수인 f와 g로 변경하겠다. 아까 우리가 만든 카테고리를 이렇게 추상적인 구조로 변경하고나니, 이런 구조를 가진 카테고리는 왠지 카테고리 $C$ 말고도 더 있을 것 같다는 생각이 든다. 그도 그럴것이 저런 구조의 대상과 사상을 가지는 카테고리는 굉장히 흔하고, 솔직히 어디다가 가져다 붙혀도 왠만한 정의에는 껴맞출 수 있는 보편적인 카테고리이기 때문이다. 그렇다면 대상에 사상을 적용하여 다른 대상으로 만들 수 있듯이, 카테고리에도 사상을 적용하여 다른 카테고리로 만들 수 있지 않을까? 이때 등장하는 것이 바로 펑터(Functor)이다. 즉, 펑터는 카테고리를 다른 카테고리로 변경할 수 있는 사상(함수)인 것이다. 카테고리 $C$가 아무리 복잡하게 구성되어있다고 해도 우리는 $F(C)$와 같이 카테고리에 펑터를 덮어 씌우기만 하면 간단하게 펑터를 사용할 수 있다. 그러면 펑터가 적용된 카테고리 내부에 있는 모든 대상과 사상들 또한 $F$라는 함수로 감싸져 있는 형태로 변경된다. 여기서 가장 중요한 것은 펑터로 감싸도 카테고리 자체의 구조는 절대 변하지 않는다는 점이다. 위 그림에서도 대상과 사상들에게 $F$라는 함수가 적용되었을 뿐 화살표 자체의 모양은 변하지 않은 것을 볼 수 있다. 12345const 대상x = 1;const 대상y = 2;const 사상f = x => x + 1;사상f(대상x) === 대상y; // true 12345const 대상x = 펑터(1);const 대상y = 펑터(2);const 사상f = 펑터(x => x + 1);사상f(대상x) === 대상y; // true 즉, 펑터를 사용하더라도 카테고리가 가지고 있는 규칙 자체는 절대 변경되지 않는다. 쉽게 말해 안전하게 대상이나 사상을 감싸기만 할 뿐, 그 외에 아무런 사이드 이펙트를 일으키지 않는다는 것이다. 아까 전에 프로그래밍에서 발생하는 모든 일도 일종의 카테고리로 표현할 수 있다고 했던 것을 기억하는가? 아무리 복잡한 카테고리라고 해도, 단순히 펑터로 감싸기만 하면 기존 카테고리의 구조를 전혀 건드리지 않으면서 다른 카테고리로 변경할 수 있기 때문에 우리가 찾고있던 “값을 감싸서 안전하게 값을 사용하고 싶다”라는 니즈에 부합하는 개념인 것이다. 펑터를 직접 만들어보자!펑터라는 것은 그렇게 거창한 개념이 아니다. 간단하게 말해서 카테고리를 다른 카테고리로 바꿔주는 행위를 할 수 있으면 펑터인 것이다. 이때 펑터가 카테고리를 변경하는 행위를 매핑(mapping)이라고 하며, 조금 더 자세히 말해 카테고리에 함수를 적용하여 다른 카테고리로 변경하는 행위라고 할 수도 있다. 이렇게 추상적인 개념을 가진 펑터이기에, 누구는 펑터를 이래서 쓴다, 누구는 펑터를 저래서 쓴다와 같이 이야기가 전부 다른 것이다. 사실 펑터 자체는 그저 매핑이라는 행위를 할 수 있는 무언가에 불과하기 때문에 펑터를 어떤 방식으로 응용하냐에 따라 그 사용 방법 또한 무궁무진하다. 사실 펑터라는 개념 자체는 특정 메소드를 통해 펑터 내부의 값을 변경할 수 있도록 만들어주기만 하면 되기 때문에 프로그래밍으로 표현하는 것이 그렇게 어렵지는 않다. 123interface Functor { map(f: (x: T) => U): Functor} Function: 이 펑터는 T 타입의 값을 가지고 있다.map: 이 펑터의 사상을 적용하면 U 타입의 값을 가진 새로운 펑터를 얻는다.f: (x: T) => U: 이 펑터의 사상이 작동하는 방식은 T 타입의 값을 입력으로 받아 U 타입의 값을 출력하는 것이다. map 메소드는 인자로 받은 (x: T) => U 꼴의 함수를 펑터 내부의 값에 적용하고, 변경된 값을 감싸고 있는 새로운 펑터를 반환한다. 결국 map 메소드에 인자로 넘기는 이 함수가 실질적으로 펑터 내부의 값을 변경하는 역할을 하는 것이며, 값을 실질적으로 변경하는 역할을 하는 이 함수를 트랜스폼(Transform) 함수라고 부른다. 매핑을 하고 난 뒤 변경된 값 자체가 아니라 변경된 값을 감싸고 있는 펑터가 반환되는 이유는 펑터라는 것이 본질적으로 카테고리 -> 카테고리로 변경하는 구조체일 뿐, 카테고리를 부숴버리고 내부에 있는 대상을 꺼내는 역할을 하는 게 아니기 때문이다. 그리고 펑터는 단지 새로운 카테고리를 표현하는 수단이기 때문에 기존 카테고리의 대상을 변경해서는 안된다. 그렇기 때문에 기존 펑터의 값을 업데이트하는 것이 아니라 변경된 값을 담고 있는 새로운 펑터를 생성해서 반환해야하는 것이다. 이 개념이 약간 혼란스럽게 느껴진다면 Array.prototype.map 메소드를 생각해보자. 잘 생각해보면 배열도 어떠한 박스 안에 값을 담고 있는 일종의 펑터라고 할 수 있다. 12345678// Functorconst array: Array = [1, 2, 3];// 트랜스폼 함수: (x: number) => stringconst toString = v => v.toString();// 매핑!array.map(toString); 12// 새로운 펑터 Functor['1', '2', '3'] 우리는 트랜스폼 함수인 toString 함수를 사용하여 배열이라는 펑터 내부의 값을 변경할 수는 있지만, 배열이라는 박스 자체를 없애버리지는 않는다. 일반적으로 우리가 자주 사용하는 map이라는 메소드가 Array에 물려있기 때문에 매핑이라는 행위를 이터레이션과 연결지어 생각하기 쉬운데, 매핑은 그렇게 구체적인 행위가 아니다. 뭐 map 메소드 내부에서 이터레이션을 돌던 북을 치던 브레이크댄스를 추던 최종적으로 Functor -> Functor라는 변환만 수행할 수 있으면 되는 것이다. 이제 펑터가 어떤 느낌으로 돌아가는 개념인지 살짝 감을 잡을 수 있을 것이다. 자, 이제 펑터를 직접 구현해볼 시간이다. 펑터라는 게 워낙 추상적인 개념인 만큼 어떻게 응용하냐에 따라 무궁무진한 펑터를 만들 수 있지만, 이미 포스팅이 꽤나 길어졌기 때문에 많은 펑터를 선보이기는 힘들 것 같다. 그리고 펑터를 사용한 구현체들은 다른 분들이 작성해주신 포스팅에도 많으니 궁금하신 분들은 그 쪽을 참고해보도록 하자. 이 포스팅에서는 가장 간단한 형태의 펑터인 Just 펑터와 Nothing 펑터를 만들어보고, 이 두 펑터를 조합하여 값의 유무로 인한 사이드 이펙트를 관리할 수 있는 Maybe 펑터만 만들어보도록 하겠다. JustJust 펑터는 아무런 추가적인 기능없이 값을 그냥 감싸고 있기만 하고 map 메소드를 통해서 그 값을 변경할 수 있는 펑터이다. 123456789class Just implements Functor { value: T; constructor (value: T) { this.value = value; } map (f: (x: T) => U) { return new Just(f(this.value)); }} Just는 자신 내부에 값을 가지고 있는 단순한 펑터이다. 이 펑터의 map 메소드를 사용한다는 것은 펑터가 가지고 있는 T 타입의 값을 U 타입의 값으로 변경하고, 이 값을 다시 새로운 Just 펑터에 감싸서 반환하는 것을 의미한다. 1234new Just(3) .map(v => v + 1000) .map(v => v.toString) .map(v => v.length); 1Just { value: 4 } NothingNothing 펑터는 이름 그대로 내부에 어떠한 값도 가지고 있지 않은 펑터를 의미한다. 그리고 펑터 내부에 값이 없으니 트랜스폼 함수를 적용할 수도 없으므로 이 펑터의 map 메소드는 아무 행동도 하지 않고 그대로 Nothing 펑터를 반환하기만 한다. 1234567class Nothing implements Functor { map () { return new Nothing(); }}new Nothing().map().map().map(); 1Nothing {} 굳이 이렇게 값의 유무를 표현하는 펑터가 필요한 이유는 무엇일까? 한번 펑터를 사용하여 함수를 합성하기 시작하면 그 연산 과정에서 필요한 모든 값들 또한 펑터로 감싸져 있어야하기 때문이다. 만약 펑터로 감싸진 값에 그냥 함수를 적용하려고 하면 당연히 에러가 발생한다. 12const foo = new Just(3);foo + 2; 1Operator '+' cannot be applied to types 'Just' and 'number'. 그렇기 때문에 한번 펑터를 사용하여 함수를 합성하기 시작했다면 합성이 끝날 때까지 계속 펑터를 사용해야한다. 애초에 펑터를 사용하는 이유는 함수를 합성하는 동안 타입 안정성을 유지하고 사이드 이펙트를 관리하기 위해서인데, 이 과정에서 펑터가 아닌 녀석이 하나라도 끼어들게 되면 합성한 연산 전체의 안정성을 보장할 수 없기 때문이다. (미꾸라지 한 마리가 물을 흐린다) 이렇게 들으면 뭔가 불편하다고 느껴질 수도 있지만 아까 위에서 대표적인 펑터라고 이야기했던 Array를 사용하는 경우를 생각해보면 이게 그렇게 특이한 개념이 아니라는 사실을 알 수 있다. 만약 new Array(3)이라는 배열이 있을 때 이 배열이 가지고 있는 값에 2를 더하고 싶다면 어떻게 해야할까? 단, 함수형 프로그래밍의 세계에서는 상태의 변경을 허용하지 않으므로 new Array(3)[0] += 2와 같은 개념으로 접근해서는 안된다는 사실을 잊지말자. 즉, 불변성을 중시하는 함수형 프로그래밍의 세계에서 배열 내부의 값을 변경하고 싶다면, “변경된 값을 가지고 있는 새로운 배열”을 생성할 수 밖에 없다. 그래서 우리는 불변성을 지키며 배열 내부의 값을 변경하기위해 무조건 map이라는 메소드를 사용해야하는 것이다. 이제 펑터의 map 메소드가 왜 값을 변경한 후 새로운 펑터를 생성해서 반환하는지 조금은 이해가 갈 거라고 생각한다. Maybe자, 여기까지 이해했다면 조금 더 복잡한 펑터를 만들어보도록 하자. Maybe라는 펑터의 map 메소드는 펑터 내부에 값이 있다면 인자로 받은 함수를 값에 적용하고, 값이 없다면 값이 없음을 의미하는 펑터인 Nothing 펑터를 반환하는 펑터이다. 12345678910111213141516171819class Maybe implements Functor { value: Just | Nothing; constructor (value?: T) { if (value) { this.value = new Just(value); } else { this.value = new Nothing(); } } map (f: (x: T|null) => U) { if (this.value instanceof Just) { return this.value.map(f); } else { return new Nothing(); } }} 12345678910111213const getFirstLetter = s => s[0];const getStringLength = s => s.length;const foo = new Maybe('hi') .map(getFirstLetter) .map(getStringLength);const bar = new Maybe('') .map(getFirstLetter) .map(getStringLength);console.log(foo); // Just { value: 1 }console.log(bar); // Nothing {} Maybe 펑터를 사용하면 우리는 중간에 null이나 undefined가 반환되어 함수의 합성이 깨져버리는 걱정 없이 안심하고 함수를 합성할 수 있다. 물론 최종적으로 연산 결과가 Just인지 Nothing인지 구분하려면 if 문을 통해서 조건 검사를 해야하기는 하지만, 적어도 함수를 합성하는 중간중간마다 검사하지는 않는다. 즉, 함수를 합성할 때는 합성에만 집중할 수 있다는 뜻이다. 12345678// 펑터가 없다면 함수를 함부로 합성할 수 없다const firstLetter = getFirstLetter('');if (firstLetter) { console.log(getStringLength(firstLetter));}else { console.log('함수 합성 실패');} 123456789101112// Maybe 펑터를 사용하면 마음놓고 합성이 가능하다const result = new Maybe('') .map(getFirstLetter) .map(getStringLength);if (result instanceof Just) { console.log(result);}else { console.log('함수 합성 실패');} 이렇게 단순히 값을 감싸고, 내부에 있는 값을 변경할 수 있다는 단순한 개념만으로 우리는 함수의 안전한 합성을 할 수 있게 되었다. 이 포스팅에서는 값의 유무로 인한 사이드 이펙트를 관리할 수 있는 Maybe 펑터 만을 예시로 들었지만, 여러 번 이야기 했듯이 펑터는 그냥 값을 감싸고 있는 박스이기 때문에 어떤 로직을 구현하냐에 따라 천차만별로 다른 펑터를 만들어낼 수 있다. 예를 들면 현재에는 아직 값이 없지만 미래에 값이 결정되는 것을 약속해주는 Promise 같은 개념도 일종의 펑터라고 볼 수 있고, 여러 개의 값을 순차적으로 저장할 수 있는 Array도 일종의 펑터라고 할 수 있다. 펑터라는 것은 추상적인 개념일 뿐이지 구체적으로 특정 로직만을 수행하는 구현체가 아니라는 말이다. 말 그대로 코에 붙히면 코걸이고 귀에 붙히면 귀걸이기 때문에 단순히 뭔가로 값을 감싸고 그 값을 변환할 수 있다는 개념만으로도 마음껏 상상의 나래를 펼치며 다양한 펑터 구현체들을 만들어낼 수 있다. 마치며이 포스팅을 읽는 독자 분들 중 펑터에 대한 설명을 읽으면서 “어? 이거 모나드 아닌가?”라고 하신 분들도 있을 것이라 생각한다. 정확히 말하면 반은 맞고 반은 틀리다. 모나드도 결국 함수를 안전하게 합성하기 위한 펑터의 한 종류이기 때문이다. 간단하게 말하면 모나드라는 것은 수학적으로 특별한 몇 가지 조건을 만족시키는 두 개의 펑터 사이의 사상이라고 할 수 있다. 어플리케이티브 펑터나 모나드를 이 포스팅에서 따로 설명하지는 않았지만, 뭐 원리가 어쩌고 저쩌고를 떠나서 그냥 이런 개념들을 추가적으로 사용하는 이유는 그냥 딱 한 가지 밖에 없다. 어 뭐여…? 펑터로도 해결이 안되네…? 함수의 안전한 합성이라는 목표를 이루기 위해 펑터를 사용했지만 사실 프로그래밍을 하다보면 펑터로 해결이 안되는 케이스도 수두룩하기 때문이다. 뭐 펑터로 여러 번 감싸져 있는 값에 매핑해야한다거나 하는 케이스말이다. 이런 경우에는 펑터의 매핑만으로는 함수를 합성할 수 없다. 결국 어플리케이티브 펑터나 모나드는 펑터로도 해결되지 않는 예외 상황들까지 모두 커버할 수 있도록 더 추상적이고 강력하게 만든 펑터라고 생각하면 된다. 사실 이번 포스팅에서 모나드의 개념까지 설명을 해보려고 했지만, 이 포스팅에서 펑터를 설명했던 방식으로 모나드를 설명하기 위해서는 개요 수준의 카테고리 이론이 아니라 조금 더 깊숙한 설명이 필요하기 때문에 포기했다. (모나드는 다음 포스팅에서 한 번 조져보겠다) 물론 함수의 합성과 펑터와의 관계를 파악하는 것은 꽤나 추상적인 개념이기 때문에 이해하기에 조금 어렵긴 하다. 그런 이유로 어떤 개발자들은 펑터와 모나드의 사용 방법 정도만 익히고 프로그래밍하기도 하지만, 개인적으로는 이러한 개념들이 왜 사용되는 것인지, 어디서 아이디어를 얻은 것인지 알고 있다면 프로그래밍이 더 재밌어지지 않을까라는 생각이 든다. 이상으로 어떻게 하면 안전하게 함수를 합성할 수 있을까? 포스팅을 마친다.","link":"/2020/01/27/safety-function-composition/"},{"title":"프론트엔드와 백엔드가 소통하는 엔드포인트, RESTful API","text":"이번 포스팅에서는 프론트엔드 개발자와 백엔드 개발자가 만나는 지점인 API에 대한 이야기를 해보려고한다. 일반적으로 앱이나 웹 상에서 작동하는 어플리케이션을 개발할 때는 주로 HTTP나 HTTPS 프로토콜을 사용하여 API를 만들게 되는데, 이 API의 정의가 얼마나 직관적이고 명확하냐에 따라 프로젝트의 복잡도가 크게 낮아지게 될 만큼 시스템 설계에 있어서 꽤나 중요한 자리를 차지하고 있다. 그래서 우리는 일종의 약속을 통해 이 API가 어떤 동작을 수행하는 API인지를 명확하게 정의해야 하며, 이 API 정의 과정에서 우리가 사용할 수 있는 요소들이 바로 HTTP 메소드와 URI(Uniform Resource Identifiers)이다. 1GET https://evan.com/users/1 HTTP API의 엔드포인트는 위와 같이 HTTP 메소드와 URI를 사용하여 이 API가 어떠한 동작을 수행하는 API인지를 표현하게 된다. 여기서 중요한 포인트는 사용자가 이 표현을 읽고난 뒤 API에게 기대하는 동작과 실제로 서버가 수행하는 동작이 명확하게 일치되어야 한다는 것이다. 우리가 서버에게 “앞으로 한 걸음 가줘!”라고 요청했는데 서버가 응답으로 “ㅇㅋ 뒤로 한 걸음 갔음!”이라고 한다면 꽤나 당황스럽지 않겠는가? 사람끼리든 컴퓨터끼리든 표현을 제대로 안하면 못 알아먹는다 그래서 우리는 REST와 같은 가이드라인을 사용한다. REST는 지난 2000년, 로이 필딩(Roy Fielding) 아저씨가 자신의 박사학위 논문에서 소개한 API 아키텍처 가이드라인이며, 무려 20년이 지난 현재까지도 널리 사용되고 있다. 하지만 지난 번 서버의 상태를 알려주는 HTTP 상태 코드 포스팅에서 이야기했듯이, 이건 말 그대로 가이드라인이기 때문에 지키지 않는다고 해서 에러가 발생하거나 하는 게 아니지만, 그렇다고해서 이런 가이드라인을 무시하고 마음대로 개발해도 된다는 것은 아니다. REST라는 용어와 개념은 이미 업계에 널리 퍼져있기 때문에, 많은 개발자들이 HTTP API를 만났을 때 이 API가 당연히 RESTful하게 작성되었을 것이라고 생각하기 때문이다. (사실 상 표준이라고 봐도 무방할 정도의 영향력이다) 그런 이유로 이번 포스팅에서는 이 REST라는 것이 도대체 왜 나오게 된 것인지, 또 REST가 뭘 의미하길래 사람들이 매번 RESTful, RESTful 하는 것인지에 대한 이야기를 나눠보려고 한다. REST가 의미하는 것이 무엇인가요?REST는 REpresentational State Transfer의 약자이다. 이 거창해보이는 단어의 핵심은 바로 Representational State, 한국말로 간단히 직역하면 대표적인 상태 정도의 뜻을 가진 단어이며, 이를 조금 더 유연하게 번역해보자면 표현된 상태라고 할 수 있다. 이때 이야기하는 상태라 함은 서버가 가지고 있는 리소스의 상태를 이야기한다. 즉, REST는 통신을 통해 자원의 표현된 상태를 주고받는 것에 대한 아키텍처 가이드라인이라고 할 수 있다. REST에 대한 이야기를 할 때, 많은 분들이 이 표현된 상태(Representational State)에 대한 이해를 어려워하는데, 이는 클라이언트와 서버가 API 통신을 통해 주고 받고 있는 것들이 리소스 그 자체라고 생각하기 때문이다. 하지만 조금만 생각해보면 우리가 통신을 통해 리소스를 직접 주고받고 있지 않다는 사실을 알 수 있다. 사실 주고 받는 것은 리소스가 아니다.우리가 API를 통해 주고 받는 리소스는 어떤 문서일수도 있고, 이미지 또는 단순한 JSON 데이터일 수도 있다. 하지만 사실 우리는 리소스를 직접 주고 받는 것이 아니다. 한번 간단한 예시를 통해 이 말이 어떤 의미인지 살펴보도록 하자. 자, 여기 클라이언트가 서버에게 특정 유저의 정보를 받아오는 API 엔드포인트를 통해 요청을 보냈다고 가정해보자. 123GET https://iamserver.com/api/users/2Host: iamserver.comAccept: application/json 클라이언트는 이 API 엔드포인트를 사용하여 서버에게 2번 유저의 자원을 요청했고, 서버가 요청을 성공적으로 처리했다면 클라이언트는 서버로부터 대략 이런 느낌의 응답을 받을 수 있다. 123456789HTTP/1.1 200 OKContent-Length: 45Content-Type: application/json{ id: 2, name: 'Evan', org: 'Viva Republica',} 자, 서버가 보내준 응답의 바디에는 2번 유저의 데이터가 담겨있다. 일반적으로 우리는 이 상황을 /api/users/2라는 엔드포인트를 통해서 2번 유저 데이터 리소스를 받아왔다고 표현하고는 한다. 사실 필자도 편의상 이런 표현을 자주 사용하고는 한다. 그런데…정말로 지금 서버가 보내준 저 JSON 데이터가 리소스 자체일까? 땡. 저건 2번 유저의 리소스가 아니다! 사실 서버에서 보내준 저 JSON은 리소스 원본이 아니라 데이터베이스에 저장된 2번 유저의 데이터 리소스를 표현한 것에 불과하다. 서버는 클라이언트의 요청을 받고 2번 유저의 정보를 데이터베이스에서 조회한 후 요청의 헤더에 담겨있던 application/json이라는 방식으로 표현하여 응답에 담아준 것이다. 곰곰히 생각해보면 당연한 이야기인 것이, 서버가 접근하는 진짜 리소스 원본은 그저 데이터베이스에 담겨있는 하나의 로우이거나 파일에 작성된 데이터일 것이다. 물론 서버의 로컬 시스템에 리소스를 JSON 파일로 저장하고 있을 수도 있지만 어쨌든 포인트는 서버가 보내준 저 JSON이 원본 리소스가 아니라는 것이다. 서버가 보내준 JSON은 단지 데이터베이스에 저장되어있는 원본 데이터 리소스의 현재 상태를 표현한 것이다. 리소스를 표현한 상태라는 것의 의미앞서 이야기했듯이 REST가 이야기하는 Representation State라는 단어는 원본 리소스를 표현한 상태라는 것을 의미한다. 원본 리소스는 데이터베이스에 저장된 하나의 로우로써 존재하지만 클라이언트에게 이걸 그대로 넘겨줄 수는 없으니 서버가 원본 리소스를 읽어와서 적당한 상태로 표현해주는 것이다. 그리고 이 적당한 상태에 대한 힌트는 HTTP 요청 헤더나 응답 헤더에 전부 나와있다. 123GET https://iamserver.com/api/users/2Host: iamserver.comAccept: application/json 위에서 예시로 들었던 상황에서 클라이언트는 서버에게 2번 유저의 리소스를 요청하며 요청 헤더의 Accept라는 키에 application/json이라는 값을 담아서 보냈다. 클라이언트가 서버에게 “2번 유저의 상태를 json으로 표현해줘”라는 요청을 보낸 것이다. 만약 클라이언트가 application/json이 아닌 application/xml을 담아보냈고, 서버가 XML 포맷의 표현을 지원하도록 작성되어있다면 2번 유저의 리소스는 XML 형태로 표현되어 내려왔을 것이다. 그리고 서버는 응답 헤더에 Content-Type이나 Content-Language와 같은 키를 사용하여 이 리소스가 어떤 방식으로 표현된 상태인지 클라이언트에게 알려주고, 클라이언트 또한 이 정보를 읽은 후 각 컨텐츠 타입에 맞게 정보를 파싱한다. 즉, 클라이언트는 2번 유저의 리소스를 받은 것이 아니다. JSON으로 표현된 2번 유저 리소스의 현재 상태를 받은 것이다. 이처럼 REST는 클라이언트와 서버가 리소스의 타입이나 원하는 언어 등을 사용하여 자원을 자유롭고 명확하게 표현할 수 있는 것에 집중한다. RESTful API앞서 이야기했듯이, REST는 결국 리소스를 어떻게 하면 명확하게 표현할 수 있을지에 대한 것에 집중하는 아키텍처 스타일이다. 하지만 우리가 HTTP API를 사용할 때는 단순히 리소스의 표현 상태만으로는 클라이언트가 API를 호출했을 때 서버에서 정확히 어떤 일이 발생하는지 알기가 어렵다. REST는 단지 리소스가 표현된 상태만을 이야기할 뿐, 어떠한 “행위”에 대해서는 이야기하고 있지 않기 때문이다. 하지만 클라이언트가 서버의 API를 사용할 때 원하는 것은 소스를 생성하거나 삭제하거나 수정하는 등 명백히 어떠한 행위이다. 그래서 RESTful API에서는 REST 아키텍처를 통해 표현된 리소스와 더불어 어떠한 행위를 명시할 수 있는 HTTP 메소드와 URI까지 활용하게 되며, 각 요소들이 표현하고 있는 것들은 다음과 같다. 리소스가 어떻게 표현되는지? - REST 어떤 리소스인지? - URI 어떤 행위인지? - HTTP 메소드 즉, 이 요소들을 사용하여 명확하게 정의된 API를 사용하는 클라이언트는 굳이 API에 대한 구구절절한 설명이 없이 GET /users/2와 같은 엔드포인트만 보고도 “음, 2번 유저의 정보를 가져오는 API겠군”이라고 추측할 수 있게 되는 것이다. 이 3가지 요소 중 리소스를 표현하는 방법인 REST에 대해서는 앞서 이미 이야기했으니, 이번 섹션에서는 어떤 리소스인지를 표현하는 URI와 어떤 행위인지를 표현하는 HTTP 메소드에 대해 알아보도록 하자. URI를 사용하여 어떤 리소스인지 표현하자RESTful API의 URI는 이 API가 어떤 리소스에 대한 API인지를 나타내는 요소이다. 예를 들어, 서비스를 사용하는 유저의 목록을 가져오는 API가 있다고 생각해보자. 이 API를 사용하는 클라이언트가 접근하고자 하는 리소스는 유저가 될 것이고, 이 API의 URI는 명확하게 유저를 표현하고 있어야한다. 1GET /users 솔직히 이 정도 URI면 지나가던 중학생이 봐도 뭔가 유저와 관련이 있다는 것을 알 수 있을 정도로 명확하다. 그런데 유저라는 리소스를 왜 user라고 표현하지 않고 굳이 users라는 복수형으로 표현하고 있는 것일까? 그 이유는 유저라는 리소스가 특정한 하나의 객체가 아니기 때문이다. 이건 영어로 “나는 고양이를 좋아해!”라는 문장을 이야기할 때 “I love a cat“가 아닌 “I love cats“라고 하는 것과 같은 맥락이다. 나는 어떤 특정한 고양이를 좋아하는 것이 아니라 고양이라는 생물 자체를 좋아하는 것이고, 이때 고양이라는 단어는 우리 아파트 앞에서 쓰레기통 뒤지고 있는 점박이 고양이, 이름 모를 사람이 인스타그램에 이쁘다고 자랑하는 지네 집 고양이, 길 가다가 우연히 마주치는 고양이까지 모두 포함되는 다소 추상적인 리소스를 의미하는 것이기 때문이다. 자, 그럼 유저들이라는 추상적인 리소스에서 한 단계 더 구체화 시켜보도록 하자. 유저라는 추상적인 리소스를 조금 더 구체화한 다음 레벨의 리소스는 특정 유저이다. 리소스의 계층을 표현하기일반적으로 유저들은 각각 고유한 ID를 가지고 있는 경우가 많으니, 이 ID를 사용하면 특정한 유저를 대충 이런 URI로 표현할 수 있을 것 같다. 1GET /users/2 굳이 설명하지 않더라도 다들 눈치채셨겠지만, 이 URI는 유저들을 의미하던 /users라는 URI 뒤 쪽에 각 유저들이 고유하게 가지고 있는 ID를 추가하여 특정한 유저를 식별할 수 있도록 만든 것이다. 또한 이 URI가 표현하고 있는 리소스인 2번 유저는 유저라는 리소스의 하위 집합이라고 할 수 있고, RESTful API는 이러한 리소스 간의 계층 구조를 /를 사용하여 표현할 것을 권장하고 있다. 유저 > 특정 유저 > 특정 유저의 프로필 사진으로 이어지는 계층 구조 이러한 리소스 간의 계층 구조는 어플리케이션에서 사용하는 리소소들의 관계를 어떻게 설정할 것이냐에 대한 문제이기 때문에 API를 설계할 때 굉장히 중요하고 예민한 요소인데다가, 대부분의 설계 패턴이 그러하듯이 이건 정답이 정해져있는 것도 아니기 때문에 고민을 깊게 할 수 밖에 없는 문제다. 정답이 정해져있지 않다는 것은 프로필 사진이라는 리소스를 /users/2/profile-image라는 계층 구조가 아니라 /profile-images/users/2와 같은 계층으로 설계해도 논리 상으로는 아무런 문제가 없다는 것을 의미한다. 여기에는 단지 프로필 사진이라는 리소스가 어떤 의미를 가질 것이냐의 차이만 있을 뿐이다. /users/2/profile-image유저들 중 2번 유저의 프로필 사진 /profile-images/users/2프로필 사진들 중 유저들의 프로필 사진 중 2번 유저의 프로필 사진 이처럼 같은 프로필 사진이지만 리소스의 계층을 어떻게 설계하냐에 따라 의미가 완전히 달라지게 된다. 이 케이스의 경우 유저 외에 프로필 사진을 가질 수 있는 다른 리소스가 존재하지 않는다면 /users/2/profile-image가 적당하지만 유저 외에도 다양한 리소스가 프로필 사진을 가져야 하는 상황이라면 profile-images/users/2라는 계층 구조도 고민해볼 수 있을 것이다. 결국 우리가 고민해야 할 문제는 특정 유저의 프로필 사진이라는 리소스를 포함하는 상위 계층 리소스가 “유저가 더 명확하냐”, “프로필 사진이 더 명확하냐”인 것이다. 물론 앞서 이야기했듯이 정답은 없으니, 항상 빠르게 변화하는 비즈니스 상황에 유연하게 대처할 수 있도록 팀원들과 협의해보고 URI를 설계하도록 하자. URI에는 행위가 표현되면 안된다RESTful API의 URI를 설계할 때 또 한 가지 중요한 것은 URI에 어떠한 행위를 의미하는 표현이 포함되어서는 안된다는 것이다. 예를 들어 유저를 삭제하는 엔드포인트가 하나 있다고 생각해보자. 이때 HTTP 메소드에 익숙하지 않다면, 대략 이런 느낌의 엔드포인트를 설계할 수도 있다. 1POST /users/2/delete 이 엔드포인트의 URI에는 삭제 행위를 의미하는 delete라는 표현이 포함되어 있다. 뭐 사실 이대로도 이 API가 어떤 역할을 수행하는 API인지 인지하기에는 큰 무리가 없지만, RESTful API는 URI를 사용하여 행위를 표현하지 않을 것을 권고한다. URI가 가지는 의미는 철저히 어떤 리소스인지, 그리고 리소스의 계층 구조에 대한 것 뿐이어야한다. API가 수행하는 행위는 되도록이면 올바른 HTTP 메소드를 사용하여 표현해주는 것이 좋다. 그리고 단순히 이건 RESTful API가 이런 설계를 권고하기 때문인 것도 있지만, RESTful API의 가이드라인을 지키지 않도록 개발된 여러분의 어플리케이션이 이미 RESTful API의 가이드라인을 지키며 개발된 다른 어플리케이션들과 통신할 때 어떤 부작용이 발생할지 모르기 때문이기도 하다. (당장 웹 브라우저만 해도 HTTP 메소드와 상태 코드에 상당히 종속되어 설계되어 있다) 그리하여 올바르게 작성된 엔드포인트는 삭제를 의미하는 HTTP 메소드인 DELETE를 사용한 요런 엔드포인트가 될 것이다. 1DELETE /users/2 자, 지금까지 URI를 사용하여 리소스를 표현하는 방법에 대해 살펴보았으니, 이제는 API의 행위를 표현하는 방법에 대해서 알아볼 차례이다. HTTP 메소드를 사용하여 어떤 행위인지 표현하자RESTful API는 HTTP 메소드를 사용하여 API가 수행하는 행위를 표현하도록 권고하고있다. HTTP 메소드는 나름 RFC-2616에서 정의된 표준이기 때문에 상황에 맞지 않는 메소드를 사용하게 되면 어플리케이션이 예상하지 못한 동작을 일으킬 수 있다는 사실을 기억하도록 하자. 사실 API를 사용하여 하게되는 행위는 대부분 CRUD(Create, Read, Update, Delete) 이기 때문에, 몇 가지 특수한 경우를 제외하면 단 5가지의 HTTP 메소드만으로도 대부분의 API를 정의할 수 있다. Method 의미 GET 리소스를 조회한다 PUT 리소스를 대체한다 DELETE 리소스를 삭제한다 POST 리소스를 생성한다 PATCH 리소스의 일부를 수정한다 이 외에도 HEAD, OPTION, TRACE 등의 메소드도 존재하기는 하지만, 사실 이 5가지의 메소드가 가지는 역할만 확실히 알고 있어도 HTTP 메소드를 사용하여 올바른 행위를 표현하거나 RESTful API를 설계하기에는 전혀 무리가 없다. 여기서 한 가지 헷갈릴만한 것은 바로 PUT과 PATCH 메소드인데, 이 메소드들은 동일하게 “리소스를 수정한다”라는 의미로 해석되는 경우가 많기 때문에 정확히 어떤 경우에 PUT을 사용하고 어떤 경우에 PATCH를 사용해야 하는지 구분하기 어려운 경우가 많다. PUT과 PATCH의 차이는 무엇인가요?흔히들 PUT 메소드를 리소스를 수정한다는 개념으로 설명하고는 하지만, 실제 PUT 메소드가 의미하는 것은 리소스를 수정하는 것이 아니라 리소스를 요청 바디에 담긴 것으로 대체하는 것이다. 한번 { id: 1, name: 'evan' }이라는 유저 리소스의 이름을 ethan으로 수정해야하는 상황을 생각해보자. 만약 우리가 PUT 메소드를 사용하여 이 리소스를 수정한다면 우리는 반드시 요청 바디에 유저 리소스 전체를 표현하여 보내야한다. 즉, 수정할 사항만 보내는 것이 아니라 수정하지 않을 사항까지도 모두 보내야한다는 것이다. 12PUT /users/1{ id: 1, name: 'ethan' } PUT 메소드는 리소스를 수정하는 것이 아니라 대체하는 것이다 이렇게 리소스를 대체한다는 PUT 메소드의 특성 상, 실수로 { id: null, name: 'ethan' }과 같은 리소스를 전송해버리기라도 하면 이 유저는 영영 ID를 잃어버린 비운의 유저가 되어버리는 경우도 발생할 수 있다. (사실 이 정도 예외처리는 다들 기본적으로 해놓긴 한다.) 하지만 그냥 리소스를 받아서 대체하면 된다는 동작 자체가 워낙 심플하기 때문에 리소스를 수정하는 쪽이든 받아서 처리하는 쪽이든 이것저것 신경써줘야 할 일이 별로 없어서 편하기는 하다. 반면 우리가 PATCH 메소드를 사용하여 방금과 동일한 행위를 하려고 하면 어떨까? 12PATCH /users/1{ name: 'ethan' } PATCH 메소드는 리소스의 일부분을 수정하는 것이다 PUT 메소드와 다르게 PATCH 메소드는 진짜로 현재 저장되어 있는 리소스에 수정을 가하는 행위를 의미하기 때문에 굳이 수정하지 않은 사항을 요청 바디에 담아줄 필요도 없다. PATCH메소드는 PUT처럼 수정하지 않을 사항까지 보낼 필요가 없고 진짜 수정하고 싶은 사항만 깔끔하게 보내면 되기 때문에, 쓸데없이 큰 요청 바디를 만들지 않을 수 있다. 또한 실제로 이러한 수정 동작을 수행하는 API를 사용할 때는 SQL의 UPDATE와 동일한 의미를 떠올리는 경우가 많기 때문에, 리소스를 대체하는 PUT 메소드보다 리소스의 일부를 수정하는 PATCH 메소드가 수정이라는 의미를 가지기에도 더 적합하다고 할 수 있다. 아직까지는 리소스를 수정하는 행위를 표현할 때 PUT 메소드를 주로 사용하는 경우가 많기는 하지만, PUT 메소드와 PATCH 메소드의 의미적인 차이는 분명히 존재하므로 API의 엔드포인트를 설계할 때 리소스를 대체, 리소스를 수정 중 원하는 행위와 일치하는 메소드를 사용하는 것을 권장한다. 그러나 PUT 메소드와 PATCH 메소드의 진짜 중요한 차이점은 이런 행위의 의미가 아니라, PUT 메소드는 반드시 멱등성을 보장하지만 PATCH 메소드는 멱등성을 보장하지 않을 수도 있다는 것이다. 메소드가 멱등성을 보장하는가?멱등성이란, 수학이나 전산학에서 어떤 대상에 같은 연산을 여러 번 적용해도 결과가 달라지지 않는 성질을 이야기한다. 즉, 단순히 HTTP 메소드에만 국한된 이야기는 아니고 이는 데이터베이스나 파일에 자원을 읽고 쓰는 등 컴퓨터가 수행하는 모든 연산에 해당되는 이야기이다. 가장 대표적으로 멱등성이 보장되는 연산은 바로 어떠한 수에 1을 곱하는 연산이다. x => x * 1과 같은 함수는 어떠한 값에 1번을 적용하든, 10,000번을 적용하든 항상 x를 반환한다. 그러나 1을 곱하는 것이 아니라 1을 더하거나 빼는 함수라면 한번 호출될 때마다 인자로 주어진 값을 계속 증가시키거나 감소시킬 것이므로 항상 같은 값을 반환하지 않는다. 이러한 성질의 연산이 바로 멱등성을 보장하지 않는 연산의 대표적인 예이다. HTTP 메소드 또한 결국 어떠한 자원을 읽고 쓰고 수정하고 지우는 CRUD에 대한 의미를 가지기 때문에, 우리는 어떤 행위가 멱등성을 보장하고 어떤 행위가 멱등성을 보장하는지 알고 있어야 어플리케이션이 예상하지 못한 방향으로 동작하는 것을 방지할 수 있다. Method 멱등성 보장 GET O PUT O DELETE O POST X PATCH X 일단 깊게 생각하지말고 위 테이블을 한번 보자. GET 메소드는 단지 리소스를 읽어 오는 행위를 의미하기에 아무리 여러 번 수행해도 결과가 변경되거나 하지는 않을 것이다. 마찬가지로 요청에 담긴 리소스로 기존 리소스를 그대로 대체해버리는 PUT 메소드 또한 여러 번 수행한다한들 요청에 담긴 리소스가 변하지 않는 이상 연산 결과가 동일할 것이다. 즉, 어떤 리소스를 읽어오거나 대체하는 연산은 멱등성을 보장한다고 이야기할 수 있다. 그렇다면 멱등성이 보장되지 않는 케이스는 어떤 것이 있을까? POST 메소드의 경우 리소스를 새롭게 생성하는 행위를 의미하기 때문에 여러 번 수행하게 되면 매번 새로운 리소스가 생성될 것이고, 그 말인 즉슨 결국 연산을 수행하는 결과가 매번 달라진다는 것을 의미한다. POST 메소드와 같이 멱등성을 보장하지 않는 동작은 한 번 수행될 때마다 어플리케이션의 상황이 전혀 다르게 변화시킬 수도 있다. 이러한 HTTP 메소드의 멱등성에 대한 지식은 에러에 대한 정보가 별로 없는 상태에서 디버깅을 진행할 때도 활용될 수 있기 때문에 여러모로 알고 있는 편이 좋다고 생각한다. 똑같이 통신 후에 발생하는 에러라고 해도 GET을 여러 번 수행했을 때 발생하는 에러와 POST를 여러 번 수행했을 때 발생하는 에러는 전혀 다른 컨텍스트를 가지고 있을 수 있다는 것이다. PATCH는 왜 멱등성이 보장되지 않는다는걸까?위 테이블에서 PATCH 메소드는 POST 메소드와 동일하게 멱등성이 보장되지 않는 메소드로 표기되어있다. 그러나 사실 정확하게 이야기하면 PATCH 메소드는 구현 방법에 따라서 PUT 메소드처럼 멱등성이 보장될 수도 있고, 혹은 보장되지 않을 수도 있다고 할 수 있다. PATCH 메소드는 PUT 메소드처럼 리소스를 대체하는 행위가 아니기 때문에 요청을 어떤 방식으로 사용하는지에 대한 제한이 딱히 없기 때문이다. RFC 스펙 상의 PATCH 메소드는 단지 리소스의 일부를 수정한다는 의미만을 가질 뿐이다. 예를 들어, 앞서 필자가 설명했던 예시처럼 PATCH 메소드에 수정할 리소스의 일부분만 담아서 보내는 경우에는 당연히 멱등성이 보장된다. 123456// 기존 리소스{ id: 1, name: 'evan', age: 30,} 12PATCH users/1{ age: 31 } 123456// 새로운 리소스{ id: 1, name: 'evan', age: 31, // 변경!} 이 PATCH 요청은 명확하게 age라는 필드를 31로 수정하는 행위만을 의미하므로 아무리 여러 번 수행한다고 해도 늘 age는 31이라는 값을 가질 것이기 때문이다. 이건 굉장히 일반적인 PATCH 메소드의 구현 방법이고, 실제로 필자도 PATCH 메소드를 사용해야한다면 이렇게 구현한다. 근데 왜 PATCH 메소드는 멱등성 보장이 안될 수도 있다는 것일까? 어쩌면 이게 PATCH를 구현하는 올바른 방법이 아닐 수도 있을 것이라는 킹리적 갓심이 들기 시작했다 뭐든지 원조가 중요하니 PATCH 메소드를 처음으로 정의해놓은 RFC-5789 문서를 한번 까보도록 하자. 보통 RFC 문서에는 정의된 개념에 대한 설명과 간략한 예시도 포함되어 있는 경우가 많으니, PATCH 메소드의 올바른 구현 방법 또한 적혀있을 것 같다. ... …는 그딴 건 없었습니다 놀랍게도어이없게도 RFC-5789 문서에 있는 예시 요청의 바디에는 단지 description of changes라는 설명만 적혀있을 뿐, 어떤 제약 조건도 적혀있지 않다. 즉, 별다른 제약없이 개발자 마음대로 API의 인터페이스를 정의해도 된다는 의미이기 때문에 이런 느낌으로 API를 구현하는 것도 가능하다는 것이다. 12345PATCH users/1{ $increase: 'age', value: 1,} 이 요청의 $increase 필드의 값은 증가시키고 싶은 속성을 의미하고, value 필드의 값은 그 속성을 얼마나 증가시킬 것인지를 나타내고 있다. 이 경우 API가 호출될 때마다 에반의 나이는 1씩 증가(…😢)할 것이기 때문에 이 API는 멱등성을 보장하지 않는다. 물론 필자도 PATCH 메소드를 이렇게 사용하는 경우를 실제로 보지는 못했지만, 앞서 이야기했듯이 RFC-5789에는 PATCH 메소드를 어떻게 구현해야 하는지에 대한 제약이 존재하지 않으니 이런 방식으로 사용한다고 해서 표준을 어기는 것도 아니다. 즉, 자세한 스펙 상 구현 방법에 대한 제약이 없으니 API를 어떻게 구현하느냐에 따라서 PATCH 메소드는 멱등성을 보장할 수도 있고 아닐 수도 있는 것이다. 마치며사실 REST는 네트워크 아키텍처를 설계하는 가이드라인이기 때문에 필자가 이야기했던 리소스의 표현 상태는 REST의 일부분에 불과하다. 그러나 애초에 이 포스팅은 RESTful API를 설명하는 것이 목적이었기 때문에 더 자세한 내용을 굳이 이야기하지는 않았다. 혹시 REST에 대해 더 관심이 가시는 분들은 로이 필딩 아저씨의 논문 중 REST 챕터을 한번 읽어보는 것을 추천한다. RESTful API는 필자가 지금까지 프론트엔드 개발자로 일을 하면서 백엔드 개발자와 가장 많은 논의를 했던 주제였다. 그렇게 논의를 했던 이유는 개발자로써 명확한 API를 정의하고 싶다는 욕심이기도 했고, 어떻게 하면 명확한 API를 정의해서 새로 조직에 합류한 개발자들이 바로 API에 익숙해지게 만들 수 있을지에 대한 욕심이기도 했다. (오늘도 역시 사무실에서 이런 이야기를 나누다 왔다) 필자는 개인적으로 가장 좋은 API는 기능이 많은 API도 아니고 공짜로 사용할 수 있는 API도 아닌, 어떠한 정보도 없는 누군가가 구구절절 다른 설명 없이 엔드포인트만 봐도 어떤 동작을 하는 API인지 바로 이해할 수 있을 정도로 명확한 API가 가장 좋은 API라고 생각한다. 물론 RESTful API와 같은 아키텍쳐 가이드라인을 학습하고 준수하는 것이 다소 번거로울 수는 있지만, 이런 표준이나 가이드라인이 가지는 의미가 전 세계의 수 많은 개발자들이 소통할 수 있는 획일화된 교통 정리인 만큼 가이드라인을 준수하기 위한 개개인의 작은 노력이 모여서 거대한 웹 아키텍처를 유지할 수 있게 만드는 것은 아닐까. 이상으로 프론트엔드와 백엔드가 소통하는 엔드포인트, RESTful API 포스팅을 마친다.","link":"/2020/04/07/about-restful-api/"}],"tags":[{"name":"궤도 구현하기","slug":"궤도-구현하기","link":"/tags/궤도-구현하기/"},{"name":"케플러 궤도 방정식","slug":"케플러-궤도-방정식","link":"/tags/케플러-궤도-방정식/"},{"name":"케플러 6요소","slug":"케플러-6요소","link":"/tags/케플러-6요소/"},{"name":"JavaScript","slug":"javascript","link":"/tags/javascript/"},{"name":"머신러닝","slug":"머신러닝","link":"/tags/머신러닝/"},{"name":"딥러닝","slug":"딥러닝","link":"/tags/딥러닝/"},{"name":"Machine Learning","slug":"machine-learning","link":"/tags/machine-learning/"},{"name":"Deep Learning","slug":"deep-learning","link":"/tags/deep-learning/"},{"name":"중력 구현하기","slug":"중력-구현하기","link":"/tags/중력-구현하기/"},{"name":"중력","slug":"중력","link":"/tags/중력/"},{"name":"Gravity","slug":"gravity","link":"/tags/gravity/"},{"name":"Web","slug":"web","link":"/tags/web/"},{"name":"SPA","slug":"spa","link":"/tags/spa/"},{"name":"Single Page Application","slug":"single-page-application","link":"/tags/single-page-application/"},{"name":"SSR","slug":"ssr","link":"/tags/ssr/"},{"name":"Server Side Rendering","slug":"server-side-rendering","link":"/tags/server-side-rendering/"},{"name":"서버사이드 렌더링","slug":"서버사이드-렌더링","link":"/tags/서버사이드-렌더링/"},{"name":"정렬 알고리즘","slug":"정렬-알고리즘","link":"/tags/정렬-알고리즘/"},{"name":"Sort Algorithms","slug":"sort-algorithms","link":"/tags/sort-algorithms/"},{"name":"Algorithm","slug":"algorithm","link":"/tags/algorithm/"},{"name":"알고리즘","slug":"알고리즘","link":"/tags/알고리즘/"},{"name":"TypeScript","slug":"typescript","link":"/tags/typescript/"},{"name":"타입스크립트","slug":"타입스크립트","link":"/tags/타입스크립트/"},{"name":"타입스크립트 인공 신경망","slug":"타입스크립트-인공-신경망","link":"/tags/타입스크립트-인공-신경망/"},{"name":"Aamazon Web Service","slug":"aamazon-web-service","link":"/tags/aamazon-web-service/"},{"name":"HTTP","slug":"http","link":"/tags/http/"},{"name":"HTTP/2","slug":"http-2","link":"/tags/http-2/"},{"name":"Network","slug":"network","link":"/tags/network/"},{"name":"배열 성능 최적화","slug":"배열-성능-최적화","link":"/tags/배열-성능-최적화/"},{"name":"Array","slug":"array","link":"/tags/array/"},{"name":"에세이","slug":"에세이","link":"/tags/에세이/"},{"name":"Essay","slug":"essay","link":"/tags/essay/"},{"name":"Good Programmer","slug":"good-programmer","link":"/tags/good-programmer/"},{"name":"좋은 프로그래머","slug":"좋은-프로그래머","link":"/tags/좋은-프로그래머/"},{"name":"Jira","slug":"jira","link":"/tags/jira/"},{"name":"애자일","slug":"애자일","link":"/tags/애자일/"},{"name":"Agile","slug":"agile","link":"/tags/agile/"},{"name":"협업","slug":"협업","link":"/tags/협업/"},{"name":"Atlassian","slug":"atlassian","link":"/tags/atlassian/"},{"name":"Webpack","slug":"webpack","link":"/tags/webpack/"},{"name":"NodeJS","slug":"nodejs","link":"/tags/nodejs/"},{"name":"webpack dev server","slug":"webpack-dev-server","link":"/tags/webpack-dev-server/"},{"name":"webpack dev middleware","slug":"webpack-dev-middleware","link":"/tags/webpack-dev-middleware/"},{"name":"memory-fs","slug":"memory-fs","link":"/tags/memory-fs/"},{"name":"MFS","slug":"mfs","link":"/tags/mfs/"},{"name":"Memory leak","slug":"memory-leak","link":"/tags/memory-leak/"},{"name":"수학","slug":"수학","link":"/tags/수학/"},{"name":"평균구하기","slug":"평균구하기","link":"/tags/평균구하기/"},{"name":"Average","slug":"average","link":"/tags/average/"},{"name":"누적평균","slug":"누적평균","link":"/tags/누적평균/"},{"name":"역전파 알고리즘","slug":"역전파-알고리즘","link":"/tags/역전파-알고리즘/"},{"name":"Backpropagation","slug":"backpropagation","link":"/tags/backpropagation/"},{"name":"SEO","slug":"seo","link":"/tags/seo/"},{"name":"렌더 최적화","slug":"렌더-최적화","link":"/tags/렌더-최적화/"},{"name":"Render Optimizing","slug":"render-optimizing","link":"/tags/render-optimizing/"},{"name":"컴퓨터 공학","slug":"컴퓨터-공학","link":"/tags/컴퓨터-공학/"},{"name":"네트워크","slug":"네트워크","link":"/tags/네트워크/"},{"name":"IP","slug":"ip","link":"/tags/ip/"},{"name":"traceroute","slug":"traceroute","link":"/tags/traceroute/"},{"name":"Temporal Dead Zone","slug":"temporal-dead-zone","link":"/tags/temporal-dead-zone/"},{"name":"TDZ","slug":"tdz","link":"/tags/tdz/"},{"name":"호이스팅","slug":"호이스팅","link":"/tags/호이스팅/"},{"name":"Hoisting","slug":"hoisting","link":"/tags/hoisting/"},{"name":"V8","slug":"v8","link":"/tags/v8/"},{"name":"프로그래밍","slug":"프로그래밍","link":"/tags/프로그래밍/"},{"name":"논리학","slug":"논리학","link":"/tags/논리학/"},{"name":"난수생성알고리즘","slug":"난수생성알고리즘","link":"/tags/난수생성알고리즘/"},{"name":"메르센 트위스터","slug":"메르센-트위스터","link":"/tags/메르센-트위스터/"},{"name":"조직문화","slug":"조직문화","link":"/tags/조직문화/"},{"name":"철학","slug":"철학","link":"/tags/철학/"},{"name":"퇴사","slug":"퇴사","link":"/tags/퇴사/"},{"name":"회고","slug":"회고","link":"/tags/회고/"},{"name":"Audio","slug":"audio","link":"/tags/audio/"},{"name":"오디오 이펙터","slug":"오디오-이펙터","link":"/tags/오디오-이펙터/"},{"name":"JavaScript Audio API","slug":"javascript-audio-api","link":"/tags/javascript-audio-api/"},{"name":"소프트웨어 장인정신","slug":"소프트웨어-장인정신","link":"/tags/소프트웨어-장인정신/"},{"name":"프로페셔널","slug":"프로페셔널","link":"/tags/프로페셔널/"},{"name":"소프트웨어 장인 리뷰","slug":"소프트웨어-장인-리뷰","link":"/tags/소프트웨어-장인-리뷰/"},{"name":"Git","slug":"git","link":"/tags/git/"},{"name":"Commit","slug":"commit","link":"/tags/commit/"},{"name":"Merge","slug":"merge","link":"/tags/merge/"},{"name":"Merge and squash","slug":"merge-and-squash","link":"/tags/merge-and-squash/"},{"name":"Rebase","slug":"rebase","link":"/tags/rebase/"},{"name":"Heap","slug":"heap","link":"/tags/heap/"},{"name":"힙","slug":"힙","link":"/tags/힙/"},{"name":"자료 구조","slug":"자료-구조","link":"/tags/자료-구조/"},{"name":"프로토타입","slug":"프로토타입","link":"/tags/프로토타입/"},{"name":"자바스크립트","slug":"자바스크립트","link":"/tags/자바스크립트/"},{"name":"Prototype","slug":"prototype","link":"/tags/prototype/"},{"name":"TCP","slug":"tcp","link":"/tags/tcp/"},{"name":"흐름 제어","slug":"흐름-제어","link":"/tags/흐름-제어/"},{"name":"오류 제어","slug":"오류-제어","link":"/tags/오류-제어/"},{"name":"Flow Control","slug":"flow-control","link":"/tags/flow-control/"},{"name":"Error Control","slug":"error-control","link":"/tags/error-control/"},{"name":"Stop and Wait","slug":"stop-and-wait","link":"/tags/stop-and-wait/"},{"name":"Go bacK N","slug":"go-back-n","link":"/tags/go-back-n/"},{"name":"Selective Repeat","slug":"selective-repeat","link":"/tags/selective-repeat/"},{"name":"슬라이딩 윈도우","slug":"슬라이딩-윈도우","link":"/tags/슬라이딩-윈도우/"},{"name":"Sliding Window","slug":"sliding-window","link":"/tags/sliding-window/"},{"name":"Feedback","slug":"feedback","link":"/tags/feedback/"},{"name":"피드백","slug":"피드백","link":"/tags/피드백/"},{"name":"자유와 책임","slug":"자유와-책임","link":"/tags/자유와-책임/"},{"name":"넷플릭스","slug":"넷플릭스","link":"/tags/넷플릭스/"},{"name":"Netflix","slug":"netflix","link":"/tags/netflix/"},{"name":"Freedom & Responsibility","slug":"freedom-responsibility","link":"/tags/freedom-responsibility/"},{"name":"함수형 프로그래밍","slug":"함수형-프로그래밍","link":"/tags/함수형-프로그래밍/"},{"name":"Functional Programming","slug":"functional-programming","link":"/tags/functional-programming/"},{"name":"순수 함수","slug":"순수-함수","link":"/tags/순수-함수/"},{"name":"사이드 이펙트","slug":"사이드-이펙트","link":"/tags/사이드-이펙트/"},{"name":"Pure Functions","slug":"pure-functions","link":"/tags/pure-functions/"},{"name":"Side Effects","slug":"side-effects","link":"/tags/side-effects/"},{"name":"Retrospective","slug":"retrospective","link":"/tags/retrospective/"},{"name":"질문의 힘","slug":"질문의-힘","link":"/tags/질문의-힘/"},{"name":"Question Driven Thinking","slug":"question-driven-thinking","link":"/tags/question-driven-thinking/"},{"name":"Question","slug":"question","link":"/tags/question/"},{"name":"질문","slug":"질문","link":"/tags/질문/"},{"name":"학습론","slug":"학습론","link":"/tags/학습론/"},{"name":"개발자","slug":"개발자","link":"/tags/개발자/"},{"name":"공부","slug":"공부","link":"/tags/공부/"},{"name":"소크라테스","slug":"소크라테스","link":"/tags/소크라테스/"},{"name":"앎이란","slug":"앎이란","link":"/tags/앎이란/"},{"name":"Paypal","slug":"paypal","link":"/tags/paypal/"},{"name":"페이팔","slug":"페이팔","link":"/tags/페이팔/"},{"name":"Vue","slug":"vue","link":"/tags/vue/"},{"name":"Express","slug":"express","link":"/tags/express/"},{"name":"자료구조","slug":"자료구조","link":"/tags/자료구조/"},{"name":"해시테이블","slug":"해시테이블","link":"/tags/해시테이블/"},{"name":"Hash Table","slug":"hash-table","link":"/tags/hash-table/"},{"name":"Data structure","slug":"data-structure","link":"/tags/data-structure/"},{"name":"애자일 프로세스","slug":"애자일-프로세스","link":"/tags/애자일-프로세스/"},{"name":"소프트웨어 개발 방법론","slug":"소프트웨어-개발-방법론","link":"/tags/소프트웨어-개발-방법론/"},{"name":"Data Driven","slug":"data-driven","link":"/tags/data-driven/"},{"name":"데이터 기반 의사결정","slug":"데이터-기반-의사결정","link":"/tags/데이터-기반-의사결정/"},{"name":"튜토리얼","slug":"튜토리얼","link":"/tags/튜토리얼/"},{"name":"Tutorial","slug":"tutorial","link":"/tags/tutorial/"},{"name":"Versioning","slug":"versioning","link":"/tags/versioning/"},{"name":"버전관리","slug":"버전관리","link":"/tags/버전관리/"},{"name":"Event Loop","slug":"event-loop","link":"/tags/event-loop/"},{"name":"이벤트 루프","slug":"이벤트-루프","link":"/tags/이벤트-루프/"},{"name":"Study","slug":"study","link":"/tags/study/"},{"name":"프라하","slug":"프라하","link":"/tags/프라하/"},{"name":"한 달살기","slug":"한-달살기","link":"/tags/한-달살기/"},{"name":"프라하 생활 팁","slug":"프라하-생활-팁","link":"/tags/프라하-생활-팁/"},{"name":"Synchronous","slug":"synchronous","link":"/tags/synchronous/"},{"name":"Asynchronous","slug":"asynchronous","link":"/tags/asynchronous/"},{"name":"동기","slug":"동기","link":"/tags/동기/"},{"name":"비동기","slug":"비동기","link":"/tags/비동기/"},{"name":"Blocking","slug":"blocking","link":"/tags/blocking/"},{"name":"Non Blocking","slug":"non-blocking","link":"/tags/non-blocking/"},{"name":"블록킹","slug":"블록킹","link":"/tags/블록킹/"},{"name":"논블록킹","slug":"논블록킹","link":"/tags/논블록킹/"},{"name":"번아웃","slug":"번아웃","link":"/tags/번아웃/"},{"name":"Burn Out","slug":"burn-out","link":"/tags/burn-out/"},{"name":"글쓰기","slug":"글쓰기","link":"/tags/글쓰기/"},{"name":"Writing","slug":"writing","link":"/tags/writing/"},{"name":"Essay Tips","slug":"essay-tips","link":"/tags/essay-tips/"},{"name":"코딩테스트","slug":"코딩테스트","link":"/tags/코딩테스트/"},{"name":"헤더","slug":"헤더","link":"/tags/헤더/"},{"name":"ECN","slug":"ecn","link":"/tags/ecn/"},{"name":"TCP 플래그","slug":"tcp-플래그","link":"/tags/tcp-플래그/"},{"name":"SYN","slug":"syn","link":"/tags/syn/"},{"name":"ACK","slug":"ack","link":"/tags/ack/"},{"name":"불안감","slug":"불안감","link":"/tags/불안감/"},{"name":"불안요소 마주보기","slug":"불안요소-마주보기","link":"/tags/불안요소-마주보기/"},{"name":"면접","slug":"면접","link":"/tags/면접/"},{"name":"Congestion Control","slug":"congestion-control","link":"/tags/congestion-control/"},{"name":"혼잡 제어","slug":"혼잡-제어","link":"/tags/혼잡-제어/"},{"name":"AIMD","slug":"aimd","link":"/tags/aimd/"},{"name":"Slow Start","slug":"slow-start","link":"/tags/slow-start/"},{"name":"CWND","slug":"cwnd","link":"/tags/cwnd/"},{"name":"Tahoe","slug":"tahoe","link":"/tags/tahoe/"},{"name":"Reno","slug":"reno","link":"/tags/reno/"},{"name":"FIN","slug":"fin","link":"/tags/fin/"},{"name":"TCP Handshake","slug":"tcp-handshake","link":"/tags/tcp-handshake/"},{"name":"불변성","slug":"불변성","link":"/tags/불변성/"},{"name":"Immutable","slug":"immutable","link":"/tags/immutable/"},{"name":"HTTP 상태코드","slug":"http-상태코드","link":"/tags/http-상태코드/"},{"name":"Web Push","slug":"web-push","link":"/tags/web-push/"},{"name":"Notification API","slug":"notification-api","link":"/tags/notification-api/"},{"name":"PWA","slug":"pwa","link":"/tags/pwa/"},{"name":"Progressive Web Application","slug":"progressive-web-application","link":"/tags/progressive-web-application/"},{"name":"회고록","slug":"회고록","link":"/tags/회고록/"},{"name":"개발서적","slug":"개발서적","link":"/tags/개발서적/"},{"name":"프로그래밍서적","slug":"프로그래밍서적","link":"/tags/프로그래밍서적/"},{"name":"공동집필","slug":"공동집필","link":"/tags/공동집필/"},{"name":"커피 한잔 마시며 끝내는 VueJS","slug":"커피-한잔-마시며-끝내는-vuejs","link":"/tags/커피-한잔-마시며-끝내는-vuejs/"},{"name":"Sound Engineering","slug":"sound-engineering","link":"/tags/sound-engineering/"},{"name":"사운드 엔지니어","slug":"사운드-엔지니어","link":"/tags/사운드-엔지니어/"},{"name":"나이퀴스트","slug":"나이퀴스트","link":"/tags/나이퀴스트/"},{"name":"비전공 개발자","slug":"비전공-개발자","link":"/tags/비전공-개발자/"},{"name":"부트캠프","slug":"부트캠프","link":"/tags/부트캠프/"},{"name":"코딩학원","slug":"코딩학원","link":"/tags/코딩학원/"},{"name":"컴퓨터공학","slug":"컴퓨터공학","link":"/tags/컴퓨터공학/"},{"name":"HTTP3","slug":"http3","link":"/tags/http3/"},{"name":"UDP","slug":"udp","link":"/tags/udp/"},{"name":"함수형 사고","slug":"함수형-사고","link":"/tags/함수형-사고/"},{"name":"Functional Thinking","slug":"functional-thinking","link":"/tags/functional-thinking/"},{"name":"명령형 프로그래밍","slug":"명령형-프로그래밍","link":"/tags/명령형-프로그래밍/"},{"name":"객체지향 프로그래밍","slug":"객체지향-프로그래밍","link":"/tags/객체지향-프로그래밍/"},{"name":"선언형 프로그래밍","slug":"선언형-프로그래밍","link":"/tags/선언형-프로그래밍/"},{"name":"OOP","slug":"oop","link":"/tags/oop/"},{"name":"Object Oriendted Programming","slug":"object-oriendted-programming","link":"/tags/object-oriendted-programming/"},{"name":"펑터","slug":"펑터","link":"/tags/펑터/"},{"name":"모나드","slug":"모나드","link":"/tags/모나드/"},{"name":"Functor","slug":"functor","link":"/tags/functor/"},{"name":"Monad","slug":"monad","link":"/tags/monad/"},{"name":"합성 함수","slug":"합성-함수","link":"/tags/합성-함수/"},{"name":"Composition","slug":"composition","link":"/tags/composition/"},{"name":"카테고리 이론","slug":"카테고리-이론","link":"/tags/카테고리-이론/"},{"name":"범주론","slug":"범주론","link":"/tags/범주론/"},{"name":"REST","slug":"rest","link":"/tags/rest/"},{"name":"RESTful API","slug":"restful-api","link":"/tags/restful-api/"},{"name":"HTTP 메소드","slug":"http-메소드","link":"/tags/http-메소드/"},{"name":"REST API","slug":"rest-api","link":"/tags/rest-api/"}],"categories":[{"name":"Programming","slug":"programming","link":"/categories/programming/"},{"name":"Graphics","slug":"programming/graphics","link":"/categories/programming/graphics/"},{"name":"Machine Learning","slug":"programming/machine-learning","link":"/categories/programming/machine-learning/"},{"name":"Web","slug":"programming/web","link":"/categories/programming/web/"},{"name":"Algorithm","slug":"programming/algorithm","link":"/categories/programming/algorithm/"},{"name":"Network","slug":"programming/network","link":"/categories/programming/network/"},{"name":"JavaScript","slug":"programming/javascript","link":"/categories/programming/javascript/"},{"name":"Essay","slug":"essay","link":"/categories/essay/"},{"name":"Soft Skills","slug":"soft-skills","link":"/categories/soft-skills/"},{"name":"Agile","slug":"soft-skills/agile","link":"/categories/soft-skills/agile/"},{"name":"Organization","slug":"soft-skills/organization","link":"/categories/soft-skills/organization/"},{"name":"Audio","slug":"programming/audio","link":"/categories/programming/audio/"},{"name":"Git","slug":"programming/git","link":"/categories/programming/git/"},{"name":"Design","slug":"programming/design","link":"/categories/programming/design/"},{"name":"Data","slug":"soft-skills/data","link":"/categories/soft-skills/data/"}]}